---
title: Assessment - Data Finding and Wrangling  
subtitle: ""
author: Andreas Handel
institute: "University of Georgia"
date: "`r file.mtime(knitr::current_input())`"
#bibliography: ../media/references.bib
output: 
  html_document:
    toc_depth: 3
---


# Quiz

Get the quiz sheet for this module from the general [__Assessments__](./Course_Assessments.html) page. Fill it in, then submit to the online grading system before the deadline. 

# Exercise

This exercise lets you do a bit of data loading and wrangling. And of course more group work and GitHub `r emoji::emoji("grin")`. 


## Setup

We'll also use the same group setup as last week. Assign each member in your group an (arbitrary) number (I'm calling them M1, M2, ...). The order should be different than last week so you get to interact with a different group member. Everyone will first work on their own and finish this part by Wednesday. Then M1 will contribute to M2's repository, M2 will work on M3, etc. The last person (M3 or M4), will work on M1's repository. This way, everyone will work on their own and one group member's repository. 

This is going to be a small data analysis. I generally recommend making each project its own GitHub repository, and always using a structure like the one provided in the `Data Analysis Template` (or something similar). However, **for this small exercise and for logistic reasons, you'll use your portfolio/website repository.** If you want, you can make a new folder for this exercise inside your portfolio repository (call it, e.g., `dataanalysis-exercise`) and in that folder, create sub-folders that are similar to the one from the `Data Analysis Template` (e.g.,  a `data` folder and a `code` folder).


## Finding Data

Previously, you did a quick exploration of a dataset that came with an R package (`gapminder` data inside `dslabs` package). A lot of datasets can be found inside R packages. For instance, [this page lists what is likely only a small fraction](https://vincentarelbundock.github.io/Rdatasets/datasets.html). There is even a [Reddit group dedicated to R datasets](https://www.reddit.com/r/datasets/). The good and the bad about datasets that come with R packages is that they are often fairly clean/tidy. That's unfortunately not how most "real world" datasets look like. Getting dirty and messy datasets and wrangling them into a form that is suitable for statistical analysis is part of most workflows and often takes a lot of time. We'll start practicing this here by getting data that might or might not be very clean.

Go to the CDC's data website at [https://data.cdc.gov/](https://data.cdc.gov/). Browse through the site and identify a dataset of interest. 

Which dataset you choose is up to you. I suggest you pick a dataset that has at least 5 different variables, and a mix of continuous and categorical ones. Often, 5 variables means 5 columns. That would be the case in properly formatted data. However, some of the data provided by the CDC is rather poorly formatted. For instance [this dataset](https://data.cdc.gov/Traumatic-Brain-Injury-/Rates-of-TBI-related-Deaths-by-Age-Group-United-St/nq6q-szvs) has the same variable (age) in separate columns, and it is also discretized. As we'll discuss, these are two really bad things you can do to your data, so I recommend staying away from such datasets. There are plenty on that website, so I'm sure you'll find one that is suitable and interesting to you.

If you absolutely can't find a good dataset on the CDC website, you can get one somewhere else. It needs to be _real world_ data (so no training/teaching data repositories), decently documented, and readily available.


## Loading data

Get the dataset you selected into R. You can do that by downloading the file to your computer first and placing it inside your repository. **Remember that GitHub doesn't like large files, so if you pick a large data file (>100MB), first reduce it by e.g., writing some R code that selects only a portion of the data, before placing it into the GitHub repository.** If no file is available for easy download, or if it is too large to download and place in the repo, you can instead write code to pull it directly from the source (Google is your friend to figure out what commands you need to write).

Now, write code (either as a single Quarto file, or a combined R script + Quarto, like the examples in the `Data Analysis Template`) that reads/loads and processes the data. Comprehensive and full cleaning of all the data is not necessary. Instead, decide on a few variables of interest and clean those. Think of one variable as the main outcome of interest (in a plot, that would generally be shown on the y-axis) and some other variable(s) as the predictors of interest. Similar to the height/weight example code in the `Data Analysis Template`. Or it could be some quantity changing over time. It's your choice.

Once you have written code that processes the bits of the data you are interested in, save the cleaned data as an RDS file.

**Add enough commentary to your Quarto file and R code such that your classmate who will take over next knows what you are doing and what variables they should work with!**

Then add a `dataanalysis_exercise.qmd` file to the main folder of your repository (the place where the other `_exercise.qmd` files are).

To that file, add a brief description of the data, where you got it, what it contains. Also include a summary table of your cleaned data. If you want, you can copy and paste relevant components from `Manscript.qmd` file of the `Data Analysis Template`. 

When all these parts are done and work, commit and push your changes to GitHub. Then let the group member who will take over (see above) know where to find your repository.



## Wrangling and exploring data

Fork and clone your fellow group member's repository using the same workflow you used in a previous exercise. Once you have it on your local computer, start a new Quarto file (or Quarto+R files) and write code that loads the data cleaned by your colleague and produces a few plots and/or tables. 

Next, open the main manuscript Quarto file and add text and code to include the figures/tables you just produced. Make sure to provide enough comments and text so it's clear what you are doing.

Once all done, commit, push to your repo on Github (the fork of the original), then initiate a pull request (PR) to the original repository. 



## Posting the "final" product

The original repository owner should check the PR they received from their colleague, request changes if needed, approve if all looks okay, then merge and update their own repo. 

In a final step, update the `_quarto.yml` file and include a menu item for "Data Analysis Exercise" pointing to the new file. Re-create your website and make sure it all works and the new project shows up.

Since this will be part of your portfolio site, and you already posted a link to that previously, you donâ€™t need to post anything, I know where to find it.




# Discussion

No discussion assignment this week. Instead, submit project part 1.


# Project

Submission of part 1 is due. **Submit a link with a URL to your project repository to the `project_related` Slack channel.** 
