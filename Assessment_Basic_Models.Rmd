---
title: Assessment - Fitting Basic Statistical Models 
author: Andreas Handel
date: "`r file.mtime(knitr::current_input())`"
output: 
  html_document:
    toc_depth: 3
---


```{r, include = FALSE}
library(emo)
```



# Quiz

Get the quiz sheet for this module from the general [__Assessments__](./Course_Assessments.html) page. Fill it in, then submit to the online grading system before the deadline. 


# Exercise

We'll practice a bit of model fitting using the `tidymodels` framework and continuing with an exercise you started previously, namely the exercise where you obtained and wrangled some CDC data. We'll now add some fitting.


## Setup

We'll again add a group exercise component, using the - by now familiar - M1, M2, ... setup. Assign each other a number, as much as possible, do it such that you end up working with group members you have not (or not in a while) worked together. The goal is that again as in past exercises, everyone will work on their own and one other person's repository. If you are in a group of 4, you can do pairs. That of course doesn't work in a group of 3, so you need to use the 'circular' setup there.

For this exercise, we'll revisit and extend the project you started in the _wrangling_ module. Make sure you have the latest version of the repository on your local machine (it might not be, since last time your classmate contributed to your repository). As needed pull/push/merge to make sure remote and local repositories are in sync. Open the project in RStudio. 

The dataset you previously got from the CDC should have at least 5 variables, with a mix of continuous and categorical. If for some reason that's not the case for your dataset, you have two options. Either, you create some new variables and fill them with artificial data or you switch to a different dataset. 

If you have to create artificial data, do that with R commands, either at the beginning or end (or both) of your cleaning script. **No editing of raw data files by hand!** (We only did that once in the beginning exercise for convenience, but we won't do that again.)



## Wrangling and EDA revisited

We already spent time on the important steps of data wrangling/cleaning and exploring/visualizing. But practice makes perfect `r emo::ji('grin')` so we'll talk a bit more and do a bit more if it here.

By now you will have figured out that the whole wrangling/exporing process is iterative. Without exploring your data, you don't know how to clean/wrangle it. Without at least some cleaning, exploration often doesn't work (e.g. if you have missing values, some R summary functions don't work properly.) 

For this exercise, you'll likely have to do some more cleaning and exploring, depending on how much you did last time.

As part of this exercise, continue to update your `processingscript.R` code such that it does a full cleaning of all or most variables. 

**No matter how you do it, write comments into your R script explaining each cleaning decision. State briefly why you are making a decision, and then if the code is not obvious, explain the how. At this point, you should also delete any bits from your code that are not part of your cleaning (e.g., leftovers from the template).**

As examples, of things you might need to clean:

* If you have categorical variables, check the categories and decide if maybe some should be combined. 
* You might already be aware of outliers or mistakes in the data. Decide what to do with them, clean accordingly.
* If you know you have a variable with a lot of missing data, explain that you are dropping this variable (and why), then write the code that drops the variable. Or alternatively you might remove observations (rows) that contain any missing data.


:::note
With the tidyverse and the pipe operator, a lot of these steps can be done in one long chain of commands. While convenient, I generally consider this not the best idea. It is often better to save the result of some cleaning operation as a new intermediate variable. That lets you more easily check for bugs, and to see how the data changed from step to step and if it does what you think it should. So instead of coding like this:
```{r, eval = FALSE}
data %>% filter() %>% 
         select() %>%
         fct_lump() %>%
         mutate() %>% ...
```
I suggest instead to write code like this
```{r, eval = FALSE}
dat_small1 <- data %>% filter()
dat_small2 <- dat_small1 %>% select()
dat_small3 <- dat_small2 %>% fct_lump()
...
```
That style of coding defeats a bit the purpose of the pipe operator, but I think it helps initially to be able to inspect what happens at each step. Once you get more comfortable with cleaning steps and coding in general, and make few mistakes, you can start chaining things together. But if you start out writing code that way, it's much harder to follow along and find bugs.
:::

At the end of your cleaning process, you need to end up with at least 5 variables, a mix of continuous and categorical. You might have many more variables, that is ok. At the end of your cleaning script, you should save the new dataset to an `Rds` file, as before.

As part of the data wrangling/cleaning, and also to some extent as stand-alone exploration to prepare you for the formal statistical fitting, you need to explore your data. You can either have exploration code as part of your processing script, such that this script does both, clean data and produce some EDA outcomes (tables, figures, etc.). Or you could do a separate R/Rmd file, e.g. one called `exploration.Rmd`. That's your choice.

At minimum, your exploration code (either in a separate file or part of the cleaning script) should do the following:

* For each (important) variable, produce and print some numerical output (e.g. a table or some summary statistics numbers).
* For each (important) continuous variable, create a histogram or density plot. 
* Create scatterplots or boxplots or similar such plots for the variable you decided is your main outcome of interest and the most important (or all depending on number of variables) independent variables/predictors.
* If applicable to your data, make some pairwise correlation plots.
* If needed for your data, explore the pattern of missing values.
* Any other exploration steps that might be useful.

If you only have a limited number of variables, e.g., less than 10, you can explore them all. If you have a lot, focus your exploration on the ones you think are most interesting and important, which of course is the outcome and the main predictors of interest.

Again, make sure you document well. Prior to each code piece that produces some output, you should add text describing what you are about to do and why. After you produced the result, add some text that comments on what you see and what it means. E.g. you could write something like _Histogram for height shows two persons at 20in, everyone else else is above 50in. Checked to see if those are wrong entries or not. Decided to remove those observations._


**Once you are done with this part, you should have code that explores and cleans the data, and saves the final cleaned data set to an Rds file. You should also have documentation, either as comments in the code, text in and Rmd file, notes in a readme (or all of those), that explain what variables the data contain, what each variable means, which 2 (one continuous, 1 categorical) you decide are the main outcomes of interest. Also decide on a predictor variable that is of main interest. And state which other predictor/input variables are worth considering. For the predictors, you can say "all" if you don't have too many. If you have a lot of variables, specify whiche ones are of main importance.**

::note
For any good data collection and analysis, the data needs to be documented with meta-data to describe what it contains, what units things are, what variables are allowed, etc. Without good meta-data, analyses often go wrong. There are famous examples in the literature where the coding 0/1 was assumed to mean the opposite of what it was, so all conclusions were wrong. To guard against this, careful documentation is crucial. Also, giving variables easy to understand names and values is best. E.g. instead of coding sex as 0/1/99, code it as Male/Female/Other, or something along those lines.
::

You need to finish this part by Wednesday. Make sure everything works, then push to GitHub and let your teammate know that they can take over.


# Model fitting

Fork and clone your partner's repository. (Alternatively, you can also add each other as collaborator and then each of you can directly push/pull the main repository. That's up to you.) Once you have the repository on your local computer, open the project in R Studio.

Read through their code so you understand what the two outcomes of interest are, the main predictors, and what the rest of the data contain. 

**Hopefully their code is nicely documented. If it is, you should see how this is much better than going into some Excel file, manually deleting stuff, and nobody ever knows what happened. Always be reproducible and explicit!**

Start with a clean analysis script. You can either use an Rmd file, e.g., the existing `analysisscript.Rmd` and deleting everything that's there. Or you can do an R script.


Add code that does the following operations:

* Loads cleaned data.
* Fits a linear model with to continuous outcome and the main predictor of interest.
* Fits another linear model to the continuous outcome with all important predictors of interest.
* Fits a logistic model to the categorcial outcome and the main predictor of interest.
* Fits another logistic model to the categorical outcome with all important predictors of interest.


MORE HERE


Once completed, make sure the analysis script runs. You can make it write results to files (e.g. figure files or Rds files for tables). Or just print on the screen and add comments to the text. Or if you are a big fan of R Markdown files, you can convert the script to an Rmd file and then knit it to html.

There is no need for this exercise to update the manuscript file. We'll do that in a future exercise. But do make sure everything is documented well so your partner (and any future project collaborators) can understand what is going on.

Once your part is done, commit and push. If you used a fork, send a pull request to your partner. If you are working on the same repository as collaborators, you should be able to just push directly. In either case, you might want to let your partner know that you finished and that they can do a final check and any further cleanups they want to do.

Since this is the same repository as the one you used previously, and previously posted the link to the slack channel, you don't need to re-post. I know where to find the repository and will take a look and review/grade after the deadline.


# Discussion

Write a post in this week's discussion channel that answers this question:

Which major new insight did you gain from this week's topic (or any topic related to model fitting you got exposed to over the last few weeks) and how will that maybe affect your analyses in the future. And what is or are points that you find rather confusing about the model fitting material we covered so far? 

Post by Wednesday, then reply to each other by Friday. See if you can help each other reduce any existing confusion. I'll be participating too of course.

