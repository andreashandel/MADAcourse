---
title: Assessment - Exploration
author: Andreas Handel
date: "`r file.mtime(knitr::current_input())`"
output: 
  html_document:
    toc_depth: 3
---


```{r, include = FALSE}
library(emo)
```



# Quiz

Get the quiz sheet for this module from the general [__Assessments__](./Course_Assessments.html) page. Fill it in, then submit to the online grading system before the deadline. 


# Exercise



## Setup

We'll again add a group exercise component, using the - by now familiar - M1, M2, ... setup. Assign each other a number, as much as possible, do it such that you end up working with group members you have not (or not in a while) worked together. The goal is that again as in past exercises, everyone will work on their own and one other person's repository. 

For this exercise, we'll revisit and extend the project you started in the _wrangling_ module. Make sure you have the latest version of the repository on your local machine (it might not be, since last time your classmate contributed to your repository). As needed pull/push/merge to make sure remote and local repositories are in sync. Open the project in RStudio. 

The dataset you previously got from the CDC should have at least 5 variables, with a mix of continuous and categorical. If for some reason that's not the case for your dataset, you have two options. Either, you create some new variables and fill them with artificial data (like we did in an earlier exercise) or you switch to a different dataset. If you have to create artificial data, do that with R commands, either at the beginning or end (or both) of your cleaning script. **No editing of raw data files by hand!** (We only did that once in the beginning exercise for convenience, but we won't do that again.)


## Cleaning

Cleaning can and has to happen before and after exploration. It is iterative. Without exploring your data, you don't know how to clean it. Without at least some cleaning, exploration often doesn't work (e.g. if you have missing values, some R summary functions don't work properly.) So for this exercise, it's going to be a back and forth between the cleaning part and the exploration part. You already explored the data a bit in a previous exercise. So if you know anything of that type is in the data and needs cleaning, you can do it here.

For now, update your `processingscript.R` code such that it cleans and retains all or most variables. You can check variables and take some cleaning steps already here, or do the exploration first, then come back.

**No matter how you do it, write comments into your R script explaining each cleaning decision. State briefly why you are making a decision, and then if the code is not obvious, explain the how.**

As examples, of things you might need to clean (either now or after exploration):

* If you have categorical variables, check the categories and decide if maybe some should be combined. 
* You might already be aware of outliers or mistakes in the data. 
* If you know you have a variable with a lot of missing data, explain that you are dropping this variable (and why), then write the code that drops the variable. Or alternatively you might remove observations (rows) that contain any missing data.


:::note
With the tidyverse and the pipe operator, a lot of these steps could be done in one long chain of commands. While convenient, I generally consider this not the best idea. It is often better to save the result of some cleaning operation as a new intermediate variable. That lets you more easily check for bugs, and to see how the data changed from step to step and if it does what you think it should. So instead of coding like this:
```{r, eval = FALSE}
data %>% filter() %>% 
         select() %>%
         fct_lump() %>%
         mutate() %>% ...
```
I suggest instead to write code like this
```{r, eval = FALSE}
dat_small1 <- data %>% filter()
dat_small2 <- dat_small1 %>% select()
dat_small3 <- dat_small2 %>% fct_lump()
...
```
That style of coding defeats a bit the purpose of the pipe operator, but I think it helps initially to be able to inspect what happens at each step. Once you get more comfortable with cleaning steps and coding in general, and make few mistakes, you can start chaining things together. But if you start out writing code that way, it's much harder to follow along and find bugs.
:::
  
Make sure your cleaning/processing script is updated enough that you end up with at least 5 variables, a mix of continuous and categorical. You might have many more variables, that is ok. Some of these variables might not be cleaned, that is ok too. Your updated script should save the new dataset to an `Rds` file, as before.



## Exploration

Create a new R Markdown file called `exploration.Rmd` somewhere inside the `code` folder. You can give it its own sub-folder or not. 
Then write code, combined with text/comments that describe what you do, into your `exploration.Rmd` file. The code should do the following:

* Load the data you just created and saved.
* For each variable, produce and print some numerical output (e.g. a table or some summary statistics numbers).
* For each continuous variable, create a histogram or density plot. 
* Create scatterplots or boxplots or similar such plots for the variable you decided is your main outcome of interest and the most important (or all depending on number of variables) independent variables/predictors.
* If applicable to your data, make some pairwise correlation plots.
* If needed for your data, explore the pattern of missing values.
* Any other exploration steps that might be useful.

Prior to each code piece that produces some output, you should add text describing what you are about to do and why. After you produced the result, add some text that comments on what you see and what it means. E.g. you could write something like _Histogram for height shows two persons at 20in, everyone else else is above 50in. Needs further checking to see if those are wrong entries or not._ 

By the end of this section/script, you should have a good understanding of what your data looks like, and what analysis approaches might be most appropriate.
As you explore, you'll likely find additional things that need cleaning, like the example I just gave. Add comments in your Rmd file such that it is clear what needs further investigation (your classmate/team member will take care of that). 

Once you have explored the data as just described, knit your R Markdown file, make sure everything looks ok. Then commit your changes and push to Github. **Do this by Wednesday.** Inform your partner that they can take over.


## More exploration and cleaning

Fork and clone your partner's repository. (Alternatively, you can also add each other as collaborator and then each of you can directly push/pull the main repository. That's up to you.) Once you have the repository on your local computer, open the project in R Studio.

Read through their comments. Add code to do that cleaning script that does any further cleaning they identified as needed. All additional code and decisions should be documented. For instance, continuing the example I gave above, you or your partner might have looked at age and weight and decided that based on those, a height of 20in is not reasonable. Unfortunately, you don't know how to fix this. Since you plan to use height in your models, You decide that the best approach is to remove those individuals. Therefore, you add a few lines of code to your cleaning/processing script that remove those individuals, with some comments explaining what you are doing and why. 

**Note how this is much better than going into some Excel file, manually deleting stuff, and nobody ever knows what happened? Always be reproducible and explicit!**

Once you are done with the additional cleaning, and any updates to the exploration part, make sure everything runs well, then move to the next part.


## Preliminary statistical analyses

Open the `analysisscript.R` file. Remove most or all of the content. (We are now doing all the exploring in `exploration.Rmd` anyway.) Add some code that performs some basic preliminary statistical analyses. That could be adding a linear regression line to a ggplot scatterplot, applying some bivariate analysis to the main outcome and some predictors of interest, or any other fairly simple stats operations you are familiar with. The idea here is not to draw any firm conclusions, but to continue the theme of exploration you started above, now with the help of some quick and simple statistical approaches. 

Once completed, make sure the analysis script runs. You can make it write results to files (e.g. figure files or Rds files for tables). Or just print on the screen and add comments to the text. Or if you are a big fan of R Markdown files, you can convert the script to an Rmd file and then knit it to html.

There is no need for this exercise to update the manuscript file. We'll do that in a future exercise. But do make sure everything is documented well so your partner (and any future project collaborators) can understand what is going on.

Once your part is done, commit and push. If you used a fork, send a pull request to your partner. If you are working on the same repository as collaborators, you should be able to just push directly. In either case, you might want to let your partner know that you finished and that they can do a final check and any further cleanups they want to do.

Since this is the same repository as the one you used previously, and previously posted the link to the slack channel, you don't need to re-post. I know where to find the repository and will take a look and review/grade after the deadline.


# Discussion

Write a post in this week's discussion channel that answers this question:

