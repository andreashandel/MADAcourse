---
title: "Example of a data analysis"
author: "Andreas Handel"
date: "January 3, 2017"
output: html_document
---


# Overview

This is a very short example illustrating a full data analysis process.

The analysis is based on and inspired by the [Kaggle Titanic challenge.](https://www.kaggle.com/c/titanic) [Kaggle (owned by Google)](https://www.kaggle.com/) is a website that hosts data analysis competitions. Everyone can participate and compete for - sometimes rather large - prizes. The website also has a lot of good datasets and code, as well as other resources related to data analysis. Definitely worth checking out. 

Their introductory challenge, which they recommend newcomers work through to practice, asks one to predict who did and did not survive the sinking of the Titanic, based on a variety of characteristics of each individual (e.g. sex, age, cabin number, etc.). 

The [Kaggle Titanic challenge website](https://www.kaggle.com/c/titanic) shows the data and explains how to do the competition. This project does produce predictions in a file that could be submitted to the Kaggle webpage for scoring.

Some of what's going on in this script might not fully make sense to you (yet). That's ok, I'm sure you still get the overall idea. By the end of the course, you should be able to understand it all.


# Scientific Question
We'll look at data from passengers on the Titanic. We get a number of characteristics (e.g. gender, age) about each passenger, and if they survived or not. We want to know if we can build a model that could have accurately predicted based on passenger characteristics if they survived or not.

Question: Can we predict survival of passengers based on their characteristics? 

Hypothesis: We can use passenger characteristics to predict (better than randomly guessing) if they survived or not.


# Package loading
We will make use of the package "caret". The caret package is a front-end package that allows for easy use of many different fitting and auxilliary packages in R. We'll need several of these other packages, install them.
```{r, echo=FALSE}
#Install if needed before you load
library('caret')
library('rpart')
library('rattle')
library('corrplot')
library('readr')
```




# Data loading and cleaning

Load and clean data. We'll use the "famous" titanic dataset. It's one of Kaggle's starter datasets to let people try out their machine learning/predicting skills and getting used to the whole Kaggle platform. We already downloaded the data files titanic_train.csv and titanic_test.csv. For now we focus on titanic_train.csv

Load the data and make a copy
```{r}
titanic_raw <- read_csv('titanic_train.csv')
titanic <- titanic_raw #make a copy
```

Take a quick look at the data
```{r}
str(titanic)
```

Data cleaning. We'll remove some variables that we think might not be important (and will make our life a bit easier). We'll recode some from numeric/integer to factor. 

```{r}
titanic$PassengerId <- NULL #remove this variable
titanic$Name <- NULL #remove this variable
titanic$Ticket <- NULL #remove this variable
titanic$Cabin <- NULL #remove this variable
titanic$Survived <- as.factor(titanic$Survived)
titanic$Pclass <- as.factor(titanic$Pclass)
titanic$Sex <- as.factor(titanic$Sex)
titanic$Embarked <- as.factor(titanic$Embarked)
```

Removing all observations that are not complete (i.e. any that have an NA anywhere).
Usually not a good idea since one might loose too much data and therefore predictive power. Ok for our purpose.

```{r}
print(dim(titanic))
titanic <- titanic[complete.cases(titanic),]
print(dim(titanic))
```

Check to make sure everything looks good
```{r}
str(titanic)
```


## Exploratory analysis 

Plotting a predictor, as example. We should really go through all of them more carefully.

```{r}
hist(titanic$Fare)
```

Doesn't look normal, let's transform.

## Data preprocessing

The preProcess function needs to operate on data frames or matrices. We transform only a single variable (Fare), which R represents automatically as vector. We need to force it to a data frame.
```{r}
preproc <- preProcess(data.frame(titanic$Fare), method = c("center","scale","YeoJohnson")) #compute transformations
farenew <- predict(preproc, data.frame(titanic$Fare))[,1] #apply transformation, convert to vector
hist(farenew) 
```

Note: I first tried a BoxCox transformation, but that didn't work well due to 0 values in the Fare column. Those entries are probably not correct and one might to recode those 0 as NA. But if we don't recode for now and assume those 0 values are correct, we can't use BoxCox. There is however an alternative transformation available (called Yeo-Johnson) that can deal with 0/negative values. We use that one.


```{r}
titanic$Fare <- farenew #replace old values with new transformed ones
```


# More exploration

Look for correlations in continuous variables. Check the codebook on the Kaggle page for explanations of what each variable means.

```{r}
correlations <- cor(titanic[,c(4:7)])
corrplot(correlations, order = "hclust")
highCorr <- findCorrelation(correlations, cutoff = .7) #cut-off specifies how high a correlation we'll tolerate
print(highCorr) #shows predictors/variables that are suggested for removal 
```
No large correlations.



# Splitting the data
Let's set some data aside so we can later pretend those are new observations and see how well the model does.
Do a random split of data 80/20.

```{r}
set.seed(1234) #for reproducibility, set random number seed
trainset <- createDataPartition(y = titanic$Survived, p = 0.8, list = FALSE)
new_obs <- titanic[trainset,] #our test observations
titanic <- titanic[-trainset,] #remove those from the main dataset
```



# Fitting a tree to a categorical outcome

Looking at survival as outcome, all others as predictors
```{r, echo=TRUE}
set.seed(123) #for reproducibility, set random number seed
fitControl <- trainControl(method="repeatedcv",repeats=5,number=5) #repeated cross-validation
treeGrid=expand.grid(maxdepth=1:10)
fit1 = train(Survived ~ ., data=titanic, method="rpart2",  trControl = fitControl, tuneGrid = treeGrid) 
```

Plot the results
```{r, echo=TRUE}
fancyRpartPlot(fit1$finalModel)
```


# Making some test predictions

Let's make some predictions.
Recall, we set some data aside above. We can use that and see how well the model predicts.

Let's split the new observations into predictors and outcomes. The outcome is the 1st variable/column, so we can split it by subsetting like this:
```{r, echo=TRUE}
new_obs_predictors <- new_obs[,-1] 
new_obs_outcome <- new_obs[,1]
```

Now we'll give the model the predictors for the new observations, and see what it predicts the outcomes to be.

```{r, echo=TRUE}
new_obs_predictions <- predict(fit1, newdata=new_obs_predictors)
```

Note we didn't tell the model what the outcomes are, those are not part of new_obs_predictors. So the model can't "cheat" by looking at outcomes (of course it never does, that would defeat the whole purpose).



Now we check for which observations the model predicted correctly. The simplest check is to just print them and compare. Only works for few new observations.
```{r, echo=TRUE}
print(new_obs_outcome)
print(new_obs_predictions)
```

A more convenient way to compare observations and predictions is with the confusion matrix.
```{r, echo=TRUE}
confusionMatrix(as.data.frame(new_obs_outcome)[,1],new_obs_predictions)
```

# Note on "real" predictions

In the "real world" we often don't know the outcome of a new observation and can't check prediction quality. For instance your email spam filter was trained on many messages for which it was known if the message was spam or not. Now if a new message arrives, it looks at "the predictors" of the new message (e.g. content, where it was sent from, etc.) and decides if it's spam or not. Unless you relabel a message by hand as spam or not, the software will never know if it got the prediction right or not.

# Back to the Titanic

To simulate the "real world", Kaggle provides a test dataset that does not have the outcome.
Let's load it and get it into the same shape as the training set, then predict outcomes for this new test.

Note that we need to do the same cleaning and pre-processing for this test set as for our training set. Train and test set need to look the same (same number and type of predictors). If we don't do that and we have different predictors or differently scaled predictors between train and test, the results will be nonsense (or R won't compute any).

```{r}
titanic_test_raw <- read_csv('titanic_test.csv')
titanic_test <- titanic_test_raw
titanic_test$PassengerId <- NULL #remove this variable
titanic_test$Name <- NULL #remove this variable
titanic_test$Ticket <- NULL #remove this variable
titanic_test$Cabin <- NULL #remove this variable
titanic_test$Pclass <- as.factor(titanic_test$Pclass)
titanic_test$Sex <- as.factor(titanic_test$Sex)
titanic_test$Embarked <- as.factor(titanic_test$Embarked)
preproc <- preProcess(data.frame(titanic_test$Fare), method = c("center","scale","YeoJohnson")) #compute transformations
farenew <- predict(preproc, data.frame(titanic_test$Fare))[,1] #apply transformation, convert to vector
titanic_test$Fare <- farenew #replace old values with new transformed ones
str(titanic_test)
```

Observe that the variable "Survived" is missing from this dataset. 

# Making and saving predictions

Now we can use the test set to predict.

```{r, echo=TRUE}
real_predictions <- predict.train(fit1, newdata=titanic_test , na.action = na.pass)
print(real_predictions)
```


Next, we'll save our predictions into a CSV file. We could upload that to Kaggle to see how well our model did (hint, it's not a good model).

```{r, echo=TRUE}
real_pred_final <- data.frame(PassengerId = titanic_test_raw$PassengerId, Survived = real_predictions)
write_csv(real_pred_final,'titanicpredictions.csv')
```


