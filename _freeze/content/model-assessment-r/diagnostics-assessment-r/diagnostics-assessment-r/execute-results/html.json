{
  "hash": "142e1238feb2147fdf98f6966db38162",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model diagnostics with R\"\ndate: 2024-01-25\ndate-modified: last-modified\n---\n\n\n# Overview\n\nThis unit shows how to use R to run different diagnostics that are helpful to assess models.\n\n\n# Learning Objectives\n\n* Know how to implement different diagnostic approaches with R.\n\n\n\n# Introduction\n\nWe previously discussed how to use different diagnostics to assess models. \nHere, we'll focus on their implementation with R. We'll mostly work with the `tidymodels` set of packages.\n\n\n\n# Algorithm assessment\n\n\n\n# Null model comparisons\n\nIn `tidymodels`, you can use the `null_model()` function to compute the performance for a null/base model.\n\n\n# Single predictor comparisons\n\n\n# Summary\n\nWhen assessing your models, it is important to go beyond the performance metric and look at individual model predictions and how closely they agree with the data. If you spot residual patterns, that might indicate that you could (but don't have to) make your model more complex/flexible to try and capture additional details of the data.\n\n\n# Further Resources\n\nNone ATM.\n\n\n# Test yourself\n\n\nWhat's the term for the differences between data and model that you want to see distributed like a symmetric cloud? <div class='webex-radiogroup' id='radio_MESCLFDINI'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MESCLFDINI\" value=\"\"></input> <span>Overfitting</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MESCLFDINI\" value=\"\"></input> <span>Variance</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MESCLFDINI\" value=\"answer\"></input> <span>Residuals</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_MESCLFDINI\" value=\"\"></input> <span>Bias</span></label></div>\nOn a predicted versus observed plots, the points should cluster along a horizontal line to indicate a good model fit. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select> <br><br>If your overall model metric (e.g. Accuracy) is very good, you don't need to look at individual predictions. <select class='webex-select'><option value='blank'></option><option value=''>TRUE</option><option value='answer'>FALSE</option></select>\n\n\n\n\n# Practice\n\n* Revisit any of the papers you found in one of the previous exercises. See if the authors used any of the approaches discussed here to assess their model(s). Often, this kind of information would be in the supplement. At the minimum, you'd want the authors to mention that they did these checks. Unfortunately, you'll often see it missing. It seems that at times, authors/analysts don't want to look too closely, otherwise they would need to acknowledge that their cherished model is actually not that good üòÅ.\n\n\n",
    "supporting": [
      "diagnostics-assessment-r_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}