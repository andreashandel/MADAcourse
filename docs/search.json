[
  {
    "objectID": "resources/resources-overview.html",
    "href": "resources/resources-overview.html",
    "title": "Resources Overview",
    "section": "",
    "text": "This section contains pages with brief descriptions and links to other resources.\nThe Course Resources page lists materials that are most relevant to the course. Some of this material will assigned as part of this course.\nThe General Resources page lists other resources in case you might find them helpful. All materials are (should be) available freely online.\nThe Glossary page provides brief definitions of terms used in this course.\nIf you come across any other good resources, or find some important term is missing from the glossary, please let me know (or contribute a pull request with your updates to the course website)."
  },
  {
    "objectID": "resources/resources-general.html",
    "href": "resources/resources-general.html",
    "title": "More Data Science Resources",
    "section": "",
    "text": "I kept adding resources until things got too unwieldy and the Course Resources page was becoming too large üòÅ. So I decided to split things into two pages. The Course Resources page lists materials directly related to and used/mentioned in the course. This page lists a lot of other resources that are not heavily featured in the course, but that might be useful and interesting. Everything listed here is broadly related to the course topic, i.e.¬†the resources focus on Data Science/Stats/R Coding/GitHub/etc. For even more materials, see the links to various lists by others at the end of this document.\nMost materials described below are (should be) freely available online. For better or for worse, a lot of the resources I list below are dynamic and ever changing. That means occasionally links might not work, sites go offline, chapters in online books get re-arranged, etc. If any link does not work and you can‚Äôt access the materials for some reason, let me know so I can update this document.\nI placed them into categories according to main topic, but there is a lot of overlap. Many R coding resources focus on data analysis, and most data science resources I list focus on R.\nI am familiar with some, but not all of these resources. Sometimes I just took a quick glimpse to decide if it was worth including them here. If you find particular resources especially helpful or unhelpful (both listed and not listed), I‚Äôd love to receive feedback."
  },
  {
    "objectID": "resources/resources-general.html#researcher-degrees-of-freedom-p-hacking",
    "href": "resources/resources-general.html#researcher-degrees-of-freedom-p-hacking",
    "title": "More Data Science Resources",
    "section": "Researcher degrees of freedom (p-hacking)",
    "text": "Researcher degrees of freedom (p-hacking)\n\nThe concept of Researcher degrees of freedom, which is related to Data Dredging and p-hacking are important ideas to keep in mind when doing a data analysis. Note that this issue is often cast in the language of p-values since those are still (unfortunately) the most common approach to statistical analyses. But the concept applies even if one doesn‚Äôt use p-values.\nYou can find a fun hands-on exploration of the potential problem of researcher degrees of freedom in this 538 visualization and another choose-your-own adventure story.\nFor further discussions of this general problem, see e.g.¬†this article from 538 (which goes with the hands-on example just mentioned) or Gelman‚Äôs and Loken‚Äôs article, with their closely related article from 2013.\nSimmons et al.¬†2011 provides a nice and easy to follow illustration how researcher degrees of freedom, combined with incomplete reporting, can lead to apparently nonsensical results. The study is a (fake) psychology study, but everything applies in general and it is easy to follow.\nNot surprisingly, xkcd has also covered the topic of p-hacking."
  },
  {
    "objectID": "resources/resources-general.html#reproducible-research",
    "href": "resources/resources-general.html#reproducible-research",
    "title": "More Data Science Resources",
    "section": "Reproducible research",
    "text": "Reproducible research\n\nTrisovic et al. 2022 provides a nice glimpse at the problems that still exist when trying to reproduce/replicate prior studies by re-running the code.\nR Workflow is an online book describing how to do reproducible research using the R ecosystem and the still fairly new Quarto framework.\nFor more Quarto, the Awesome Quarto repository has a nice curated list of links to resources."
  },
  {
    "objectID": "project/templatefiles/project-review-template.html",
    "href": "project/templatefiles/project-review-template.html",
    "title": "Project Review Template",
    "section": "",
    "text": "Title of project:\nName of project author(s):\nName of project reviewer:"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#background-context-and-motivation",
    "href": "project/templatefiles/project-review-template.html#background-context-and-motivation",
    "title": "Project Review Template",
    "section": "3.1 Background, Context and Motivation",
    "text": "3.1 Background, Context and Motivation\nHow well is the context of the project described? Is a comprehensive background, including summary of previous/related work given? Is the project well placed into the context of existing work (including proper referencing of existing work). Is it clear why the project was undertaken and what new information it hopes to provide?\n\n3.1.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.1.2 Summary assessment (PICK ONE, DELETE THE OTHERS)\n\nvery poor contextualization and motivation\nsome contextualization and motivation\nstrong contextualization and motivation"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#question-description",
    "href": "project/templatefiles/project-review-template.html#question-description",
    "title": "Project Review Template",
    "section": "3.2 Question description",
    "text": "3.2 Question description\nHow well and clear are the question(s)/hypotheses the project aims to address described? Is it clear how the questions relate to the data?\n\n3.2.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.2.2 Summary assessment\n\nquestion/hypotheses unclear\nquestion/hypotheses somewhat explained\nquestion/hypotheses fully clear"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#data-description",
    "href": "project/templatefiles/project-review-template.html#data-description",
    "title": "Project Review Template",
    "section": "3.3 Data description",
    "text": "3.3 Data description\nHow well is the data overall described? Is the source provided? Is a codebook or other meta-information available that makes it clear what the data is?\n\n3.3.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.3.2 Summary assessment\n\nsource and overall structure of data poorly explained\nsource and overall structure of data somewhat explained\nsource and overall structure of data well explained"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#data-wrangling-and-exploratory-analysis",
    "href": "project/templatefiles/project-review-template.html#data-wrangling-and-exploratory-analysis",
    "title": "Project Review Template",
    "section": "3.4 Data wrangling and exploratory analysis",
    "text": "3.4 Data wrangling and exploratory analysis\nHow well is the data cleaned/processed and explored? Are all steps reasonable and well explained? Are alternatives discussed and considered? Are meaningful exploratory results shown (e.g.¬†in the supplementary materials)?\n\n3.4.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.4.2 Summary assessment\n\nmajor weaknesses in wrangling and exploratory component\nsome weaknesses in wrangling and exploratory component\nessentially no weaknesses in wrangling and exploratory component"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#appropriateness-of-analysis",
    "href": "project/templatefiles/project-review-template.html#appropriateness-of-analysis",
    "title": "Project Review Template",
    "section": "3.5 Appropriateness of Analysis",
    "text": "3.5 Appropriateness of Analysis\nWere the analysis methods appropriate for the data? Was the analysis done properly? Were different components of the analysis (e.g.¬†performance measure, variable selection, data pre-processing, model evaluation) done in the best way possible and explained well?\n\n3.5.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.5.2 Summary assessment\n\nwrong/inadequate analysis\ndefensible but not optimal analysis\nstrong and reasonable analysis"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#presentation",
    "href": "project/templatefiles/project-review-template.html#presentation",
    "title": "Project Review Template",
    "section": "3.6 Presentation",
    "text": "3.6 Presentation\nHow well are results presented? Are tables and figures easy to read and understand? Are the main figures/tables publication level quality?\n\n3.6.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.6.2 Summary assessment\n\nresults are poorly presented, hard to understand, poor quality\nresults are presented ok, with room for improvement\nresults are very well presented"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#discussionconclusions",
    "href": "project/templatefiles/project-review-template.html#discussionconclusions",
    "title": "Project Review Template",
    "section": "3.7 Discussion/Conclusions",
    "text": "3.7 Discussion/Conclusions\nAre the study findings properly discussed? Are strengths and limitations acknowledged? Are findings interpreted properly?\n\n3.7.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.7.2 Summary assessment\n\nmajor parts of discussion missing or wrong\nminor parts wrong, missing or unclear\nstrong, complete and clear discussion"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#further-comments",
    "href": "project/templatefiles/project-review-template.html#further-comments",
    "title": "Project Review Template",
    "section": "3.8 Further comments",
    "text": "3.8 Further comments\nAdd any other comments regarding the different aspects of the project here. Write anything you think can help your classmate improve their project."
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#structure",
    "href": "project/templatefiles/project-review-template.html#structure",
    "title": "Project Review Template",
    "section": "4.1 Structure",
    "text": "4.1 Structure\nIs the project well structured? Are files in well labeled folders? Do files have reasonable names? Are all ‚Äújunk‚Äù files not needed for analysis/reproduction removed? By just looking at files and folders, can you get an idea of how things fit together?\n\n4.1.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n4.1.2 Summary assessment\n\npoor and confusing structure\nmostly clear, but some confusing parts (e.g.¬†useless files, things in the wrong folders)\nwell structured"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#documentation",
    "href": "project/templatefiles/project-review-template.html#documentation",
    "title": "Project Review Template",
    "section": "4.2 Documentation",
    "text": "4.2 Documentation\nHow well is the project documented? Are you able to understand each step of the whole analysis, each decision that was made, and each line of code? Is enough information provided as comments in code or as part of Rmd files?\n\n4.2.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n4.2.2 Summary assessment\n\npoorly documented\ndecently documented with some gaps\nfully and well documented"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#reproducibility",
    "href": "project/templatefiles/project-review-template.html#reproducibility",
    "title": "Project Review Template",
    "section": "4.3 Reproducibility",
    "text": "4.3 Reproducibility\nAre all results fully reproducible? Is documentation provided which clearly explains how to reproduce things, and does it work without the need for any manual intervention? Are you able to re-run the whole analysis without having to do manual interventions/edits?\n\n4.3.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n4.3.2 Summary assessment\n\nmajor parts not reproducible\nsmall parts not reproducible or required manual intervention\nfully reproducible without issues"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#thoroughness",
    "href": "project/templatefiles/project-review-template.html#thoroughness",
    "title": "Project Review Template",
    "section": "4.4 Thoroughness",
    "text": "4.4 Thoroughness\nHow thorough was the overall study? Were alternatives (e.g.¬†different ways of processing the data or different models) considered? Were alternatives discussed? Were the questions/hypotheses fully and thoroughly addressed?\n\n4.4.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n4.4.2 Summary assessment\n\nweak level of thoroughness\ndecent level of thoroughness\nstrong level of thorougness"
  },
  {
    "objectID": "project/templatefiles/project-review-template.html#further-comments-1",
    "href": "project/templatefiles/project-review-template.html#further-comments-1",
    "title": "Project Review Template",
    "section": "4.5 Further comments",
    "text": "4.5 Further comments\nAdd any other comments regarding the overall project here. Write anything you think can help your classmate improve their project."
  },
  {
    "objectID": "project/project-overview.html",
    "href": "project/project-overview.html",
    "title": "Class Project Overview",
    "section": "",
    "text": "Some of this will only fully make sense once we are a few weeks into the course. It‚Äôs nevertheless a good idea to read through it and get an overall idea before we have discussed all the different tools and details mentioned. Re-visit/re-read as needed."
  },
  {
    "objectID": "project/project-overview.html#your-own-data",
    "href": "project/project-overview.html#your-own-data",
    "title": "Class Project Overview",
    "section": "Your ‚Äúown‚Äù data",
    "text": "Your ‚Äúown‚Äù data\nIf you have data that you are using for some research project(s) you are doing, you are welcome and encouraged to work on this as part of the class project. Of course, what you do for this class project needs to be new work, not a previously done and recycled analysis. Also, since the analysis needs to be fully reproducible, you need to provide the data at least within the class (no need to make it publicly available). I encourage you to use the class project toward helping you do an analysis and write a report that can help you with a project you want to publish as part of your research!"
  },
  {
    "objectID": "project/project-overview.html#publicly-available-data",
    "href": "project/project-overview.html#publicly-available-data",
    "title": "Class Project Overview",
    "section": "Publicly available data",
    "text": "Publicly available data\nYou can use any data you can get access to. Since it needs to be reproducible, you need to be able to share the data at least with me and the classmates who will review your project. If you need some ideas for data, check out this website I maintain with various links to resources, among them is a list with links to various data sources. Of course you are not limited to data listed there.\nOften, the most interesting questions can be asked by combining data from more than one source."
  },
  {
    "objectID": "project/project-details.html",
    "href": "project/project-details.html",
    "title": "Class Project Details",
    "section": "",
    "text": "Overview\nThis documents provides more detailed instructions and grading rubrics for each part of the project.\nNote that you should not consider the scores below as corresponding to a letter grade. Instead, I use those to differentiate between different aspects of the project. Only at the end while I make actual grade-like scores, which will likely involve curving (up, never down).\nAt my discretion, I might consider assignments even if they are submitted after the deadline. In that case, I might take off a point for lateness.\n\n\nPart 1\nThe main objective for part 1 is to describe data and question in enough detail that I can assess if the planned project is feasible.\nUse the Manuscript.qmd file. Remove or replace the template information. Add your proposed project title and your name(s).\nTo that end, provide the following pieces of information:\n\nBriefly describe what the data is, how it was collected, where you will get (or got) it from. How many observations do you have, what was measured? Anything else important to report about the data?\nAt this stage you are not required to already have and show the data, but if you do, even better. Then add a few lines of code which load the data and using some of the commands you learned about, provide summary descriptions of the data.\nExplain the question you want to answer using the data. What will be your outcome(s) of interest (if any)? What (if any) specific predictors will you focus on? What relations/patterns are you looking for in the data?\nAs much as you know, suggest how you will analyze it. At this stage in the course, we haven‚Äôt covered analysis approaches yet, so you can keep things vague and non-technical here.\nYou are allowed, but not yet required, to provide background information for the question you plan to answer. For instance you can describe why it‚Äôs an interesting question, who else has done similar analyses, how your analysis will be new/different, etc. Similar to what you read in an introduction to a research paper. For the final report, you‚Äôll need these parts. For part 1, they are not required, but you are welcome to already write down some of that.\nEventually, for your final report, what you write for this part will go into different sections of the full report. Some will go into the introduction, some in the methods section. You can already place these items there, or for now just write them as a single section.\n\nGrading for this part will follow the following rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete, contains enough information to allow assessment of feasibility of proposed project. Fully reproducible.\n3\n\n\nSomewhat insufficient\nMost components are present, but noticeable gaps exist, or existing materials can‚Äôt be fully reproduced.\n2\n\n\nNot sufficient\nSubmission is rather incomplete, has major missing parts which does not allow assessment of feasibility of proposed project\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\n\nPart 2\nThe main objective for part 2 is to have mostly completed the data loading/cleaning/wrangling/exploring part.\nTo that end, provide the following pieces of information:\n\nEverything from part 1. That doesn‚Äôt mean you need to keep what you wrote for part 1 frozen. Just that the description you provided there should be part of this submission. As appropriate, you can rewrite/reformat things to get it closer to a final report structure (e.g.¬†start moving some parts into a method section).\nA somewhat detailed description containing text and code showing your cleaning/wrangling/exploring steps. Place this in separate Quarto/R files inside the code folder or its sub-folders. Make sure it‚Äôs clear which files are the relevant ones, delete any non-relevant files.\nUpdate the main manuscript. Add a few of the exploratory results and any other content you think should be shown.\nRemoval or replacement of any left-over files and leftover text and code from the templates. Update all readme files, delete any files and folders that are not part of your project. Remove any comments and bits of code that are not relevant. At this stage, only information, code and files relevant to your project should be present, with appropriate documentation.\nThe main text should show plots or tables that explore the data, with a focus on the quantities of main interest (outcome, main predictor, co-variates of specific interest, etc.).\nIt is up to you how you structure things. You can use a combination of R or Rmd scripts. As long as things are well documented, reproducible and logical, the exact setup is your choice.\nEverything needs to be fully reproducible and you need to provide somewhere (e.g.¬†in the main text file or in the readme file in your repository) instructions on what one needs to do to completely reproduce everything.\nYour main article and - if applicable supplementary files - should knit into a word or pdf or html documents.\nIf you start including references, you should use a reference manager and a bibtex file from which you cite references in your manuscript. I recommend managing the bibtex file with the free Zotero reference manager, but if you have another reference manager that can handle bibtex files, you can use that too. Your bib file should be part of the project repository (for instance in the same folder as the manuscript). Feel free to pick any citation style you like (you can get CSL files from e.g.¬†Zotero‚Äôs style repository).\n\nGrading for this part will follow the following rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\n\nPart 3\nThe main objective for part 3 is to have started the analysis part of the project and continued to improve everything.\nTo that end, provide the following pieces of information:\n\nAll relevant files and documents needed to reproduce everything.\nAll non-relevant files (e.g., leftovers from the template) removed or updated.\nSome documentation (e.g.¬†a readme.md file) explaining how your project is set up and which scripts need to be run in what order to reproduce everything.\nFiles with well-documented code (either R scripts or Rmd files) that do all the previous tasks (cleaning/processing), as well as running a few analyses. You can start with simple bivariate ones, looking for patterns between your outcome(s) and individual predictors of interest. I suggest as much as possible you use the tidymodels framework. You can also try a few multivariable GLM. Results from those explorations should be saved in whatever form you consider most appropriate (figures or tables).\nA main article/manuscript file which contains the most pertinent results and findings from everything you have done so far. You can include more figures/tables here than you would in a regular manuscript. However, it should still be nicely readable and somewhat focused, so don‚Äôt produce page-long raw R output or a ton of exploratory figures or similar things. The main results of your analysis should be in this manuscript. Any further explorations and results (figures and tables) should go into a separate Rmarkdown file that comprises the supplementary material. For some idea on how this can be structured, you can for instance revisit Brian‚Äôs project we looked at in this unit.\n\nGrading for this part will follow this rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, minor parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\nThis part of your project will be assessed by some of your classmates. See the Project Review document for more details.\n\n\nPart 4\nThe objective is to have mostly completed implementation of analyses following the approaches covered in the course.\nTo that end, provide the following pieces of information:\n\nAll relevant files and documents needed to reproduce everything.\nDocumentation for everything you are doing. Any files/documentation not related to your project should be removed, only relevant information should be present.\nCode that performs statistical analyses of your data using the approaches we cover in class, such as: train/test split, cross-validation, trying different models, exploring model quality (performance, uncertainty, diagnostics, etc.). Depending on your data and question, not all approaches will make sense for your data. Choose the ones that make sense. E.g., if you happen to do an analysis of text or high-dimensional data, use methods/models appropriate for that data. The main point is that you should show you understand the main concepts regarding analysis and model evaluation and can apply them to your data with the tools we covered.\nUpdate your manuscript and supplementary files with the new results (figures/tables) from your analysis.\nAt this point, make sure you are also far along with your background section, including referencing (using a reference manager and bibtex, file, no manual references) and everything starts looking like a full analysis similar to what one could submit to a journal for publication.\n\nGrading for this part will follow this rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, minor parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\n\nPart 5\nThe objective for part 5 is to have a fully completed project, ready for peer review.\nTo that end, provide the following pieces of information:\n\nEvery file needed to reproduce your complete analysis and report in your project repository (that includes files such as readme, bibtex, style files, etc).\nDetailed instructions (e.g.¬†as a readme.md file) explaining how to reproduce all your results.\nA complete, nicely readable and well formatted report, written in structure like a research paper (Abstract/Introduction/Methods/Results/Discussion/Citations).\nWell documented supplementary material\nWell-documented code as Rmd or R scripts that explain clearly all steps in your analysis (including the wrangling/exploring parts).\nMeta-data explaining your data as needed.\nEverything very well documented and polished as much as possible.\n\nCheck the Project Review Template file to see how this submission will be assessed by your peers. I will not grade this part but it will be assessed by your peers, as described in the Project Review document.\nA great project would be at the level of Brian McKay‚Äôs paper example you checked out in the Motivating Examples document, or one of the projects on the Project Examples page.\n\n\nPart 6\nThe main objective for part 6 is to have a finished project, ready for final grading.\nTo that end, provide the following pieces of information:\n\nA complete project, with as many further improvements as you want to implement, based on feedback from your classmates and any other improvements you can think of.\n\nI will grade the final project using the same criteria your classmates used for peer review. This will be graded on a 100 points scale and combined with the other project scores for a final overall project score."
  },
  {
    "objectID": "courseinfo/course-syllabus.html",
    "href": "courseinfo/course-syllabus.html",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "",
    "text": "This course provides a survey of modern statistical approaches to analyze data. We will cover a variety of modern approaches for analyzing and interpreting data commonly encountered in public health, biomedical sciences, and related areas. This is an applied, hands-on class. We will use real data (bring your own data if you have it) to learn different methods of analysis. We will discuss all the steps of a data analysis, including obtaining and cleaning data, exploratory and full analysis, and presentation of results.\nWe will discuss how to formulate scientifically solid questions for a given set of data, how to decide on the right method of analysis, how to implement the analysis in R, and how to present and communicate the results. We will cover statistical topics such as regression, tree based models, cross-validation, bootstrapping, and model selection."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#overview",
    "href": "courseinfo/course-syllabus.html#overview",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "",
    "text": "This course provides a survey of modern statistical approaches to analyze data. We will cover a variety of modern approaches for analyzing and interpreting data commonly encountered in public health, biomedical sciences, and related areas. This is an applied, hands-on class. We will use real data (bring your own data if you have it) to learn different methods of analysis. We will discuss all the steps of a data analysis, including obtaining and cleaning data, exploratory and full analysis, and presentation of results.\nWe will discuss how to formulate scientifically solid questions for a given set of data, how to decide on the right method of analysis, how to implement the analysis in R, and how to present and communicate the results. We will cover statistical topics such as regression, tree based models, cross-validation, bootstrapping, and model selection."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#learning-objectives",
    "href": "courseinfo/course-syllabus.html#learning-objectives",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe main goal for this course is for you to learn the whole process of performing a data analysis project. This starts with identifying a suitable question-data pair, proceeds to getting, cleaning and exploring the data, culminates with fitting statistical models and producing materials that communicate what you did and found. A second goal is to introduce you to some modern analysis approaches that these days often go by the name of ‚ÄòMachine Learning‚Äô. Finally, a related goal is to introduce you to a set of tools that allow for a modern, reproducible workflow of your analysis.\nThe specific learning objectives that I hope you will achieve by going through this course are:\nDefine meaningful data analysis questions and assess the feasibility of answering these questions with the available data.\n\nGiven a data set, define the questions that can be answered and formulate and implement suitable analytic approaches.\nGiven a data analysis question, determine the type of data and analytic approach needed to answer it.\n\nBe able to obtain, organize and process data for detailed analysis.\n\nKnow how to obtain data from a variety of different sources.\nBe knowledgeable about data types and standards and how to process them.\nBe able to organize and process data in a reproducible, automated and documented manner.\nBe able to thoughtfully and critically assess strengths and weaknesses of specific data sets and process the data appropriately.\n\nBe knowledgeable of different data analysis methods and select the appropriate approach for a given project based on data and question.\n\nCritically compare and evaluate the strengths and weaknesses of different data analysis approaches.\nJudge the appropriateness of different approaches for specific questions and data sets and know how to apply an appropriate analytic approach.\nDesign and implement successful data analyses using state-of-the-art analysis software to translate data to information and knowledge that leads to actionable insights.\n\nEfficiently communicate results from data analyses to a variety of stakeholders.\n\nSummarize analysis results in ways that provide actionable conclusions and that are easily understandable by different audiences, such as laypersons, decision makers, and expert colleagues.\nAssess the strengths and weaknesses of different formats for representing the results of data analyses.\n\nUse modern coding and analysis tools to implement automated, reproducible analysis and project management workflows.\n\nExplain the importance of workflow, project management, and reproducibility tools, and know how to use those tools.\nBe proficient in R coding to implement and execute a complete data analysis project in a reproducible and automated manner.\nBe comfortable using R and Github to do data analysis in a reproducible manner.\nBe able to quickly learn how to use new software and tools, figure out how to get help when stuck, and make it work for you.\n\nDevelop skills to critically assess your own and others‚Äô analyses and conclusions.\n\nJudge the usefulness and appropriateness of data analyses described in the primary research literature.\nLearn to look over your shoulder and critically assess what you are doing, what assumptions you make by doing certain things, and if and how you can justify these."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#topics",
    "href": "courseinfo/course-syllabus.html#topics",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Topics",
    "text": "Topics\nHere is a non-exhaustive lists of topics that this course does and does not cover.\n\nWe will cover\n\nHow to set up an analysis workflow that is as reproducible and automated as possible.\nGetting, cleaning and processing messy real-world data.\nData visualization.\nModern tools for data analysis (e.g., R, Quarto, Git/GitHub, AI systems).\nThe tidyverse for data processing and tidymodels for fitting models.\nAn introduction to some Machine Learning tools and techniques.\n\n\n\nWe will not (or barely) cover\n\nAdvanced visualization techniques using interactive tools such as R/Shiny.\nDealing with ‚Äúnon-rectangular‚Äù data, such as time-series data, images, audio, complex -omics data, etc. (we won‚Äôt cover it, but you can use such data for your class project).\nStatistical tests and basic statistical modeling (linear and generalized linear models). Some familiarity with those techniques is assumed and they do show up a few times, but won‚Äôt be covered in any depth.\n\nHow to code in R. We will use R, but this course doesn‚Äôt teach R. With enough effort, a student can learn both the subject matter and pick up enough R at the same time, but this will require extra effort. In general, some basic familiarity with R or another programming language will be assumed.\nAnything in depth. This is a survey course and covers a lot of material, thus we won‚Äôt be able to go into much depth for any topic. Resources are provided to allow anyone interested to go deeper on their own."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#prerequisites",
    "href": "courseinfo/course-syllabus.html#prerequisites",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nFormal requirement for the course is BIOS 7010. Knowledge of material from BIOS 7010 is assumed. If your lack pre-requisites prevent you from enrolling in this course, please contact me to get permission to enroll.\nThis is a quantitative course. We will not discuss the mathematical details of specific data analysis approaches, however some statistical background and being comfortable with quantitative thinking are needed. Knowledge of statistics at the level of fitting linear or logistic models to data (e.g., as obtained in our BIOS 7010 and 7020 courses) is assumed. Some R coding skills (e.g., as obtained in our EPID 7500 class) are also needed.\nIf you do not have any coding or statistics knowledge, you can still take the class, but you need to be prepared to spend extra time and effort to fill any gaps. This will be especially true for the R coding part. Some of you likely have previous R experience, while others might have little to none. If you are in the little to none category, expect to spend extra time getting up to speed. I believe it‚Äôs doable and worth the effort, but you need to be prepared for it. There will be plenty of help from myself, classmates, and the internet if you end up getting stuck with some of the coding, but your effort and commitment are still required."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#should-i-take-this-course",
    "href": "courseinfo/course-syllabus.html#should-i-take-this-course",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Should I take this course?",
    "text": "Should I take this course?\nHere are a few more pointers to what to expect from the course, and comments from previous students (so you can hear it from them, not just from me).\n\nThe course is a high-level survey course. We cover a lot of material.\nA lot of the content is fairly conceptual/broad/big-picture, we don‚Äôt go much into technical details on specific topics.\nThe pace of the course is fast and covers a lot of topics.\nThere is a good bit of hands-on work/exercises that you need to do each week, which can be quite time-consuming.\nThis course comes with a good bit of team work.\nYou will learn skills that are likely useful for your research/work career.\nYou will be using R, GitHub, Quarto and other useful tools.\n\n\nTo re-emphasize: This course requires a significant time commitment, especially if you have not used R or any other programming language before. Budget your time accordingly and plan ahead! If you do, I‚Äôm fairly certain you will find it worth it. If you are not able or willing to allot the time needed to learn enough R (and GitHub) to make things work, this course might not be ideal for you.\n\n\nSome feedback from past students\n‚ÄúI have found this course to be useful and relevant to my research interests. I think the mixture of the modules, additional resources, and coding exercises really helped strengthen my understanding of important data analysis concepts.‚Äù\n‚ÄúI have thoroughly enjoyed this class from start to finish, and I have gained a lot of knowledge, starting from Github to the entire data analysis workflow. However, I feel that we have covered a lot of material in just one semester.‚Äù\n‚ÄúI have learned so much from this class in what feels like such a short amount of time, and it has gotten me a lot more comfortable working with and interpreting my data and I now feel that I have a strong grasp on data analysis workflows. However, sometimes I feel like I‚Äôve learned about so many new things that I feel dumb all over again because there is so much to take in when it comes to machine learning and I want the opportunity to explore each facet in greater detail.‚Äù\n‚ÄúThe last half of the semester has seemed like a whirlwind. We have reviewed everything from modeling to training and even machine learning.‚Äù"
  },
  {
    "objectID": "courseinfo/course-syllabus.html#course-setup",
    "href": "courseinfo/course-syllabus.html#course-setup",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Course Setup",
    "text": "Course Setup\nThis course is a fully online, asynchronous, cohort-based course. That means there are weekly deadlines, but other than the fixed deadlines, you can do the work whenever it is convenient for you.\nAll course materials are freely available online. We will make use of several freely available textbooks and other materials. All course materials are listed on the course website. We will use the R software for data analysis. We will also use a few other software tools. All are freely available.\nThis course is very hands-on. The weekly exercises (aka homework) are usually quite in-depth and also often time-consuming. Plan accordingly. For each exercise, I will provide detailed instructions that hopefully make it clear what you need to do. The materials provided on the course website are not meant to be memorized, but to be used to be able to do the hands-on activities, such as the exercises and the class project.\nFor more details on course logistics, see the other information in the General Information section of the course website."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#privacyferpa",
    "href": "courseinfo/course-syllabus.html#privacyferpa",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Privacy/FERPA",
    "text": "Privacy/FERPA\nThe course is set up such that you work ‚Äúin the open‚Äù by having a GitHub repository and accompanying website that showcases the work you are doing in this class as an online portfolio. This is a great way to showcase your work to potential employers. However, it also means that your work is publicly visible.\nAs a student, under the Family Educational Rights and Privacy Act (FERPA) you have the right to have all your student-related information kept private, including the fact that you are taking this class. Keeping everything private can be done using private GitHub repositories. But it means you won‚Äôt be able to create a public website/portfolio, and you can‚Äôt follow the general instructions in the exercises. Instead I‚Äôll need to give you special instructions. If for some reason you want to keep the fact that you are taking this class a secret, and thus do not want to use public GitHub repositories/create a publicly accessible online portfolio please let me know. If you take this course and do not inform me that you want your attendance in this course to be kept private, I consider this as you consenting with producing public materials as part of this course, and thus make your participation in this course public. If you have any concerns about this, please contact me to discuss."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#grading",
    "href": "courseinfo/course-syllabus.html#grading",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Grading",
    "text": "Grading\nThe grade will be made up as follows:\n\n20% quizzes\n30% exercises/homework\n10% participation/discussions\n40% a course long project, broken up into pieces.\n\nThe following grading scale will be used, final grades might be curved (upward, never down): A 93-100, A- 90-93, B+ 87-90, B 83-87, B- 80-83, C+ 77-80, C 73-77, C- 70-73, D 60-70, F &lt; 60\nMore detailed descriptions of the different assessments is provided on the Assessments page."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#class-attendance-make-up-policy",
    "href": "courseinfo/course-syllabus.html#class-attendance-make-up-policy",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Class Attendance, Make-up Policy",
    "text": "Class Attendance, Make-up Policy\nThis class is online. You are expected to submit all assignments by their due dates. Excused misses of due dates are only provided by prior agreement with the instructor or for special reasons (medical, etc.)."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#getting-help",
    "href": "courseinfo/course-syllabus.html#getting-help",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Getting Help",
    "text": "Getting Help\nIf you have questions about any aspect of the course, please do not hesitate to ask for help. The course materials describe in detail the ways you can ask for help."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#university-honor-code-and-academic-honesty-policy",
    "href": "courseinfo/course-syllabus.html#university-honor-code-and-academic-honesty-policy",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "University Honor Code and Academic Honesty Policy",
    "text": "University Honor Code and Academic Honesty Policy\nAll academic work must meet the standards contained in A Culture of Honesty. All students are responsible to inform themselves about those standards before performing any academic work. More detailed information about academic honesty can be found on UGA‚Äôs honesty webpage.\nDiscussions with your classmates and the instructor are encouraged. However, the final work should be your own."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#students-with-disabilities",
    "href": "courseinfo/course-syllabus.html#students-with-disabilities",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Students with Disabilities",
    "text": "Students with Disabilities\nStudents with disabilities who require reasonable accommodations in order to participate in course activities or meet course requirements should contact the instructor."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#general-disclaimers",
    "href": "courseinfo/course-syllabus.html#general-disclaimers",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "General Disclaimers",
    "text": "General Disclaimers\nThis syllabus is a general plan, deviations announced to the class by the instructor may be necessary."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#course-outline",
    "href": "courseinfo/course-syllabus.html#course-outline",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Course Outline",
    "text": "Course Outline\nFor an outline of the course, please see the Course Schedule document."
  },
  {
    "objectID": "courseinfo/course-syllabus.html#more-details",
    "href": "courseinfo/course-syllabus.html#more-details",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "More Details",
    "text": "More Details\nThe General Information section and the introductory unit of this course contains all the logistic details you need to know."
  },
  {
    "objectID": "courseinfo/course-overview.html",
    "href": "courseinfo/course-overview.html",
    "title": "Course Information Overview",
    "section": "",
    "text": "This section contains most of the logistic information. The Syllabus page explains all the general logistic parts, the Schedule page lists what content is covered and what assessments you need to do each week, and the Communication page provides details on how we‚Äôll interact throughout this course.\nA few more logistic details can be found in other places, e.g., the introductory unit of this course. You‚Äôll be pointed to the sources."
  },
  {
    "objectID": "courseinfo/assessments-overview.html",
    "href": "courseinfo/assessments-overview.html",
    "title": "Course Assessments - Overview",
    "section": "",
    "text": "Each module comes with assessments. Those are things you need to do that will be graded. In general, expect that each module contains a Quiz, an Exercise, and a Discussion. Sometimes there are additional/alternative components, such as submission of project parts.\nDue dates are usually Friday 5 pm the week of the module. Some might be at other times, e.g., the beginning or middle of the week. See the schedule or the module specific assessment document. If unclear, ask.\nAll assessments are located in the Assessments section of the website. Their labeling matches that of the module, so it should be clear for each module which assessment to complete.\nThe assessments, especially the exercises, are the main way of learning the material. They are therefore meant as a help for you, giving you incentives to work through and learn the course material. It is very easy to take shortcuts (cheat) for any of the assessments. Of course, our academic honesty policy prohibits you from doing so. But more importantly, I assume you want to learn the material. And if you take shortcuts by getting answers from a classmate, you are only hurting your own learning. That said, I encourage getting help from any source you can, including your classmates. I let you decide how much helping each other is acceptable and at what point it turns into having someone else do the work for you that you should be doing yourself."
  },
  {
    "objectID": "courseinfo/assessments-overview.html#assessments-overview",
    "href": "courseinfo/assessments-overview.html#assessments-overview",
    "title": "Course Assessments - Overview",
    "section": "",
    "text": "Each module comes with assessments. Those are things you need to do that will be graded. In general, expect that each module contains a Quiz, an Exercise, and a Discussion. Sometimes there are additional/alternative components, such as submission of project parts.\nDue dates are usually Friday 5 pm the week of the module. Some might be at other times, e.g., the beginning or middle of the week. See the schedule or the module specific assessment document. If unclear, ask.\nAll assessments are located in the Assessments section of the website. Their labeling matches that of the module, so it should be clear for each module which assessment to complete.\nThe assessments, especially the exercises, are the main way of learning the material. They are therefore meant as a help for you, giving you incentives to work through and learn the course material. It is very easy to take shortcuts (cheat) for any of the assessments. Of course, our academic honesty policy prohibits you from doing so. But more importantly, I assume you want to learn the material. And if you take shortcuts by getting answers from a classmate, you are only hurting your own learning. That said, I encourage getting help from any source you can, including your classmates. I let you decide how much helping each other is acceptable and at what point it turns into having someone else do the work for you that you should be doing yourself."
  },
  {
    "objectID": "courseinfo/assessments-overview.html#quizzes",
    "href": "courseinfo/assessments-overview.html#quizzes",
    "title": "Course Assessments - Overview",
    "section": "Quizzes",
    "text": "Quizzes\nThe quizzes cover the basics and ensure that you have gone through some of the material. Each quiz consists of around 10-20 short questions. Not all answers to the quiz questions are directly found in the materials, sometimes you need to do a - hopefully straightforward - online search.\nQuizzes are auto-graded on submission based on a simple counting of the right answers. At the end, I‚Äôll average across all quizzes. Note that since there are &gt;10 quizzes and all quizzes together count for only a fraction of the total grade, each quiz counts for a small part of your grade. So try to get them as correct and complete as possible, but don‚Äôt stress out about missing a question here and there.\n\nThe link to the website for quiz submission is in the important-information Discord channel. Before submitting any quizzes, you need to set up a user name and password by filling the Google Form that was shared with you in the welcome email. The link to the Google Form can also be found in the important-information Discord channel. The submission system becomes available once everyone has submitted their user names/passwords.\nUnless otherwise specified, you can submit each quiz once, and it needs to be submitted by the deadline (generally Fridays), otherwise the system will not take your submission.\n\n\nQuiz sheets\nFor each module, you are asked to fill out an Excel sheet with questions related to the course material. Download the quiz files from the links below, fill in the Answer column, and submit to the online quiz submission system by the deadline specified in the Schedule document.\n\nYou can expand and resize the columns/rows of the Excel file so you can make sure to see all content. Other than that, please only fill in the Answer column and don‚Äôt make other changes (e.g.¬†don‚Äôt rename or delete rows/columns). If you do make changes, submission might fail. If that happens and you get an error message during submission, simply come back here, download a fresh sheet and copy and paste your answers into it, making sure to not change any other part of the sheet.\n\nQuizzes will become available here as the course progresses. Once a quiz is available, you should be able to submit it at any time before the deadline.\n\nModule 1 ‚Äì Course and tools introduction - you have 2 attempts available for this one.\nModule 2 ‚Äì Data analysis introduction"
  },
  {
    "objectID": "courseinfo/assessments-overview.html#exercises",
    "href": "courseinfo/assessments-overview.html#exercises",
    "title": "Course Assessments - Overview",
    "section": "Exercises",
    "text": "Exercises\nThe exercises are the most important component of the course! The material we cover in this class can only be learned by doing it yourself. As such, the focus is really on the exercises (i.e.¬†the homework), with the readings and videos mainly meant to help you find and learn the material you need to do the exercises.\nBecause the exercises are so important, they are usually quite in-depth and also often time-consuming. Plan accordingly. For each exercise, I will provide detailed instructions that hopefully make it clear what you need to do.\nIf you get stuck during the exercises (quite possible), feel free to ask for help. Use the various course materials, the course discussion board as well as outside resources. But do try to do as much as possible yourself (of course with the help of Google).\nUnless otherwise specified, exercises are due by Friday. Quite often, some part of the exercise needs to be done before that deadline to allow for group work.\n\nExercise grading\nI grade the exercises based on 4 categories:\n\n3 points = complete or only minor omissions/mistakes.\n2 points = some noticeable bits are missing or wrong.\n1 points = some parts are present, but overall very incomplete.\n0 points = completely or almost completely missing.\n\nAt my discretion, I might consider assignments even if they are submitted after the deadline. In that case, I might take off a point for lateness.\nDon‚Äôt interpret these scores as grades, I just use them to differentiate, then I‚Äôll adjust at the end when I compute the overall project score.\nFor exercises that include multiple parts (e.g., you start something, then your colleague takes over), each part will be assigned a score."
  },
  {
    "objectID": "courseinfo/assessments-overview.html#discussions",
    "href": "courseinfo/assessments-overview.html#discussions",
    "title": "Course Assessments - Overview",
    "section": "Discussions",
    "text": "Discussions\nWe can‚Äôt have in-class discussions in an online course, but hopefully we can still have robust discussions as part of the course. I hope that lively online discussions will allow us to create a sense of class community that usually comes more naturally in an in-class course. I want everyone to feel like they are part of a class, a community of students learning together, instead of just being on their own interacting with the course in front of a computer screen. To achieve that, having a lot of discussion activity is vital.\nYou will be asked to participate in discussions throughout the course, and discussion participation is part of the grade. The discussions are meant to connect the module content to the broader context and hopefully will spark interesting further thoughts and insights.\nUnless otherwise specified, the first posting of the discussion is due by Wednesday, the discussion replies are due by Friday. Post to the discussion channel for the applicable module (e.g.¬†module2_discussion, module3_discussion).\n\nDiscussion guidelines\nTo make sure discussions work well for everyone, here are some general guidelines.\n\nYou can and will be asked to both start new threads/posts or reply to others. If you start a new topic, make sure you are in the right channel. (If needed, you can also move posts to other channels later).\nYou are encouraged to include links to additional material that you find online, including but not limited to research papers, or sources like Wikipedia, and other useful resources. There are no rules on formatting other than that it should be readable.\nYou don‚Äôt always have to provide references/links to resources, nor do you need to be sure that ‚Äòyou got it right‚Äô. It‚Äôs ok to post speculations/ruminations/thoughts occasionally.\nMake meaningful contributions. You are welcome to react to each others‚Äô posts with üòÉ or üëç or other such emotions, but that does not replace more substantial responses. Give reasons why you think something is one way or another.\nFor the purpose of assessment, you need to participate in the discussions at the level specified in each module‚Äôs assessment document. I encourage you to engage in further discussions. Those further posts won‚Äôt be counted toward the requirement for a given module but will factor into the overall participation portion of the grade.\nIt should be obvious that any posts should be helpful and civilized. I will remove any posts that are demeaning or otherwise inappropriate. Please let me know if you see such posts.\nDiscussions will remain available for the whole course, and you can keep contributing at any time.\n\n\n\nDiscussion grading\nFor most modules, you‚Äôll have to do the following (unless specifically directed otherwise):\n\nYou need to write a post (generally a few hundred words) as instructed. Post it to the appropriate channel by Wednesday evening.\nYou need to write at least 3 replies to posts from classmates. Those can be replies to 3 different posts or multiple back-and-forth (of substance) for the same post/thread. You must post your replies by Friday evening.\nFor each module (i.e.¬†each week) you need to post a minimum of 4 posts (1 initial, 3 replies) with around 500 meaningful words total by the deadlines. I won‚Äôt count words (unless it looks really short), and probably also won‚Äôt count the number of posts, unless both seem well below the minimum.\nI will use the same 0-3 point score for each discussion as for the quizzes.\nGoing above the minimum (more posts, longer contributions, especially thoughtful, etc.) will count positively toward the participation portion of the final grade.\nFor each module, the assessment document contains specific discussion instructions. If the specific discussion instructions are different than the general ones here, you should follow the specific ones."
  },
  {
    "objectID": "courseinfo/assessments-overview.html#projects",
    "href": "courseinfo/assessments-overview.html#projects",
    "title": "Course Assessments - Overview",
    "section": "Projects",
    "text": "Projects\nSee the Projects section on the website for more information."
  },
  {
    "objectID": "content/module-workflow/data-models-not-covered.html",
    "href": "content/module-workflow/data-models-not-covered.html",
    "title": "More Data and Models we didn‚Äôt cover",
    "section": "",
    "text": "Overview\nIn this unit, I mention very briefly a few types of data and models that we unfortunately didn‚Äôt have time to cover.\n\n\nLearning Objectives\n\nBe familiar with some other types of common data.\nKnow about some additional analysis approaches.\n\n\n\nIntroduction\nThis course already covers a lot of material (maybe more than should be covered in a single semester üòÅ. Of course, there is much more in the area of data analysis that we were not able to cover.\nOur focus here was on the analysis of what is sometimes called rectangular data. We have observations (individual units of data) generally as rows, and variables (outcome and predictors) as columns. This is still the most common type of data, especially in public health and more generally the life sciences. However, more complex types of data are rapidly increasing in frequency and importance. If the data has a different structure, it will usually require different, specialized analysis approaches. The good news is that most of the general approaches we covered in this course still apply. The difference is often in how the data is processed, and the specific way data and models are combined.\nThe following sections very briefly mention some common types of more complex data and provide a few pointers towards their analysis. Those categories are not mutually exclusive, some data sources can have more than one property (e.g., real-time text data).\n\n\nHierarchical data\nThis type of data is actually quite common. A hierarchy (also at times called multi-level or nesting structure) occurs when there is some type of grouping inherent. For instance, if you analyze heart attacks from different hospitals, there might be systematic differences between hospitals that are worth taking into account. You can then build models that take that into account. I think there is a strong case to be made that one should often start with a hierarchical modeling approach, and only drop it if one is convinced that there are no systematic differences in the data. Unfortunately, such hierarchical models are still a little bit harder to implement, and often a lot harder to understand and interpret. For such models, a Bayesian framework is often very useful. A good introductory resource for hierarchical modeling, and especially how to do it with R, is Statistical Rethinking (the second half of the book). The new edition of R4DS has a chapter on working with hierarchical data (but does not cover the fitting/analysis part.) There is also a CRAN Task view that lists R packages relevant to hierarchical modeling.\n\n\nTime-series data\nTime series are a specialized data type that are autocorrelated, and we can therefore get better predictions by using specialized models that take this data structure into account. A good resource is the free textbook Forecasting: Principles and Practice and a lot of the other work by Rob Hyndman.\nA very useful set of tools to allow times-series work in R is the set of packages called the tidyverts. The modeltime R package allows one to use the tidymodels framework to analyze time-series data.\nCRAN also has a Task View for Time Series Analysis. (A Task View on CRAN is a site that tries to combine and summarize various R packages for a specific topic). Another task view that deals with longitudinal/time-series data is the Survival Analysis Task View.\n\n\nSpatial data\nLike time series data, spatial data also feature autocorrelation, but typically in two dimensions. (i.e., latitude and longitude.) Spatiotemporal data, typically collected as repeated measurements of spatial data over time, is also somewhat common. One could also have an elevation coordinate and have 3D (or even 4D spatiotemporal data) spatial data, but most analyses focus on 2D spatial autocorrelation structures.\nWhile there is (to my knowledge) no current way to fit specific spatial models in tidymodels (i.e.¬†no modeltime analogue), one can use spatial resampling through the spatialsample package, which provides resampling methods that take spatial autocorrelation into account.\nYou can find a community-curated list of resources for spatial data in The Big Book of R or in the Spatial Data CRAN task view.\n\n\nText\nWorking with and analyzing larger sections of text is becoming increasingly important. Complex and powerful AI tools (e.g., ChatGPT) have become rather good at working with text. The analysis of text often goes by the term natural language processing. Such text analysis will continue to increase in importance, given the increasing data streams of that type. If you are interested in doing full analyses of text data, the tidytext R package and the Text mining with R book are great resources. A short introduction to this topic is the Text Mining chapter (27) of IDS.\n\n\nGenetics and -omics data\nThe main genetic type of data is based on sequences. A lot of specialized tools exist to work with what is often fairly noisy data. Aligning sequences, comparing them, placing them on phylogenetic trees, and other such operations are so common and important that there is a large area of tools for those purposes.\nData that is often called -omics (e.g., metabolomics, glycomics) is often rectangular in structure, but often has distinguishing features, such as few individuals/rows and many observations/columns. Such data needs special treatment. Often, variable/feature reduction is a common step in the analysis workflow.\nThe bioconductor website is your source for (almost) all tools and resources related to genetics and omics-type data analyses in R.\n\n\nBig data\nThis term is a bit fuzzy. Operationally, it often means anything you can‚Äôt work with on your local computer. Sometimes data is somewhat big, you can still work with it on your local computer but it takes very long and you have so many observations that everything is statistically significant. Thus, big data generally requires both special approaches and tools towards wrangling/exploring/cleaning and often also special approaches for its analysis. A short discussion of Big Data is provided in the AI/Deep Learning module.\n\n\nImages, Audio, Video\nImages are generally converted into multiple matrices of values for different pixels of an image. For instance, one could divide an image into a 100x100 grid of pixels, and assign each pixel RGB (color) values and intensity. That means one would have 4 matrices of numeric values, each of size 100x100. One would then perform operations on those values. As you can imagine, that quickly leads to fairly large amounts of data. These days, most successful image analysis is done using some form of neural nets, which are generally considered an artificial intelligence (AI) method (but recall that ML and AI terminology is somewhat fuzzy. The general current usage is that AI are a type of ML, specifically the neural net type.)\nVideos are basically time-series of images. Of course, analysis of videos is even harder than analysis of images. Again, neural-net based AI methods are most often used.\nAudio also has a time-series structure that needs to be taken into account when analyzing it. It is conceptually similar to video, a time-series of sounds. In fact, if the video has sound, then audio analysis could be part of the video analysis.\nI have close to zero experience trying to analyse images/sounds. Unfortunately, because of my lack of experience, I don‚Äôt even know what a good introductory source would be to get started if you were interested (other than the generic ‚Äúlook around online‚Äù). I do think that if you wanted to analyze that kind of data, R is probably not the best tool.\n\n\nFurther reading\nSee the references provided in the sections above, as well as the Resources section on the course website."
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html",
    "href": "content/module-workflow/assessment-complete-workflow.html",
    "title": "Assessment - Complete Workflow",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#overview",
    "href": "content/module-workflow/assessment-complete-workflow.html#overview",
    "title": "Assessment - Complete Workflow",
    "section": "Overview",
    "text": "Overview\nFor this exercise, we will participate in one more Tidy Tuesday exercise. This time, we‚Äôll apply a full analysis workflow to the data.\nThis is another solo exercise, so no group work this week, but of course more GitHub. This exercise will go into your portfolio."
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#setup",
    "href": "content/module-workflow/assessment-complete-workflow.html#setup",
    "title": "Assessment - Complete Workflow",
    "section": "Setup",
    "text": "Setup\nUse your portfolio website. Make sure it‚Äôs up to date and fully synced. Open it in R Studio. From last week‚Äôs exercise, you should have an empty Rmd file called tidytuesday_exercise2.qmd. Open it, that‚Äôs where you‚Äôll write your Tidy Tuesday analysis.\nGo to the TidyTuesday Github repository. Look for the dataset for this week, and read the instructions on how to get the data. You will also be provided with a data dictionary. If the data is available for download, place it somewhere in your portfolio repository (e.g., you can make a new folder called data)."
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#what-to-do",
    "href": "content/module-workflow/assessment-complete-workflow.html#what-to-do",
    "title": "Assessment - Complete Workflow",
    "section": "What to do",
    "text": "What to do\nWrite code and text to perform the following steps:\n\nLoad, wrangle and explore the data. By now you know this is an iterative procedure, so it‚Äôs ok to have these parts of the process/code intertwined.\nOnce you understand the data sufficiently, formulate a question/hypothesis. This will determine your outcome of interest and, if applicable, main predictor(s) of interest. Since we don‚Äôt know the data yet, it might be that the question is a bit contrived and not actually too interesting, but I‚Äôm sure there will be more than one potentially reasonable question one can ask, no matter what the data will be.\nOnce you determine the question and thus your outcome and main predictors, further pre-process and clean the data as needed. Then split into train/test. (It might be that the data is too small for this split to make sense in real life but for this exercise, we‚Äôll just do it.)\nFit at least 3 different model types to the data using the tidymodels framework we practiced. Use the CV approach for model training/fitting. Explore the quality of each model by looking at performance, residuals, uncertainty, etc. All of this should still be evaluated using the training/CV data. You can of course recycle code from previous exercises, but I also encourage you to explore further, e.g.¬†try different ML models or use different metrics. You might have to do that anyway, depending on the data and your question/outcome.\nBased on the model evaluations and your scientific question/hypothesis, decide on one model you think is overall best. Explain why. It doesn‚Äôt have to be the model with the best performance. You make the choice, just explain why you picked the one you picked.\nAs a final, somewhat honest assessment of the quality of the model you chose, evaluate it (performance, residuals, uncertainty, etc.) on the test data. This is the only time you are allowed to touch the test data, and only once. Report model performance on the test data.\nSummarize everything you did and found in a discussion. Make sure you discuss your findings with regard to your original question/hypothesis. What did you learn? If feasible, show a summary figure or table that illustrates your main scientific finding from this analysis."
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#general-pointers",
    "href": "content/module-workflow/assessment-complete-workflow.html#general-pointers",
    "title": "Assessment - Complete Workflow",
    "section": "General pointers",
    "text": "General pointers\nAdd ample information/documentation in the form of both code comments and explanatory text You should provide a running commentary on what you do, why you do it, and how your R code accomplishes that (so that if you stare at it in a few weeks, you can remember what in the world you did back then üòÑ).\nI don‚Äôt want to see any p-values anywhere üòÅ! This is an exploratory analysis, thus there is no place for p-values."
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#finish-up",
    "href": "content/module-workflow/assessment-complete-workflow.html#finish-up",
    "title": "Assessment - Complete Workflow",
    "section": "Finish up",
    "text": "Finish up\nOnce done with your Tidy Tuesday analysis, rebuild your portfolio site to make sure everything works and looks good, that all the links work, etc. Then push to GitHub by the deadline. Since this will be on your portfolio website, and I know where to find it, there is no need to post any link this week."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-overview.html",
    "href": "content/module-synthetic-data/synthetic-data-overview.html",
    "title": "Overview of the synthetic data content",
    "section": "",
    "text": "Overview\nThis module covers why and how to generate and use synthetic (also known as simulated/artificial/fake) data.\n\n\nLearning Objectives\n\nKnow about reasons why one might want to use simulated data.\nBe familiar with ways to generate synthetic data.\nBe aware of several R packages that can be used to generate synthetic data.\nKnow how to use synthetic data for testing.\nKnow how to use synthetic data with AI tools.\n\n\n\nContent\nThe module consists of the following units:\n\nIntroduction and Motivation\nCreating new data\nCreating new data based on existing data\nScrambling existing data\nR packages for data creation\nUse synthetic data with AI tools\nUse synthetic data to test models"
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html",
    "title": "Generating synthetic data based on existing data",
    "section": "",
    "text": "In this unit, we discuss how to generate synthetic data based on existing data."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#ai-help",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#ai-help",
    "title": "Generating synthetic data based on existing data",
    "section": "AI help",
    "text": "AI help\nYou probably don‚Äôt want to feed your real data to the AI. So the first part of summarizing existing data has to be done in a safe/secure space. Once you have good data summaries, you can use AI to generate synthetic data and code."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#example",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#example",
    "title": "Generating synthetic data based on existing data",
    "section": "Example",
    "text": "Example\nWe‚Äôll use the data we generated in example 1 of the previous tutorial and now assume that this is ‚Äúreal‚Äù data and that we want to generate synthetic data that‚Äôs similar to the real data.\n\nSetup\nFirst, we do the usual setup steps of package loading and other housekeeping steps.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(here)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(gtsummary)\n\n\n# Set a seed for reproducibility\nset.seed(123)\n# Define the number of observations (patients) to generate\nn_patients &lt;- 100\n\n\n\nLoad and explore data\nIn a first step, we want to understand how each variable in the real data set is distributed, so we can create synthetic data that looks very similar.\nWe can use various helper functions from different packages to get good descriptive summaries of the data and variables. This code below shows two such helper functions.\nNote that we added more information to the table output. The table summary function treats the date variable and patient ID variable as numeric, so the output is somewhat nonsensical. But this is just for internal ‚Äúquick and dirty‚Äù use, so we don‚Äôt need to make things pretty. Of course you could if you needed/wanted.\n\ndat &lt;- readRDS(here::here(\"content/module-synthetic-data/synthetic-data-new-existing-r/syn_dat.rds\"))\nskimr::skim(dat)\n\n\nData summary\n\n\nName\ndat\n\n\nNumber of rows\n100\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n4\n6\n0\n2\n0\n\n\nTreatmentGroup\n0\n1\n1\n7\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nEnrollmentDate\n0\n1\n2022-01-08\n2022-12-30\n2022-06-25\n86\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPatientID\n0\n1\n50.50\n29.01\n1.0\n25.75\n50.50\n75.25\n100.0\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nAge\n0\n1\n45.90\n9.14\n21.9\n40.08\n45.60\n51.92\n66.9\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÇ\n\n\nBloodPressure\n0\n1\n127.96\n21.31\n91.3\n110.67\n130.85\n147.38\n159.5\n‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá\n\n\nCholesterol\n0\n1\n174.67\n32.58\n88.9\n153.23\n171.85\n196.62\n271.0\n‚ñÅ‚ñÜ‚ñá‚ñÉ‚ñÅ\n\n\nAdverseEvent\n0\n1\n0.29\n0.46\n0.0\n0.00\n0.00\n1.00\n1.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n\n\n\n\ngtsummary::tbl_summary(dat, statistic = list(\n  all_continuous() ~ \"{mean}/{median}/{min}/{max}/{sd}\",\n  all_categorical() ~ \"{n} / {N} ({p}%)\"\n),)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 1001\n    \n  \n  \n    PatientID\n51/51/1/100/29\n    Age\n46/46/22/67/9\n    Gender\n\n    ¬†¬†¬†¬†Female\n49 / 100 (49%)\n    ¬†¬†¬†¬†Male\n51 / 100 (51%)\n    TreatmentGroup\n\n    ¬†¬†¬†¬†A\n43 / 100 (43%)\n    ¬†¬†¬†¬†B\n30 / 100 (30%)\n    ¬†¬†¬†¬†Placebo\n27 / 100 (27%)\n    EnrollmentDate\n2022-06-30/2022-06-25/2022-01-08/2022-12-30/107.06744593852\n    BloodPressure\n128/131/91/160/21\n    Cholesterol\n175/172/89/271/33\n    AdverseEvent\n29 / 100 (29%)\n  \n  \n  \n    \n      1 Mean/Median/Minimum/Maximum/SD; n / N (%)\n    \n  \n\n\n\n\nWe can also look at the distribution of the different variables individually, using e.g., base R commands (or any other package of your choice).\n\n# using base R to explore variable distributions\ntable(dat$Gender)\n\n\nFemale   Male \n    49     51 \n\ntable(dat$TreatmentGroup)\n\n\n      A       B Placebo \n     43      30      27 \n\ntable(dat$AdverseEvent)\n\n\n 0  1 \n71 29 \n\nhist(dat$Age)\n\n\n\n\n\n\n\nhist(dat$BloodPressure)\n\n\n\n\n\n\n\nhist(dat$Cholesterol)\n\n\n\n\n\n\n\n\nLooks like a normal distribution for age and uniform distribution for blood pressure should work well. That‚Äôs of course not surprising since we produced the data that way in an earlier tutorial. But for real data you don‚Äôt know what process produced it, you just want to see how things are distributed so you can recreate it that way.\nFor Cholesterol, the distribution doesn‚Äôt look quite normal. That‚Äôs because in the original data generating process, we made it dependent on treatment.\nIf you want to see if there are correlations in the data that you might want to also have in your synthetic data, you can explore those with tables or plots like these.\n\n# explore some correlations between variables\ntable(dat$AdverseEvent, dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 28 19      24\n  1 15 11       3\n\nplot(dat$Age, dat$BloodPressure)\n\n\n\n\n\n\n\nggplot(dat) + geom_histogram(aes(x = Cholesterol, fill = TreatmentGroup)) \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAt this stage, it is up to you to decide if you want to try to include correlations between variables that might or might not exist in the real data, or if you just want to give each variable an independent distribution.\n\n\nGenerate data\nNow we‚Äôll create synthetic data that is similar to the real data. In this code example, we pull directly from the actual data stored in dat. However, you can also save that information into an intermediary object or file (e.g., save the mean and standard deviation of age) and then just use those summary statistics to generate the synthetic data. This prevents for instance issues with confidentiality if you use AI to help write the synthetic data code.\n\n# Create an empty data frame with placeholders for variables\nsyn_dat &lt;- data.frame(\n  PatientID = numeric(n_patients),\n  Age = numeric(n_patients),\n  Gender = character(n_patients),\n  TreatmentGroup = character(n_patients),\n  EnrollmentDate = lubridate::as_date(character(n_patients)),\n  BloodPressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  AdverseEvent = integer(n_patients)\n)\n\n# Variable 1: Patient ID\n# can be exactly the same as the original\nsyn_dat$PatientID &lt;- 1:n_patients\n\n# Variable 2: Age (numeric variable)\n# creating normally distributed values with the mean and SD taken \n# from the real data\nsyn_dat$Age &lt;- round(rnorm(n_patients, mean = mean(dat$Age), sd = sd(dat$Age)), 1)\n\n# Variable 3: Gender (categorical variable)\n# create with probabilities based on real data distribution\nsyn_dat$Gender &lt;- sample(c(\"Male\", \"Female\"), \n                         n_patients, replace = TRUE, \n                         prob = as.numeric(table(dat$Gender)/100))\n\n# Variable 4: Treatment Group (categorical variable)\n# create with probabilities based on real data distribution\nsyn_dat$TreatmentGroup &lt;- sample(c(\"A\", \"B\", \"Placebo\"), \n                                 n_patients, \n                                 replace = TRUE,\n                                 prob = as.numeric(table(dat$TreatmentGroup)/100))\n\n# Variable 5: Date of Enrollment (date variable)\n# use same start and end dates as real data\nsyn_dat$EnrollmentDate &lt;- lubridate::as_date(sample(seq(from = min(dat$EnrollmentDate), \n                                                        to = max(dat$EnrollmentDate), \n                                                        by = \"days\"), n_patients, replace = TRUE))\n\n# Variable 6: Blood Pressure (numeric variable)\n# use uniform distribution as indicated by histogram of real data\n# use same min and max values as real data\nsyn_dat$BloodPressure &lt;- round(runif(n_patients, \n                                     min = min(dat$BloodPressure), \n                                     max = max(dat$BloodPressure)), 1)\n\n# Variable 7: Cholesterol Level (numeric variable)\n# here, we re-create it based on the overall data distribution pattern\n# since the data didn't quite look like a normal distribution, \n# here we'll just use it as its own distribution and sample right from the data\n# note that this breaks the association with treatment group \n# for real data, we wouldn't know if there is any, but if we suspect, we could\n# generate data with and without such associations and explore its impact on model performance\nsyn_dat$Cholesterol &lt;- sample(dat$Cholesterol, \n                                    size = n_patients, \n                                    replace = TRUE)\n\n\n# Variable 8: Adverse Event (binary variable, 0 = No, 1 = Yes)\n# we implement this variable by taking into account different probabilities stratified by treatment\nprobA = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,1])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,1])\nprobB = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,2])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,2]) \nprobP = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,3])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,3])\n\n# this re-creates the correlation we find between those two variables\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"A\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"A\"), replace = TRUE, prob = probA)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"B\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"B\"), replace = TRUE, prob = probB)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"Placebo\"), replace = TRUE, prob = probP)\n\nYou can always make your synthetic data partially different from the real data for important quantities (e.g., your main input/exposure or outcome of interest) to explore different implications and model performance.\n\n\nCheck and save data\nQuick peek at generated data to make sure things look ok, then we can save it.\n\n# Print the first few rows of the generated data\nhead(syn_dat)\n\n  PatientID  Age Gender TreatmentGroup EnrollmentDate BloodPressure Cholesterol\n1         1 36.0   Male              A     2022-06-26         114.3       151.7\n2         2 45.1 Female        Placebo     2022-04-13         152.3       120.1\n3         3 55.7   Male              A     2022-02-02          93.5       183.8\n4         4 44.6   Male              A     2022-08-25         107.5       148.7\n5         5 35.3   Male              B     2022-06-04         138.1       190.3\n6         6 38.4 Female              A     2022-12-23         106.7       142.6\n  AdverseEvent\n1            0\n2            0\n3            0\n4            1\n5            0\n6            1\n\n# quick summaries\nsummary(syn_dat)\n\n   PatientID           Age           Gender          TreatmentGroup    \n Min.   :  1.00   Min.   :16.40   Length:100         Length:100        \n 1st Qu.: 25.75   1st Qu.:40.10   Class :character   Class :character  \n Median : 50.50   Median :45.10   Mode  :character   Mode  :character  \n Mean   : 50.50   Mean   :45.83                                        \n 3rd Qu.: 75.25   3rd Qu.:50.33                                        \n Max.   :100.00   Max.   :65.50                                        \n EnrollmentDate       BloodPressure    Cholesterol     AdverseEvent \n Min.   :2022-01-15   Min.   : 92.7   Min.   : 97.6   Min.   :0.00  \n 1st Qu.:2022-04-12   1st Qu.:107.5   1st Qu.:158.0   1st Qu.:0.00  \n Median :2022-06-28   Median :126.9   Median :170.6   Median :0.00  \n Mean   :2022-06-27   Mean   :125.8   Mean   :175.4   Mean   :0.31  \n 3rd Qu.:2022-09-23   3rd Qu.:144.3   3rd Qu.:196.1   3rd Qu.:1.00  \n Max.   :2022-12-26   Max.   :159.0   Max.   :248.7   Max.   :1.00  \n\ndplyr::glimpse(syn_dat)  \n\nRows: 100\nColumns: 8\n$ PatientID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ Age            &lt;dbl&gt; 36.0, 45.1, 55.7, 44.6, 35.3, 38.4, 52.2, 43.0, 33.9, 4‚Ä¶\n$ Gender         &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Fe‚Ä¶\n$ TreatmentGroup &lt;chr&gt; \"A\", \"Placebo\", \"A\", \"A\", \"B\", \"A\", \"Placebo\", \"B\", \"A\"‚Ä¶\n$ EnrollmentDate &lt;date&gt; 2022-06-26, 2022-04-13, 2022-02-02, 2022-08-25, 2022-0‚Ä¶\n$ BloodPressure  &lt;dbl&gt; 114.3, 152.3, 93.5, 107.5, 138.1, 106.7, 113.0, 103.2, ‚Ä¶\n$ Cholesterol    &lt;dbl&gt; 151.7, 120.1, 183.8, 148.7, 190.3, 142.6, 151.7, 212.1,‚Ä¶\n$ AdverseEvent   &lt;int&gt; 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0‚Ä¶\n\n# Frequency table for adverse events stratified by treatment\ntable(syn_dat$AdverseEvent,syn_dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 34 12      23\n  1 16 12       3\n\n# ggplot2 boxplot for cholesterol by treatment group\nggplot(syn_dat, aes(x = TreatmentGroup, y = Cholesterol)) +\n  geom_boxplot() +\n  labs(x = \"Treatment Group\", y = \"Cholesterol Level\") +\n  theme_bw()\n\n\n\n\n\n\n\n# Save the simulated data to a CSV file\nwrite.csv(syn_dat, \"syn_dat_new.csv\", row.names = FALSE)"
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#summary",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#summary",
    "title": "Generating synthetic data based on existing data",
    "section": "Summary",
    "text": "Summary\nThe process of generating synthetic data based on existing data is fairly straightforward:\n\nLoad existing data.\nLook at distributions/frequencies of variables of interest.\nGenerate new data with variables that are distributed like original data.\n\nSometimes, you can just look at your original data and make up new data that‚Äôs roughly similar. At other times you want to be very close, in that case you need to either draw new values from distributions that are well-describe the original data, or you need to re-sample the original data (with or without replacement).\nFortunately, most of the time it‚Äôs good enough to get your data somewhat similar to the original. You would hope/expect any kind of statistical method to work well on different sets of data that are roughly similar. If that‚Äôs not the case, your method might not be very robust and might require changes."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html",
    "title": "Why create and use synthetic data?",
    "section": "",
    "text": "In this unit, we discuss why you might want to create synthetic (simulated/artificial/fake) data as part of your analysis and what general approaches there are for doing so."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#knowing-the-truth",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#knowing-the-truth",
    "title": "Why create and use synthetic data?",
    "section": "Knowing the truth",
    "text": "Knowing the truth\nFor your simulated data, you know exactly how it is created (because you do it). Let‚Äôs say for your data analysis, you want to look at the relation between the daily dose of some statin drug a person takes, and if that has some association with their LDL cholesterol. Depending on the exact structure of your data, you might fit various simple or complex models and get some answers. You can - and should! - look carefully at the results to see if they make sense. But you never know for sure if the model ‚Äúgot it right‚Äù since you don‚Äôt know what the true (whatever that might mean) answer is. In contrast, if you created the data, you know exactly what the true answer should be. This allows you to test your models and see if they can recover what you know the truth to be. For instance, if you set up the data such that there is a linear relation between drug dose and outcome of interest, and you set the slope of this relation to some value, then when you fit a linear model you should expect to get that value back."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#no-problems-with-sharing",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#no-problems-with-sharing",
    "title": "Why create and use synthetic data?",
    "section": "No problems with sharing",
    "text": "No problems with sharing\nThere are often limitations (real or perceived) for sharing the actual data. An important reason might be confidentiality. This not a problem with simulated data. So if you want to be able to share your analysis with collaborators or readers when you publish, and for some reason you can‚Äôt share the actual data, you can share simulated data. This allows others to run all your code and reproduce your analysis workflow, even if they won‚Äôt get quite the same results since they are using simulated and not the real data.\nAnother important aspect of being able to share is the increasing use of AI models to help with data analysis. You can upload data to AI tools and request them to perform certain analyses and return the code that does the analysis. This can be very powerful and time-saving. However, you probably often do not want to give the AI your real data, as whatever you upload is often stored and re-used by the companies owning the AI tool. What you can do instead is feed the AI synthetic data, ask it to write code to do the analyses you are interested in, then take the code and with probably only a few modifications, apply it off-line on your real dataset."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#easier-to-play-around-with",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#easier-to-play-around-with",
    "title": "Why create and use synthetic data?",
    "section": "Easier to play around with",
    "text": "Easier to play around with\nOften, real-world datasets are very messy and require a good bit of time to clean them up (wrangle them) to get data in a form that allows analysis. If you are not even sure if your idea/analysis makes sense, it would be a waste to spend a lot of time cleaning data that ends up not being useful. What you can do instead is simulate data that has similar structure and content to the real data, but since you create it, you can make sure it‚Äôs clean and easy to work with. Then you try your analysis ideas on that simulated data. If it works ok, there‚Äôs a chance (unfortunately no guarantee!) that it might also work for the real data. As you play around with your simulated data, you might realize that certain ideas/approaches you initially had won‚Äôt work, and you can modify accordingly. Once your explorations using the simulated data suggest that your analysis plan can work, then you can start the often arduous task of cleaning and wrangling your real data."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#make-it-all-up",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#make-it-all-up",
    "title": "Why create and use synthetic data?",
    "section": "Make it all up",
    "text": "Make it all up\nCompletely making up data is the most flexible approach. You can try to produce synthetic versions of the full dataset, or just parts that are of interest. Let‚Äôs say in the statin-cholesterol example above, you real data set also contains information on the age and sex of the individuals, and if they have any pre-existing heart condition. Maybe in a first pass, you don‚Äôt want to look at those and just explore the main statin-cholesterol relation. Then you can just simulate data for that part. You can add further information - including simulated characteristics that are not in the original data! - as needed."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#summarize-the-original-data-then-re-create",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#summarize-the-original-data-then-re-create",
    "title": "Why create and use synthetic data?",
    "section": "Summarize the original data, then re-create",
    "text": "Summarize the original data, then re-create\nEven if you fully make up your synthetic data, you want it to somewhat resemble the original one. The easiest way is to just glimpse at the variables in the original data and generate new data that looks approximately similar.\nIf you want to be a bit more rigorous and get closer to the original data, you can also statistically summarize the original data, then use those summary distributions to generate new data. For instance, let‚Äôs say you find that the cholesterol variable can be well approximated with a normal distribution. If you fit the normal distribution to the data, you can get the mean and standard deviation. Then you can generate synthetic cholesterol data coming from a normal distribution with those estimated mean and standard deviation values."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#scramble-the-data",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#scramble-the-data",
    "title": "Why create and use synthetic data?",
    "section": "Scramble the data",
    "text": "Scramble the data\nIf you already have the data in a clean form that you can work with, you can use that data and scramble things to make new data. Basically you can randomly re-arrange variable values for different individuals, such that the new data does not correspond to the real data anymore, but has the overall same structure, and the same values (just re-arranged between individuals).\nFor instance you can take the age of each person, and randomly re-assign it to someone else. Note that this breaks potential patterns. For instance if in the original data, there happen to be an association between cholesterol and age, once you re-shuffled age values, this pattern will change. So the results you get from scrambled data will possibly be different, but since it has exactly the same structure and the same values as the original data, your whole analysis pipeline should work on the scrambled data. Of course, since you didn‚Äôt create the data, you don‚Äôt know the ‚Äútruth‚Äù and as such can‚Äôt assess fully if your analysis gives you the right results.\nIf you have already done your analysis and want/need scrambled data for sharing, e.g.¬†as part of a submission to a journal where you can‚Äôt submit the original data, you can try to do the reshuffling in a way that preserves patterns you identified. For instance if you saw that age was correlated with LDL, you can do your reshuffle such that age and LDL pairs stay together, while other variables (e.g., sex) get randomly reassigned. Of course you need to change at least some values for each individual, you can‚Äôt just move all variables for an individual from one row to another, it will still be the same data for that person, thus associated with potential confidentiality issues."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#do-a-mix",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#do-a-mix",
    "title": "Why create and use synthetic data?",
    "section": "Do a mix",
    "text": "Do a mix\nIt is of course entirely possible to combine the two approaches above. For instance, you can start with the original data and do some scrambling if needed. Then you can replace some variables with the simulated data you generated. This allows you to test your analysis more thoroughly since now you know what you put in, so you can check if you get that out."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "",
    "text": "In this unit, we explore the use of LLM AI tools to generate synthetic data with R."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#a-simple-example",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#a-simple-example",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "A simple example",
    "text": "A simple example\nUsing AI to generate code that creates synthetic data can be very efficient. Here is an example. I typed the following prompt into ChatGPT 3.5.\n\nWrite R code that generates a dataset of 100 individuals with ages from 18 to 49, BMI values from 15 to 40 and smoking status as yes or no. Assume that age and BMI are uncorrelated. Assume that smokers have a somewhat lower BMI. Then use the patchwork R package to generate a panel of ggplot2 plots. The first panel should show a violin plot with BMI on the y-axis and smoking status on the x-axis. The second panel should show a scatterplot with age on the y-axis and BMI on the x-axis. Add thorough documentation to your code.\n\nOnly the first part is really the data generation bit, but since it‚Äôs so quick and easy, I wanted some code that also explores the data so I can see if what it produced is what I had in mind.\nThe result was this fully-working code.\nSince the AI systems are continuously updated, it is possible that if you type the same prompt into ChatGPT, your code might look slightly differently, but hopefully it is still working.\nIn my experience, you usually don‚Äôt quite get what you want on the first try. But it‚Äôs easy to tell the AI to update the code until it does what you want it to, or at least is close and then you can do the rest by hand."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#one-more-example",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#one-more-example",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "One more example",
    "text": "One more example\nIt is possible to ask the AI to create more complex data. You might not always get exactly what you want, but it‚Äôs often worth a try. I gave this prompt to Mcirosoft Bing/Copilot in precise mode.\n\nWrite R code that creates data for N=100 individuals. Individuals ages are randomly distributed between 18 and 50 years. Assume that individuals belong to 3 treatment groups: placebo, low dose, and high dose. Individuals in each group receive either no drug, or 100mg of drug or 200mg of drug every week. Drug concentration is measured every other day. Assume that drug concentration follows an exponential decline between doses, with the decay rate being the same for the low dose and high dose groups.\nIn addition to drug concentration, cholesterol levels are measured daily. Assume that higher drug concentrations correlate with lower cholesterol levels.\nCreate a data frame that contains Patient ID, treatment group, age, time, drug concentration, and cholesterol level.\nFor function calls, specify explicitly the package from which the function comes from.\nAdd thorough documentation to the code so it is clear what each line of code does.\n\nThe code I got back did not quite work, so I provided this prompt:\n\nThe code above does not work. It gives this error :\nError in map(): In index: 1. With name: ID. Caused by error in patient$ID:\nPlease fix the code. Also add code at the end that plots the raw data for drug concentration as a function of time for all patients, stratified by treatment status. Also plot the mean for each group.\n\nThis is the code I got back.\nThe code runs and does some of the things I asked for, but not all. I specified that the drug is given weekly, but the code only contains a single dose at the start. I forgot to tell the AI for how long the data should be simulated. It picked 13 days, which means there should have been 2 doses of drug.\nOf course I could keep talking to the AI, telling it to update things such that dosing is weekly, maybe explicitly say that I want to run for 28 days, fix whatever else might not be quite right (I didn‚Äôt check the code too carefully). It‚Äôs so quick and easy, there‚Äôs no reason not to do a few more iterations and see how close the AI can get to what I want. Eventually, I will likely intervene and finish things off manually."
  },
  {
    "objectID": "content/module-stats-overview/statistical-analysis-introduction.html",
    "href": "content/module-stats-overview/statistical-analysis-introduction.html",
    "title": "Statistical Analysis Overview",
    "section": "",
    "text": "Overview\nIn this Unit, we will take another look at the whole data analysis process. Here, we focus on the actual (statistical) analysis (which I also call model fitting) component of the data analysis process. A general conceptual understanding is useful before we jump into looking at and applying specific modeling/statistical approaches.\n\n\n\nSource: VALDO.com\n\n\n\n\nLearning Objectives\n\nBecome familiar with different types of analyses.\nLearn how the type of data determines the modeling approach and be able to choose the right methods.\nUnderstand the overall goals of fitting statistical models to data.\n\n\n\nIntroduction\nSo far in this course, you briefly encountered a few simple statistical models in some of the readings and exercises (e.g., applying a linear model), but we didn‚Äôt focus on it. Instead, we looked at a lot of other components that are important for a full data analysis project but are less often taught. You might have come across the statement (e.g., in R4DS) that 80% of the time spent on data analysis is in the non-modeling/non-statistical parts. From my experience, that is true. While the statistical modeling/analysis part is certainly important, it often takes a fairly small amount of the whole project time. And while it is certainly possible to use the wrong statistical model, it seems to me that the most important and consequential mistakes that might invalidate an analysis do not happen in the modeling part. Sure, people often use the wrong statistical method, but that has - in my experience - often surprisingly (and thankfully!) little impact on the results. Not properly cleaning and processing data (e.g., not realizing that weight is reported in pounds and not kilograms when computing BMI) often has a much bigger impact on results.\nNo matter what statistical analysis you end up using, you will need to do the steps of getting and cleaning/processing/wrangling the data. During that process, you also explore your data, e.g., through plots and tables. Doing those steps efficiently and accurately is crucial. After you are done with the preliminary steps of getting/cleaning/wrangling data, you can move on to your main goal, fitting models to data. Once you reach the stage where you are ready to fit models, most of the hard work is done.\nOnce you reach the state at which your data is ready for statistical analysis, you should have a good idea of the types of models that might be appropriate. The choice of model is partly driven by the data, and partly by the kind of question you want to answer. There are several reasons why you might want to apply statistical models to your data, we‚Äôll discuss them briefly.\n\n\nWhy fit models to data\nThere are several reasons why we might want to fit models. The following is a brief list. The next section goes into more detail.\n\nTo produce or test hypotheses (e.g., ‚Äúthere is a (linear) correlation between BMI and diabetes‚Äù)\nTo estimate parameters (e.g.¬†‚Äúa 1 unit increase of particulate matter leads to 2 more asthma attacks per person per year‚Äù)\nTo draw causal conclusions (e.g.¬†‚Äútaking statins causes reduction in cholesterol‚Äù)\nTo draw mechanistic conclusions (e.g.¬†‚Äúinterferon reduces HCV virus load by stopping production of new virus‚Äù)\nTo make predictions (e.g.¬†‚Äúexposure to N cholera bacteria leads to an infection with probability p‚Äù)\n\nDepending on the objective, you will likely be using different statistical approaches. Note however that there is no clear mapping. For instance you can use a linear model to come up with hypotheses (i.e., do an exploratory analysis), to test hypotheses (e.g., in a clinical trial), to estimate parameters, or make predictions. A lot of models can be used for different purposes. This might be initially confusing. Just keep in mind that while some models are better for some purposes (e.g.¬†a complex machine learning or artificial intelligence model might be good for predictions, but bad for causal conclusions), most models can be applied to more than one type of objective.\n\n\nData Analysis Types\nThe following way of categorizing types of data analyses follows The Art of Data Science (ADS) ‚Äì specifically, chapter 3. Now might be a good time to give that chapter another quick re-read. A very similar, shortened version of the ADA chapter 3 discussion can be found in Jeff Leek and Roger Peng‚Äôs article. The following figure from their paper gives a good summary of what follows.\n\n\n\nSource: Leek and Peng 2015\n\n\nThe most straightforward analysis is a descriptive one. At that stage, you process, summarize and present the data, and do not go further. You don‚Äôt need to fit any statistical models. A lot of data collected by surveillance systems or government agencies falls into the descriptive category. For most scientific projects, we often start with a descriptive presentation. E.g. Table 1 in a study often describes and summarizes the data source. Note that some authors have the bad habit of including model fitting quantities, such as p-values, in a descriptive table. Measures that involve fitting a model (such as p-values), go beyond a descriptive analysis and should therefore generally not be in a descriptive table.\nSometimes, a descriptive study is interesting and sufficient by itself. But often, we then want to go beyond the descriptive presentation of data. The most common analysis approach is associative. Here, we are looking for associations (i.e., patterns) in the data. We are interested in seeing if patterns exist (e.g., if there is a correlation between age and speed of solving mathematical problems) and what the shape of the pattern is (e.g., linearly increasing/decreasing or non-monotone). This is also called correlation analysis.\nDepending on the way we came up with our question and the data, the results from such an associative analysis can be interpreted as an exploratory or hypothesis-generating approach, or an inferential or hypothesis-supporting approach. In general, if you asked the question/posed the hypothesis first, then went out and collected the data and analyzed it to see if your hypothesis holds, you can interpret your findings as supporting or refuting your hypothesis. If you had data that were not collected to answer your question specifically, and you analyzed the data to see if you can find some interesting patterns, then your findings should be interpreted as hypothesis-generating.\nFor both exploratory and inferential settings, you are usually interested in understanding how specific inputs/predictors affect the outcome(s) of interest. For that reason, you generally want to keep your models fairly simple and easy to interpret.\nEssentially all standard statistical tests and approaches you are likely already familiar with (e.g., various statistical tests like t-tests, simple linear regression, or simple classification such as logistic regression) fall into these categories of associative, exploratory, or inferential. We will cover some of those modeling approaches in future units.\nOften, we would like to go from association to causation, i.e., we would like to say that not only does an increase in X correlate with an increase in Y, but that X causes that increase in Y. There are two ways of doing so. One is to collect the data in the right way, namely using a randomized clinical trial or equivalent lab science approach where all contributing factors but the input of interest, X, are controlled. This way, we can say that a change in X directly causes a change in Y. If the data does not come from such a study design, methods of causal inference (which we won‚Äôt discuss in this course) can sometimes help in trying to determine causality.\nClassical statistical models can get us as far as determining potential causal relations. If we want to go even further and not only try to answer if X causes Y, but how X causes Y, we will need to employ studies or models that are mechanistic. Such models explicitly include postulated mechanisms, and by comparing such models to data, one can often determine which mechanisms are more plausible. Mechanistic models are also beyond what we cover in this course (if you are interested in those, I teach two courses on mechanistic modeling in infectious diseases üòÉ).\nWhile the main goal of science is generally understanding a system as well as possible, outside of science, other goals are often more important. In applied/industry/commerce settings, one often does not care too much if or how exactly certain inputs influence outcomes of interest. Instead, the main purpose is to try and predict future outcomes given a set of inputs. In this case, the interpretability of your model is not that important. Instead, a predictive modeling framework is more important. That‚Äôs where complex models, such as machine learning and artificial intelligence (neural net) approaches come into play. Those models are too complex to allow much interpretation and understanding of the system, but often these kinds of models are very good at prediction and real world performance (e.g., differentiating cats from dogs in images).\n\n\nExamples of data analysis types\nAn example of an inferential analysis might be the question which, if any, immunological markers (e.g., cytokines in blood) are most influential for a given clinical outcome. Building a simple model here, e.g.¬†a linear model if the outcome is continuous (e.g., blood sugar level) or a logistic model if the outcome is binary (e.g., heart attack within 5 years or not), allows us to quickly and transparently investigate how each variable in our model affects the outcome and which variables (immunological markers) might be important to study further.\nA good example of a prediction model is the monitoring of credit cards by the issuing companies, who try to predict fraudulent transactions. To that end, they feed all the data they can get about you into their models, and if something happens that is unusual, you might get flagged, and your card denied, or you will have to call to confirm. In this case, the interest is not too much on how exactly all these data points about you and your behavior lead to the prediction of legitimate vs.¬†fraudulent, only that the accuracy of those predictions is high. Because of this, in situations where prediction is important, models tend to be more complex, and one is willing to trade simplicity and interpretability of a model for increased predictive performance.\n\n\nModeling methods based on data\nWhile the goal of the analysis will guide you toward choosing a general type of modeling approach, the data usually dictate in more detail what kinds of models are suitable. The main determinant of the model type to use is the outcome(s) one wants to analyze.\nFirst, is there even an outcome? While the majority of datasets have an outcome(s) of interest, that is not always the case. Data without a clear outcome are sometimes called unlabeled. For instance, we might have collections of images of cell types, and our question is if these images cluster into specific types of cells - without knowledge of what those types might be. Another example is a scenario where we might have a large dataset of customers and lots of information about each customer. We might want to know if those customers can somehow be grouped based on the data we have about them (with the goal to design a marketing campaign directed at specific groups). Methods for those tasks are generally called clustering methods or unsupervised learning methods. Examples are k-means clustering, principal component analysis, and neural networks. (Note that some of these methods can also be used with data that include outcomes.)\nThe more common data structure is one with a specific outcome(s) of interest. This is also referred to as labeled data. Since labeled data is the most common, we focus on it in this course. In this case, we use approaches referred to as supervised learning methods. Those can be further divided based on the type of outcome. If the outcome is continuous (or can be treated as such), we use regression approaches, or if the outcome is categorical, we use classification approaches.\nYou are likely already familiar with some of these approaches. Most basic statistical tests are simple models for regression or classification, i.e., they try to detect patterns in data with quantitative or categorical outcomes. Some other statistical methods are generalized linear models (which include the basic linear and logistic models), generalized additive models, trees, support vector machines, neural nets, k-nearest neighbors, linear discriminant analysis, and a lot of further methods, many of which are variants of others. Some, but not all, of the more complex methods can be applied to both quantitative and categorical outcomes. We will cover a few of these methods later in the course.\nNote that there is, unfortunately, no one method that is universally best for all data/questions. Both the type of question and the details of the data will influence the model choices. Often, there are several models that would be reasonable choices for a given setting, and in such instances it is often worthwhile (and fairly easy) to explore multiple alternative models.\n\n\nFurther reading\nFor some additional (as well as overlapping) information to what I wrote, read Chapters 1 and 2.1. An Introduction to Statistical Learning (ILS). You don‚Äôt need to work through it in detail and can skip over the math bits if you want to. But do try to get an overall idea of the concepts these chapters are trying to convey. Chapter 1 of HMLR is another good source that you should skim through. Again, try to get the main points (which will of course overlap with the text above and the other readings).\nThis recent paper A practical guide to selecting models for exploration, inference, and prediction in ecology provides a nice discussion of different modeling goals and what approaches one should use. We have not yet discussed some of these approaches, but will soon. You could skim through the paper now, then revisit later once we covered more of the topics discussed in there."
  },
  {
    "objectID": "content/module-model-ml-2/svm-models.html",
    "href": "content/module-model-ml-2/svm-models.html",
    "title": "Support Vector Machine Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover support vector machines (SVM), another useful type of machine learning/statistical model.\n\n\nLearning Objectives\n\nBe familiar with SVM.\nUnderstand advantages and disadvantages of SVM.\n\n\n\nIntroduction\nSVM were developed in the computer science and engineering communities and were among the best-performing types of models for a while. They are not that exceptional anymore (now the big trend is neural networks), but still often perform rather well, especially sometimes for data where other models don‚Äôt perform quite as well. So SVM are often a good idea to try.\n\n\nGeneral Idea behind SVM\nSVM have cost/objective functions that do not penalize discrepancy between true and predicted outcome if the two are within a certain range, and do penalize outside that range. This is different than penalties based on basic least squares, which implement a penalty for any difference between model and data. SVM divide the predictor space into areas using multi-dimensional hyperplanes until observations within the same ‚Äúvolume‚Äù are sufficiently accurately predicted (a bit like trees).\nSVM can come in different flavors, for instance depending on how discrepancies between data and model are penalized (absolute distance, quadratic, etc.) or how the boundaries of the hyperplanes are defined.\n\n\nStrengths and Weaknesses of SVM\nSVM can be used for both classification and regression. They are very flexible and often have very good predictive performance. They are often well suited for data that contain outliers.\nThe math behind SVM is not that easy to follow, and since SVM models are somewhat complex, they tend to be a bit of a black box and the model itself is hard to interpret.\n\n\nSVM in tidymodels\ntidymodels/parsnip has several SVM models, such as svm_linear() and svm_poly(). They can be used as part of the tidymodels workflow like any other ML model you have already seen. They also need tuning.\n\n\nFurther Resources\nTo learn a bit more about SVM, you can check out chapter 9 of ISL which discusses SVM. So does chapter 14 of HMLR."
  },
  {
    "objectID": "content/module-model-ml-2/da-models.html",
    "href": "content/module-model-ml-2/da-models.html",
    "title": "Discriminant Analysis Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover discriminant analysis, which is a set of machine learning/statistical models that is very useful for classification.\n\n\nLearning Objectives\n\nBe familiar with discriminant analysis models.\nUnderstand advantages and disadvantages of these kinds of models.\n\n\n\nIntroduction\nWhile logistic regression is the most widely used approach for classification, it has several limitations. For one, it doesn‚Äôt easily generalize to outcomes that have more than 2 categories. Also, for certain types of data, logistic regression might not perform very well. Other methods like tree-based models, KNN or SVM can be used for multi-category outcome classification and often provide good performance. Another option are Discriminant Analysis (DA) methods. Those approaches are fairly quick and easy to implement and fit. At times, they provide better performance than logistic regression and might do as well as more complicated models.\n\n\nGeneral Idea behind DA\nDA make assumptions about the distributions for each predictor, and then uses those distributions to model and predict the outcome. This is called a generative approach.\nThere are different forms of DA. Linear Discriminant Analysis (LDA) is the simplest approach, it assumes that predictors can be modeled with a multivariate normal distribution. Nonlinear discriminant analysis approaches, e.g., quadratic DA, allow for a more flexible distribution of the predictors. This makes the model more flexible, but also bigger (so possibly more prone to overfitting). One can show that LDA and logistic regression are mathematically rather similar (see e.g.¬†the ISLR book). Other nonlinear variants of DA exist, e.g.¬†regularized, mixture, flexible, ‚Ä¶\nAs for any model, there is always a bias-variance/underfitting-overfitting trade-off between less flexible approaches like LDA and more flexible approaches like some of the nonlinear DAs.\n\n\nStrengths and Weaknesses of DA\nA nice feature of DA is that it is one of the algorithms that allows for classification with more then 2 outcomes. For two outcomes, there are situations where logistic regression struggles (if the outcomes are well-separated), and DA might perform better.\nDA models are not as common as logistic regression, thus it might require a bit more explanation to your audience if you decide to use them. Also, DA models make assumptions about the distributions of the predictors. If those assumptions do not hold, DA might not perform well. In situations where logistic regression works well, it usually performs better. Like logistic regression, DA is used for classification, not regression.\n\n\nDA in tidymodels\ntidymodels has the discrim pacakge, which implements several forms of discriminant analysis.\n\n\nFurther Resources\nChapter 4 of ISLR discusses discriminant analysis. So does the Generative Models section in the Examples of Algorithms chapter in IDS."
  },
  {
    "objectID": "content/module-model-ml-1/polynomial-spline-models.html",
    "href": "content/module-model-ml-1/polynomial-spline-models.html",
    "title": "Polynomial and Spline Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover some statistical models that provide more flexible extensions to generalized linear models.\n\n\nLearning Objectives\n\nBe familiar with polynomial and spline models.\nUnderstand advantages and disadvantages of these models.\nKnow when to use them and how to minimize overfitting.\n\n\n\nGLM review\nSo far, we‚Äôve focused on generalized linear models (GLM), and mainly on the two workhorses of statistical analysis: linear models for regression modeling of continuous outcomes, and logistic models for classification modeling of categorical (binary) outcomes.\nAs mentioned, what GLMs have in common is that the predictor variables show up as a linear combination, which is then mapped to the outcome with some function. In general, one can write a GLM like this:\n\\[f(Y)=b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n,\\] where \\(Y\\) is the outcome, the \\(X_i\\) are the predictors, and the choice of the function \\(f(Y)\\) (called the link function) determines the specific model.\nFor a standard linear model, it‚Äôs simply \\(f(Y) = Y\\) and the model is \\[\\textrm{linear model:} \\qquad Y=b_0 + b_1X_1 + b_2X_2 + \\ldots + b_n X_n.\\] For a logistic model, it is \\(f(Y)=\\log \\left( \\frac{Y}{1-Y} \\right)\\) (this function is called the logit function) and the whole model becomes \\[\\textrm{logistic model:} \\qquad \\log \\left( \\frac{Y}{1-Y} \\right) = b_0 + b_1X_1 + b_2 X_2 + \\ldots + b_nX_n.\\] You can re-write this such that there is just \\(Y\\) on the left and exponentiated terms of the \\(X\\) on the right, you might have seen either notation for the model before in one of your statistics classes.\nIf, instead, your outcome was integer counts of something, you could use a Poisson model, with \\(f(Y) = log(Y)\\), and the model would be\n\\[\\textrm{Poisson model:} \\qquad \\log \\left( Y \\right) = b_0 + b_1X_1 + b_2 X_2 + \\ldots + b_n X_n.\\]\nSimilar GLMs for outcomes that are distributed in other ways are possible, see e.g.¬†the list on Wikipedia. In R, if you use the glm() function, you can set the link function by specifying the family argument. If you use tidymodels, there are similar ways to set the link function, depending on the underlying fitting function you use.\nThis model structure is easy to understand, one can test the impact of individual predictor variables by taking them in and out of the model (subset selection) or alternative approaches (e.g.¬†regularization), and one can see in the final model which predictor variables survive, and what their impact is, as given by the coefficients in front of the variables.\n\nIf you have a model that has a mix of categorical and continuous input variables, and you want to compare their impact by looking at the values of their coefficients, you need to standardize your continuous variables so they are on a similar scale as your categorical variables.\n\nBasic linear and generalized linear models (LMs and GLMs) such as logistic or poisson regression impose a specific structural relation between your inputs/predictors and your outcomes. This means that if you have more complex patterns relating your inputs with your output(s), basic GLMs might not capture it fully. For instance if you had a scenario where you measured the activity/movement of some animal and how that might be impacted by temperature, it is quite feasible to expect a curved relationship. At low temperatures, the animals try to stay warm and don‚Äôt move much. At high temperature, the animals also don‚Äôt move much and instead try to stay cool. At some intermediate temperature, their movement is largest. So if you had data for animal movement as outcome, and temperature as your predictor variable, a linear model would not be good. You wanted a model that allows the pattern to curve. If you start with a linear model and look at the plot of your residuals, or an outcome-prediction plot, you would see systematic deviations, telling you that your model is not missing important patterns in the data and you are therefore underfitting.\nIf that‚Äôs the case, you might want to move on to more flexible models.\n\nEven if you suspect (or know) that the true relation between input and output is not linear - and in most cases, that is true, - it is still often good to start with a linear model. Often, things are approximately linear, and if you don‚Äôt have a lot of data, have a good bit of noise, and are trying to capture the relation between multiple input variables and the outcome, it is quite possible that the data doesn‚Äôt allow you to go beyond checking for linear relations. So it‚Äôs generally good to start with a linear model, then explore the model results (e.g., residual plots) and if you see indications for underfitting, start extending your model.\n\n\n\nPolynomial models\nA natural extension of GLM is to include higher-order terms for the predictors, either with itself or with other predictor variables (the latter case is called an interaction term). For instance for a simple linear model, this model includes second-order terms for \\(X_1\\) and \\(X_2\\) and also an interaction term between the two variables:\n\\[Y=b_0 + b_1X_1 + b_2X_2 + b_3 X_1^2 + b_4 X_2^2 + b_5 X_1X_2\\]\nSuch models with higher order terms (\\(X^2\\), \\(X^3\\), ‚Ä¶) are often called polynomial models. Note that statisticians still call this a linear model, since the coefficients show up in linear combinations, even though the variables do not. (Statisticians or statistics books will often say that a linear model is ‚Äúlinear in the parameters.‚Äù This means the same thing as having a linear combination of coefficients.) It took me years to figure out that terminology, since a physicist/engineer would call this a non-linear model!\nThe advantage of polynomial models is that they can more flexibly capture patterns in the data. A major shortcoming of polynomial models is that as the predictor values get very large or small, the predicted outcomes go ‚Äúthrough the roof‚Äù (a technical statistical term üòÅ). Let‚Äôs say you had data for the animal movement-temperature example that had temperature up to 30 degrees Celsius and showed a marked decline in movement going from 20C to 30C. A 2nd order polynomial (also called a degree 2 polynomial‚Äìthe number is the highest power that the dependent variable is raised to in the equation) that curved down as temperature went up did a good job at modeling the data. It is quite likely that if you try to predict higher temperatures, say 35C or 40C, movement might turn negative! That of course can‚Äôt happen, an animal can‚Äôt move less than not at all. These are general problems with polynomial functions, they impose an overall structure on the data and generally therefore behave poorly for values outside the range of the data. Thus, while easy to use, those models are generally not the best choice, the ones we discuss next are generally better.\n\n\nSpline models\nSpline models are similar to polynomial models, inasmuch as they allow higher order terms of the predictor variables to show up in the model, and thus can capture patterns that go beyond linear relations. They try to deal with the potential problem of imposing a global structure done by polynomial models by applying polynomial combinations of predictors only to parts of the data, with connection points (knots) between those parts. The end result is a smooth function that allows for capturing of potentially non-linear patterns without the need to impose a global structure. This often improves the quality of the fits. Though even for these kinds of functions, extrapolation outside the range of observed/measured predictor values can be tricky and often lead to wrong results (in fact, a general problem for models). These models are often more difficult (or at least more complicated) to interpret as well.\n\n\nLocal regression\nWhen using ggplot2, you likely came across a local regression curve. The default smoothing for small data if you use geom_smooth() is a LOESS (LOcally Estimated Scatterplot Smoothing) model, which is a type of local regression. You can think of it as being conceptually very similar to spline models, though the details of how it‚Äôs implemented differ. It is perfectly fine to use smoothing functions without making any adjustments during data exploration. But if you want to use any of these for actual statistical fitting, you have to tweak/tune them as discussed below.\n\n\nStep functions\nA version of the models just discussed are those that allow non-smooth jumps in the outcome as a predictor varies. As a simple example, say you want to model current time as function of geographical location. Since humans divided the world arbitrarily into time zones, as you move from one zone to another, there is an abrupt change in the outcome (time) with a small change in the predictor (longitude). A bad way of modeling that is to discretize your predictors, since most often you don‚Äôt know where the cut-offs are. Maybe the example I just gave is a bad one, since this is a rare situation where we do know the cut-offs, because they are human made üòÑ. But say we assume some outcome changes somewhat abruptly with age. It is unlikely that we know exactly what age(s) we should set the cut-off(s) at. By using a model, we can let the computer decide where to set the cut-off (see below). Other model types that are good at handling rapid changes in outcome for a change in a predictor are tree-based models, which you‚Äôll be learning about shortly.\n\n\nGeneralized additive models (GAM)\nGeneralized additive models (GAMs) are in some sense (though not completely, see below) a general form of the different models just discussed. The preserve the feature that each predictor enters the model additively. But now, each predictor can be related to the outcome with a ‚Äì potentially complicated, potentially unknown ‚Äì function. In equation form, a GAM can be written as:\n\\[g(Y) = x_0 + f_1(x1) + f_2(x2) + \\ldots + f_n(x_n).\\] Here, the \\(f_n\\) are functions of the precictor \\(x_n\\), and \\(g()\\) can again be some overall link function.\nFor instance one could have a GAM where some predictor is sinusoidally related to the outcome, e.g., \\(f_1(x_1) = sin(x_1)\\). GAMs allow for more flexible relations between predictors and outcome than GLMs, and are still fairly easy to fit and interpret (but not quite as easy). Also, to provide good functions to map predictors to outcome, one needs to know something about the underlying system. If that‚Äôs not the case, one often uses spline function, and let the data determine the exact shape of the splines. One restriction of GAM is the same as all additive models: having each predictor enter the model separately can lead to interactions being missed.\n\n\nFitting polynomial/spline models\nThe models just discussed are more flexible and allow capturing more intricate patterns in the data. Of course, the downside of this is that they might not only capture patterns that are due to real signals in the data, but might also try to capture spurious patterns that are entirely due to noise/randomness in the data. In other words, these more flexible models are easily prone to overfitting.\nBy now, you have learned what you can do to try and minimize overfitting. Most importantly, you can build your models and decide how complex to make them by cross-validation. You can think of model parameters that determine if you should have a 2nd/3rd/4th/etc. order spline functions as a tuning parameter, and you let the cross-validation procedure decide if a 2nd order or 3rd order model is better ‚Äì based on model performance during the cross-validation process. As you will see, using the tidymodels framework allows you to easily do this model training/tuning.\n\n\nModel fitting in R\nA lot of separate packages exist to fit the models discussed here. Not all, but several of the most important ones, can be accessed through the tidymodels framework. We will explore some of them in the exercises so you get a general idea of how this works. You can then explore more models as interested or as needed for your specific research questions.\n\n\nFurther information\nThis short summary closely followed the information in the Moving Beyond Linearity chapter of ISL. Check out that chapter for further information on the models covered here, as well as a few additional variations. HMLR also discusses several models of this type. See for instance the Multivariate Adaptive Regression Splines chapter of HMLR. The Smoothing chapter of IDS also covers the topic discussed here. For specific ways of using some of these models through tidymodels, check out some of the tutorials in the tidymodels ‚Äúlearn‚Äù section.\nThe references above for each class of models provide further reading. Those 3 sources, namely ISL, IDS and HMLR are very good starting points for learning more about different machine learning methods. The Resources section of the course provides some further pointers to additional material, and of course there is a lot of other, often free, information available online. You should be able to find more details on any of these methods with just a few online searches."
  },
  {
    "objectID": "content/module-model-ml-1/many-tree-models.html",
    "href": "content/module-model-ml-1/many-tree-models.html",
    "title": "Many-Tree Based Models",
    "section": "",
    "text": "Overview\nIn this unit, we will cover an extension of simple decision tree models, namely models that combine multiple trees for improved performance (but reduced interpretability).\n\n\nLearning Objectives\n\nBe familiar with several tree-based methods\nUnderstand the advantages and disadvantages of single trees versus tree ensembles\n\n\n\nMany trees\nAs discussed in the last unit, trees have many good properties, such as being interpretable, able to work for both regression and classification, deal with missing values, and auto-select predictors.\nThe main disadvantage of single-tree models is that their performance is often not that great‚Äìand complicated trees which perform better are vulnerable to overfitting. However, building models which are combinations of many single trees is possible. Usually these so-called ensemble models (the general name for an ML model that gets an aggregate result by combining many other models) sacrifice some interpretability for performance. The idea behind more sophisticated tree-based methods is to take multiple individual trees and add/average them together in a smart way to end up with an ensemble of trees that often performs much better than a single tree. Some of the most common approaches are Bagging (Bootstrap aggregation), Random Forests, and Boosting. We‚Äôll cover each one very briefly.\n\n\nBagging\nThe main goal of bagging is to reduce variance, which is the main factor that generally negatively affects the performance of a single tree (even if one uses pruning or cross-validation). Remember that if we have N independent observations, each with the same variance/standard deviation, SD, the total variance is SD/N. If we had M datasets for the same process, each with N observations, and we built a model for each dataset, we could average over the models and thereby reduce the variance by 1/M. We don‚Äôt have M different datasets, but we can re-sample our existing data (using bootstrapping), and build a model for each sample. We can then, in the end, average over the different models to reduce variance. Here, each model is a tree, but we could also apply this approach to other models. The final model is the average of all the trees/individual models. Since the bagging procedure reduces the variance, individual trees are not pruned. Bagging leads to models with less variance and thus generally better model performance/predictive power. What is lost now is the ease of interpretation, since our final model is now the sum of a (possibly large) number of individual trees.\n\n\nRandom forest\nThe random forest model/algorithm is similar to bagging. The difference is that as we split each tree, instead of considering all possible predictors, we pick a random subset of predictors and only split on the best among those. Since we are artificially limiting ourselves, we obtain many trees that don‚Äôt perform too well on their own. However, this random choosing of subsets of predictors leads to more ‚Äúdiversity‚Äù in the tree structure (i.e., it avoids the greedy nature of the standard tree building algorithm). This helps in de-correlating the trees when we sum across them in the final model. Often this further improves model performance. The cost is the same as for bagging, namely the final model is a sum of trees which is hard to understand and interpret.\n\n\nBoosting\nBoosting takes a somewhat different approach than the other two. In this case, instead of averaging over full trees, we build many small trees. The procedure starts by building a tree with a specified, often small number of splits (this number is a tuning parameter). This small tree is added to the model, the change in performance is computed (e.g., reduction in RMSE or misclassification error) and a new tree is built that tries to reduce the leftover (residual) errors. In this form, many small trees are added, each one trying to take care of the under-performance produced by previous trees. In the end, one ends up with a sum of many small trees as the final model. This tree ensemble is expected to perform much better than each of the individual trees. Again, the final model is somewhat hard to interpret.\n\n\nAdvantages and disadvantages of many-tree models\nAll the advantages mentioned for a single tree model apply to many-tree models. The main additional advantage is that these models often have (much) improved performance compared to a single tree. It is often possible to use an algorithm that implements one of these many-tree models and with little adjustment and tuning, obtain a model with excellent performance. It is, however, still important to tune/train the model.\nI already mentioned the main disadvantage: These models are hard to interpret. If the goal is to have a user-friendly model that could be used by humans, trees are best. If the user is ok with typing values into a computer (or smartphone) and letting some algorithms run in the background and produce answers, then more complex models might be ok.\nAnother possible disadvantage of many-tree models is that they generally take longer to train, so if speed is important, one might not be able to use all of these model variants to their full extent.\n\n\nAdditional comments\nAs mentioned, bagging and boosting are approaches that can be applied to methods other than trees. For instance, one can bag/boost linear models, GAMs, etc.\nThe many-tree methods described above are examples of what is often called ensemble methods. Boosting is an example of using an ensemble of weak learners, i.e., a combination of models that individually don‚Äôt perform that well, but when combined, they often have outstanding performance. The combination of different individual models/learners (weak or not) is often called model averaging or model stacking, and such methods lead to some of the best-performing models in machine learning. For more on this, see e.g., the Stacked Models chapter of HMLR. We will not further cover these models, but if you have a situation where you need the best model performance available, such models might be worth looking at.\n\n\nImplementing tree models in R\nThe tidymodels framework allows access to many different types of packages/algorithms that implement tree-based models. You can see the full list on the parsnip model search page.\nMany of these methods and packages are similar and differ in the details of the model implementation, the speed at which they run, if one can parallelize them, etc. Some common ones that people use are rpart for individual trees, ranger for random forests, and xgboost for boosting. Many others are available too, e.g., the bonsai package has several other methods. Often it doesn‚Äôt matter much which package you use. Sometimes you might need specific features that only a certain package can give you, so you need to go with that one.\nSince tidymodels is still under development, there are many other packages for tree-based and related model fitting that can‚Äôt yet be accessed through it. If you absolutely need one that‚Äôs not yet supported, you can either try to implement it through tidymodels or just write code that uses the model directly.\n\n\nFurther information\nFor more details on the many-tree methods described here, see chapter 8 of ISLR and chapters 10-12 of HMLR."
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html",
    "title": "Assessment - Machine Learning Models 1",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#setup",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#setup",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Setup",
    "text": "Setup\nThis is a solo-exercise, and it‚Äôs part of your portfolio.\nThe goal is to have both a complete and well-organized data analysis example at the end of the exercise. So as you go through this exercise, in addition to doing the indicated tasks and writing the code, keep organizing things as needed. You want to make sure you have readme files or other documentation that briefly explain what files are where and in which order to run them. Make sure your R/Quarto files are easy to understand and that they all run.\nAs you re-organize, you can decide the mix of R and Quarto files and what you want to combine and what to split. However you do it, make sure it‚Äôs documented and easy to understand."
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#pre-processing",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#pre-processing",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Pre-processing",
    "text": "Pre-processing\nWe previously covered pre-processing, but haven‚Äôt specifically looked at that yet in an exercise. So let‚Äôs add that part. This can be added in various places to the code/files you already created previously. You can for instance make it part of the cleaning/wrangling code, or include it in the recipe part of your tidymodels workflow (or a mix).\n\nFeature/Variable removal\nYou probably realized that some of the variables contain the same information in slightly different ways. Specifically, Weakness, Cough and Myalgia exist on both a severity score and as Yes/No.¬†Further, there are 2 variables for cough yes/no. These variables are strongly correlated and thus at a minimum, don‚Äôt help when you model, and are actually more likely to confuse your model and lead to errors/warnings/poor model performance (e.g.¬†‚Äúpredictions from a rank-deficient fit may be misleading). So let‚Äôs fix that. For those symptoms where you have both multiple levels and yes/no, remove all the yes/no versions. That should remove 4 variables.\n\n\nCategorical/Ordinal predictors\nSome of your predictors are categorical (e.g., Yes/No) and the 3 symptom severity factors are ordinal, with None &lt; Mild &lt; Moderate &lt; Severe.\nWe can code the categorical variables as unordered factors and the others as ordered factors. I want you to do that as practice. The functions step_dummy() and step_ordinalscore() will help. See e.g.¬†the help file example for step_ordinalscore(). Not that to deal with ordered factors in a statistical analysis, one needs special approaches (and we won‚Äôt actually do those for this exercise.)\n\n\nLow (‚Äúnear-zero‚Äù) variance predictors\nIf you look at your data, you‚Äôll see that some predictors are fairly unbalanced, with most patients reporting No and only a few Yes. If almost everyone is in one category and almost nobody in others, that often (but not always) means those predictor variables are not very helpful in fitting/predicting the outcome. Furthermore, if you do cross-validation and one of your samples happen to not include one predictor level at all, things might go wrong when applied to the holdout-sample. (There are methods to deal with this specific problem, but they are more complicated than what we will do here.) Thus, it is worth considering if we want to remove them. The recipes package in tidymodels has the function step_nzv() which can do that for you automatically. If you have lots of predictors, that might be good to use. But it‚Äôs often better to decide manually for each variable based on your scientific expertise if you want to remove it or not. We‚Äôll take that approach here. After looking at the data, we decide to remove those binary predictors that have &lt;50 entries in one category (there are 2). Write code to remove them.\nYou should end up with a data frame that has 730 observations and 26 variables. This is the dataset we‚Äôll use for modeling."
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#analysis-code",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#analysis-code",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Analysis code",
    "text": "Analysis code\nStart a new Quarto file for this analysis, call it machinelearing.qmd, place it into the code folder. Note that for this exercise, some of your code might take long to run. This is one of the situations where it is often good to have a setup where some of the heavy computations are done by separate R scripts/functions, and the results saved, pulled into and displayed in the Quarto file. How you want to do it here is up to you, as long as the Quarto file shows all the main results and your code/file(s) are well documented.\nTo keep things a bit simpler, for this exercise we focus on a single outcome, the continuous, numerical value of Body temperature. Thus, we are fitting regression models here. Once you finished the whole workflow for that outcome, you are welcome to (optionally) do it again and run a classification for the categorical outcome of Nausea. Since we are doing a regression, and we don‚Äôt have any specific expert knowledge that tells us how we should build our performance metric, we‚Äôll go with one of the usual ones, namely RMSE. But remember to always think about your performance metric and don‚Äôt just use the default without at least considering other options.\n\nData Setup\n\nStart by setting the random seed to 123. This should make everything reproducible and everyone should get the same results.\nSplit the dataset into 70% training, 30% testing. Also use the outcome BodyTemp as stratification. This allows for more balanced outcome values in the train and test sets. See e.g., section 3 of the Get Started tutorial.\nWe want to do 5-fold cross-validation, 5 times repeated. (There‚Äôs no specific reason to do this 5x5 pattern, other than to show you that there are different ways to pick the sample, and that I want you to not use the default.) For the CV folds, we also want to stratify on BodyTemp, as we did for the main train/test split. Use the vfold_cv() function to create a resample object for the training data with these specifications.\nCreate a recipe for the data and fitting. You won‚Äôt need to do much, just make sure you code the categorical variables as dummy variables, otherwise things might not work smoothly. For that, you want to use the step_dummy() function and pick all nominal predictor variables (which are actually all predictor variables here, since the only continuous variable is our outcome).\n\n\n\nNull model performance\nWrite some code to compute the performance of a null model, i.e.¬†a ‚Äúmodel‚Äù that doesn‚Äôt use any predictor information. For a continuous outcome and RMSE as our metric, a null model is one that always predicts the mean of the outcome. Compute the RMSE for both training and test data for such a ‚Äúmodel‚Äù. We‚Äôll use that later to compare it to the performance of our real models. Of course, we expect/hope our real models that use predictor information to be better. If they aren‚Äôt that means they are no good.\n\n\nModel tuning and fitting\nWe‚Äôll fit a tree, a LASSO model, and a random forest. I chose those because they are used in the tutorial on the tidymodels website. You can of course add further models. For the tree, see the Tune model parameters section of the Get Started tutorial. For LASSO and the random forest, check out the Case Study section of the Get Started tutorial. Note that you will need to adjust the code for our scenario since we have a continuous outcome.\nIf you follow the tutorial, you‚Äôll likely use the packages rpart, glmnet and ranger to fit those 3 models. Make sure they are installed and loaded.\nI suggest you write code for each model separately. A lot of the code will look similar, so once you got the first one set up, the other two should be easier. They mainly differ in the commands specifying the tuning parameters and the tuning grid.\nEach of these models requires some tuning. For the choices regarding the tuning parameters, you can follow the examples. Most of the models have more things that can be tuned, but for now you can stick to what they show in the tutorial. Follow the examples by setting up a workflow, set a tuning grid, and then use the tune_grid() function to tune the model using cross-validation.\nNote that the tuning part, i.e., calling tune_grid() might take a good bit of time to run (possibly minutes).\nThe steps (block of code) you should have here are 1) model specification, 2) workflow definition, 3) tuning grid specification and 4) tuning using cross-validation and the tune_grid() function.\n\n\nModel evaluation\nOnce you have done the tuning, you can take a look at some diagnostics by sending your object returned from the tune_grid() function to autoplot(). For instance if you tuned the tree and saved the result as tree_tune_res, you can run tree_tune_res %&gt;% autoplot(). Depending on the model, the plot will be different, but in general it shows you what happened during the tuning process.\nNext, you want to get the model that the tuning process has determined is the best. You can get the best-fit model with select_best() and finalize_workflow() and then do one more fit to the training data with this final workflow using the fit() function. Follow the examples in the tutorial.\nTo evaluate the final fit for each model, do the following.\n\nMake two plots, one that shows model predictions from the tuned model versus actual outcomes, and one that plots residuals. The actual outcomes you get straight from the data, the predicted outcomes you can get by applying the predict() function to the final fit.\nLook at/print the model performance and compare it with the null model (still only on training data). Here, we want the performance of the tuned, best-fitting model on the CV dataset (we are not yet touching the test data). You can get that for instance with the show_best() function, which gives you the mean cross-validated performance for the best models. It also shows the standard deviation for the performance. Compare that model performance with the null model.\n\nThe mean and standard deviation of the performance give you a measure of overall performance and variability in that measure. The plots show you if there are any systematic deviations between model and data. Taken together, these can be compared for the different models and based on those (and as wanted, other considerations) a final model can be chosen.\nImplement the model tuning/fitting and evaluating steps for all 3 models.\n\n\nComments for specific models\nHere are some more suggestions and hints. Most of this is optional but worth trying.\nFor the tree model, if you want to plot the tree, you can use the rpart.plot package and run this command rpart.plot(extract_fit_parsnip(best_tree_fit)$fit) (assuming your result from the final workflow fit is called best_tree_fit). You might get a warning message, but the tree will show. You will likely find when you look at the actual/predicted plot or the residual plot that the tree model does not perform very well, and the model only predicts a few discrete outcome values. That‚Äôs also noticeable when you compare RMSE for the tree model and the null model, they are very similar.\nFor the lasso model, you will likely find that it performs a bit better than the tree, but not a lot. If you want to see a plot for how the number of predictors included in the LASSO model changes with the tuning parameter, you can extract the model from your final fit (say it‚Äôs called best_lasso_fit) with x &lt;- best_lasso_fit$fit$fit$fit and then plot(x, \"lambda\"). I know, this is awful code having to dig that deep into the best_lasso_fit object. You might also be able to use the extract_fit_engine() function to get the underlying fit object produced by LASSO, e.g., x &lt;- extract_fit_engine(best_lasso_fit) might work (this is new in tidymodels and I haven‚Äôt fully tried it yet).\nFor the random forest model, you will likely again find that it performs a bit better than the tree, but not a lot. The tuning setup might require some fiddling around, I had a few initial tries where the whole tuning failed. For a model like random forest, all variables stay in the model. There are ways to look at the variables that are most important. If you want to do that, you again need to pull out the fit object. Say it‚Äôs called best_rf_fit, you can do that with x &lt;- best_rf_fit$fit$fit$fit and then use the vip() function from the vip package to plot the importance of the variables. Alternatively extract_fit_engine() should also work.\n\n\nModel selection\nOnce you have implemented above steps for the 3 models, you should have a ‚Äúbest‚Äù fit for each one based on the tuning process. For each best model you should have performance, uncertainty around the performance measure, and some diagnostic plots. While for any real research project, you likely want to look deeper (e.g.¬†at uncertainty in predictions instead of just overall performance), for now this is enough. Pick one of the three models. Explain why you pick it. There is no single answer that‚Äôs correct, I just want you to practice reasoning for why you are doing something: in this case justify why you are picking the model you do.\n\n\nFinal evaluation\nOnce you picked your final model, you are allowed to once ‚Äì and only once ‚Äì fit it to the test data and check how well it performs on that data. This gives you a somewhat honest estimate of how the model might perform for new, unseen data. You can do that using the last_fit() function applied to the model you end up choosing. For the final model applied to the test set, report performance and the diagnostic plots as above.\nAnd that concludes what is likely a fairly long exercise. The code itself is not that long, but it will take you time to cobble it together from the tidymodel tutorial and possibly other sources."
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#test-and-website-update",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#test-and-website-update",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Test and website update",
    "text": "Test and website update\nMake sure your analysis and all results are nicely documented and everything runs/renders correctly. Then, add the newly created Quarto document as an entry into your _quarto.yml file, as a sub-menu of Flu fitting. Call it Machine Learning. Recompile your portfolio website and make sure everything works and shows up as expected. Then commit and push.\nSince this is part of your portfolio site, you don‚Äôt need to post anything, I know where to find it. Therefore there is no exercise Slack channel for this module."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-subset-selection.html",
    "href": "content/module-model-improvement/model-improvement-subset-selection.html",
    "title": "Subset Selection",
    "section": "",
    "text": "In this unit, we will cover the idea of subset selection."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-subset-selection.html#forward-selection",
    "href": "content/module-model-improvement/model-improvement-subset-selection.html#forward-selection",
    "title": "Subset Selection",
    "section": "Forward selection",
    "text": "Forward selection\nIn forward selection, you start with the null-model (no predictors), then evaluate all single-predictor models. The best-performing 1-predictor model is chosen, and you build all 2-predictor models on top of that chosen 1-predictor model. You choose the best 2-predictor model, then go on to add a third, and do that until adding any further predictors does not lead to a model that performs better (again, measured by CV on the test set) than the smaller model. E.g., you might find that adding any of the remaining 3rd predictors does not lead to a better-performing model than the 2-predictor model you already have. Then you stop here and pick the 2-predictor model as your best.\nLet‚Äôs consider an example. Say you want to predict BMI based on age, sex, calorie intake, and exercise level. Since BMI is continuous, we can consider a linear regression model, and we might try to minimize RMSE between model-predicted BMI and actual measurements. We start by computing RMSE for the null model. Then we compute RMSE for all 1-predictor models. Assume that all 1-predictor models have RMSE lower than the null model, and the lowest RMSE (all computed using cross-validation) of the 1-predictor models is the one that includes calorie intake. Next, you try all 2-predictor models that include calorie intake. Among those, a model with calorie intake + exercise level has the lowest RMSE, and it‚Äôs also lower than the model with calorie intake only. Next, you try all 3-predictor models that include the 2 chosen predictors. In this example, only 2 of those 3-predictor models are possible, namely calorie intake + exercise level + age and calorie intake + exercise level + sex. Let‚Äôs assume both of those models have RMSE that is larger than the 2 predictor model. Note, it is important to compute RMSE through cross-validation since the RMSE evaluated on the data used for fitting will always be lower for the bigger model. We thus found our best model, namely one that includes the 2 predictors calorie intake + exercise level."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-subset-selection.html#backward-selection",
    "href": "content/module-model-improvement/model-improvement-subset-selection.html#backward-selection",
    "title": "Subset Selection",
    "section": "Backward selection",
    "text": "Backward selection\nBackward selection is essentially the reverse to forward selection: You start with the model with all predictors, then evaluate all models with one predictor dropped. The smaller model with the best performance is your choice. You then drop each predictor at a time from that model. You continue until dropping predictors does not improve performance anymore. Then you keep the model with the best performance. Note that this method does not work on datasets that have more predictors than observations (e.g., most -omics type data).\nLet‚Äôs consider the above example again. We now start with a model that includes all 4 predictors and compute cross-validated RMSE. Next, we drop each predictor and try all 3-predictor models. Let‚Äôs say the one that dropped exercise level is the 3-predictor model with the lowest RMSE, lower than all other 3-predictor models, and lower than the full model. Next, we drop each variable from the current best candidate 3-predictor model, which is age + sex + calorie intake. All of the 2-predictor models have RMSE larger than the 3-predictor model. We thus found our final model, with the 3 predictors age + sex + calorie intake.\nNote that it is also possible to do ‚Äúforward-backward‚Äù selection: you can choose to start with the full or reduced model, but the method is allowed to add/remove predictors at every step depending on what improves performance most."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-subset-selection.html#exhaustive-subset-selection",
    "href": "content/module-model-improvement/model-improvement-subset-selection.html#exhaustive-subset-selection",
    "title": "Subset Selection",
    "section": "Exhaustive subset selection",
    "text": "Exhaustive subset selection\nYou might have noticed that in my example, forward selection and backward selection do not necessarily produce the same model. The reason for that is that neither forward- or backward-selection try all possible models (predictor combinations). Both approaches use what is called a greedy strategy. Once they included or excluded a predictor, it can never be dropped or included. Once you picked your 1-predictor model, all subsequent models will consist of that 1 predictor model - even if there might be a better performing 2-predictor model that doesn‚Äôt include the 1st predictor you picked. The same holds for dropping predictors in backward selection.\nAlso, as in my example, often (but not always), the final model determined through backward selection tends to include more predictors than a model determined through forward selection. In such a case, you have to decide which model to choose. Alternatively, you can try a different way of choosing the model. One approach is to try models with all combinations of predictors. This is called exhaustive subset selection, all subsets selection, or best subset selection (although note that ‚Äúbest subset selection‚Äù can also refer to a different method which still hasn‚Äôt gained much traction). If you have only a few predictors, you can try every combination of predictors. However, once you have more than 5-10 predictors (depending on the number of observations), trying every combination of sub-models would be too time-consuming. Therefore, most of the time, such an exhaustive feature selection is not feasible."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-subset-selection.html#other-subset-selection-methods",
    "href": "content/module-model-improvement/model-improvement-subset-selection.html#other-subset-selection-methods",
    "title": "Subset Selection",
    "section": "Other subset selection methods",
    "text": "Other subset selection methods\nThere have recently been a number of approaches that try to be somewhat smarter in finding the best model, without having to try every combination. The caret package has several methods, such as Genetic Algorithms and Simulated Annealing. Those methods often do a good job finding a very good sub-model in a reasonable amount of time. While the only guarantee that you found the single best model is to try them all using exhaustive subset selection, these modern approaches usually find a model that is either the best or at least close to the best while doing so rather efficiently. See sections 18-22 of the caret manual for more information if you are interested. The mlr/mlr3 packages also have similar such selection methods. tidymodels currently does not have any subset selection procedure implemented (see more below)."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-subset-selection.html#manual-subset-selection",
    "href": "content/module-model-improvement/model-improvement-subset-selection.html#manual-subset-selection",
    "title": "Subset Selection",
    "section": "Manual subset selection",
    "text": "Manual subset selection\nAll the approaches above use cross-validated performance of some metric to determine which predictors should be included in the model. Another, still commonly used strategy is to do subset selection manually. The analyst usually starts with a simple model containing just one predictor (the most important one), and then decides based on model results (e.g., systematic deviations in the residuals) and/or based on scientific knowledge, if adding additional predictors is justified. For a skilled analyst with a lot of subject matter expertise, this can lead to models that are all-around very good. They have good performance, make scientific sense, and are still simple. Unfortunately, this requires a lot of knowledge, time and careful analysis. Also, the analyst needs to honestly report any model they tried, not just the one they end up with. Otherwise they are using and not reporting researcher‚Äôs degrees of freedom, which leads to overfitting. Most analyses are not done that way.\n\nI mentioned this before in various places and it‚Äôs a theme throughout the course, that unless data is collected and analyzed using pre-specified protocols (e.g., a clinical trial for FDA approval) it is almost always the case that scientists have so much flexibility during the data collection and analysis stages that p-values and similar statistical measures are meaningless. The Researcher‚Äôs degrees of freedom issue is everywhere and essentially impossible to get rid of. For more on this topic, see the Pitfalls section on the General Resources page.\nAll of this applies to manual subset selection (which is why I‚Äôm thinking about it here and adding this note), but it is more general and applies to every step in a scientific project."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-overview.html",
    "href": "content/module-model-improvement/model-improvement-overview.html",
    "title": "Model Fit Improvement Overview",
    "section": "",
    "text": "Overview\nIn this module, we will discuss ways one can try to improve model performance.\n\n\n\nSource: xkcd.com\n\n\n\n\nLearning Objectives\n\nLearn how to process your data to improve model performance.\nBecome familiar with subset selection and regularization approaches.\nLearn about model tuning.\n\n\n\nIntroduction\nWhile, as we discussed, having a model that performs great is not the only important goal, it is often one of the most important ones. We generally want a model with good performance. To get such a model, there are several things we can do to either the data or the model to help improve performance.\nTrying different models is, of course, always an option, and possibly a good one. But even if we don‚Äôt use lots of different models, there are often things we can do to improve the performance of the model (or type of model) we are using.\nWe are covering several important approaches to model performance improvement here. But as we go through these topics (and the rest of the course) always keep these things in mind:\n\nGood performance alone doesn‚Äôt mean a model is good!\nGood performance on a scientifically wrong metric is useless!\nGood performance assessed with the data used to build the model is not too meaningful!"
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html",
    "href": "content/module-model-improvement/assessment-model-improvement.html",
    "title": "Assessment - Improving Models",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#setup",
    "href": "content/module-model-improvement/assessment-model-improvement.html#setup",
    "title": "Assessment - Improving Models",
    "section": "Setup",
    "text": "Setup\nYou‚Äôll continue working on the Flu Analysis exercise you started previously. We‚Äôll also do some group work again, using the ‚Äì by now familiar ‚Äì M1, M2, ‚Ä¶ setup. Assign each other a number. As much as possible, do it such that you end up working with group members you have not (or not in a while) worked with. If you are in a group of 4, you can do pairs. That of course doesn‚Äôt work in a group of 3 or 5, so you need to use the ‚Äòcircular‚Äô setup there. The goal is that, as in past exercises, everyone will work on their own repository and on one other person‚Äôs repository.\nAt this stage, I assume all the data wrangling and EDA code, as well as the model fitting code you worked on previously is present and fully functional. If there are still some issues that need to be resolved, go ahead and do so.\nPlace a new Quarto file called modeleval.qmd into the code folder (or sub-folder, if you use them). As always, you can of course either use a single Quarto file with code or put the code into an R script and pull it into the Quarto file. In either case, document/comment well."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#data-splitting",
    "href": "content/module-model-improvement/assessment-model-improvement.html#data-splitting",
    "title": "Assessment - Improving Models",
    "section": "Data splitting",
    "text": "Data splitting\nWrite code that takes the data and splits it randomly into a train and test that, following for instance the example in the Data Splitting section of the Get Started tidymodels tutorial."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#workflow-creation-and-model-fitting",
    "href": "content/module-model-improvement/assessment-model-improvement.html#workflow-creation-and-model-fitting",
    "title": "Assessment - Improving Models",
    "section": "Workflow creation and model fitting",
    "text": "Workflow creation and model fitting\nNext, following the example in the Create Recipes section of the Get Started tidymodels tutorial, create a simple recipe that fits a logistic model to our categorical outcome of interest, using all predictors. (We start with categorical and all predictors since that‚Äôs the closest to the shown example). For now, you can ignore the concept of roles and features they mention.\nSet a model as you did in the previous exercise, then use the workflow() function to create a simple workflow that fits a logistic model to all predictors using the glm engine. To that end, follow the Fit a model with a recipe section of the tutorial and adjust for your case.\nYou should end up with a fit object similar to the one shown at the end of that section in the tutorial - of course, yours will look somewhat different since you are using a different dataset, but overall things should look similar."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#model-1-evaluation",
    "href": "content/module-model-improvement/assessment-model-improvement.html#model-1-evaluation",
    "title": "Assessment - Improving Models",
    "section": "Model 1 evaluation",
    "text": "Model 1 evaluation\nFollow the example in the Use a trained workflow to predict section of the tutorial to look at the predictions, ROC curve and ROC-AUC for your data. Apply it to both the training and the test data. ROC curve analysis and ROC-AUC is another common performance measure/metric for categorical outcomes. If you are not familiar with it, you can read more about them by following the link in the tutorial. It‚Äôs not too important to go into the details for now. The focus here is on getting the code to work. In general, a ROC-AUC of 0.5 means the model is no good, 1 is a perfect model. Generally, somewhere above 0.7 do people think the model might be useful. If your model has a ROC-AUC a lot less than 0.5, you likely have an issue with how your factors are coded or how tidymodels is interpreting them."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#alternative-model",
    "href": "content/module-model-improvement/assessment-model-improvement.html#alternative-model",
    "title": "Assessment - Improving Models",
    "section": "Alternative model",
    "text": "Alternative model\nLet‚Äôs re-do the fitting but now with a model that only fits the main predictor to the categorical outcome. You should notice that the only thing you have to change is to set up a new recipe, this time one that only has the name of the predictor of interest on the right side of the formula (instead of the . symbol, which is shorthand notation for ‚Äúall predictors‚Äù.) Then you can set up a new workflow with the new recipe, rerun the fit and evaluate performance using the same code as above. In general, if you do multiple models/recipes, you might want to write a loop to go over them, or parallelize/vectorize things. For now, just copying and pasting most of the code is ok."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#wrap-up-part-1",
    "href": "content/module-model-improvement/assessment-model-improvement.html#wrap-up-part-1",
    "title": "Assessment - Improving Models",
    "section": "Wrap up part 1",
    "text": "Wrap up part 1\nMake sure everything runs and works as expected. Also make sure everything is well commented/documented/explained! Then commit, push and tell your classmate that they can take over. Finish this by Wednesday."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#continuous-outcome",
    "href": "content/module-model-improvement/assessment-model-improvement.html#continuous-outcome",
    "title": "Assessment - Improving Models",
    "section": "Continuous outcome",
    "text": "Continuous outcome\nFork and clone (or if you are added as collaborator, clone directly) your classmate‚Äôs repository. Open their modeleval.qmd file. Add a heading at the bottom that says # This section added by YOURFULLNAME. I need this so I can grade accordingly.\nAdd code that repeats the above steps, but now fits linear models to the continuous outcome. One model with all predictors, one model with just the main predictor. For that, you need to change your metric. RMSE is a good one to choose. You should find that a lot of the code your classmate wrote can copied/pasted/re-used with only minimal modifications. Make sure to add comments/documentation."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#wrap-up-part-2",
    "href": "content/module-model-improvement/assessment-model-improvement.html#wrap-up-part-2",
    "title": "Assessment - Improving Models",
    "section": "Wrap up part 2",
    "text": "Wrap up part 2\nMake sure everything runs and works as expected. Then commit, push and if you forked the repo, initiate a pull request. Tell our classmate that its done."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#test-and-website-update",
    "href": "content/module-model-improvement/assessment-model-improvement.html#test-and-website-update",
    "title": "Assessment - Improving Models",
    "section": "Test and website update",
    "text": "Test and website update\nThe original repository owner should make sure everything works. Then, add the newly created Quarto document as an entry into your _quarto.yml file, as a sub-menu of Flu fitting. Call it Model Evaluation. Recompile your portfolio website and make sure everything works and shows up as expected. Then commit and push.\nSince this is part of your portfolio site, you don‚Äôt need to post anything, I know where to find it. Therefore there is no exercise Slack channel for this module."
  },
  {
    "objectID": "content/module-model-improvement/assessment-model-improvement.html#looking-ahead",
    "href": "content/module-model-improvement/assessment-model-improvement.html#looking-ahead",
    "title": "Assessment - Improving Models",
    "section": "Looking ahead",
    "text": "Looking ahead\nWe also covered overfitting and strategies to minimize it (e.g., cross-validation), and further model assessment strategies in the materials. We‚Äôll practice those in future exercises. I figured the above will already take a good bit of getting used to, and we will be able to practice more when we look at further model types in the coming weeks."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-performance.html",
    "href": "content/module-model-evaluation/model-evaluation-performance.html",
    "title": "Model Performance Evaluation",
    "section": "",
    "text": "Up to now, I have not shown much math or equations. Below, you will encounter some. The equations are fairly simple and I‚Äôm sure you‚Äôll be able to follow once you spend a few minutes to think through them. Most often, there are functions in R that compute all those quantities for you. But sometimes, you will have to compute a few of them yourself. It is therefore good to know their definitions. Once you do, you realize that you can simply compute them yourself with just a few lines of R code. Again, most often you won‚Äôt need to do that, but sometimes it is useful to have that ability and know how to do it."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-performance.html#least-squares",
    "href": "content/module-model-evaluation/model-evaluation-performance.html#least-squares",
    "title": "Model Performance Evaluation",
    "section": "Least squares",
    "text": "Least squares\nYou are likely familiar with one of the most widely used functions for f, the least-squares method. If you have ever fit a linear model (e.g.¬†using lm() in R or an equivalent function in a different statistical software), chances are you used least squares as your objective function (maybe without knowing that you did). For least squares, we compute the squared difference between model prediction and data for each observation and sum it all up. In equation form:\n\\[C = f(Y_m,Y_d) = \\sum_i (Y_m^i - Y_d^i)^2\\] Again, \\(Y^i_m\\) are all the predictions made by the model for the outcome, and the \\(Y^i_d\\) are the actual outcomes from the data. The quantity \\(C\\) for this equation has many names. A common one is least squares error, or sum of square residuals (SSR), or residual sum of squares (RSS), or sum of squares (SS), or residual square error (RSE), and a bunch of similar names. You will usually be able to tell from the context what is being used as the performance metric.\nYou will often see a variation where one divides by the sample size, i.e.¬†\\(C\\) will look like\n\\[C = \\frac{1}{N} \\sum_i (Y_m^i - Y_d^i)^2\\]\nThis is called mean squared error. Other names of exist of course.\nDividing by the sample size has the advantage of allowing you to compare values across samples of different size from the same dataset (but it doesn‚Äôt really work for comparing across different datasets). For instance if you compare model performance on training and test data (to be discussed shortly), and each has different sample size, you need to make sure you standardize by it.\nIf you want to compare different models on the same dataset which might include some missing values, and one of your models can deal with missing data while the other cannot, you need to be careful. One option is to fit both models only to the data without missing values. If you decide to allow one model to use the observations that have some missing values, while the other model does not, you definitely need to standardize by the sample size. Even then, care is needed, since the samples with some missing data might be systematically different from those without and thus the datasets might not be equivalent anymore.\nAnother variant is a version where at the end you take the square-root, i.e.\n\\[C = \\sqrt{\\frac{1}{N} \\sum_i (Y_m^i - Y_d^i)^2}\\]\nwhich is called the root mean squared error (RMSE). The advantage of taking the square-root at the end is that now the units of \\(C\\) are the same as those of your outcomes. THis often makes interpretation of the results easier. In general, it is best to use MSE or RMSE. In tidymodels, the yardstick package has the rmse() metric built-in."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-performance.html#coefficient-of-determination",
    "href": "content/module-model-evaluation/model-evaluation-performance.html#coefficient-of-determination",
    "title": "Model Performance Evaluation",
    "section": "Coefficient of determination",
    "text": "Coefficient of determination\nAn equivalent alternative to SSR is to use a quantity called Coefficient of determination, or more commonly \\(R^2\\) (R-squared). This quantity is defined as \\[1-RSS/TSS\\] where RSS is the residual sum of square introduced above and TSS is the total sum of squares. The latter is defined as\n\\[TSS =  \\sum_i (Y_{av} - Y_d^i)^2\\]\nwhere \\(Y_{av}\\) is the mean of the data. Therefore, the equation for \\(R^2\\) is\n\\[R^2 = 1-  \\frac{\\sum_i (Y_m^i - Y_d^i)^2}{\\sum_i (Y_{av} - Y_d^i)^2}\\]\nSince TSS is fixed for a given dataset, minimizing SSR is equivalent to maximizing \\(R^2\\). You might see \\(R^2\\) reported in papers, and it is highly likely that the fitting was performed by minimizing SSR (or MSE or RMSE). The SST is a useful quantity since it tells us the performance of a dumb/null model which uses no information from any predictor variables, but instead just predicts the mean of the outcomes. Any model you build that includes predictors needs to do better than this dumb null model.\nLeast squares fitting is simple and frequently used. A lot of standard routines in major statistical packages use least squares. It often makes good sense to penalize predictions that deviate from the actual value with the squared distance (and under certain conditions, this is equivalent to maximizing the likelihood). However, sometimes a different way to define the function f might be useful. We‚Äôll discuss a few of them briefly."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-performance.html#beyond-least-squares",
    "href": "content/module-model-evaluation/model-evaluation-performance.html#beyond-least-squares",
    "title": "Model Performance Evaluation",
    "section": "Beyond least squares",
    "text": "Beyond least squares\nAn alternative to least squares is to penalize not with the distance squared, but linearly with the (absolute) distance. This metric is called (mean) absolute error or (least) absolute deviation, and the model is\n\\[C = f(Y_m,Y_d) = \\sum_i |Y_m^i - Y_d^i|\\]\nThis approach can be useful if the data contains outliers (that are real, and one can‚Äôt justify removing them during cleaning). With a squared distance penalty, outliers have a strong impact on the model fit. With a linear penalty, such outliers carry less weight. Because of this, the linear difference approach is sometimes called a robust estimation.\nRobust estimation methos such as MAE and many others often require the use of specialized R packages. A large number of R packages exist that allow fitting this way, see e.g., the CRAN Robust Task View. The yardstick package as the metric mae() which computes the mean absolute error.\nAnother way to define f is with step functions. The idea is that as long as model and data are within some distance, the penalty is zero. Once model and data differ by some threshold, a penalty is given, e.g., a fixed value or a linear or quadratic penalty. Such types of schemes to define f are common in the class of models called Support Vector Machines, which we will look at later in the course.\nSome other metrics are implemented in yardstick. You can check out other metric types in this yardstick article. For instance there is one called huber_loss() which is a combination of least squares and absolute error.\nNo matter what scheme you choose, it might at times be useful to weigh data points. In the examples given above, each model-data pair was given the same importance. Sometimes it might be that you have some data points that should carry more weight. A common case is if you have measurements not on the individual level but some aggregate level. As an example, assume you have a fraction of heart attacks among all patients for some time period in different hospitals, and you want to fit that fraction. You don‚Äôt know the number of people who had heart attacks, only the fraction. But you do know something about the total number of beds each hospital has. You could then argue that hospitals with more beds have more patients and thus likely more heart attacks, and therefore the data from larger hospitals should get more weight, and you could e.g., multiply each term in the sum by bed size. Note that this is a scientific decision based on your expert knowledge of the data. Almost all fitting routines allow you to provide weights for your data and you can then perform weighted least squares (or a weighted version of whatever other approaches you choose). tidymodels has some support for weighted fitting."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-performance.html#accuracy",
    "href": "content/module-model-evaluation/model-evaluation-performance.html#accuracy",
    "title": "Model Performance Evaluation",
    "section": "Accuracy",
    "text": "Accuracy\nThe simplest way to determine performance of a model for categorical data is to count the fraction of times the model did (not) correctly predict the outcome. If we instead count the fraction of correct predictions made by the model, it is called accuracy. If we focus on the number of times the model got it wrong, it is called the (mis)classification error. The yardstick package has it as metric accuracy().\nWhile accuracy is often not a bad choice of metric, sometimes just counting the number of times a model didn‚Äôt get it right might not be the best idea. A common situation where accuracy (just counting how often the model prediction is right/wrong) is not very useful is for rare (but important) outcomes. Data of this type is often called unbalanced data. As an example, say we had some algorithm that tried to use brain images to predict if people have brain cancer. Fortunately, brain cancer is rare. Let‚Äôs say (I‚Äôm making this number up) that 1 in a million people who undergo this screening procedure actually have this cancer. Therefore, a model that predicts that nobody has cancer would be a very accurate model, it would only make one mistake in a million. However, missing that one person would be a very important mistake. We likely would prefer a model that flags 10 people as (potentially) having a cancer, including the person who really does and 9 false positives. This model has worse accuracy since it gets 9 out 1 million wrong. But it‚Äôs likely more important to catch the one true case, even if it means temporarily scaring several other individuals, until further checks show that they are healthy. Of course this trade-off is very common. You likely know it in the context of balancing sensitivity and specificity of say a clinical test. Other areas where this happens is, e.g., predicting the (hopefully rare) errors when building a plane engine, flagging the (rare) credit card fraud, and many others of that type."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-performance.html#beyond-accuracy",
    "href": "content/module-model-evaluation/model-evaluation-performance.html#beyond-accuracy",
    "title": "Model Performance Evaluation",
    "section": "Beyond accuracy",
    "text": "Beyond accuracy\nIn instances where accuracy/misclassification is not a good performance metric, other metrics are more helpful. A large variety of such metrics exist. Some only apply to the special (but very common) case of a binary outcome, and others apply more generally. Fairly common ones are:\n\n(Cohen‚Äôs) Kappa, which is implemented in yardstick as kappa()\nCross entropy, which is\nArea under a Receiver operating curve (AUC ROC) (roc_auc() in yardstick)\nMatthews correlation coefficient (mcc() in yarstick)\nF-score (f_meas() in yardstick)\n\nRemember that yardstick is part of the tidymodels set of packages, so if you work in the tidymodels framework, you have access to these (and other) metrics."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-performance.html#the-confusion-matrix",
    "href": "content/module-model-evaluation/model-evaluation-performance.html#the-confusion-matrix",
    "title": "Model Performance Evaluation",
    "section": "The confusion matrix",
    "text": "The confusion matrix\nFor the common case of a binary outcome, one can construct what is called the confusion matrix (also known as 2x2 table in epidemiology). The confusion matrix tracks 4 quantities: true positives (TP, model predicts positive, and data is positive), true negative (TN, both model and data are negative), false positive (FP, model wrongly predicts positive) and false negative (FN, model wrongly predicts negative).\nThe confusion matrix comes with very confusing terminology since many of the quantities are labeled differently in different fields. For instance, epidemiologists tend to call the quantity TP/(TP+FN) sensitivity, while in other areas, such as machine learning, it is called recall. For a good overview of those different quantities and terminology, see this Wikipedia article.\nOne can of course also make a table for a case with more than 2 outcomes and track the true values versus the predictions. Some of the metrics generalize to such a scenario of more than 2 outcomes, but not all do."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-performance.html#footnotes",
    "href": "content/module-model-evaluation/model-evaluation-performance.html#footnotes",
    "title": "Model Performance Evaluation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, for lm() and some other simple models, it is possible to find the best parameter values other than by ‚Äòtrial and error‚Äô, but for many other models, this is more or less what is happening. Developing routines that can quickly, efficiently, and reliably find the optimum value of a function (here the minimum of C) is part of the large field of optimization. Fortunately, for our purposes, we can let the different functions in R do this task without us having to worry too much about how the optimal value is determined. However, for more advanced analyses, the built-in optimizers sometimes do not work. In that case, I recommend using the nloptr package - but we won‚Äôt need it in this course.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-overfitting.html",
    "href": "content/module-model-evaluation/model-evaluation-overfitting.html",
    "title": "Model Performance and Overfitting",
    "section": "",
    "text": "Introduction\nWe discussed how to evaluate the performance of a model. We went through the idea of defining a single numerical value (cost function) and optimizing (usually minimizing) it to find the best model. There is however a very big caveat to this. The main point is: It usually doesn‚Äôt matter how well your model performs on the data that you used to build your model!\nThis is a very important point, and one that unfortunately a majority of scientific papers still get completely wrong! It is one area where modern machine learning is much more careful compared to the traditional way statistics is taught/used. In the machine learning field, it is very much recognized that it doesn‚Äôt matter a whole lot how well your model performs on the data that you used to build your model! What matters is performance on similar new data like the data that was used to fit the model.\nI‚Äôm going to repeat this and similar sentences a bunch of times throughout the rest of the course üòÅ. If you only take away 2 main points from this course, this would be one if them. The other is that doing data analysis in a reproducible (automated) manner is critical.\nSo let‚Äôs go into some more details regarding this important point.\n\n\nShould we really minimize the cost function?\nWe discussed that once we have chosen a cost function for our problem, we are trying to find a model that minimizes this cost function (e.g., minimizes the RMSE or the misclassification error), and models with smaller cost functions are better. The problem with this concept is that in general, a bigger model will be more flexible and thus able to fit the data better. However, when we do data analysis, we generally are not (only) interested in having a model that works well for the specific data sample we used to fit your model. Our main question/hypothesis usually does not concern the actual data we have/fit. Instead, we generally want to say something about ‚Äòthe larger world‚Äô.\nIf we are asking inferential questions, we are interested in what the data analysis teaches us about this system in general. E.g., if we analyze data to see if there is a correlation between levels of atmospheric pollutants and cases of asthma among our study population, we are usually really interested in knowing if such a correlation is real in general.\nIf we are asking predictive questions, we are interested in a model that can predict future observations, not the ones we already have. E.g., if we analyze data for a specific treatment, we are not very interested how well the model predicts the effect of the drug on the people for which we collected the data (we already know that). Instead, we want to make general predictions about the effectiveness of the treatment on future patients.\nIn either case, what we want is a model that is generalizeable (also sometimes called externally valid), and that applies equally well to new and similar data beyond the data we already collected.\nWhat truly matters is how well our model can explain/predict other/future data, not just the data we are already observed!\nIf we build a very complex model in an effort to match our existing data as closely as possible, what generally happens is that our model overfits. That means it becomes very good at modeling the data we use to build the model, but it won‚Äôt generalize very well to the general, broader context of other/future data. The reason for that is that there is noise (random variability) in any dataset, and if we have a model that is too flexible, it will not only match the overall signal/pattern (if there is any) but will also capture all the noise in our sample, which leads to worse performance on future data that have different amounts and types of noise/variability.\n\n\nBias-variance trade-off\nThe general concept that models tend to fit the data that is used to build the model better as model complexity increases, but not perform so well on future/new data is also known as bias-variance trade-off.\nBias describes the fact that a model that is too simple might get the data ‚Äúsystematically wrong‚Äù. A more restricted model like a simple linear model usually has more bias. Another way of saying this is that the model underfits, i.e., there are still patterns in the data that the model does not capture. More complex models generally reduce the bias and the underfitting problem..\nVariance describes how much a model would vary if it were fit to another, similar dataset. If a model goes close to the training data, it will likely produce a different fit if we re-fit it to a new dataset. Such a model is overfitting the data. More complex models tend to be more likely to overfit.\nThe following figure illustrates this concept. In this example, the data was produced by taking the black curve and adding some noise on top. This gives the data shown as circles. Three models are fit. A linear model (yellow) is too restrictive and misses important patterns. The next model (blue line) is more flexible and is able to capture the main patterns. The most complex model (green line) gets fairly close to the data. But you can tell that it is trying to get too close to the data and thus overfits. If we had another data sample (took the black line and added some noise on top), the green model would not do so well. This is shown on the right side, where the grey line plots the MSE for each model for the given dataset. As the model gets more complex/flexible, they get closer to the data, and the MSE goes down. However, what matters is the model performance on an independent dataset. This is shown with the red curve. Here, you can see that the blue model has the lowest MSE.\n\n\n\nBias-variance tradeoff. Source: ISLR.\n\n\nThe same concept holds for categorical outcomes, and for models with multiple predictors. No matter what the model, there is always a sweet spot for model complexity somewhere ‚Äúin the middle‚Äù. This ‚Äúmiddle‚Äù depends on the data and the question. Often, linear models are as good as one can get, and more complex models will overfit. Even for linear models, we might have to remove predictors to prevent overfitting (we‚Äôll discuss that later). At other times, somewhat complicated models (e.g., neural nets) might perform best. In general, the more data (both quantity and richness), the less likely it is that a more complex model will lead to overfitting. However, we always need to check.\n\n\nOverfitting and machine learning\nIf you only fit simple models (e.g., a linear model), and maybe decide based on scientific knowledge which predictors need to be in the model, then your risk of overfitting ‚Äì while still present ‚Äì is not that large. However, in machine learning, you often have complex models with many components that can be adjusted/tuned (we‚Äôll get into that) to improve model performance. The danger is that if you have a very flexible model that can be finely tuned to perform well on the data, you have a very large risk of overfitting, namely of ending up with a model that is well tuned and performs very well on the data you use to build the model, but does not work so well on other data. Therefore, while overfitting is always something the be careful about, once you start using larger and more flexible models, you definitely need to guard against overfitting.\n\n\nDealing with overfitting\nSo now that you learned that the model that performs best (using whatever metric you chose) is not necessarily the best one, how can we evaluate model performance in a better way? There are different options.\nMore data: The ultimate test is, of course, to check your model on newly collected data. Or a completely independent dataset. However, collecting new data generally takes a lot of time and money, and often you might not have a completely independent dataset that measures the same variables as the one you are trying to analyze/fit. Therefore, for the remainder of the course, we assume that getting more data is not an option. (Though if you have that option, certainly go for it!)\nReserving some data: If you have enough data, you can perform an evaluation of model performance that tries to imitate the idea of evaluating the model on newly collected data. What you do is that right at the start of your analysis, you set some of your data aside and do not look at it until the very end! A common fraction of data that is set aside is 20%-40%, but there is no clear rule. This data you set aside is called your test/validation data. You need to completely ignore that data and are not allowed to use it while doing any statistical model fitting. You then take the rest of your data, referred to as training data, and do all your model building and fitting and model choosing, etc.\nAt the end and only once at the very end do you run your model on the test data and compute the performance. The way you do this is to give your model the input variables from the new data, ask the model to make predictions for the outcome, and use your metric/cost function to evaluate the performance of your model on that part of the data that your model has never seen before. This gives you an estimate of your model performance that is as honest as you can get without collecting new data. Be aware that even with this approach, you are likely still getting slightly better results than if you had evaluated your model on independently collected data (with somewhat different study design and data collection approaches). But in the absence of truly independent data, this is the best/most honest model performance evaluation you can do.\nIf you look at data analysis competitions such as Kaggle, this is how they operate. People need to submit their model predictions, and the model will be evaluated on a final dataset, only once, at the end of the competition. And whoever scores highest on that dataset wins, no matter how well they did on the leaderboard.\nWhile such a train/test split is an excellent thing to do, there are some constraints. First, you need to have enough data. If your dataset is small (&lt;100-1000 observations, depending on the number of predictors and model complexity), then this approach will make you give away data that you need to build your model, and won‚Äôt provide robust insights. Second, you still need a way to decide which model to chose as your final model, and as mentioned above, using the one with the smallest cost function as evaluated on the data that was used to build the model is not good. Thus, you still need to figure out how to evaluate which model is best during the model building process. To that end, an approach called cross-validation has become very widely used and is currently - in my opinion - the best method to evaluate and choose your model.\nTo do the train/test split in R, you can use the rsample package which is part of tidymodels.\nCross-validation during model building: Cross-validation (CV) is a reasonably straightforward approach. It mimics the train/test idea just described, but is now applied during the model fitting process. The idea is that you take the data you set aside for the model building process (i.e., your training data), and during the model building procedure, you repeatedly split your data into a portion to which you fit our model, and a portion which you use to evaluate model performance. You repeat this many times to get several samples and always use some portion of the data to fit the model, and the remaining part to evaluate it.\n\nYou might not be surprised to learn that this is another place where terminology is confusing and inconsistent. The words training data and test/validation data are used both for the initial split described above, and the split done in the CV procedure. A recent suggestion by Max Kuhn is to call the splits done by CV the analysis and assessment portions of the data. Tidymodels has terminology and a nice schematic. I will try to use that terminology, but I might not be consistent. And other materials you see/read certainly do not follow that terminology, as you will see already below.\n\nThere are different ways to do the data splitting during cross-validation. A common setup is to randomly divide the data set into k blocks (called folds). You then use the data in all but one of the blocks to fit the model, then compute the cost function/performance metric (e.g., the RMSE) using the remaining block. You repeat that for all k blocks. The average value of the model performance over the k blocks is the target that you want to optimize. One usually chooses k = 5 or 10 blocks/folds, which has shown in simulation studies to give the best variance-bias trade-off. The figure shows an example of a 4-fold cross-validation for a categorical outcome (color of the balls). Note that this figure uses the train/test terminology for labeling the blocks, not the analysis/assessment terminology.\n\n\n\nSource: Cross Validation Wikipedia Page\n\n\nIn addition to the standard way of splitting the data, there are different sampling variants. For instance, if you have longitudinal data, or otherwise data with structure, you might want to sample in a different way (e.g., sample the earlier times and predict later times). One variant of the cross-validation approach is the leave-one-out (LOO) cross-validation, which removes each data point one at a time as the test data and therefore performs the train/test part N times. You will also often see repeated cross-validation, which just adds another round of sampling on top to get better distributions (e.g., a 10-fold CV 10 times repeated gives 100 samples, which will be used to compute model performance).\nCross-validation works for most situations and is quite robust. It is also fairly easy to implement. In R, the tidymodels framework allows for easy CV, and we‚Äôll make use of that. Thus CV is a good default strategy. However, sometimes it is not possible or desirable to use CV. Occasionally, datasets are so small that you need all the data just to fit your model. More commonly, your data are too large and/or your model too complex to make CV run in a reasonable amount if computational time. (Often, one can speed up things by down-sampling the data, using faster algorithms, or running code in parallel on a computer cluster.)\n\n\nOther ways to minimize overfitting\nModel selection criteria: Are an alternative option if CV is not possible/wanted. Information criteria, such as AIC, BIC, DIC and similar, compute a measure that is a trade-off between good fit to the data (low-cost function/high performance) and model complexity (number of parameters). Approaches based on such selection criteria essentially try to guess how the model would perform if it were to be fit to new data, without actually trying to do it (in contrast to CV). The disadvantage is that these guesses as to how the model might perform on new data are not as reliable as actually evaluating model performance on such data through CV. The advantage is that no sampling is needed, which means these approaches are much less computationally intensive. For all those approaches (AIC, BIC, etc.), things are set up that a model with a smaller value is considered better. These measures - thankfully! - do not have the arbitrary p&lt;0.05 value cut-off common in frequentist statistics. For AIC, a rule of thumb is that a difference of 10 between 2 models is meaningful. (Unfortunately, people seem to not be able to make their own decisions and need crutches, so arbitrary cut-offs for AIC/BIC/etc. have started to show up in the literature.)\nThere is a lot of math behind information criteria (AIC and similar), see the book Model Selection and Multimodel Inference. Fortunately, one does not need to know the mathematical details to use those measures, just common sense. A lot of statistical methods in R can report such selection criteria and the broom package of tidymodels can often be used to further process such reported outcomes.\nRegularization: If one fits models in a Bayesian framework, one needs to supplies priors for each parameter. If one provides informative priors that constrain the possible values of the model parameters based on prior scientific information, it leads to what is called regularization and generally reduces overfitting. In a frequentist approach, there are methods such as LASSO which introduce a extra terms in the performance metric to try to prevent overfitting.\n\n\nModel fitting done the right way\nTo repeat (again): We generally want to know how well a model performs in general and on new data - not the sample we fit it to. Testing/reporting model performance for the data the model was fit to very often leads to overfitting and optimistic/wrong conclusions about new/future data. To minimize overfitting, here is my recommended strategy (I‚Äôm sure it‚Äôs not the only one, so what matters most is that you clearly think about what each step in your analysis means/implies):\n\nIf you have enough data and care about predictive performance, set some data aside for a final validation/test. If you don‚Äôt have a lot of data, you might need to skip this split.\nIf you choose to use all your data for model fitting purposes, and don‚Äôt evaluate your model on data not used during model building/training, you need to interpret your findings as exploratory and hypothesis generating, and you need to be careful about trying to draw generalizable conclusions.\nIf you have enough data (&gt;100s observations) and CPU power, use cross-validation (CV) approaches to determine the best model. If for some reason (mainly computational time or small data) CV is not feasible, use AIC & Co.\nThink carefully about your cost function/metric! A model that is great at predicting the wrong outcome is useless! (See e.g., the brain cancer example.)\nNo matter what approach you use, choosing a model based on performance alone is not enough. Perform additional evaluations (see next units).\n\n\n\nFurther comments\nDon‚Äôt forget: Performance is not everything. Even if you have a model that performs best on the independent/cross-validated data, you might want to opt for a ‚Äúworse‚Äù model that is easier to interpret and potentially use. Having smaller and simpler models can sometimes be of little importance and other times of great importance.\nFor instance, if you want to build a model that allows doctors to predict the chance that a patient has a certain disease, you might want to have a model that only uses the fewest (or easiest/cheapest to measure) variables to obtain good performance. So if you collect a lot of data, some based on checking patient symptoms and some on lab results, you might not want to use all those variables in your model. Let‚Äôs say that you had data on 10 predictors, 5 for easy to measure symptom variables (e.g., body temperature and similar), and 5 variables that come from different lab tests. You‚Äôll evalute models with different predictors (performing e.g., subset selection or LASSO, which we‚Äôll discuss soon) and find that the best performing model retains 3 symptom variables and 2 lab tests. Let‚Äôs say its performance is 95% (I‚Äôm purposefully fuzzy about what that performance exactly is since it doesn‚Äôt matter. It could be accuracy, or F1 score, or AUC, or‚Ä¶). But you also find that another model that contains 4 symptom variables and no lab tests has 85% performance. Which do you choose? That comes back to our discussion about assessing model quality: Performance is an important measure, but it‚Äôs not the only one. In this case, since you could get data on the 4 symptoms very quickly and cheaply, you might want to recommend that model for most doctors offices, and only use the better, but more time-consuming and expensive model with the 2 lab tests in settings such as high-risk populations in the hospital.\nIn contrast, if you are a bank that tries to predict fraud by having complicated models that constantly analyze various data streams, you might not care how complicated and big your model is, only that the performance in flagging fraudulent transactions is as good as possible.\n\n\nFurther learning\nMLNAR‚Äôs blog post provides a nice further discussion of the idea of generalization and how different areas of data science (statistics, machine learning, causal modeling) think about this problem. I think the most important paragraph is actually the short last one. I want to add to this that while different areas might think about the question of generalization differently, all of them more or less agree, that in the end, what is important is the general conclusions you can draw from your statistical modeling analysis and it doesn‚Äôt matter (by itself) how well your model performs on the data that you used to build your model! What matters is what it means more generally.\nFrank Harrell‚Äôs blog post discusses some of the potential caveats for using methods such as cross-validation.\nParts of chapter 2 of HMLR cover similar ground to what I discussed above. You might want to give it a read-through. Some of the topics that are discussed in that overview chapter are topics we‚Äôll cover later. Similarly chapters 2 and 5 of ISL also covers some of these topics, e.g.¬†the bias-variance trade-off. You looked at some of that material previously, revisiting might be worth it.\nChapters 11 and 12 of the Tidy Modeling with R book provide additional discussion of the sampling/CV approaches. Another discussion of CV is chapter 30 of IDS.\nTake a look at these and other sources and work through any that you find most helpful."
  },
  {
    "objectID": "content/module-model-evaluation/assessment-model-evaluation.html",
    "href": "content/module-model-evaluation/assessment-model-evaluation.html",
    "title": "Assessment - Evaluating Models",
    "section": "",
    "text": "Quiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\n\n\nExercise\nWe definitely need to practice some of the topics we covered in this (and previous modules). But since at the end of the week there is another project deadline, I want you to focus on that. Thus no exercise this week, we‚Äôll practice the topics we covered in upcoming exercises.\n\n\nProject\nFinish (and make sure to push) part 3 of the project. There is no need to submit a link. Unless you renamed your repository, I, and your classmates, know where to find it based on your previous submissions. Make sure the link you posted previously to the project_related Slack channel is accurate. If you are using a private repository, add the students who will review your project as collaborators (see announcement channel).\n\n\nDiscussion\nWrite a post in this week‚Äôs discussion channel that answers this question:\nWhich of the concept(s) we covered in this module is/are the most interesting/surprising to you, and why? And which concept(s) or topic(s) from this module‚Äôs materials do you find the most confusing, and why/how so?\nPost by Wednesday, then reply to each other by Friday. See if you can help each other reduce any existing confusion. I‚Äôll be participating too of course."
  },
  {
    "objectID": "content/module-model-basics/basic-statistical-models.html",
    "href": "content/module-model-basics/basic-statistical-models.html",
    "title": "Basic Statistical Models",
    "section": "",
    "text": "Overview\nIn this unit, we will discuss a few basic models, and explore how to apply them to data in R. This will set the stage for later, more in-depths explorations of statistical model concepts.\n\n\nLearning Objectives\n\nLearn about several types of basic models.\nUnderstand the relation between basic models and statistical tests.\n\n\n\nIntroduction\nThere are lots of models out there, too many to cover in much detail. In this module, we will briefly discuss some of the most common and basic ones, and then explore them.\nIn further modules, we will look at several more complex models. Most of the time, you do not understand every detail of a model. You need to understand enough about how the model works, its strengths and limitations, and to determine if a certain model is useful for a given situation or not.\nIn this course, you will get a very cursory introduction to several different models. If you happen to work on a problem where you think a certain kind of model might be useful, you will then want to revisit that model and learn some more, so you can ensure you are using the right model and are using it properly.\n\n\nSimple models for continuous outcomes (regression)\nThe most common model type applied to continuous outcomes is a linear regression model. This model is covered in many of the course materials we have been using (and of course other places).\nIf you are new to regression models, a good place to start are the chapters in the Model section of R4DS. I recommend you code along as you go through those chapters, or at least read through the code to make sure you understand what is going on.\nAnother good overview and introduction, at a slightly more abstract level, is given in the Linear Regression chapter in ISLR. You should read through this chapter in at least enough detail to get the big picture ideas. Depending on your interest and willingness to look at equations, try to follow along with the math as well. But the important part is to understand the overall ideas.\nIf you want further discussion of the linear model ‚Äì and see it applied to baseball data ‚Äì check out the Linear Models chapter of IDS. In this chapter, the author also discusses a few important concepts like confounding and regression to the mean.\nBut wait, there is more! Yet another discussion of linear models is in the Linear Regression chapter in HMLR.\nAs you can tell, this is such a fundamental type of model and is thus covered in a lot of resources. Definitely check the R4DS source, skim through ISLR, then take a look at the other ones I list here or any other source you find and like (let me know if you found a nice free one so I can add it to the resources list). Make sure you read and learn enough about these models that you feel comfortable understanding conceptually what they are used for and how they work.\n\n\nSimple models for categorical outcomes (classification)\nFor categorical outcomes, at least those with 2 categories (also called binary or dichotomous outcomes), logistic regression is the most common approach. Logistic regression has a bit of an unfortunate name since it is used for classification even though we call it a regression model. It is actually a regression model, since logistic regression predicts a numeric value between 0 and 1, which is the probability of having the outcome or not. We then usually (but not always) decide that 0.5 is the cut-off, and call any probability below as 0/not present/no and above that as 1/present/yes. Just keep in mind that logistic regression is usually used for classification problems.\nTo learn about logistic regression, or refresh your knowledge if you have previously been exposed to those models, read through the Logistic Regression chapter in HMLR. This reading goes into maybe more detail than you want or need at this point. So just make sure you go through it to get the main points of what logistic models are all about. We‚Äôll come back to some of the other topics discussed there later.\nLogistic models are also covered in the Classification chapter in ISLR. Read through the first part of this chapter in enough detail to get the overall concepts. You can also go through the sections following the logistic regression section to learn about some additional methods for classification. We‚Äôll revisit some of those types of models later.\nOf course, you can also find lots of additional resources online describing logistic regression models. If you find a good one, let me know.\n\n\nGeneralized linear models\nBoth linear and logistic models belong to a class of models called generalized linear models (GLM). Those types of models allow you to fit outcomes of specific types, for instance, if you have outcomes that are counts (integers), you can use Poisson regression, or if you have continuous outcomes that are all positive, you could use a Gamma regression.\nGLMs all have the same structure: namely there is a linear combination of predictor variables (e.g., \\(b_1*age + b_2*height\\)) and those are connected to the outcome through what is called a link function.\nGLM are very commonly used. Among GLM, linear and logistic regression models are by far the most commonly used ones. Other models that assume specific distributions of the outcome, e.g.¬†a Poisson distribution, can be accommodated by choosing the appropriate link function, which connects a linear combination of predictor variables with the outcome. GLM models are fast and easy to fit (using e.g.¬†the glm function in R), fairly easy to interpret, and often provide performance that is hard to improve upon with more complicated models, especially if the data is on the small side. To prevent overfitting, variable/feature selection or regularization approaches are often used. GLM assume a specific relation between predictors and outcome (e.g.¬†linear, logistic) and as such might not perform well on data that does not show the assumed pattern.\nAt the end of the Classification chapter in ISLR, there is a nice brief discussion of GLMs, I suggest you read through that in enough depth to get the overall ideas, but there is no need to go through all the math.\n\n\nKNN - another simple model worth mentioning\nAnother model that you might have already encountered (e.g., in the ISLR reading) is one called k-nearest neighbors (KNN). The idea is very simple: For any new observation, you compare the values of the predictor variables with those in your data. You then predict the outcome of the new observation to be the average of the outcomes of the K observations whose predictors most closely resemble the predictors of the new observation.\nAs an example for a continuous outcome (regression), if you want to predict height as the outcome and you have age and weight as the predictors, you would take an observation, look at the age and weight values for that observation/individual and compare to the K individuals in your dataset with the closest values (we won‚Äôt go into detail how ‚Äúclose‚Äù is defined). Then you take the average height of those K closest individuals and that‚Äôs your prediction for the new observation.\nThe same idea can be applied to categorical outcomes (classification). Say you wanted to predict sex instead of height. You again looked at the K individuals whose predictor values (here age and weight) are closest, and use the majority to predict the outcome for the new observation. (Say K=5, and 3 of the 5 closest individuals are male. Then you would predict the new observation to be male.)\nKNN often perform well, but they are not very ‚Äúportable‚Äù. For other models, once you trained/fit a model, you can ‚Äútake it with you‚Äù and apply the model to new data, leaving the old data you used for model building behind. KNN are a somewhat strange since the data is the model. All the data is used to predict new observations by comparing them to existing ones and predicting outcomes based on closeness of predictor variables. Which means you always need to have the data to predict new outcomes, often making it not too useful in practice. You also don‚Äôt learn too much in terms of inferential insight. Still, it is a useful model to know about, and it works for both continuous and categorical outcomes. It is also commonly used for imputation of missing values. In that situation, you treat the predictor you want to impute as the outcome and use the remaining predictors as the data. KNN are described in the K-Nearest Neighbors chapter in HMLR and also show up in chapters 2 and 3 of ISLR and the Cross-validation and Examples of Algorithms chapters of IDS. Take a look at any of those resources if you want to learn a bit more about KNN.\n\n\nSimple models for no outcomes\nAs we discussed previously, if you have an outcome variable, your data is called labeled data and the methods you use are called supervised methods/models. Such data with one (or several) outcome variable(s) are most common. But as previously discussed, sometimes you might have data without clear outcomes (unlabeled data) and you still want to determine if there is some pattern in your data. This calls for unsupervised methods.\nThere isn‚Äôt really one single, standard go-to method for unsupervised learning ‚Äì in contrast to the very common linear and logistic models for supervised learning. There are often used methods, such as Principal Component Analysis (PCA) (which falls into a larger group of methods called dimensionality reduction methods) and a group of methods called clustering methods (the most common among these are probably k-means clustering and hierarchical clustering). Since most of you will work on labeled data using supervised methods most of the time, we won‚Äôt go much into unsupervised methods. If you want to learn a bit more, check out for instance the Unsupervised Learning chapter of ISLR. There are also chapters on PCA and different types of clustering in HMLR.\n\n\n\nChoosing K is an important task. 3 sometimes works. Source: xkcd.com.\n\n\n\n\nThe zoo of statistical tests\nIn some of your (bio)statistics classes, you likely came across a variety of statistical tests, e.g., t-test, Wilcoxon, Kruskal-Wallis, and others from the huge collection of tests. Each test makes certain assumptions and is most adequate for certain types of data. For instance you might remember hearing about parametric tests (which are most applicable if the underlying data has a certain distribution, usually a normal distribution) and non-parametric tests, which make fewer assumptions about the distribution of the data, but don‚Äôt have quite the same statistical power. Those more traditional statistical tests certainly have their uses (though are quite often overused and misused). We just don‚Äôt have time to cover them.\nPretty much all of those tests have equivalent formulations as multivariable models (either a GLM type as described here, or others that we‚Äôll cover later). The advantage of more general models, such as GLM, is that they easily allow for as many predictor variables as you want to add. Also, once you understand the general setup of GLM (and other models), this understanding transfers to other methods. For statistical tests, they are often taught as ‚Äúif the data look like this, then test like that‚Äù without any underlying fundamental and transferable understanding. Overall, I am not a big fan of the way many intro (bio)stats courses are still taught these days.\nBecause of that, and since this course is about modern data analysis, we will not cover any classical statistical tests and instead start with multivariable, GLM-type models and then move on to more complex/modern machine learning models.\nIf you never thought about the relation between classical statistical tests and GLM-type models, you might want to check out Common statistical tests are linear models by Jonas Kristoffer Lindel√∏v which explains the relation between the two in a good bit of detail. Statistical tests as linear models by Steve Doogue is also good, it is based on the original text by Lindel√∏v and covers some topics in more detail. Both resources show examples in R, and have links to further materials. At some point in your career as a data analyst, you might need to use or interpret classical statistical tests, and having a general idea that they can map to GLM-type and other models might be helpful.\n\n\nFurther reading\nChapters 3 and 4 of ISL discuss linear and logistic models. So do chapters 4 and 5 of HMLR.\nFor another source that discusses almost all the models just mentioned, each one very briefly, see the Examples of Algorithms chapter in IDS."
  },
  {
    "objectID": "content/module-intro-tools/tools-tips-guidance.html",
    "href": "content/module-intro-tools/tools-tips-guidance.html",
    "title": "Guidance and Tips for R/RStudio/GitHub",
    "section": "",
    "text": "This document is a collection of guides and tips related to getting started and using R, R Studio and GitHub, based on stumbling blocks that I have noticed students encounter somewhat regularly."
  },
  {
    "objectID": "content/module-intro-tools/tools-tips-guidance.html#avoid-large-files",
    "href": "content/module-intro-tools/tools-tips-guidance.html#avoid-large-files",
    "title": "Guidance and Tips for R/RStudio/GitHub",
    "section": "Avoid large files",
    "text": "Avoid large files\nGitHub is not suited for tracking large files. If you try to push/pull files larger than say 20MB, things might not work. Therefore, don‚Äôt try to track large files with GitHub! Large files are the #1 reason students have problems with their GitHub repository!\nIf you do have large files for a project, there are a few options:\n\nShrink the large file outside of GitHub. For instance if you have raw data in CSV format, you can remove parts you don‚Äôt need for your project and save the rest as an Rds or other compressed format. That file might be small enough to be tracked by GitHub.\nYou can use Git LFS to track them. But that‚Äôs a bit more advanced.\nYou can place the large file into a special folder in your GitHub repository (e.g.¬†one called largefiles) and then add an entry to the ‚Äô.gitignore` file to tell GitHub to ignore this folder. The problem with that is that if someone else wants to work on your project, they won‚Äôt automatically have those large files. If the files are generated by your code (e.g., they are the result of running a simulation), they can just re-run your code and get themselves a local copy of these files. If that‚Äôs not possible, either because the files are input (such as data) or it takes too long to re-run the code, you will have to manually share these files/folder with them."
  },
  {
    "objectID": "content/module-intro-tools/tools-tips-guidance.html#resolve-merge-conflicts",
    "href": "content/module-intro-tools/tools-tips-guidance.html#resolve-merge-conflicts",
    "title": "Guidance and Tips for R/RStudio/GitHub",
    "section": "Resolve merge conflicts",
    "text": "Resolve merge conflicts\nAt times, you will get merge conflicts. For instance you might have made changes to your local repository that you didn‚Äôt mean to. Or someone else (or you on a different computer) added changes to the Github.com repository and things don‚Äôt merge with your local repo. If you have some local updates that you want to keep, here is a method that often works.\nAt that stage, it gets a bit tricky. GitKraken has a good tool to help you resolve conflicts, and it works well for text files (code, Rmd, md, etc.). It doesn‚Äôt work well for other files (word, Excel, etc.). Start by using the GitKraken tools, compare files and decide which version of any conflicting ones to keep (or to combine things).\nIf the GitKraken tools don‚Äôt help, you can do some manual intervention as follows:\n\nMove the files you changed and that create the problems to some ‚Äúsafe‚Äù location (outside of your repository).\n\n2a. If you are using GitKraken and you haven‚Äôt committed your local changes, there is a red trash button symbol in the top left. If you click on it, you can discard your changes. once you have done that, you can sync with the remote. Your local and remote should be in sync again, and you should be able to move to step 3.\n2b. If you already committed your changes, you can right-click in GitKraken on the latest version on the remote commit (those symbols with comments in the main window). Then pick reset main/master to this commit and then choose a hard. This overwrites anything you‚Äôve done locally with the remote version on Github.com, so make sure any local changes you want to keep have been copied to a safe place (step 1). Your local and remote should be in sync again, and you should be able to move to step 3.\n\nOnce all is ok again and your local repository and the remote one are in sync, you can copy the files (or the content from those files) you placed in a safe location in step 1 back into your repository. Then continue with the usual work flow.\n\nIn general, to minimize conflicts, it is good to regularly create issues and push/pull. You should definitely do that any time you stop working on a project. But sometimes doing updates in-between is also good. It is better to change a few files and work on just one topic, then commit and push. After that, start the next topic.\nThis is also true if you work with someone else and send them your updates as pull requests. By breaking them up into smaller units, it is more likely that conflicts are avoided or localized."
  },
  {
    "objectID": "content/module-intro-tools/tools-tips-guidance.html#start-over",
    "href": "content/module-intro-tools/tools-tips-guidance.html#start-over",
    "title": "Guidance and Tips for R/RStudio/GitHub",
    "section": "Start over",
    "text": "Start over\nSometimes the previous fixes doesn‚Äôt work. For instance if you tried to track large files, even though you were just told not to do so, you might end up in a situation where there‚Äôs no simple fix, other than to start over. Here is how you can do that:\n\nRename the repo on GitHub.com to something like myrepo_old. You can do that in the settings of your repository. Also give your local repo/folder the same name.\nCreate a new repo on GitHub.com with the name of the prior one. Clone this new repo to your local computer.\nCopy all the good stuff from your old repo into the new one. Make sure to not copy over the bits that cause the problem.\nPush the updated local repo to GitHub.com.\nOnce certain that everything works, delete the old repo both online and on your local computer. You might decide to keep it around for a while, since by creating a new repo you lost the history, so you can‚Äôt go back to prior versions in the new repo."
  },
  {
    "objectID": "content/module-intro-tools/tools-reference-management.html",
    "href": "content/module-intro-tools/tools-reference-management.html",
    "title": "Reference Management",
    "section": "",
    "text": "Overview\nIn this short unit, you will learn how to manage your references within the R/Quarto/Markdown stack of tools.\n\n\nLearning Objectives\n\nKnow how to do reference management that plays nicely with Quarto/Markdown\n\n\n\nReference management with Quarto/R Markdown\nFor almost any data analysis, especially if it is meant for publication in the academic literature, you will have to cite other people‚Äôs work and include the references in your work. In this class, possibly for some of our exercises and definitely for the class project, you need to include references and cite other people‚Äôs work like in a regular research paper.\nI assume you are familiar with how citing references works, and hopefully, you are already using a reference manager. You might be using EndNote or Mendeley or PaperPile or‚Ä¶\nTo have something that plays well with Quarto/R Markdown, you need references in a format called bibtex. This is essentially a large file of structured plain text. You don‚Äôt ever want to edit the bibtex file yourself. Instead, you use a reference manager that is bibtex friendly. I mostly use Zotero, which is free. If you use Zotero, you should use the Better BibTeX extension, which allows you to keep your bib file in sync as you update references in Zotero.\nYou could probably use others. I don‚Äôt have much experience with any others. Whichever reference manager you use, the important part is that you need to be able to save or export your references as a bibtex file.\nTo use references in your Quarto or RMarkdown files, specify the name and location of your bibtex file in the YAML header. You can also specify a CSL style file there. This is a file which controls how the references are formatted. Styles for almost any journal are available for download from Zotero‚Äôs styles page. When you create your bibtex file, each reference gets a key, which is a shorthand that is generated by the reference manager or you can create yourself. For instance, I use a format of lower-case first author last name followed by 2 digit year for each reference. If I cite a paper by the same first author that was published in the same year, then a lower case letter is added to the end. For instance, for a paper that I wrote as 1st author in 2010, my bibtex key might be handel10 or handel10a. You can decide what scheme to use, just pick one and use it forever.\nIn your Quarto document, you can then cite the reference by adding the key, such as ...in the paper by Handel et al. [@handel10].... When you process the file, it looks for a reference with the specified tag in your bibtex file and places all references at the end of your document using the formatting you specified.\nFor references with Quarto, see this Quarto documentation.\nSome more details on citations and references in R Markdown/bookdown can be found in the RMarkdown Cookbook bibliography section and citations section.\nYou will see a few examples of bibtex use together with Quarto/R Markdown in the course, so when it‚Äôs time for you to use it (e.g.¬†for the class project), you should know how to go about doing it."
  },
  {
    "objectID": "content/module-intro-tools/tools-github-r-workflow.html",
    "href": "content/module-intro-tools/tools-github-r-workflow.html",
    "title": "The GitHub and R Workflow",
    "section": "",
    "text": "Overview\nThis document gives a brief discussion of working with R projects through GitHub. There is more than one way to do this, they should be equivalent in most instances.\n\n\nLearning Objectives\n\nKnow how to get your R projects synced through GitHub.\n\n\n\nPrerequisites\nI assume you read through the Git/GitHub introduction and have created a GitHub account.\nI also assume that you are using the GitKraken client. If you don‚Äôt, you need to adjust things accordingly.\n\n\nApproach 1 - start on GitHub\n\nGitHub\nFor this approach, you start by creating your repository on GitHub. The most common place to create a repository is under your own user name/account. Note that it is also possible to create repositories inside an organization (if you are part of an organization). As an example, my Github user name is andreashandel and the name of my research group/organization is ahgroup. Depending on if I want a repository under my name or under the research group/organization, I‚Äôll go to https://github.com/andreashandel or https://github.com/ahgroup/).\nOnce you are in the location/account where you want the new repository, click on Repositories to see your list of repositories (which might initially be empty). Now create a new repository by clicking the green New button. A page should show up where you can specify details for your new repository, something like this:\n\n\n\n\n\nSpecify the repository owner. Make sure it‚Äôs the one you want, e.g., yourself or an organization you are part of. Pick a good name for your repository. Set the repository as private or public. In general, you should consider doing it public, unless there is an important reason to have it private (e.g.¬†confidential data). Note that if you set it to private, you might not be able to access it with the free version of GitKraken. Check the box Initialize this repository with a README. Set the .gitignore option to R. You can keep the license box unchecked, or choose a license for your page. Click ‚Äúcreate repository‚Äù.\n\n\nGitKraken\nOpen Gitkraken, under File choose Clone Repo go to Github.com, find the name of the repository you just created and clone it to some place on your local computer (i.e.¬†copy it from Github to your local computer.) E.g. if your repository was called mynewrepo, and you place it on your desktop under windows, you might choose C:\\Users\\yourname\\Desktop as the target. I recommend having a main folder for a specific organization or course or project, and into which you place all your repositories related to it (e.g.¬†place all your repositories for this course into a folder called MADA, such as C:\\Data\\GitHub\\MADA\\.)\nRemember to not place your repository in a location that is synced by other software such as DropBox or Onedrive. Also do not place GitHub repositories inside other GitHub repositories.\n\n\nRStudio\nOpen RStudio on your computer. Choose File, New Project, Existing Directory and find the directory/folder you just created on Github and copied to your local computer (e.g.¬†C:\\Users\\yourname\\Desktop\\mynewrepo). If things work, you should see in the bottom right window of RStudio (at least) 2 files, one with .Rproj ending and a README.md.\nClick to open the README.md file. It will only contain the name of your repository with a # sign in front. At some point, it is useful to add meaningful information. For now, just to practice, you can add whatever you want.\nIn the future, if you want to edit your project, click or double-click on the .Rproj file. You can try it. Close RStudio, click the .Rproj file and the same setup as before should show up. Using R projects has several advantages. One is that it makes how you write file paths easier, so while having an .Rproj file is not strictly required I recommend you do it that way.\nNow create your content, place any files you create inside your project directory or its sub-folders.\n\n\nSyncing\nOnce you are done editing your content (for now), go to Gitkraken (or your preferred Git client) and push your changes to Github.\nIf you use GitKraken, in the top right you should see a notification about changed files. Click view changes, then Stage all Changes. Write a commit message, the Commit the changes. In the main area of GitKraken, you should see that the repository ‚Äì usually main or master, unless you work on a branch ‚Äì shows the computer icon above the one with some random logo (or your picture). That means your local repository is ahead of the one on GitHub.com. To get them in sync, you click the Push button. If things work, the two symbols should now be in the same line.\nIf your code cannot sync you will likely receive an option from GitKraken to perform a force push. A force push will overwrite the remote repo with the local repo forcibly. This means that the remote will be updated with any changes that exist on your local computer however, if any additional changes have been made to the code since you began editing (i.e.¬†someone else has made a commit to the repo while you were working on it) they will be overwritten by the force push since they are not on your local repo. Thus, use with care.\nNow that everything is established, the workflow is simple, you just need to remember to go through all the steps.\n\nBefore working on a repo/project, open GitKraken to make sure the local and remote are in sync. If the remote is ahead (e.g.¬†because someone else worked on it, or you did on a different computer), do a Pull. If the local one is ahead, you don‚Äôt need to push (but you should have right after you finished your last local session).\nOpen your project in RStudio (assuming it is an RStudio project). Make any changes you want to make. Once done for the day, go back to GitKraken and stage, then commit your changes, then push.\n\n\n\n\nApproach 2 - start with an R Project\nAbove we created the repository on GitHub first, then cloned it to the local machine, then initialized it as an RStudio project. It is also possible to do this in reverse. If you want to start a Git repo this way, you need to install Git standalone, and it must be configured to work with third-party applications. You can read chapter 6 of happygitwithr for more detailed instructions.\n\nRStudio\nOpen RStudio. Choose File, New Project, New Directory and (usually) you‚Äôll choose New Project. Pick a project name and location. Check the Create a git repository box. This button might not appear if you only installed Gitkraken and not Git.\n\n\n\n\n\nCreate your content in the project folder.\n\n\nGitKraken\nOnce you finished working on your project for now, open GitKraken. Go to File -&gt; Init Repo and choose the folder where your project is in. You‚Äôll likely get a message about an initial commit that needs to be created. Agree to it. Your repository should show up in GitKraken. In the top right, look at and stage your changes, write a commit message, then commit. Next, press the Push button.\nAt this point, if you didn‚Äôt properly connect GitKraken and GitHub previously, you‚Äôll likely get an error message. Follow the error message and the connect Gitkraken to your Github account information to get it to work.\nYou‚Äôll see a message about no remote existing and if you want to add one. Say yes. A menu on the left should show up. Make sure the repository name is the same as your local repository/folder name. Then click the green button. If things worked, your local repository has been created on GitHub.com.\n\n\nSyncing\nOnce your repository exists both locally and on Github.com, the syncing process is exactly as described under Approach 1.\n\n\n\nMerge conflicts\nSooner or later, you will encounter a merge conflict. That happens if the same file(s) changed both locally and on GitHub.com. That can happen if multiple people work on the same files, or if you work on multiple computers and forget to always push/pull before and after you work on a project. GitKraken can help you resolve conflicts. Sometimes you have to do it manually. Moving the offending files out of your local repository, then doing the merge and after that adding the updates again can help. If you know that one source is the definite one, you can force a push or pull. A more advanced approach is to use branches.\nMore information on merge conflicts and how to deal with them using GitKraken can be found in this merge conflict blog post and merge conflict tutorial."
  },
  {
    "objectID": "content/module-intro-tools/tools-ai.html",
    "href": "content/module-intro-tools/tools-ai.html",
    "title": "AI Tools for Data Analysis",
    "section": "",
    "text": "Overview\nI started to write a unit on AI tools. Then it got too big üòÅ and I decided to make it its own module, which you can find towards the end of the content list sidebar.\nPlease go through the content of the AI module. You don‚Äôt need to read through all of it this week, but do so soon. I strongly encourage you to use AI throughout this course, and you will be explicitly asked to do so in some of the exercises."
  },
  {
    "objectID": "content/module-intro-tools/assessment-course-tools-introduction.html",
    "href": "content/module-intro-tools/assessment-course-tools-introduction.html",
    "title": "Assessment - Course and Tools Introduction",
    "section": "",
    "text": "Reminder\nEach module comes with assessments. Those are things you need to do that will be graded. The table on the Schedule page lists the assessments for each module.\nIf you haven‚Äôt done so, please read through the Assessments Overview page so you know how each type of assessment should be done and how grading works.\nEach module has an Assessments page, like this one, which describes what the assessments are and what you should do. Often, there is a good bit of information for the exercises.\n\n\nQuiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\nFor this first module, you have 2 submission attempts. This allows you to get used to the submission system.\n\n\nDiscussion\nThe discussion for this module is all about getting to know each other. For your original post, complete the Exercise below, then post the URL to the website you createed by doing the exercise below into the introductions Discord channel. Do so by Wednesday 5pm.\nFor your replies, visit some of your classmates websites and leave feedback as a reply to their initial post. Do so by the Friday deadline.\nI hope this will allow you to start getting to know each other, even if only virtually.\n\n\nExercise\nFor this exercise, you are asked to start using Github, R, RStudio, and Quarto to create a website which will become your portfolio for this class. You‚Äôll write and post a short personal introduction of yourself.\n\nWorkload Note\nFor those of you who are new to Github/R/Quarto, this first project will make you do a lot of things that you are not familiar with. I know that this will be time-consuming and also might feel a bit intimidating. It‚Äôs partly unavoidable and partly on purpose. You need to learn how to quickly get up to speed with all kinds of new tools that come your way. So practicing it is a good idea. You are welcome to draw on any sources for help that you want (online, classmates, instructor, etc.). I‚Äôm confident with a bit of trial and error you‚Äôll get it to work.\nIf you get stuck (quite possible), check the materials listed in the Resources section of the course, or use Google. Also use the discussion boards to ask questions. You may use any method to get help (Google, R4DS Slack channel, AI, colleagues/friends/relatives with R/Quarto/Markdown experience, etc.). Still, try to do as much as possible yourself. We‚Äôll use all the bits you are learning here repeatedly during this course.\n\n\nPre-requisites\nYou should gone through all content in this module and should be set up with all the tools we need, namely R, RStudio, GitHub and GitKraken (or your preferred Git client).\n\n\nGithub setup\nI made a template for you to get started. Go to the Online Portfolio Template. Make sure you are logged into GitHub, then click on Use this template. Create a new repository, call it yourname-MADA-portfolio (yourname should include both your first and last name, e.g.¬†andreashandel-MADA-portfolio.) Choose public repository, then create it.\nNext, clone the repository (which will be something like https://github.com/USERNAME/yourname-MADA-portfolio/) from Github.com to your local computer (using Gitkraken or whatever software/method you decided to use). I recommend that you make a folder somewhere on your computer (e.g., called MADA-course) and place all class-related repositories (of which this is the first one) into this folder. Remember that these GitHub repositories/folders should not be synced with some other software (e.g., Dropbox/OneDrive).\n\n\nQuick local website check\nOnce your repository is on your local computer, go into the folder and click on online_portfolio.Rproj, which should open RStudio. In the bottom right, you‚Äôll see a number of files and folders that are part of your repository/project. We‚Äôll edit a few of them as described below. The rest you can ignore for now, we‚Äôll come back to them in later exercises.\nIn the top right panel in RStudio, you should see a Build tab. Click on it. It should show a Render website button, click it. You will probably get complaints about missing packages. If that‚Äôs the case, install them. Once everything works and renders without problems, a local preview of your soon-to-be new website should open in your browser.\n\nIf for some reason, you can‚Äôt see the Build tab in the top-right corner of RStudio, you can alternatively click on the Terminal tab (next to the Console tab). Type quarto render into the terminal and this should build your website (the RStudio button is basically just calling that command.)\n\nIf you can‚Äôt get this far and can‚Äôt figure out what‚Äôs going wrong, ask for help on Discord. Once you got that far, let‚Äôs create some content.\n\n\nQuarto content creation\nFor this part of the task, the comprehensive Quarto guide and the Quarto section of the 2nd edition of R for Data Science will likely be useful.\nOpen index.qmd. This is the main landing page for your website. Change the text however you want to. What you are writing here is Markdown, so now would be a good time to check out some of the Markdown resources and play with them. You will also see at the top of your R Studio window two buttons Source and Visual. This allows you to switch between editing Markdown in plain format, and using a visual editor. It‚Äôs a good idea to switch back and forth to start to learn how certain graphical elements look like in plain text formatting. You‚Äôll likely be using a mix of visual and source view a lot.\nNext, open aboutme.qmd and write some text introducing yourself, as follows:\n\nTalk a bit about yourself, your background, training, research interests. Let me/us know what kind of statistics, programming, data analysis experience you already have. I‚Äôd also be curious to know what you most hope to learn in this course. And then tell us some quirky/interesting/curious thing about you.\nGive your text a nice structure with headings, subheadings, etc.\nInclude a picture of yourself. To do so, place the picture in the repository and link it from the Quarto file.\nFinally, include a link to a picture/video/website/etc. The linked material should be somehow related to data analysis. Find something that you think is really cool/interesting/weird/etc. Provide a few explanatory sentences.\nIf you want, feel free to get creative and include other things. You can play with Quarto if you wish to, e.g., you can try to include a table or a video or any other elments you want to explore.\n\nIf you make changes and want to see how they look, save your changes and rebuild the website. Load/refresh in your browser and it should show any changes.\n\nThis portfolio website and everything you put on there is public. So don‚Äôt put anything on there that you don‚Äôt want to be publicly known.\n\n\n\nWebsite control\nFor Quarto, a lot of the look/feel and structure of a website is controlled in what‚Äôs called a YAML file (it has the ending .yml). The main one is called _quarto.yml. Open it. You‚Äôll see that it contains a bunch of information about the website, including the title, the menu structure, and some styling information.\nMake the following changes:\n\nUpdate the title of your website.\nReplace the URL-TO-THIS-REPOSITORY-HERE placeholder text with the Github URL of the repository you created (e.g.¬†for me, that would be something like https://github.com/andreashandel/andreashandel-MADA-portfolio).\nIt is possible to change the look of your website. Full styling requires knowledge of CSS and HTML. We won‚Äôt get into this. But we‚Äôll explore a quick way to change the look. Replace theme with a different one. Many themes are available. Try some, see how things change. You can also change the highlight scheme. For further styling, you can put custom CSS code into the styles.css file. We won‚Äôt do this here, but feel free to explore on your own.\n\n\nYAML (which is the format of _quarto.yml) is VERY finicky when it comes to formatting. You need to have exactly the right number of empty spaces. If you ever change a YAML file and you get a weird error message when trying to rebuild your website, it‚Äôs likely because some minor formatting is wrong, e.g.¬†an empty space is missing or too much. Just keep that in mind. (It happens to me all the time.)\n\n\n\nSending updates to Github\nOnce you are happy with your updates, or if you want to take a break, make sure to push your updates to GitHub. First close RStudio. Then go to the program you use for Github (I will assume that it‚Äôs GitKraken. If you use something else the details might vary). In the top right of Gitkraken, there should be a notification about changed files. Click view changes, then Stage all changes. Write a commit message, commit the changes. You should see the main with the computer symbol moved above the one with some random logo. That means your local repository is ahead of the one on github.com. To get them in sync, you click the push button. If things work, the two symbols should now be in the same line.\n\nSometimes, there might be issues that prevent pushing/pulling, e.g., if someone else changed the same files you did. If you are sure your local version is the correct one, you can perform a force push. A force push will overwrite the remote repo with the local repo forcibly. This means that the remote will be updated with any changes that exist on your local computer. If any changes have been made to the code since you began editing (i.e.¬†someone else has made a commit to the repo while you were working on it) they will be destroyed by the force push since they are not on your local repo. For this assignment, you are the only person working on your introduction so it is unlikely you will encounter any issues with force push, but it is good to be aware of the function of the action. Use with care. It‚Äôs generally better to resolve merge conflicts.\n\n\n\nMaking the website public\nWe want to set things such that your repository can at the same time become a public website. Github allows you to do that with its Github Pages feature. To turn on the Github Pages feature, go to your repository on github.com (e.g.¬†something like https://github.com/andreashandel/andreashandel-MADA-portfolio). On the top of the website for your repository, you should see a Settings button. Click on it and then find the Pages section. In the Branch section, switch none to main and folder as /docs. Click Save.\nThis should now have turned on your website. You might have gotten a notice about the location of your new website. If yes, follow the link. If not, you can get there by going to a URL that has the form https://username.github.io/yourname-MADA-portfolio/ (e.g.¬†for me, that is https://andreashandel.github.io/andreashandel-MADA-portfolio/). It might take a few seconds before the page is available, if you get a ‚Äònot found‚Äô error, wait a bit and try again. If everything works, your website is now public!\n\n\nGithub pages\nWe used the Github pages feature to create the website. You might have noticed that the files for your new website live on the domain github.com, while your nice-looking website lives at the domain github.io. This is how Github Pages work. It take files from your repository and turns them into a website with a distinct URL. Those files can either be in the main repository folder or in a /docs subfolder (our setup). Your website URL and your repository URL are distinct. All github actions, like pushing, pulling, cloning, etc. are done with the repository on the github.com domain. The github.io domain is only there to display websites.\nAs an example, this course website is displayed at https://andreashandel.github.io/MADAcourse/ and all the files that make up the webpage are at (https://github.com/andreashandel/MADAcourse). Note the text at the bottom of the repository view. That text comes from the Readme.md file. Speaking of which‚Ä¶\n\n\nUpdate your repository Readme file\nRe-open your project with RStudio. Click to open the README.md file. It only contains a single sentence I placed there. This file will not be part of your website, but it is what people see if they land at your github repository (i.e.¬†the behind-the-scenes location). Add text to it that points users to the URL for your new website. (You‚Äôve done the reverse above, allowing users to quickly go from your website to your Github repository).\nMake any other edits you like.\nOnce done with edits, remember to re-build/re-render your website with Quarto (by clicking ‚ÄòRender Website‚Äô in the Build tab) and then committing/pushing your local changes to GitHub (using Gitkraken or another Git client).\n\nOnce your website is up and running, don‚Äôt forget to post the link into the introductions channel!\n\n\n\nGeneral Github workflow\nIn the future, if you work on repositories like this one (or any others), your workflow should be as follows:\n\nOpen GitKraken or your preferred Git client and make sure the local copy of your repository is up-to-date (this is important if you collaborate with someone or you are working on multiple computers). If the remote is ahead of the local, do a pull.\nOnce your local repo is up-to-date, open your project in RStudio by double-clicking the name of the project (ends in .Rproj). Make all the edits you like. Make sure to render your quarto files (or knit your Rmd files). Once you are done with edits, close Rstudio, go to Gitkraken, stage/commit/push your changes back to the remote server.\n\nYou will be using this workflow a lot.\n\n\nFurther information\nI wrote a tutorial a while back describing the setup with Github pages. It has some additional information, which you can check out on my website. Note that this tutorial still used R Markdown instead of Quarto. Some of the information still applies, other parts are different with Quarto.\nUsing Github pages makes it fairly easy to create websites. I currently host all my course websites in this way. What we created was a fairly basic page. It is easy with Quarto to create other, more full-featured websites. For instance my personal website uses Quarto. If you are interested in such a more full-featured approach, check out this part of the Quarto documentation. You can also find a few more links to other resources."
  },
  {
    "objectID": "content/module-data-presentation/visualization-in-r.html",
    "href": "content/module-data-presentation/visualization-in-r.html",
    "title": "Visualization in R",
    "section": "",
    "text": "Visualization and graphics in R\nVisualization and graphics are strengths of R. There are currently three main ‚Äì and unfortunately incompatible ‚Äì ways of making graphs in R.\nThe first approach is to use the base R commands, with the plot function as the main workhorse, and related functions for more specialized plots (e.g.¬†boxplot). You can make a lot of very good graphs using base R, but it often takes many lines of code and a good bit of fiddling.\nThe second approach is using the lattice package, and extensions that build on this package. lattice seems to have largely ‚Äúlost‚Äù against option number 3 and I don‚Äôt see it used much anymore these days.\nThe third, and currently most popular approach is to make graphs using the ggplot2 package and various extensions to it. ggplot2 is one of the many R packages written by Hadley Wickham. ggplot2 and its extensions have by now become more or less the standard for good graphics in R. Note that while the package is called ggplot2, people interchangeably refer to it as ggplot or ggplot2. (No one uses ggplot ‚Äúone‚Äù anymore.) I‚Äôll do the same.\nMy recommendation for learning to make plots in R is to focus on ggplot2 and related packages.\nFor most graphs and figures you want for a publication, ggplot2 is probably your best choice. It, and its many add-on packages, product very high quality graphics with not too much code writing required. While you could use ggplot2 exclusively for any figures you make, it is also useful to know a little bit about how to make plots using base R, since they are often very good for quick and dirty plots, especially during the data exploration stage.\n\n\nPracticing ggplot2\nIf you are new to ggplot2, a good and gentle introduction is the set of tutorials in the Visualize Data Posit Recipes section. Similar material, though a little bit more advanced, is covered in a non-interactive form in the Data Visualization, Exploratory Data Analysis and Graphics for Communication chapters of R4DS. A nice step-by-step tutorial can be found in C√©dric Scherer‚Äôs blog post. Another good source are the chapters in the Data Visualization section of IDS. Especially the Data visualization principles chapter is worth a read.\nIf you want to learn a bit more about ggplot2 in general, the ideas behind it, and also how to use it, you can check out Hadley & co-author‚Äôs free online book ggplot2: Elegant Graphics for Data Analysis.\nFor coding/R in general, and making plots in particular, it is generally a learning by doing approach. The most common approach - and one I still use a lot - is to search online for ‚Äúhow do I do X in ggplot‚Äù, and usually you find an example and code that is close enough to what you want so you can adjust it. The tricky bit is knowing what to search for. If you don‚Äôt know the type of plot you want, it is useful to learn a bit about different plots and what they are good for (something that was discussed in the previous unit). A good reference source for how do I do X with ggplot2 is the R graphics cookbook, which is meant to give you quick recipes for common questions and plots.\nFor inspiration and ideas, there is also the R graphics gallery where you can see all kinds of plots made in R, and you can also see the code. They are nicely organized into categories. It‚Äôs definitely worth browsing around.\n\n\nBeyond static graphs\nWhile you will likely always need regular figures for papers, reports, presentation slides, etc., it is becoming increasingly common to have interactive and dynamic visualizations. R has pretty good facilities for that, some in rapid development. The ggvis package is meant to be similar to ggplot2 but allow for interactive and dynamic visualizations. The plotly package also works well with R and allows interactive graphs.\nAnother great, and related set of tools is the htmlwidgets package which allows you to make a lot of very nice, interactive web-based graphs using R and various R packages.\nFor even richer, interactive graphical operations and visualizations, including writing full graphical user interfaces, you can use the shiny package and extensions to it. The shiny gallery has examples of apps. Some of my R packages, e.g.¬†DSAIDE and DSAIRM use Shiny for the interactive graphical interface (and either ggplot2 or plotly to show graphs for different models).\nOften, getting professional-looking interactive figures, dashboards, and other widgets in R is not that difficult. However, since those are all specialized products, they usually require commands and concepts that are somewhat different from basic or tidyverse R, which generally takes some time to learn.\nInteractive graphs and apps are beyond what we‚Äôll cover in this class, but it is something that might be worth learning, and you are certainly welcome to make use of some of these packages and tools for your class project üòÉ.\n\n\nFurther R visualization resources\nThere are of course a whole lot more resources available. Almost all books that teach data analysis with R contain some visualization chapter. There are likely also many other good books, websites, and R packages available. A few more are listed on the General Resources page. I‚Äôm sure you‚Äôll find many others yourself. If you come across any particularly nice sources, please let me know so I can add them."
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html",
    "href": "content/module-data-presentation/presenting-results-overview.html",
    "title": "Presenting Results Overview",
    "section": "",
    "text": "In this unit, we will discuss the main ways to show your data and results and discuss some ways to do that efficiently.\nBy results, I mean any descriptive or statistical analysis you might have done for your data. For instance if you summarize your data in a table to show the percentage of individuals that do/don‚Äôt know how to write code in common programming languages, you generated a result and are presenting it. Similarly, if you fit multiple models and compute their cross-validated performance (you don‚Äôt yet need to know what this is) and want compare model performance, this would be another result you‚Äôll want to show either as a plot or table."
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html#figures",
    "href": "content/module-data-presentation/presenting-results-overview.html#figures",
    "title": "Presenting Results Overview",
    "section": "Figures",
    "text": "Figures\nA well-done figure is able to convey a lot of complex information in an intuitive and understandable format. On the flip side, a poorly thought-out figure can be utterly confusing even if it is presenting something simple. Worse, figures can easily be (ab)used to mislead the reader. Your goal is to make sure that your figures are great-looking, easy to understand and not misleading. We‚Äôll go over some more details relating to figures in subsequent units."
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html#tables",
    "href": "content/module-data-presentation/presenting-results-overview.html#tables",
    "title": "Presenting Results Overview",
    "section": "Tables",
    "text": "Tables\nTables allow you to easily present summaries of your data. Many epidemiological and other papers have a Table 1 which summarizes the data by its characteristics. For instance if you had a human cohort, this summary table will likely list the number and percentages or ranges of individuals based on gender, age, BMI, smoking status, etc.\nTables are also often used to present results of your data analysis. In my opinion, figures are often better, but if the results are simple or there are a lot of numbers that need to be shown, tables can work better at times. We‚Äôll go over some more details on how to generate tables in R in a subsequent unit."
  },
  {
    "objectID": "content/module-data-intro/data-wrangling.html",
    "href": "content/module-data-intro/data-wrangling.html",
    "title": "Wrangling Data",
    "section": "",
    "text": "In this unit, we will discuss what data wrangling is and why it‚Äôs a very important part of any analysis.\n\n\n\nHow data used to be cleaned before the tidyverse."
  },
  {
    "objectID": "content/module-data-intro/data-wrangling.html#missing-data",
    "href": "content/module-data-intro/data-wrangling.html#missing-data",
    "title": "Wrangling Data",
    "section": "Missing data",
    "text": "Missing data\nIn almost any dataset, there are some missing entries. Data can be missing for different reasons. It could have not been asked/recorded, the person refused to provide it, the variable is not applicable (e.g.¬†for a non-smoker, the variable ‚Äúnumber of cigarettes per day‚Äù might be left blank.) It is important to understand why data are missing and form an action plan based on that. Many analysis approaches do not allow for missing data. In this case, you need to reduce your dataset such that nothing is missing.\nThe easiest approach is to remove all observations with missing data (this is called ‚Äúlistwise deletion‚Äù). That can get problematic if you have a lot of variables, and each variable has some missing values; you might be left with almost no observations that have complete data. Another approach is to exclude them in analyses where they have missing data in variables of interest, but include then in analyses where they have information for all variables that are being considered (this is called ‚Äúpairwise deletion‚Äù). Both of these methods can lead to bias in different ways and should be used with careful consideration.\nAnother option is to remove all variables with missing data from further analysis. Unfortunately, it is common that at least some values are missing for each variable, which means you would be left with no variables. Also, if the variable is important for the question you are trying to answer, you obviously cannot remove it.\nAnother option is to use a combination of removing variables and observations. You could start removing variables with missing values above some threshold, e.g.¬†any variable that has more than 10% or 20% (or some value you pick) missing. There is no rule for this, and you need to justify it. Then once all variables with missing values above some threshold are removed, you remove any remaining observations that still contain missing data. This mix of removing variables and observations might preserve the most amount of data.\nThere are two problems when removing data. One is the obvious fact that you lose data, and thus statistical power. The other problem is if the data are not missing at random. In that case, by removing those observations with missing data, you introduce bias into your dataset. Again, it is important to understand why and how data are missing so you can have an understanding of potential problems such as introducing bias.\nInstead of removing rows and columns (observations and variables) until you have no more missing entry, you can also impute the missing values. Basically, you make an educated, data-driven guess as to what the missing value might have been and stick that value into the missing slot. In principle, any regression and classification method that you can use to estimate and predict an outcome can be used by temporarily thinking of the variable you want to impute as your outcome and the other variables as predictors, and then predicting the missing values. Methods such as k-nearest neighbors or random forest, which we discuss later in the course, are useful for this. Imputation adds uncertainty since you made guesses for the missing values, and often, the estimated/guessed values are randomly drawn from a distribution. Multiple imputation creates several different imputed datasets, and you can then run your analysis on each of those imputed datasets, hopefully with similar results for each. In R, the recipes package, which is part of the fairly new set of tidy modeling tools called tidymodels, and others such as the mice package allow for imputation. We‚Äôll try some of those in a later unit.\nThere are additional options for dealing with missing values. For instance, you could create new variables/predictors/features with less missing. As an example, if you have data that records if a person drinks beer (yes/no), wine (yes/no), or hard liquor (yes/no) and each of those variables has some missing, maybe you can create a new variable labeled any alcohol and code it as yes/no. If a person has a yes for at least one of the 3 original variables, they would be coded as yes in the new one. If they have all no, they would be coded as no. For anyone left, you do need to decide what to do with missing values in the original variables, i.e.¬†if you interpret them as yes or no. You could either stick with one, e.g.¬†if you had some additional knowledge that suggests anyone who doesn‚Äôt have that value recorded is more likely a no. For categorical data, you could also treat missing values as their own category (this can be useful when missingness provides information about the value of the response). Or you can use imputation.\nAs you can tell, missing values can cause headaches. It is therefore very important if you design and collect data to try to do it in such a way as to minimize missing values. Of course, if you analyze data collected by someone else, there is not much you can do, and you have to decide how to deal with missing values. For that, having a good understanding of what the data mean and how they were collected is essential.\nIn R, missing values are coded as NA. When you read data into R and that data for instance codes missing as 99, you should recode to NA. NA in R is a bit tricky, since any operation on NA returns NA. The tidyverse functions tend to be pretty good in dealing with NA, but for base R code you often have to be more careful. The function is.na() is often useful. Some functions, e.g.¬†mean() and sum() can deal with NA if you tell them what to do. Sometimes, functions just deal with NA in some built-in way. You need to check that this is what you want. Always perform careful checks when handling missing values! The tidyverse packages tidyr and dplyr have tools for dealing with missing values during the wrangling process."
  },
  {
    "objectID": "content/module-data-intro/data-wrangling.html#outliers",
    "href": "content/module-data-intro/data-wrangling.html#outliers",
    "title": "Wrangling Data",
    "section": "Outliers",
    "text": "Outliers\nIt is not uncommon to have values in your data that are strange. It could be that someone at data entry made a mistake. Or your codebook doesn‚Äôt properly explain the entries. Other times, it could be real data that is just an outlier. It can be hard to decide if a value is strange but real or a mistake. Dealing with such entries is a judgment call. The best approach is to have a pre-written analysis plan that explains exactly what to do in any such cases. Outside of clinical trials seeking FDA approval, that pretty much never happens. We usually have to decide what to do with strange entries when we run into them. Some good rules are to be consistent, fully document and explain what you do, and if you are able, do it both ways (e.g.¬†do the analysis with the values as they are, and then again with them removed). Of course both ways can quickly turn into a million different ways and at some point, you probably have to stop. However, trying it more than one way can be reassuring if you get pretty much the same answer each time. If you do not get similar results, then you have to be more careful and should describe in your report/paper in detail why and how different approaches to your data cleaning lead to different results."
  },
  {
    "objectID": "content/module-data-intro/data-wrangling.html#censored-data",
    "href": "content/module-data-intro/data-wrangling.html#censored-data",
    "title": "Wrangling Data",
    "section": "Censored data",
    "text": "Censored data\nSometimes data are censored, i.e., you know that the value is ‚ÄúX or greater/less‚Äù but not the exact value. This is common in longitudinal (survival or similar) data. It is also common in experimental data, where you might have a measurement procedure with lower or upper limits of detection. The way to deal with censored data depends on what kind of censored data you have, and what you want to do with it. In R, packages that are used to analyse survival data, e.g.¬†the survival package, can deal with some censored data. Otherwise, you will likely need to google around to figure out how you should deal with the censored data scenario you have and how to do it in R."
  },
  {
    "objectID": "content/module-data-intro/data-wrangling.html#combining-categories",
    "href": "content/module-data-intro/data-wrangling.html#combining-categories",
    "title": "Wrangling Data",
    "section": "Combining categories",
    "text": "Combining categories\nAt times, you might have categorical variables with many categories, and a lot of them have only a few observations. This can make the modeling process problematic. In that case, you might want to consider combining certain categories into larger ones. For instance, if you have a variable for jobs which has many different values, it might make sense to group the jobs into categories (e.g., manual labor, clerical,..). You need to report what you did so readers can decide if this is a reasonable approach. Sometimes you might also want to group all minor categories into an ‚Äúother‚Äù category. For instance if you have a dataset of nicotine users, your main categories might be cigarettes, cigars, chewing, vaping and everything else (whatever that might be, I don‚Äôt know) could be ‚Äúother‚Äù. Note that maybe the way I‚Äôm grouping things here is really dumb. That shows that I‚Äôm not an expert on smoking. You can let the data decide the grouping by looking at numbers in each category, but there is no substitute for some level of expert topical knowledge.\nFor tasks like combining categories, and other operations that have to do with categorical (factor) variables, the forcats R package is a great tool."
  },
  {
    "objectID": "content/module-data-intro/data-wrangling.html#merging-data",
    "href": "content/module-data-intro/data-wrangling.html#merging-data",
    "title": "Wrangling Data",
    "section": "Merging data",
    "text": "Merging data\nIt is not uncommon that your raw data comes in more than one file. It could be multiple spreadsheets of a single study, or it could be different datasets from different sources (e.g., a dataset on asthma cases combined with a different data source that records air pollution levels). In such cases, you will likely need to combine the data. That is often called joining. The dplyr package has a great set of _join() functions that let you do different types of joining of data. Other tools for R exists, e.g.¬†the data.table package."
  },
  {
    "objectID": "content/module-data-intro/data-generation.html",
    "href": "content/module-data-intro/data-generation.html",
    "title": "Making Data",
    "section": "",
    "text": "Overview\nHere we go again. I started to write a unit on creating and using synthetic (artificial/fake/simulated) data. Then it got too big again üòÅ and I decided to make it its own module, which you can find towards the end of the content list sidebar."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#setup",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#setup",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Setup",
    "text": "Setup\nWe‚Äôll also use the same group setup as last week. Assign each member in your group an (arbitrary) number (I‚Äôm calling them M1, M2, ‚Ä¶). The order should be different than last week so you get to interact with a different group member. Everyone will first work on their own and finish their first part by Wednesday. Then M1 will contribute to M2‚Äôs repository, M2 will work on M3, etc. The last person, will work on M1‚Äôs repository. This way, everyone will work on their own and one group member‚Äôs repository."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#part-1",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#part-1",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Part 1",
    "text": "Part 1\nPart 1 is due by Wednesday."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#finding-data",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#finding-data",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Finding Data",
    "text": "Finding Data\nPreviously, you did a quick exploration of a dataset that came with an R package (gapminder data inside dslabs package). A lot of datasets can be found inside R packages. For instance, this page lists what is likely only a small fraction. There is even a Reddit group dedicated to R datasets. The good and the bad about datasets that come with R packages is that they are often fairly clean/tidy. That‚Äôs unfortunately not how most ‚Äúreal world‚Äù datasets look like. Getting dirty and messy datasets and wrangling them into a form that is suitable for statistical analysis is part of most workflows and often takes a lot of time. We‚Äôll start practicing this here by getting data that might or might not be very clean.\nGo to the CDC‚Äôs data website at https://data.cdc.gov/. Browse through the site and identify a dataset of interest.\nWhich dataset you choose is up to you. I suggest you pick a dataset that has at least 100 observations with 5 different variables, and a mix of continuous and categorical ones. Often, 5 variables means 5 columns. That would be the case in properly formatted data. However, some of the data provided by the CDC is rather poorly formatted. For instance CDC‚Äôs dataset on traumatic brain injury has the same variable (age) in separate columns, and it is also discretized. As we‚Äôll discuss, these are two really bad things you can do to your data, so I recommend staying away from such datasets. There are plenty on that website, so I‚Äôm sure you‚Äôll find one that is suitable and interesting to you."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#getting-the-data",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#getting-the-data",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Getting the data",
    "text": "Getting the data\nTo get the dataset you selected, it is easiest if you download the file to your computer and place it inside your portfolio repository. Note that in general, you should make each data analysis (or other) project its own GitHub repository, and always use a structure like the one provided in the Data Analysis Template (or something similar). However, for this small exercise and for logistic reasons, you‚Äôll use your portfolio/website repository, and just a single folder. Make a new folder called dataanalysis-exercise inside your portfolio repository. Place the data into that folder.\n\nRemember that GitHub doesn‚Äôt like large files. So if you pick a large data file (&gt;50MB), first place it somewhere outside your repository, then reduce it by e.g., writing some R code that selects only a portion of the data. Once it‚Äôs small enough, you can place it into the GitHub repository.\n\n\nWhile you should be able to find data for direct download from the CDC website, sometimes you need to write a bit of code to pull data from a source. This is usually done through an API. R has packages that make this relatively easy. If you ever encounter that situation, search online for instructions. Google/Stackoverflow are your friends to figure out what commands you need to write)."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#exploring-the-data",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#exploring-the-data",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Exploring the data",
    "text": "Exploring the data\nNow, write code that explores the data. Add a new Quarto document called dataanalysis-exercise.qmd to the folder you just created.\nStart by providing a brief description of the data, where you got it, what it contains. Also add a link to the source.\nThen write code that reads/loads the data. As needed, process the data (e.g., if there are weird symbols in the data, or missing values coded as 999 or anything of that sort, write code to fix it.) If your dataset has a lot of variables, pick a few of them (at least 5).\nOnce you have the data processed and cleaned, perform some exploratory/descriptive analysis on this cleaned dataset. Make some summary tables, produce some figures. Try to summarize each variable in a way that it can be described by a distribution. For instance if you have a categorical variable, show what % are in each category. If you have a continuous variable, make a plot to see if it‚Äôs approximately normal, then try to summarize it to determine its mean and standard deviation.\nThe idea is that your descriptive analysis will provide enough information for your classmate to make synthetic data that looks similar, along the lines discussed in the synthetic data module.\nRemember to add both text to your Quarto file and comments into your code to explain what you are doing.\nIn a final step, update the _quarto.yml file and include a menu item for ‚ÄúData Analysis Exercise‚Äù pointing to the new file. Follow the format of the existing entries. Remember to be very careful about the right amount of empty space. Re-create your website and make sure it all works and the new project shows up on the website."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#a-new-github-workflow",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#a-new-github-workflow",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "A new GitHub workflow",
    "text": "A new GitHub workflow\nIf everything works as expected, commit and push your changes to GitHub. Instead of using the fork + pull-request workflow we‚Äôve tried a few times, we‚Äôll explore a different collaborative approach. In this approach, you and your collaborator work on the same repository. To that end, you need to add your classmate as collaborator. Go to Github.com, find your portfolio repository, go to Settings, then Collaborators. Choose Add Collaborator and add your classmate. Your classmate should receive an invitation, which they need to accept. With this, they are now able to directly push and pull to your repository, without them needing to create a fork. (You can remove them after this exercise if you don‚Äôt want them to be able to continue having write access to your repository).\nTo avoid any conflicts, once your classmate takes over, you shouldn‚Äôt continue making changes to the repository."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#part-2",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#part-2",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Part 2",
    "text": "Part 2\nPart 2 is due by Friday."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#joining-and-cloning-the-github-repository",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#joining-and-cloning-the-github-repository",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Joining and cloning the GitHub repository",
    "text": "Joining and cloning the GitHub repository\nYou should have received an invitation to be a collaborator on your classmate‚Äôs repository. Accept it, then clone the repository to your local computer.\nFind the dataanalysis-exercise.qmd file. At the bottom, write a comment that says something like This section contributed by YOURNAME. This needs to be there for me to be able to grade your contribution."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#making-synthetic-data",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#making-synthetic-data",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Making synthetic data",
    "text": "Making synthetic data\nTake a look at the descriptive analysis your classmate did.\nNext, produce a new synthetic data set with the same structure as their cleaned/processed data. You are encouraged to use some LLM AI tools to help write the code. If you do, specify in the Quarto document or as comments the AI prompts you are using.\nWrite code that produces synthetic data, then summarizes/explores the data with a few tables and figures similar to those made by your classmate for the original data.\nAdd comments in the code and text into the Quarto document to explain what you did and how close your synthetic data is compared to the original data.\nMake sure everything works and the website renders ok."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#finishing-the-github-workflow",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#finishing-the-github-workflow",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Finishing the GitHub workflow",
    "text": "Finishing the GitHub workflow\nOnce all is done and works, commit, push to GitHub. Note that you are now directly pushing to the original repo. This is easier, you don‚Äôt need to do a pull request (PR). It‚Äôs also a bit more dangerous, since you could potentially create conflicts if the main repository owner or other collaborators work on the repository at the same time.\n\nSome more comments on GitHub workflows:\nIn general, if you work closely with someone on a project, it might make sense to add them as collaborator, and as needed coordinate with them to avoid merge conflicts. Otherwise, telling someone to contribute by forking and sending a pull request is the safer approach, and you have control if you want to accept their changes or not.\nThere is yet another common way to use GitHub, namely collaborators working in the same repository, but with different branches. Think of a branch like a fork, but it happens inside the repository. Work can occur independently on branches, and at some point one can merge branches. This allows people to work in a single repository, but minimizes possible merge conflicts. This approach is standard for larger projects with many collaborators. For this class, we won‚Äôt use branches, but note that they are useful and commonly used."
  },
  {
    "objectID": "content/module-data-intro/assessment-data-finding-wrangling.html#part-3",
    "href": "content/module-data-intro/assessment-data-finding-wrangling.html#part-3",
    "title": "Assessment - Data Finding and Wrangling",
    "section": "Part 3",
    "text": "Part 3\nThe original repository owner doesn‚Äôt really need to do anything. What their classmate added hopefully works and shows up on the website. You still might want to check to make sure that everything is ok.\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it. I will assess both the original contribution and the addition made by the second person."
  },
  {
    "objectID": "content/module-data-exploration/assessment-exploration.html",
    "href": "content/module-data-exploration/assessment-exploration.html",
    "title": "Assessment - Exploration",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-data-exploration/assessment-exploration.html#overview",
    "href": "content/module-data-exploration/assessment-exploration.html#overview",
    "title": "Assessment - Exploration",
    "section": "Overview",
    "text": "Overview\nFor this exercise, we will join the R data science community and participate in Tidy Tuesday, a weekly exercise that allows you to practice some parts of data analysis. The focus for Tidy Tuesday is on getting, cleaning (i.e., tidying) and exploring a data set, though people sometimes do statistical analyses as well.\nThis is another solo exercise, so no group work this week, but of course more GitHub. This exercise will go into your portfolio."
  },
  {
    "objectID": "content/module-data-exploration/assessment-exploration.html#what-is-tidy-tuesday",
    "href": "content/module-data-exploration/assessment-exploration.html#what-is-tidy-tuesday",
    "title": "Assessment - Exploration",
    "section": "What is Tidy Tuesday",
    "text": "What is Tidy Tuesday\nYou briefly encountered Tidy Tuesday before in the Data Analysis Motivation unit. If you can‚Äôt remember, revisit that unit and re-read that section. Also read the information at the provided links.\nIf you want to see another Tidy Tuesday example, see one I did, as part of a previous version of MADA. I did it at the same time the students did theirs. I plan to do my own Tidy Tuesday again this time.\nFor even more examples of previous analyses, see this Shiny app which scans twitter for posts mentioning #tidytuesday. Unfortunately, it doesn‚Äôt seem to be updated/maintained anymore. But you can still find people on social media posting links to their Tidy Tuesday creations (feel free to share yours as well).\n\nApparently, there is now also a Tidy Tuesday podcast. I‚Äôm not familiar with it, so I‚Äôm not sure what it‚Äôs all about and if it is worth checking out. It probably is worth giving it a try, since it‚Äôs (almost) always good to check out new things üòÅ and those are very short recordings, so I think they just introduce the data of the week. If you end up listening to some, let me know what you think (post somewhere on Slack)."
  },
  {
    "objectID": "content/module-data-exploration/assessment-exploration.html#your-tidy-tuesday-exercise",
    "href": "content/module-data-exploration/assessment-exploration.html#your-tidy-tuesday-exercise",
    "title": "Assessment - Exploration",
    "section": "Your Tidy Tuesday Exercise",
    "text": "Your Tidy Tuesday Exercise\nYour assignment is to participate in Tidy Tuesday by analyzing this week‚Äôs dataset. You can start as soon as the dataset is posted, which is Mondays. The datasets are released on GitHub. As I‚Äôm writing this, I have no idea what the data will be the week this exercise happens. That‚Äôs part of the fun of it üòÑ.\nHere are some more detailed instructions:\nUse your portfolio website. Make sure it‚Äôs up to date and fully synced. Open it in R Studio. Then open the tidytuesday_exercise.Rmd file. That‚Äôs where you‚Äôll write your Tidy Tuesday analysis.\nGo to the TidyTuesday Github repository. Look for the dataset for this week, and read the instructions on how to get the data. You will also be provided with a data dictionary. If the data is available for download, place it somewhere in your portfolio repository (e.g., in a new folder called data).\nWrite a Quarto file (with R code as part of the Quarto file or in a separate file) that loads the data, performs any needed cleaning and wrangling, and produces some results (tables, figures). For this exercise, you don‚Äôt need to implement any statistical models. If you want, you can use some simple ones (e.g.¬†adding a linear fit line to a scatterplot, or doing some simple stats tests). This is a fairly open-ended task. I trust that you get the right balance between spending too little and too much time/effort on this. Depending on your coding skills, I think around 2-4 hours (actually working, not having Facebook open at the same time üòÅ) might be a good time commitment.\nOnce done with your Tidy Tuesday analysis, rebuild your portfolio site to make sure everything works and looks good, that all the links work, etc. Then push to GitHub by the deadline. Since this will be on your portfolio website, and I know where to find it, there is no need to post any link this week.\nAnd of course, for this and any other products you create, if you are active online (e.g., social media or LinkedIn), you can post an announcement of your exercise there."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-overview.html",
    "href": "content/module-data-analysis/data-analysis-overview.html",
    "title": "Data Analysis Overview",
    "section": "",
    "text": "Overview\nIn this unit, we will talk about the whole data analysis workflow from a general, big-picture perspective. We‚Äôll then revisit the different components in more detail later.\n\n\nLearning Objectives\n\nUnderstand the elements of a data analysis.\n\nKnow the role and importance of each component of a data analysis.\n\n\n\nData Analysis Workflow\nThe following figure provides a conceptual illustration of the whole data analysis workflow.\n\n\n\nData analysis workflow\n\n\nThe spiral is meant to represent that while the different steps in an analysis are rarely linear, things should improve over time until you have the data and code in a form that everything fits and you can produce final results.\nSome things that might send you around in circles (or hopefully spirals) are:\n\nExploring data indicates that more cleaning needs to happen.\nExploring data indicates that the current question can‚Äôt be answered.\nAn analysis fails and indicates that cleaning/pre-processing wasn‚Äôt adequate.\nA change in analysis method requires different kinds of cleaning and pre-processing.\nEvaluating results indicate that something ‚Äòisn‚Äôt quite right‚Äô and needs to be fixed.\nNew analysis based on a better understanding of the data and how to best analyze the data and present results.\n\nIn the introductory chapter of their great book R for Data Science (R4DS), Garrett Grolemund and Hadley Wickham have a similar diagram. Check out the figure and read through the introductory chapter.\nNote that the process of getting the data ready for analysis goes by different names often used slightly differently by different people. In my figure above, I separated cleaning and pre-processing, while in the R4DS chapter and figure, they distinguish between tidying and transforming and together call them wrangling (some people also say munging). Sometimes, these terms mean a specific part of the cleaning process, sometimes they are used interchangeably. You need to guess from the context what is meant, but often you can think of these terms as all having similar meanings.\nLet‚Äôs look at the different components of an analysis in a bit more detail. The following sections are meant as a conceptual overview. Apart from the Question and Data Match discussion, we will get into each of the other steps and how to do them in more detail later in the course.\n\n\nQuestion & Data Match\nMaybe the most important challenge of a good scientific project (and thus an interesting and useful data analysis) is to find a combination of questions and data that lead to a project that is both interesting and doable. One could argue that having a good question and suitable data to answer it is part of science/research but not strictly part of the analysis. However, for your data analysis to be meaningful, it is critical to get the question-data match right.\nIf you start with a boring (or dumb) question or with data that is essentially garbage/noise, or you have a bad question/data match, no part of your analysis matters!\nThere are different approaches to finding a question-data match. The classical one is to have a specific question or hypothesis, then design a study to collect data, and analyze the collected data to address that specific question.\nIf you have a question, but you do not have the resources to collect data specifically to answer your question, you can look for existing data that might help you answer your question. You rarely find data that allows you to answer your initial question exactly. But you might find data that enables you to answer a very similar question. You might need to iterate a few times by modifying your question and changing the datasets you use before you have a good match, i.e., a good question and data that allows you to answer it.\nAnother approach that is becoming more common is to start with the data. You might come across one or multiple datasets that interest you. You can then investigate the data to see what questions you might be able to answer with it. Since the data was not collected to answer your specific question, you will have to be somewhat flexible with the question and let the data guide your analysis. This approach can be very powerful since more and more data are becoming available for analysis.\nAny of these different approaches for coming up with a good data-question match are valid. It is important to keep in mind that unless you start with the question and analysis plan, then collect the data, then follow your plan exactly, the usual statistics (e.g., p-values and such) do not really apply.\nBy fiddling with the data or adjusting data and question, you are using what is sometimes called investigator‚Äôs degrees of freedom or less kindly, data dredging or data torturing. Essentially, if you fiddle with your data and question long enough, you will eventually find something statistically significant. You cannot interpret this in the same way as a statistically significant result obtained by pre-specifying the question, collecting data specifically to answer the question, and analyzing the data to answer the initial question.\n538‚Äôs website provides a nice example called Hack Your Way To Scientific Glory that illustrates how you can adjust your question and data in very reasonable ways so you can get whatever statistically significant result you want. This is researchers‚Äô degrees of freedom in action, and the main reason why p-values are most often meaningless.\nThat doesn‚Äôt mean you can‚Äôt do and publish interesting work based on such a secondary data analysis approach! But it does mean that whatever you find should be considered preliminary: hypothesis generating, not hypothesis testing.\nAgain, unless you fully pre-specify your hypothesis and your exact analysis plan (which is almost never done outside of clinical trials that seek regularory approvel), statistical measures such as p-values are largely meaningless. Unfortunately, to get published, you often need to include those. It‚Äôs a sad state of affairs that a lot of published work comes with junk statistics. It will take a good bit of time (if it ever happens), before things become more reasonable (and we will get rid of p-values in most situations).\nFor some more thoughts on this topic, read chapters 1-3 of The Art of Data Science (ADS). It is a pay what you want book with a minimum price of zero, which means you can get it for free (note that the book + video bundle has a non-zero minimum price). Chapter 2 has a figure similar to the one above.\n\nWhile I am generally in favor of supporting creators of books (or other content) and thus paying for their products, in this case since I‚Äôm making you get/read the book, and also since students have very limited budgets, I think it is entirely acceptable for you to get the book for free.\n\n\n\nGetting and loading data\nIn some sense, the best, but also the most expensive and time-consuming way of getting data is to collect it yourself. Doing so ensures that the data can answer the question you have. You can also control the quality of the data that is being collected ‚Äì at least to some extent. However, it is often not feasible to collect your own data. You might not have funding or time or interest to do so. The next best source of data is directly from someone who collected it. The advantage of such data is that you can ask someone for clarification. Also, the person who collected the data is likely a subject matter expert, who can help you generate better questions. Finally, you can get data that is generally available, e.g., in some online resource. This gives you access to a lot of different datasets. The drawback is that the data was not collected to answer your specific question, and there is usually nobody you can ask for clarification.\nNo matter the source, you need to get the data into your favorite data analysis system (in our case, that will be R). Sometimes, you get data in a format that can be read in easily, e.g., a comma-separated CSV file without any strange formatting that messes up the import. At other times, you might get data in a collection of terribly formatted Excel spreadsheets, or you get it by scraping data from some online source. We‚Äôll look at ways of getting data into R in a later module.\n\n\nExploring and cleaning data\nOnce you have the data loaded into your analysis software, such as R, you want to explore the data to get an overall idea of what is going on. At the same time, you‚Äôll likely have to engage in data cleaning.\nData exploration can and should be done both graphically and by looking at text output of various summaries describing your data, e.g., summary tables for all variables. We will get into that in more detail later. As you explore, you will likely find that your data is dirty, i.e., there are things wrong with it. The amount of data cleaning (also called tidying or wrangling) you need to do depends very much on your data.\nA general rule is that you should never directly edit your raw data. If you can, load the raw data into R and do all cleaning inside R with code, so everything is automatically reproducible and documented. Sometimes, you need to edit the files in the format you got before you are even able to load them into R. In those cases, you might have to make modifications in a software other than R. If you can‚Äôt directly read the data into R and need to make some changes before, make copies of your raw data and ONLY EDIT THOSE COPIES. Also, write down the edits you made.\nThere are certain cleaning actions you will need to do with almost any dataset. You will have to find and deal with data entry errors. If the mistake is obvious, you can fix it. If you know the value is wrong but don‚Äôt know what it should be, you might need to discard that data. Sometimes you get data that seems way outside the range of all other values, but it is not obviously wrong. For instance, if you had a group of individuals and their weights, a person with a weight of 8000 pounds is obviously not possible. Someone with an 800 pound weight seems wrong, but it‚Äôs not impossible. You will have to decide how to deal with such outliers, either during the cleaning or the analysis stage.\nIn almost any datasets, some values are missing. First, you need to figure out how missing values are coded and what missing means. Sometimes an empty entry means it was not recorded. At other times, it could be that it is not applicable. For instance on a survey that asks if someone ever smokes, and the next question is for how long, if they say they never smoked, the ‚Äúhow long‚Äù question is moot and might be empty. For your analysis, you might want to code it as a duration of 0. We‚Äôll talk more about missing values later in the course.\nAs you do all the cleaning, you need to get your data into a shape that is ready for analysis. If you apply more than one analytic method, you might need different clean versions of your dataset.\n\n\nPause and ponder\nAt pretty much every stage of your analysis workflow, you should include short pauses where you sit back and briefly contemplate what it all means and what is going on. Careful thinking and reflection is the most important (and unfortunately most under-used) component of a data analysis.\nSomewhere during the exploring and cleaning process, you might want to take a longer break. You should have come to a stage where you will be fairly sure if your project is feasible or not, i.e., if you can use the data to answer your question. If not, alter or abandon. Ideally, you will want to alter. That means either changing your question to one that is similar (or sometimes quite different) to your original question, and that can be answered with the data. Or, if feasible, it is to change the data, i.e., get a different dataset or another dataset that contains information which, when combined with your first dataset, lets you answer the question.\nSometimes, you might find that changing the question or data is not possible or will lead to a project that is of no interest to you. In that case, you should abandon your analysis. This is hard, but the honest thing to do.\nDon‚Äôt keep going hoping against all hope that ‚Äúmagic might happen‚Äù and you‚Äôll get a good final product after all. If you keep going, you will get some results. Often, you might even be able to publish them. Unfortunately, they will be useless and wrong - which is a common occurrence in the academic literature - and probably even worse outside of the peer-reviewed academic literature.\n\n\nPre-process Data\nAdditional data processing could be considered part of the cleaning process, or you can think of it as a separate step. Depending on the analytic method you use, some further data processing might be suitable. For instance, some methods work better if you center and scale your predictors. Some methods, such as linear models and related approaches, are not able to handle missing values, so you need to decide how to deal with those. Sometimes it might be good to not include all variables in your model, or to use the existing variables to create new ones (e.g., instead of including height and weight in your model, you might decide to compute BMI and use that variable in your model instead.) The specific pre-processing steps that need to happen depend on the statistical modeling approach(es) you plan on using.\n\n\nAnalyzing Data\nYou finally reached the step where you can apply actual statistical analysis to your data. This topic takes up the majority of time in most (bio)statistics courses. While it is obviously very important to get the statistical analysis right, for any real data analysis you will likely spend &lt;25% of your time on this task. The majority of your time (though not computing time) goes into the other tasks.\nAt this stage in the process, you should know your data well enough to apply statistical methods and approaches that are suitable for your data and question. You can either use a single approach or explore multiple different ones. As you will learn in this course, trying more than one approach is quite easy. If you try multiple models, you need to be careful to not overfit by only picking and reporting the models and analyses that give you the results you want. If you analyze data in multiple ways, report all results.\n\n\nEvaluating and interpreting results\nOnce you have your results, and before you decide on how (or if) to report them, you should take another break and carefully contemplate what your findings mean. Do the results make sense? Occasionally, there is some new and interesting finding that goes against everything we think we know. However, most of the time if you find something that doesn‚Äôt ring true (e.g., increased exercise shortens your lifespan), it probably isn‚Äôt. You need to think carefully about what your result actually means and how to interpret results. We can certainly think of a way to get a result that suggests increased exercise shortens lifespan. For instance, we could look at a group of people who practice some kind of extreme sport that not infrequently leads to death, and compare that group to people who do not practice this sport. You might find that those with increased exercise live less long in that particular scenario. But of course, that doesn‚Äôt mean you have overall shown that normal exercise shortens lifespan.\nUnfortunately, the scientific literature is full of examples where the interpretation of the results are likely not right. A good and amusing example was published in the New England Journal of Medicine, a very prestigious journal, by Messerli 2012. The showed that chocolate consumption and Nobel prizes were correlated. While there is no arguing with the data, the interpretation that ‚Äú‚Ä¶it seems most likely that in a dose-dependent way, chocolate intake provides the abundant fertile ground needed for the sprouting of Nobel laureates‚Ä¶‚Äù is of course nonsense, see also this article for a more general discussion. You can find more rebuttals online. Unfortunately, since the original study was in a prestigious journal, the nonsense was widely reported in the media. You can see Forbes, BusinessInsider, Time Magazine, and some other news outlets reporting the finding fairly uncritically. To this day, I do not know if this article was meant as a satire or for real.\n\n\nReporting and communicating your results\nUnless you do an analysis just for yourself, you will want to communicate your findings to others. In academia, this is usually done with peer-reviewed publications, as well as presentations at conferences in the form of talks (for which most people use slides) and posters. In those settings, the readers or listeners are usually your peers.\nOutside of academia, you might also need to present your work in written or oral form, but it might have a different target audience. For instance, you might want to communicate your findings to a lay audience, to your superiors, to groups of stakeholders, to colleagues and team members, etc.\nWhile you will need to tailor your products and way of communication to your situation and audience, there are components that are common. Figures, tables, and descriptive text are almost always used.\nYou want to make as much of your reporting automated as possible. For that, all figures, tables, and numeric results should be auto-generated by code. Similarly, for an efficient and automated workflow, you will want as many aspects of the products (reports, slides, etc.) as possible to be automated. Of course, you have to write the text yourself. But results should be loaded automatically, and text layout should be automated, as should formatting of references. You also want to use a system that is as flexible as possible. (R) Markdown is ideal because you are writing in plain text and can easily send the text into different output formats, e.g., reports, slides, posters, websites, blogs, etc. This is much harder to do with other common software. E.g., going from Word to Excel to Powerpoint is much less seamless and certainly not automated.\nYou want automation for both efficiency and reproducibility purposes. Especially if you design an analysis that might be repeated often (maybe not so common in academia, but very common in other settings), you want to be able to automatically re-run and regenerate as much of the whole process as possible.\n\n\nFurther reading\nAbove, I gave you my thoughts related to the various steps of the data analysis workflow. It‚Äôs good to hear it from multiple sources. To that end, go back to The Art of Data Science (ADS) and read through chapters 4 and 9-11.\n\n\nSome practice\nThere is code shown in chapter 4 of ADS. If you want, you can try to work along and reproduce what they are showing. This is optional. If you want to try and work along, note that the link provided in the book to download the data does not work anymore (not easily reproducible!). I did find the data, which you can download as a zip folder. Download and unzip and you should be good to work along.\nIf you code along, I suggest you start a new R script and place their code into that script (instead of copying it into the console). We‚Äôll talk about that more soon. You will need to slightly adjust the code shown in the book to make it work. E.g., if you don‚Äôt place the data into a data folder, you will need to change the path in the read_csv command. Also, for that command and some others, the book has a formatting with + signs across multiple lines of code, that might not work if you just copy and paste their code, and you might need to remove them and move everything into a single line of code. You can write everything into a single line and delete all + signs.\nYou will also notice that the data is different than what they report. Their book was published on 2018-06-22, but on the website where the data comes from, a time stamp of 2019-05-07 is listed. Thus the data got updated after the book was published, again making things not quite reproducible. (Changes to data are common, which is a reason why an automated and reproducible workflow is so important!)\nFinally, there are minor errors/typos in the book, e.g., what the code does and what the text says are not the same when looking at time (page 41).\nOverall, while you should be able to reproduce most of what they show, it will be slightly different since the data has changed, and you will need to make adjustments to code. If you have no R coding experience, trying to get it to work might be too tedious, so skip for now and consider revisiting later once you learned some R coding."
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html",
    "href": "content/module-data-analysis/assessment-data-analysis.html",
    "title": "Assessment - Data Analysis",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#more-information",
    "href": "content/module-data-analysis/assessment-data-analysis.html#more-information",
    "title": "Assessment - Data Analysis",
    "section": "More information",
    "text": "More information\nSome more information on forks and pulls and what to do if things don‚Äôt go right can be found in happygitwithR. Note that a lot of the commands described for use on the command line (e.g.¬†stash) can be applied graphically through GitKraken.\n\nGithub also has branches. Those are similar to forks but meant for more internal use. For instance, if you have a project and want to implement something new, but it might not work, you can create a branch, work in that branch, and once everything is ok, you can merge the branch into the main/master. This is useful if you write software that others are using, and you don‚Äôt want to break the whole thing. It is also helpful if you work with collaborators on a project. To be able to use branches, you need to be an owner or member of a repository. In contrast, you can fork any public repository."
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#another-fork-and-pull-exercise",
    "href": "content/module-data-analysis/assessment-data-analysis.html#another-fork-and-pull-exercise",
    "title": "Assessment - Data Analysis",
    "section": "Another fork and pull exercise",
    "text": "Another fork and pull exercise\nThis is optional. You can do it at any time during this course (and more than once) üòÅ.\nHelp improve the course with your contributions! Find something wrong/unclear/worth improving with this course (e.g.¬†a typo, something confusing, a broken link, a suggestion for a new reference, or anything else). Go to the Github repository for this course. Follow the steps outlined above: Fork the course to your personal account, clone it to your local computer, implement your updates, push it back to GitHub, then initiate a pull request. I will get a notification of your pull request. If things look ok and no conflicts exist, I will merge your improvements into the course. And just like that, you have contributed to improving this course! (And of course, you will be listed in the Acknowledgments section of the main course page.)\nAnother option for helping to improve the course website is to file a GitHub Issue. Feel free to do so any time during the course to let me know of anything that needs fixing."
  },
  {
    "objectID": "content/module-coding-basics/rcoding-basics.html",
    "href": "content/module-coding-basics/rcoding-basics.html",
    "title": "R Coding Basics",
    "section": "",
    "text": "Overview\nIn this module, we will talk a bit about writing code in R. Since this is not a learning R class, we can unfortunately only dedicate one module to it, everything else will be learning by trying/doing.\n\nFor those among you who have not used R or any other programming language before, this unit (and everything else related to R in this course) will be time-consuming. Be prepared to put in a good bit of time and effort. Budget your time accordingly and plan ahead! If you do, I‚Äôm fairly certain you will find it worth it. If you are not able or willing to allot the time needed to learn enough R to make things work, this course might not be ideal for you.\n\n\n\nLearning Objectives\n\nGain starting knowledge of R coding\nBecome familiar with the tidyverse\nBecome familiar with resources that help you learn R\n\n\n\nLearning to code\nR (or any other programming language) is best learned by ‚Äúdoing it‚Äù. You will learn more R as we go along, but the focus of the class is on data analysis, so while I will provide you with resources to figure out the R bits, outside of this module we will not focus on ‚Äòlearning R‚Äô. You will learn by doing as we go through the course. As with anything, the more you practice, the better you will get. You should approach learning to code with an attitude of fearless curiosity. You will get stuck, you will get frustrated with some weird error message in your code (still happens to me at least once a week), and you will eventually figure it out. Make use of the great resources that are out there. They are all listed below and in the Resources section.\nThis figure illustrates the journey of learning to write code:\n\n\n\nThe journey of learning to code. Source: https://www.thinkful.com/blog/. Original post is not online anymore.\n\n\nMy goal is that during this course, you will reach the beginning of the upswing of awesome, at least when it comes to being able to use R to perform data analyses. But to get there, you‚Äôll have to go over the cliff of confusion and through the desert of despair, and I‚Äôm confident that you‚Äôll get there and won‚Äôt be stuck in the hand-holding honeymoon. In fact, at times I‚Äôm providing you less detailed instructions than I could to get you quickly to the stage where you have to figure out bits yourself. I guess one could say that instead of hand-holding, I let you stumble and fall some, and then will help you to get back up üòÉ. It might feel a bit more frustrating at least initially, but it‚Äôs a much better way to learn.\nIt goes without saying that learning to code (or learning anything else) is not a linear process. Even after many years of coding and using R, I regularly encounter the cliff of confusion and the desert of despair if I‚Äôm trying to do something new that I haven‚Äôt done before and invariably get stuck.\n\n\nLearning the basics of R\nThis course is not a learn-to-code course. Therefore, we‚Äôll be writing code as-needed as we go along. Depending on your level of R coding experience, this might go very quickly or rather slowly. Be prepared to spend a good bit of time learning some coding. I think it‚Äôs a skill that‚Äôs worth the effort üòÉ.\nIf you have no prior R coding experience, you might want to try a few of the swirl tutorials or work your way through the beginning chapters of Hands-On Programming with R. Chapters 1 through 3 of IDS are also a good option.\nAlternatively, pick any other source you like, e.g.¬†those listed in the resources section of this course, or anything else you find. (If you find something not listed that is really useful, feel free to share!) Of course, AI tools can also be very helpful, but to use them well you do need to know some coding yourself.\n\n\nMore R coding\nWe only focus explicitly on R coding in this module. That is obviously not enough to become proficient at it. For the rest of the course, I expect you to learn the coding bits you need to get things done on the side. I understand this will be a challenge. Don‚Äôt hesitate to seek help. The Posit Recipes cover many of the coding related topics we will use in this course, so go through any and all resources you find useful whenever you are able (or when you need to know). I also strongly encourage AI tools and StackOverflow and other online resources, which Google will help you find fairly easily.\n\n\nGetting help\nMaybe the most important skill for learning any programming language is figuring out how to find and get help with any problem. Google, StackOverflow, and the Internet are your friend. The Posit Community Forum is also a great place to ask for help.\nIf you have a problem with your code, it‚Äôs likely someone else had the same/similar problem before you and asked a question (and probably got an answer). So search the web, and you‚Äôll find something useful most of the time.\n\n\n\nFortunately rare for R. Source: xkcd.com.\n\n\nIn those rare cases when you cannot find information online that helps you figure out your problem, feel free to ask for help.\nA great source for answers is asking questions online. Sometimes, people complain that replies to questions they ask online are unfriendly or harsh. While this is at times true, consider that all the people providing answers are volunteers. They‚Äôre doing it because they want to help others, they don‚Äôt get paid for it. It is therefore important that the person asking the question does not waste people‚Äôs time by asking poorly formulated questions or questions that have been previously answered. In general, those kinds of questions get rude replies. If you have done your homework (i.e., searched online first to see if the answer is already available) and can precisely formulate the question/problem, ideally with a reproducible example, the chance that you get an unfriendly reply is very low.\nI have found that a good way of posing question is to write something like this: ‚ÄúI need help with SPECIFIC PROBLEM, I have searched around and found LINKS/DESCRIPTION OF SIMILAR ISSUES but that doesn‚Äôt quite solve my problem yet.‚Äù If you have a coding problem, add ‚ÄúHere is some code illustrating what I want to achieve and where the problem is.‚Äù and then add a minimal reproducible example.\nThe more you show you‚Äôve done your homework and are truly stuck (instead of just being lazy and wanting others to do the work for you), and the easier you make it for others to understand what your problem is, the more likely you will get good answers.\nIf you want to learn more about how to ask a good question online, check out this video by Roger Peng.\n\n\n\n\nAsking AI for help can often faster and more efficient than trying to find the answer via Google/StackOverflow. So it‚Äôs a good first stop, and if the AI is not helpful, move on to other sources.\n\n\nSome further comments\nAs you continue on your coding journey, keep in mind: The great thing about programming is that you (usually) can‚Äôt really ‚Äúbreak‚Äù things too much. In the worst case you get an error message. So experiment and try out anything you like!\nR has a bunch of quirks. You‚Äôll likely encounter a number of them. A common one is that there are two ways of assigning something to an object. One can write x &lt;- 2 or x = 2 and often (but not always) you can use either. People argue about which way to do it. You‚Äôll see both versions used frequently. If you are completely new to programming, I recommend the first version, i.e.¬†x &lt;- 2. The problem is that most other programming languages do it the second way, so if you learned to code in another language first (like I did), it‚Äôs more natural to write x = 2. It‚Äôs your choice. Just be aware that both notations exist. When I write code, I usually do a mix (without any logic to it). That‚Äôs of course a bad idea, so don‚Äôt do what I do, instead try to pick one way or the other and stick with it.\nMore confusion: In R, you can also write this in the opposite direction, i.e.¬†2 -&gt; x. This is unususal coding style and I recommend avoiding it. Just know that it exists too."
  },
  {
    "objectID": "content/module-ai/ai-write-code-r/ai-write-code-r.html",
    "href": "content/module-ai/ai-write-code-r/ai-write-code-r.html",
    "title": "AI tools for code writing",
    "section": "",
    "text": "Overview\nIn this unit, we discuss using LLM AI tools to help write code.\n\n\nLearning Objectives\n\nKnow how and why to use current LLM AI tools to help with coding.\nBe familiar with several approaches of LLM AI assisted coding.\n\n\n\nIntroduction\nIf you write code, there are (at least) two major parts to the effort. First, you have to figure out what exactly you want to accomplish with your code. Second, you need to write a bunch of commands in the programming language of your choice to get what you are hoping to accomplish. The first part is generally the intellectually more challenging one, and a step which AI (currently) are not very good at. The second part is generally less hard, but it can be very tedious, especially if you are new to coding or if you need to write a lot of code. AI are getting pretty good at helping with writing code.\nI expect that soon, instead of people writing code, most code will be written by AI under the guidance and direction of people. This should eventually lead to much more efficient, and potentially also better code. We aren‚Äôt quite there yet, but the current AI tools are already quite useful for helping with code tasks, so you should use them.\n\n\nGood prompting\nTo get good results from the AI, it is important that you be as specific as you can with your prompt.\nTry this prompt with one of the LLM AI:\n\nWrite R code that generates a scatterplot and a violin plot.\n\nYour result might or might not be close to what you had in mind. If you are not providing a lot of details, the AI can decide what to do and sometimes it is close to what you had in mind, but often it is not.\nNow try this prompt:\n\nWrite R code that generates a dataset of 100 individuals with ages from 18 to 49, BMI values from 15 to 40 and smoking status as yes or no. Assume that age and BMI are uncorrelated. Assume that smokers have a somewhat lower BMI. Then use the patchwork R package to generate a panel of ggplot2 plots. The first panel should show a violin plot with BMI on the y-axis and smoking status on the x-axis. The second panel should show a scatterplot with age on the y-axis and BMI on the x-axis. Add thorough documentation to your code.\n\nWhen I gave the second prompt to ChatGPT 3.5, it gave me this fully working code. It is possible that when you try this, the code will look slightly different. Remember, these AI tools are not fully deterministic and can produce different results each time. Hopefully, what you get will run. If not, you might need to either fix the code or ask the AI to fix it (we‚Äôll look at that in another unit).\nAs you can see, good prompts are often quite long. It makes sense to write those down outside the AI first. That also helps somewhat with reproducibility. I generally try to stick AI prompts at the top of my R/Quarto files, or into a separate file.\nYou also notice that for the second prompt, I had to know more about the programming language, for instance I had to know that there is a package called ggplot2 and one called patchwork.\nYou will find that the more you know in general about a topic, the more useful those AI tools become. This means that you still have to learn some coding (or whatever the topic is) and understand it enough on a big picture level to be able to be useful. But you don‚Äôt necessarily need to be an expert.\nAn analogy I like is that of a composer. A composer needs to know enough about the various instruments of an orchestra to be able to write music for each instrument. But they don‚Äôt need to be able to play each instrument. Similarly, you need to know enough about coding or whatever the topic is you are working on to compose prompts for the AI and evaluate what it produces, but you don‚Äôt necessarily need to be an expert coder.\n\n\nIterating\nIt‚Äôs rare that you get exactly what you want from the AI with your first prompt. Quite likely, you realize that you weren‚Äôt specific enough, or that you really wanted something slightly different but didn‚Äôt properly specify it in the prompt. Often, the code might also not quite work. The AI might have just made up a package or function that doesn‚Äôt exist, or otherwise introduced mistakes.\nWhile it would be nice to get a great product on the first try, the process is so fast that it doesn‚Äôt matter much. Just try again. You can either update your prompt and feed it to the AI again. Or you can tell it what changes you want to make. Try this as a starting prompt:\n\nWrite R code that generates a dataset of 100 individuals, half of them female, with ages from 18 to 49, weight from 100 to 500 pounds and BMI from 15 to 35. Make a figure that shows weight on the x-axis, BMI on the y-axis, and stratification by sex. Add thorough documentation to the code.\n\nWhen I fed that to Bing in ‚Äúprecise‚Äù mode it produced this code. Note that it didn‚Äôt really give me an R script, I copied what it produced into an R file.\nThe code runs and produces a figure, but there are problems. Right now, the data is generated assuming each variable is independent of the other. We know that weight is strongly correlated with BMI, since it‚Äôs part of the equation defining BMI. We also know that males are generally heavier. We can easily fix this. One approach is to update your prompt and make it more explicit, such as:\n\nWrite R code that generates a dataset of 100 individuals, half of them female, with ages from 18 to 49, weight from 100 to 500 pounds and BMI from 15 to 35. Assume that males are generally somewhat heavier than females. Also assume that BMI and weight are positively correlated, based on the definition of BMI. Make a figure that shows weight on the x-axis, BMI on the y-axis, and stratification by sex. Add thorough documentation to the code.\n\nWith this updated prompt, I got this code, here are the relevant bits:\n\n# Generate a dataset of 100 individuals\ndata &lt;- data.frame(\n  # Generate sex variable with half of them female\n  sex = rep(c(\"male\", \"female\"), each = 50),\n  \n  # Generate ages from 18 to 49\n  age = sample(18:49, 100, replace = TRUE),\n  \n  # Generate weight values from 100 to 500 pounds\n  # Assume that males are generally somewhat heavier than females\n  weight = c(runif(50, min = 100, max = 400), runif(50, min = 200, max = 500)),\n  \n  # Generate BMI values from 15 to 35\n  # Assume that BMI and weight are positively correlated\n  bmi = c(runif(50, min = 15, max = 25), runif(50, min = 25, max = 35))\n)\n\nThere are several issues. It labeled the first 50 individuals as males, but assigned them the lighter instead of heavier weight. It also didn‚Äôt properly create BMI values as function of weight.\nWe could of course modify the prompt above further, trying again. Or we can directly iterate at the prompt and ask the AI to update the code. Here is an example:\n\nUpdate the code such that males and females are randomly distributed, make sure that males are on average heavier than females, and compute values for BMI as a function of weight with some random variation added.\n\nWith this prompt, I got this further updated code, here are again the relevant bits:\n\n# Generate a dataset of 100 individuals\ndata &lt;- data.frame(\n  # Generate sex variable with males and females randomly distributed\n  sex = sample(rep(c(\"male\", \"female\"), each = 50)),\n  \n  # Generate ages from 18 to 49\n  age = sample(18:49, 100, replace = TRUE),\n  \n  # Generate weight values from 100 to 500 pounds\n  # Make sure that males are on average heavier than females\n  weight = ifelse(sex == \"male\", runif(50, min = 300, max = 500), runif(50, min = 100, max = 300))\n)\n\n# Compute values for BMI as a function of weight with some random variation added\n# Assume a height of 1.75 meters for simplicity\n# BMI = weight(kg) / height(m)^2\n# Convert weight from pounds to kg (1 pound = 0.453592 kg)\ndata$bmi &lt;- (data$weight * 0.453592) / (1.75^2) + rnorm(100)\n\nThat looks better. However, when you try to run it, you‚Äôll find that it doesn‚Äôt work. The problem is that it tries to use the variable sex before that data frame is defined. So we need to fix that code. We could of course do it by hand, but we can also see if we can get the AI to fix its own code. Which brings us to the next unit on using AI to fix code.\n\n\nSummary\nLLM AI are very helpful at assisting with writing code. The more you know what you want, and the more specific you can be (which requires some level of subject matter expertise) the better your results. Rarely do you get exactly what you want on the first try, but iterating is easy and fast.\nThe list of tips provided in an earlier unit for general LLM AI use also applies for using it as a coding helper. Here is a version of that list again:\n\nBe as detailed and specific as possible.\nIterate. Either only AI based iterations or a mix of manual and AI iterations.\nTry different AI engines or settings or prompt phrasings.\nAsk the AI to add a lot of comments into the code to explain what each line of code does.\nBreak down big tasks into smaller tasks, ask the AI to solve the smaller tasks, then put it together.\nWrite down your prompts so you and others can go back to them later and see what you did.\n\n\n\nFurther Resources\nSee the AI resource unit."
  },
  {
    "objectID": "content/module-ai/ai-resources/ai-resources.html",
    "href": "content/module-ai/ai-resources/ai-resources.html",
    "title": "A collection of links to AI resources",
    "section": "",
    "text": "Overview\nThis unit consists mainly of a collection of links to AI resources that might be useful. I tried to group them into themes and provide a bit of commentary. The focus is on generative AI tools.\n\n\nLearning Objectives\nBy the end of the unit, you should:\n\nBe aware of a few places where you can learn more about AI.\n\n\n\nIntroduction\n\n\nAI in general\n\nOne Useful Thing is a Substack blog by Ethan Mollick where he discusses ideas around (generative) AI from a very applied user perspective. You might find a lot of what he writes echoed in my writing, because that‚Äôs where I get a lot of my information from üòÅ.\nUnconfuse Me is a podcast by Bill Gates that covers all kinds of topics, but has a few interesting AI-themed ones, namely (as of this writing) episodes 2 and 5.\nDeepLearning.AI is a platform/company that offers a lot of teaching material related to AI. Some of the materials are free.\nA really good (but non-free) book is Artificial Intelligence: A Guide for Thinking Humans. The author, Melanie Mitchell, also has an AI blog that is worth checking out.\nThis Coursera course gives a brief, non-technical introduction to current LLM AI tools (such as GPT).\n\n\n\nTechnical aspects of AI\n\nThe Deep Learning Chapter of HMLR provides a nice basic introduction.\nThe Statquest Youtube channel has multiple videos with nice explanations on various AI related topics, at times going fairly deep into the technical details, while still being rather easy to understand.\n\n\n\nAI and R\n\nThese materials from a recent workshop by Phil Bowsher describe using Tensorflow to do deep learning/AI in R.\n\n\n\nAI and teaching\n\nWharton Interactive is an initiative by Ethan and Lilach Mollick and colleagues at Wharton that - among other things - looks at the use of AI for teaching and learning. The website contains some interesting ideas and links to papers that look further into AI use.\nMIT has a nice collection of AI learning resources. The focus is on using those tools for teaching/learning.\n\n\n\nAI and Coding\n\nGitHub Copilot is meant specifically for coding. It is generally not free, but if you get the GitHub student developer pack you get it included. The GitHub Copilot documentation contains a lot of good information to get started and to dig deeper."
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html",
    "title": "Introduction to current LLM AI tools",
    "section": "",
    "text": "This unit provides an introduction to the use of Large Language Model (LLM) based Artificial Intelligence (AI) tools (such as ChatGPT) for coding and data analysis."
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#privacyconfidentiality",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#privacyconfidentiality",
    "title": "Introduction to current LLM AI tools",
    "section": "Privacy/Confidentiality",
    "text": "Privacy/Confidentiality\nWhen you use AI to help with data analysis, you might want to show your data to the AI and ask it to do something with it. The problem is that pretty much all these systems take your data and move it to the servers of whichever company you use for processing. Often, these companies keep your data to help improve their models. If you have data which is sensitive, e.g.¬†human data, or data that you don‚Äôt want to share, then you need to be careful not to let the AI have access to the data. A good solution is to generate synthetic data that has the structure of your real data but is made up. Then you can ask the AI to process this synthetic data and give you the code it generated. In a later step, you can go ‚Äúoffline‚Äù and use the code the AI helped you write on your real data. We‚Äôll discuss how to generate synthetic/artificial data shortly.\nBeyond the data, it is important to keep in mind that if you have the AI running (e.g., through the R Studio CoPilot integration), it might access not just your current file but possibly also other sources you link to, e.g.¬†other stuff on your computer. If you have things you don‚Äôt want the company to ‚Äúgrab‚Äù and copy to their servers, then be careful about what you let the AI access.\nIn general, be careful what information you let the AI access and be aware that it might end up on the server of whatever company is running the AI tool."
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#non-working-code",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#non-working-code",
    "title": "Introduction to current LLM AI tools",
    "section": "Non-working code",
    "text": "Non-working code\nWhile most of the AI tools have become pretty good at coding in R and other languages, at times they don‚Äôt get it right. You might get code back that doesn‚Äôt work. AI are known to hallucinate, i.e., make up stuff. For instance it‚Äôs not uncommon that the AI invents an R command that does not exist. So when you get your code back, you will often need to do some trouble-shooting. At times, you can tell the AI that the code is not correct and ask it to fix it. If this doesn‚Äôt always work, then you can make the fixes by hand or try to reformulate your request."
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#reproducibility",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#reproducibility",
    "title": "Introduction to current LLM AI tools",
    "section": "Reproducibility",
    "text": "Reproducibility\nThe companies making AI tools constantly update and improve the algorithms. Further, the underlying methods often have random components. This means that if you give the same instructions to an AI tool on different occasions, the results/code you get might differ. This means things might be non-reproducible.\nWhile I encourage you to keep track of any input prompts you use to have the AI generate code, note that providing those prompts does not allow someone else (or future you) to exactly reproduce things. Thus, while AI tools can be useful helpers during the data analysis process, they should not be considered part of the final workflow, which instead should contain results/code (possibly generated with AI help) which can run in such a way to allow full reproducibility."
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#ethics-of-using-ai",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#ethics-of-using-ai",
    "title": "Introduction to current LLM AI tools",
    "section": "Ethics of using AI",
    "text": "Ethics of using AI\nHere I‚Äôm not talking about the fact that most AI tools were trained on data without the permission of the original data generators, or that a lot of AI algorithms produce biased (unethical?) output. Those topics are too complex to tackle here. Instead, I‚Äôm talking about the ethics of using AI to help with your data analysis, or more generally your academic/professional work.\nSince these tools are so new, nobody knows yet how to properly acknowledge AI help. I‚Äôm not sure either. If I search Google for an example to help with my code, and find something on StackOverflow that I use as basis for my code, should I cite it? I sometimes do add a link to the original post in my code. Partly to give credit, and partly to remind myself where I got it from. But I don‚Äôt think there are clear rules on this.\nSimilarly, if you copy text from Wikipedia or some other source, you need to cite it. But if you read it, then repeat in your own words, when do you need to cite it? I don‚Äôt think it‚Äôs clear. This is similar with AI. If you have a full code or large chunk of text generated by AI and you use it ‚Äúas is‚Äù, you probably need to state that. But it‚Äôs more likely that the AI will give you some parts of the code or text, and you write the rest. What is the rule for that? I don‚Äôt think there are clear rules.\nI suggest you follow the guideline of ‚Äúif in doubt, cite/acknowledge‚Äù. For instance, at the beginning of some code, you can make a statement saying ‚Äúpart of this code was generated by ChatPGT using the following prompt‚Äù and then state the prompt you use. Or if you use AI to help you interpret your data, you can state somewhere in your output (e.g., your Quarto document), that you used AI to help generate insights/text/etc. Providing this kind of information prevents you from being accused of ‚Äúcheating‚Äù (if someone thinks using AI is cheating, for this course, you are encouraged to do so), and it also helps somewhat with the reproducibility (see above)."
  },
  {
    "objectID": "content/module-ai/ai-fix-code-r/ai-fix-code-r.html",
    "href": "content/module-ai/ai-fix-code-r/ai-fix-code-r.html",
    "title": "AI tools for fixing code",
    "section": "",
    "text": "Overview\nIn this unit, we discuss using LLM AI tools to help fix code.\n\n\nLearning Objectives\n\nKnow how to use current LLM AI tools to help fix code.\n\n\n\nIntroduction\nThis topic is very similar to the approach for writing code. In that unit, we already discussed how to iteratively update and improve the code. Fixing non-working code is really just another version of asking the AI iteratively to improve things. However, it seemed distinct enough to me that I thought it‚Äôs worth placing it into a separate unit.\nThe assumption is that you have some code (likely AI generated, but could also be code you wrote) and need to fix some typos. If the code was generated in the current session, you can just refer to it in your prompt. If the code was generated previously, you will supply the code as part of the prompt. The latter approach is more general, so I‚Äôm focusing on that one.\n\n\nFixing through iteration\nThe most basic approach is to just feed the AI non-working code and tell it to fix it. Here is an example continued from the non-working code we had at the end of the AI and code writing unit. To mimic a general setting, with code possibly produced earlier or not by the AI, I started a new session with the Bing AI in ‚Äòprecise‚Äô mode and gave it this prompt.\n\nThis code does not work. Please fix it.\nPASTE ALL CODE FROM THIS FILE HERE.\n\nHere is what I got back:\n\nI see that you‚Äôre trying to generate a scatter plot of BMI against weight, stratified by sex. The code you‚Äôve provided seems to be correct. However, the issue might be due to the specific versions of the ggplot2 and dplyr libraries you‚Äôre using, or it could be a problem with your R environment.\nIf you‚Äôre getting a specific error message, could you please provide it? That would help me understand the issue better and provide a more accurate solution. If you‚Äôre not seeing an error message, could you please describe what‚Äôs happening when you say the code doesn‚Äôt work? For example, is the plot not displaying as expected, or are the calculations not producing the right results? This additional information will help me assist you more effectively.\n\nSo this didn‚Äôt work. What‚Äôs next? One option is to see if another LLM AI can do it. For instance I could try and switch to another mode in Bing (creative or balanced) or try another LLM. I‚Äôm not doing this for now, but it might at times be worth giving it a try. What I‚Äôll do here is to help the AI (as it suggested) by being more specific about the problem. Here is my updated prompt:\n\nThe code below produces this error message:\nError in eval(ei, envir) : object ‚Äòsex‚Äô not found\nPlease fix it.\nPASTE ALL CODE FROM THIS FILE HERE.\n\nWith this prompt, I got it to work. The AI gave me this helpful explanation and fully working code\n\nThe error message you‚Äôre seeing is due to the sex variable not being recognized within the ifelse function in your data.frame creation. This is because the sex variable is not yet defined at the time ifelse is called.\nYou can fix this by first creating the sex variable and then using it in the ifelse function. Here‚Äôs the corrected code:\nCODE IS HERE\n\nThe end result is still not perfect, the difference in weight by sex is too large and we might want more scatter. But that can easily be fixed with either further iterations of asking the AI to update the code, or manual interventions. Speaking of manual interventions‚Ä¶\n\n\nManual intervention\nThe overall goal is to get working code as quickly as possible, not to have things completely AI generated. Therefore, if you reach a stage where you realize it‚Äôs faster if you just fix the code yourself, go ahead and do so.\nThis occurs at two stages. The most obvious one is close to the end. If the AI has given you code that is very close to what you want, but has a few issues (either conceptual or coding errors) that you immediately know how to correct, then just go ahead and finish things off manually. At this point, you should be able to ‚Äúturn off‚Äù the AI and have working code that you can apply to your project.\nAnother situation you might want to intervene manually is if the AI gets stuck. For instance, if you have to keep asking the AI to fix/update some code and it still doesn‚Äôt produce what you want, you could go ahead and make some manual adjustments to the code that you know should happen, and then feed the code, which still may not be fully working, back to the AI and ask it to update further.\nThe nature of the AI algorithms is that they can ‚Äúget stuck‚Äù and if you give them a slight change of scenery, they might be able to get unstuck and get you closer to your goal.\n\n\nSummary\nAs with code writing, for fixing code, being as specific as possible and iterating are key. Usually, if you tell the AI exactly what needs fixing and do a few iterations, you‚Äôll get to something that fully works, or is so close that you can quickly finish the rest manually.\n\n\nFurther Resources\nSee the AI resource unit."
  },
  {
    "objectID": "content/module-ai/ai-conceptual/ai-conceptual.html",
    "href": "content/module-ai/ai-conceptual/ai-conceptual.html",
    "title": "Conceptual ways to think about AI",
    "section": "",
    "text": "Overview\nThis unit consists of some thoughts and musings about AI and how to best think of and use it in a bigger picture framework.\n\n\nLearning Objectives\n\nKnow about a few ways one can conceptualize current AI tools.\n\n\n\nIntroduction\nAI, and especially generative AI like LLM are very new tools. Everyone is still trying to figure out how to use them, what they mean for the future, etc. While one can obviously use these tools without much further thought, it can be helpful to think about them in a conceptual way to have a potentially useful framework of interaction. Below are a few conceptual frameworks that I have heard from others or that I‚Äôve been thinking about.\n\n\nAI as the intern/1st year graduate\nI‚Äôve heard this concept multiple times by now. The idea is that you should think of LLM AI tools as being good at tasks that an intern, or a new graduate could do without too much training. For example, asking ChatGPT to solve world hunger is not a good idea. However, asking it to give you a list of countries where malnutrition is the worst and a summary of likely reasons for that, is a task where it will probably produce a result that you can use as starting point for whatever your larger project is.\nWhat that means is that to get the most out of the AI, you should break your tasks into manageable, well-prescribed bits, and ask the AI to tackle each one. The more details and instructions you provide, the more likely you will get something useful.\n\n\nAI as a brainstorming partner\nWhile AI is very good in doing specific, well-prescribed tasks, it can also be useful as a type of sparring partner or brainstorming device. You can throw ideas at the AI that are more open-ended, and ask it to provide its thoughts. Then you can iterate and that way possibly explore a topic and various options much faster than if you just thought about it yourself. This doesn‚Äôt always lead to good results, but it‚Äôs so quick and easy, it‚Äôs often worth a try. Note that if you use AI in this way, you would interact with it differently compared to the above approach. To get specific work done, e.g.¬†getting the AI to write you a piece of code, you want to be as specific and detailed as possible. You will often provide very long prompts. In contrast, if you use AI as brainstorming partner, you can have shorter, more vague prompts and do more of a back and forth. Just be clear what you are trying to accomplish and adjust your interactions accordingly.\n\n\nAI as electricity\nI haven‚Äôt heard this idea too much, but it seems to me that long-term, AI is going to be a bit like electricity. It‚Äôs going to be everywhere, it will power a lot of the environment around us, and it will become both more ubiquitous and possibly also more invisible. We use electricity all the time, and we rarely think about it. My guess is that AI will become that way. It will be interesting to watch how we get there. In the early days of electricity, there were fights about AC versus DC and lots of things were tried that didn‚Äôt work and it took a while before we had a (kinda) functioning electric grid that mostly just works. It will take AI a while. But I think we need to be prepared to have it be part of ‚Äúeverything‚Äù in the not-too-distance future.\n\n\nThe composer/conductor and the orchestra\nThis is another one I haven‚Äôt seen online, but I‚Äôm sure I‚Äôm not the first one to think of it. In fact, I asked the LLM (Bing AI in creative mode) to give me its thoughts on this analogy with this prompt:\n\nWrite a half-page paragraph that compares an LLM AI user to a composer or conductor, and the LLM AI tool to an orchestra.\n\nThe returned paragraph was pretty weak and not what I had in mind (try yourself, maybe you get something better). Here is my thought: The AI is a very versatile tool and you can do a lot of things with it, kinda like an orchestra. As a composer or conductor, you don‚Äôt need to be able to play each instrument of the orchestra. But you do need to know enough about each instrument to compose meaningful instructions as to what everyone should play, and you should know what to expect, so when you tell the trumpets to play a certain tune, you should be able to assess if what they produce is what you have in mind, and correct as needed.\nOf course, this analogy goes beyond AI tools. We can say the same about other complex tools, for instance the R programming language or a car. You don‚Äôt need to understand all the details of how these complex systems work under the hood (unless you want to become a full-time programmer or car mechanic), but you do need to know enough to give useful instructions, use them effectively, and critically assess what the machine returns and correct as needed.\n\n\nSummary\nI‚Äôm not sure these thoughts and musings about AI are useful. It helps me to find conceptual frameworks to think about new things. It might help you too. Or not üòÅ.\n\n\nFurther Resources\nSome of the resources listed on the AI resources page in the ‚Äògeneral‚Äô discuss topics similar to what I wrote here."
  },
  {
    "objectID": "content/content-overview.html",
    "href": "content/content-overview.html",
    "title": "Content Overview",
    "section": "",
    "text": "This section contains almost all of the content, organized into modules. Each module has several units.\nAssessments for each module are in the separate Assessments section.\nThe Schedule page lists what content is covered and what assessments you need to do each week."
  },
  {
    "objectID": "content/module-ai/ai-analysis-r/ai-analysis-r.html",
    "href": "content/module-ai/ai-analysis-r/ai-analysis-r.html",
    "title": "AI tools to analyze data",
    "section": "",
    "text": "Overview\nIn this unit, we discuss ways you can use AI to analyze your data, aka fit models to it.\n\n\nLearning Objectives\n\nBe familiar with ways to fit models to data using AI.\n\n\n\nIntroduction\nThis is really not that much different from the EDA approach. Basically, you have your data and tell the AI to fit some models to it and evaluate them. Here are a few simple examples to give you an idea.\n\n\nUploading data to Claude\nI‚Äôm using the longitudinal drug concentration dataset we created in the Generating synthetic data with R unit. I uploaded it to Claude and gave it this prompt:\n\nWrite R code to analyze the data in the attached CSV file. Fit a linear model with drug concentration as predictor and cholesterol level as outcome. Also fit a logistic model with drug concentration as predictor and adverse events as outcome. For each model, provide model performance diagnostics. Use the tidyverse packages for data manipulation tasks, and the tidymodels framework for model fitting. Add thorough documentation to your code so it‚Äôs clear what each line of code does.\n\nI got this code back, which looked pretty good, but didn‚Äôt run. So I asked the AI to fix the code with this prompt:\n\nThis code does not work. It produces this error:\nError in augment.model_fit(lm_model) :\nargument ‚Äúnew_data‚Äù is missing, with no default\nPlease fix this and any other errors in the code.\n\nThe updated code looked like this. It fixed the first problem with augment(), but the resid() function returns NULL and something about the the specification for the logistic model is also not correct. Of course if I really wanted this to work, I‚Äôd ask the AI to keep fixing, and/or intervene manually. But for this example I don‚Äôt care that much if it works fully. The right bits seem to be there.\nInstead, I was curious if I can get it to produce some halfway reasonable code for a more complex model. Since this is longitudinal, time-series data, with multiple measurements per patient, a hierarchical approach would be better. Here is an attempt at having Claude write code for a hierarchical Bayesian model. Note that this model is still not a good one since it doesn‚Äôt really take into account the longitudinal structure, just the grouping within patients. But let‚Äôs try, with this prompt:\n\nWrite R code to analyze the data in the attached CSV file.\nUse the brms package to fit a hierarchical Bayesian linear model with drug concentration as predictor and cholesterol level as outcome, with patient as the grouping.\nFor each function call, use explicit notation to specify from which R package the function comes from.\nAdd thorough documentation to your code so it‚Äôs clear what each line of code does.\n\nHere is the code I got back. It looks promising, but again doesn‚Äôt quite work. Of course I could try to have Claude fix it or fix myself. But I don‚Äôt feel like it right now, I‚Äôve got some more content to write üòÅ. Feel free to send me a working version of the code (or any above) if you decide to fix it!\n\n\nGenerating data and fitting it\nI‚Äôm not sure if some of the problems with the code above would not show up if we used the - supposedly better - ChatGPT engine. We can try. Again, it‚Äôs difficult to feed it data. We could try the copy and paste approach. But instead, I‚Äôm trying another approach. I‚Äôll give Bing the code that generates the data, then ask it to fit models to that generated data. Here is the prompt:\n\nThe code below produces a data frame called syn_dat2. Take the data in that data frame and fit a linear model with drug concentration as predictor and cholesterol level as outcome. Also fit a logistic model with drug concentration as predictor and adverse events as outcome. For each model, provide model performance diagnostics. Use the tidyverse packages for data manipulation tasks, and the tidymodels framework for model fitting. For each function call, use explicit notation to specify from which R package the function comes from. Add thorough documentation to your code so it‚Äôs clear what each line of code does.\nCOPY AND PASTE THIS CODE HERE\n\nI fed this to Microsoft Copilot in Precise mode. I also omitted the parts of the code after the data frame has been generated.\nThis is what I got back. Again looks promising but doesn‚Äôt work. I then gave it this prompt:\n\nThe code does not work. Please fix it. Also, update it such that the actual R package from which a function comes from is called, not the tidymodels collection of packages.\n\nThe result was code that used the standard lm() and glm() functions. While ok, that‚Äôs not what I wanted, I still wanted tidymodels syntax, just not the tidymodels::function notation. So my next prompt was:\n\nChange the code above such that instead of using the lm() and glm() functions, it uses the tidymodels set of functions\n\nI ended up with this. It makes the same mistake as Claude, not properly turning the adverse event variable into a factor before trying to fit a logistic model. Some other bits also didn‚Äôt fully work, but I didn‚Äôt feel like fixing further.\nI think you get the idea. Basically, the AI can produce quite useful bits of code that can speed up things, but it‚Äôs rarely fully correct and you will always need to check it, and often at the end still manually intervene. I‚Äôm sure as time goes by, what you get on the first try will get increasingly better. Still, you need to know what exactly you want and understand if the output makes sense or not.\n\n\n\n\n\n\nSummary\nI hope you can see how using AI tools to help with data analysis can potentially save a lot of time. Also, I think these examples make it quite clear: To be able to use those tools successfully, you need to know enough to understand what the AI should do, and if what it returns makes sense or not. If things don‚Äôt run, there is an obvious error. But it is quite possible that the code runs but doesn‚Äôt actually do the right thing. You only know this if you are familiar with what you want to accomplish. So while these AI tools can speed up coding a lot, they still require you to be an expert on whatever you are working on. Sorry! (Or maybe good, this means instead of not having a job in the near future, you‚Äôll likely just have a job that involves you being the master of AI tools, among other skills such as technical and subject matter expertise.)\n\n\nFurther Resources\nSee the AI resource unit."
  },
  {
    "objectID": "content/module-ai/ai-eda-r/ai-eda-r.html",
    "href": "content/module-ai/ai-eda-r/ai-eda-r.html",
    "title": "AI tools for exploratory data analysis",
    "section": "",
    "text": "Overview\nIn this unit, we discuss using AI tools to help with exploratory data analysis.\n\n\nLearning Objectives\n\nKnow why and how to use AI to help with exploratory analysis.\nBe aware of possible confidentiality issues.\n\n\n\nIntroduction\nIn an exploratory analysis, you generally want to look at many aspects of your data to get a good idea of what you have in front of you. While R has many powerful functions that let you explore your data quickly, combining R with AI generated code can speed up things even more.\n\n\nConfidentiality and Privacy\nIt is important to re-iterate from a previous unit: If you allow the AI to ‚Äúsee‚Äù your data, this data might end up on the servers of the company running the AI (ChatGPT, Microsoft, Google) and might be used by them for future training of their models. Therefore, be careful with what you let the AI see. If you are re-analyzing publicly available data, you shouldn‚Äôt have to worry. But if the data is in any way confidential, it might not be a good idea to allow the AI to see it.\nThe best solution in that case is to generate synthetic data that looks like your real data, then ask the AI to write code to analyze this synthetic data. Once the AI gives you working code, you can take it off-line and apply it to your real data.\nSince synthetic (artificial/fake/simulated) data is very useful for many parts of the data analyis workflow, it is covered in a separate module.\nIn the following I‚Äôm assuming that you have data that can be shared with the AI.\n\n\nExploratory data analyis (EDA) with no data\nOk, this sounds dumb, but it‚Äôs not that stupid. Instead of trying to feed the AI your data, you can ask it to generate code that does EDA on hypothetical data. For instance, you could provide a prompt like this:\n\nWrite R code to perform an exploratory data analysis of a data frame called dat. The data frame contains the continuous variables age and BMI, and the categorical variables sex and favorite color. Write code that produces a summary table, univariate plots for each variable, and a bivariate plot of age versus BMI.\n\nWhen I typed the above into Bing AI in precise mode, I got this code. It‚Äôs a good start. You can take this code and modify so it can be applied to your data.\nOf course, you can take this further and ask the AI to both generate the data you have in mind, then write EDA code. Basically ask it to write code along the lines of the examples from the synthetic data module and EDA code.\n\n\nExploratory data analyis (EDA) with copy and paste\nBing and the free ChatGPT version currently do not provide a way to upload data.\nIf you have fairly simple dataset, you can paste it directly into the prompt.\nHere is an example. This dataset comes from the first example in the\nGenerating synthetic data with R unit. I opened the CSV file, and copied the whole thing into the prompt:\n\nPerform an exploratory data analysis using R code of the following data set:\nPASTE DATA HERE\n\n\nTo get a new line in your prompts, use Shift + Ctrl/Return (or whatever the equivalent is on a Mac üòÅ.)\n\nI received this code from the AI. It runs and does a few nice exploratory analyses, such as writing summary tables and making a few plots.\nIt would of course be better if I provided much more detailed instructions regarding the types of analyses, and possible even the R packages and functions I want to have. But I think you get the idea. If the data is not too big, you can write your prompt, then copy the data underneath and get code that does EDA on the data.\nNote that there are character limits. For Bing, the maximum is 4000 as of this writing. If your data is too big, it gets cut off. In the example above, the last few lines of data got cut. I think this is often not a big problem, since the data you supply isn‚Äôt the real data anyway. All you want is enough data for the AI to generate EDA code, then you take that code and apply it to your real data later, with AI turned off.\n\n\nExploratory data analyis (EDA) with file upload\nThe free version of ChatGPT or Bing do not allow upload of files. However, Claude does. I repeated the process from above, but instead of pasting the data, I uploaded the CSV file.\n\nWrite R code to perform an exploratory data analysis of the attached CSV file.\n\nI received this code from the AI. It runs and produces some figures. Of course, as you learned previously, it would be better if I provided a much more specific prompt, saying exactly what kind of tables and figures I want to see as output. But you get the idea.\nClaude allows you (as of this writing) to upload a maximum of 5 files with 10MB each. While that prevents you to perform EDA on very large datasets, it should be enough for many purposes, and is definitely more than the limit for the copy and paste method described above.\n\n\nCopilot\nI tried using Copilot to help with EDA, but it wasn‚Äôt very successful. This screenshot shows an example of me trying to get it to produce a scatterplot. It didn‚Äôt work, Copilot did not return any suggestion. It did suggest the other lines of code. I‚Äôm not sure if I‚Äôm just not using Copilot correctly, maybe you‚Äôve got better luck. It doesn‚Äôt hurt turning it on and trying to see if it can help you. If you find it useful, let me know so I can update this.\n\n\n\nCopilot coding attempt\n\n\n\n\nchattr package\nBased on the package description, it should be able to write code based on not only the content in the current file, but also by looking into variables in the current environment. So if you have a data frame called dat in your environment, you should be able to ask the AI to write code to analyze it. Unfortunately, so far I haven‚Äôt been able to get chattr to talk to OpenAI. I think this is because talking to ChatGPT via the API (which is what chattr does) requires a paid account.\n\n\nOther options\nThe subscription-level access to OpenAI‚Äôs GPT engines gives you access to their Advanced Data Analysis tool. This allows you to upload and explore data more easily. I don‚Äôt have a subscription so I haven‚Äôt tried it yet. But if you are a heavy user of AI for data analysis, this might be worth the cost. This article describes the Advanced Data Analysis tool a bit more.\nI have also found a few other tools that seem to focus on data analysis, for instance Julius, but that one only works for Python code.\n\n\nSummary\nI haven‚Äôt tried any of the premium/paid options yet. I assume they will be superior, but I wanted to focus on free options. Among those, I currently think for EDA these 2 are the best:\n\n(Make the AI) produce code that generates synthetic data that looks like you want it to, then feed that to the AI and ask it to write code to perform EDA on the synthetic data. Once you have working code, you can apply it to your real data.\nUse Claude to upload your (synthetic or otherwise ok to share) data and ask it to write code to perform EDA on it.\n\n\n\nFurther Resources\nSee the AI resource unit."
  },
  {
    "objectID": "content/module-ai/ai-introduction/ai-introduction.html",
    "href": "content/module-ai/ai-introduction/ai-introduction.html",
    "title": "Introduction to Artificial Intelligence",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly talk about Artificial Intelligence from a general perspective.\n\n\nLearning Objectives\n\nKnow what Deep Learning and Artificial Intelligence are.\nKnow how one can (maybe) use R for such tasks.\n\n\n\nIntroduction\nArtificial intelligence (AI) is everywhere these days. AI approaches have led to some impressive recent technological advances, such as self-driving cars, very good language translation, biometrics and image recognition, and computers that beat humans at games like Chess/Go/Poker, etc. Most recently, we have seen LLM AI tools (which we‚Äôll discuss separately) that can be very useful for doing data analysis and coding tasks.\nI have no doubt that progress in AI will continue to be rapid and lead to many important advances. With everything ‚Äúnew and shiny‚Äù there comes a good bit of hype. The goal for this short unit is to give you a very brief introduction to what you should think of when you hear AI, how AI is related to data analysis/machine learning, and how you could ‚Äì if you wanted ‚Äì do AI yourself (using R).\n\n\nNeural networks (NN)\nCurrently, the main workhorse of artificial intelligence (AI) is a type of method/algorithm called a neural net/network (NN).\nThese are specific kinds of models that originated as an attempt to capture the functioning of biological neurons, and, by extension, brains. While neural nets have been around for a while and have had some successes (e.g.¬†in digit recognition), they have really taken off in recent years with the advent of more data, faster computers, and better algorithms.\nWithout going into details, you can think of how neural nets work in a few ways. One is that they are collections of individual, in silico, neurons, which are combined into layers. On one end, the input is fed into the model, at the other end, the output is produced. See e.g.¬†Wikipedia page on artificial neural networks for some schematic drawing. The input are your predictor variables, e.g.¬†lots of characteristics measured for a patient; data from an -omics array; pixels of an image; words of some text; sounds of an audio file, etc. The output is whatever label you want to predict, e.g.¬†for some images it could be the 4 categories cat/dog/neither/both. You feed the data to the model and train it. The training of a NN is conceptually similar to training other machine learning (ML) algorithms. Each neuron has parameters associated with it, and as you fit the model, these parameters are tweaked to optimize performance. The bigger your neural network (more neurons and more layers), the more flexible the model. This means it is potentially more powerful, but also more data-hungry.\nWhile the analogy to a biological brain is apt, this comparison does sometimes make NN sound more complicated and fancy than they are. To demystify NN somewhat, you can switch out the comparison to a biological brain, and instead think of them as a coupled set of logistic regressions. Each neuron is more or less described by some type of logistic regression function. It gets some input from multiple variables (either the original input, or a previous layer of neurons), and then based on that input and its parameter setting returns either 0/No or Yes/1. The output is then fed to the next layer of networks. The NN is thus a combination of individual logistic regression models, coupled together in some way.\nThis idea of connecting individual models to make a bigger, better model should be familiar by now. You saw it when we discussed making models like random forests that combine individual trees, and it was also mentioned as a general idea of ensemble models. You can think of NN as an ensemble of simpler, logistic-type models.\nOf course, neural networks are complicated. At this point, there are many different versions of NN in use, and understanding them in detail is time-consuming. However, as general user, you don‚Äôt need to understand all the details. Instead, like for other complex ML models, as long as you know how to use them and evaluate what they return, it is ok to not fully understand their inner workings. Properly training/fitting NN can be tricky, because they generally have many different parameters that need tuning to achieve good performance. Thus, a lot of development has gone into methods that allow for efficient tuning/fitting. Fortunately for us as users, these methods have become fairly good such that it is possible to use NN algorithms built by others and generally trust they work well ‚Äì similar to us using other models (GLM, trees, SVM, etc.) ‚Äì without having to worry about all the details.\nIf you want to learn more about neural nets, the Wikipedia entry is a good place to start. Also check the AI Resources page.\n\n\nDeep Learning (DL)\nYou might also hear the term Deep Learning (DL) used in the context of AI. Some folks distinguish DL and AI, considering the former a subset of the latter. Deep Learning (DL) generally refers to using a specific class of neural networks, namely those that have multiple layers of (in silico) neurons (it has nothing to do with deep as in especially insightful). These days, DL is sometimes used a bit more loosely and can refer to any NN-based complex algorithm (or sometimes even a non-NN complex model) applied to a problem. However, most often if someone says/writes that they use deep learning to address some problem, they mean using a type of neural net to fit data.\n\n\nArtificial Intelligence (AI)\nArtificial Intelligence (AI) is definitely a trendy topic these days. It is widely used and also widely mis-used in many contexts these days. Roughly speaking, AI is the use of complex models, usually NN, to solve difficult problems. If one wanted to differentiate DL and AI, one might say that AI is the use of DL approaches applied to ‚Äúcomplex‚Äù problems. However, it seems these days that DL and AI are terms that are used largely interchangeably. Overall, if you hear AI or DL, you can think of it roughly as fitting a neural net model to data. Unfortunately, since DL and AI have become such hot topics, terms are often misused these days, especially outside academia.\nIf you want to learn a bit more on the distinction between DL and AI (and just in general), the AI and DL sections of Wikipedia are good starting points.\nAs with all ‚Äúnew and shiny‚Äù things, there seems to be a bit of a current trend to use DL/AI approaches even when not needed. As an example, DeVries et al.¬†2018 used deep learning to analyze data; this follow-up correspondence article from Mignan and Broccardo 2019 showed that a single logistic regression produced more or less the same results. The reply by one of the original authors, Meade 2019, is also worth reading.\nOverall, DL and AI are certainly very promising approaches and will undoubtedly lead to significant improvements in our ability to harness data. As with most technologies going through a ‚Äúbubble‚Äù there is currently work that is both substantial and important, and work that is fluffy and full of hype.\n\n\nDL and AI in R\n\nThis section is about using R to train/fit NN models, not how to use LLM AI tools like ChatGPT with R. That is covered in a separate unit.\n\nWhile R is a great tool for data analysis, it‚Äôs currently probably not the best choice for doing AI/fitting NN models. If you really want to go deep into AI work, using something like Python, Julia, or some more specialized programming language is likely better.\nThat said, there are some ways to use R to fit some NN models. The best and easiest way is likely to use R packages that allow you to interact with powerful tools such as TensorFlow and Keras through R.\nThe RStudio TensorFlow website has a lot of good information and documentation on how to use Keras through R to do DL/AI. Starting there with the Tutorials section is probably the best way to get going. After that you can branch out to some of the other resources. While I‚Äôm sure a DL/AI expert uses more than just Keras/Tensorflow as tools, you can get very far with those. And for playing around with DL/AI, Keras through R is a great place to start. See the exercise for suggested starting points.\n\nSince DL/AI usually involves fitting large amounts of data to complex models, time constraints are often crucial and, at times, powerful computers are needed. An important development is the use of GPU (graphical processing unit) computing. While modern computers usually have more than 1 CPU, they are still generally limited to a small number. Even a very powerful single desktop generally will have less than 100 CPUs. In contrast, modern graphics cards often have &gt;1000 GPUs that can all be used in parallel to perform model fitting. Products such as Tensorflow allow one to use (mainly NVIDIA) GPUs to fit complex models to a lot of data in an often reasonable amount of time without requiring a supercomputer cluster. Unfortunately, last time I tried, R still didn‚Äôt have great GPU support. I have not recently tried to use Keras with GPUs through R.\n\n\n\nFurther Resources\nSee the AI Resources page for some additional information on various AI topics."
  },
  {
    "objectID": "content/module-ai/ai-overview.html",
    "href": "content/module-ai/ai-overview.html",
    "title": "Overview of AI content",
    "section": "",
    "text": "Overview\nThis module provides a brief introduction to AI and why and how to use AI tools.\n\n\nLearning Objectives\n\nKnow some basic concepts of AI.\nKnow why to use AI tools.\nUnderstand risks and limitations of AI tools.\nKnow how to use some common AI tools.\nBe familiar with ways to use AI for data analysis tasks.\nBe aware of ways to use AI with R.\n\n\n\nContent\nThe module consists of the following units:\n\nIntroduction to AI\nIntroduction to LLM AI tools\nA conceptual framework for LLM AI tools\nRecommended AI tools\nUsing AI to write code\nUsing AI to fix code\nUsing AI to explore data\nUsing AI to analyze data\nLinks to AI resources"
  },
  {
    "objectID": "content/module-ai/ai-tools/ai-tools.html",
    "href": "content/module-ai/ai-tools/ai-tools.html",
    "title": "Currently available AI tools",
    "section": "",
    "text": "Overview\nIn this tutorial, we discuss some currently available LLM AI tools and when/how you might want to use them. This is of course a rapidly moving field, so a few months from now things might have changed. I can hopefully keep up somewhat, but check the last modified date of this page üòÅ.\n\n\nLearning Objectives\n\nBe familiar with current AI tools and when/how to use them.\n\n\n\nIntroduction\nIt seems that every month a new LLM AI tool is announced. They are expected to keep getting better fairly rapidly for the foreseeable future. As you use these tools, it‚Äôs definitely useful to play around and explore. Here, I‚Äôm describing those that seem - as of this writing - the most useful ones for coding and data analysis tasks. Other AI tools might be more suitable for other tasks.\n\n\nIn brief\nCurrently, ChatGPT 4 is the most powerful LLM AI tool. You can access it through a paid subscription via OpenAi, or for free through Microsoft‚Äôs Bing. ChatGPT 3.5 is the less powerful, free version you can access by creating a free OpenAI account. Despite being less powerful, I‚Äôve found it to work pretty well to write R code. Other LLM are worth trying too, e.g.¬†Claude for data exploration/analysis.\n\n\nOpenAI ChatGPT\nAs of this writing, for the AI tools we are interested in (the LLMs), ChatGPT is probably the most useful tool. You can access it by creating a free account.\nChatGPT is also part of the Microsoft Bing search engine (currently located under the ‚ÄúChat‚Äù tab) and GitHub‚Äôs CoPilot.\nIf you haven‚Äôt already, I suggest you create a ChatGPT/OpenAI account. As of this writing, you can get a free account. Once you log in, you see a fairly plain interface with an area into which you can type your prompts. You can provide fairly elaborate prompts. We‚Äôll look at examples in other units.\n\n\nMicrosoft Copilot/Bing (still OpenAI GPT)\nA free OpenAI account gives you access to ChatPGT 3.5. While I find it often quite useful, especially for writing code, it‚Äôs not the best AI out there. ChatGPT 4.0 is generally better. You have to pay OpenAI if you want to use it. Or, you can use Microsoft Copilot - aka Bing Chat, and not to be confused with the GitHub Copilot, which is also owned by Microsoft ü§∑Ô∏è. It is recommended to use it in ‚ÄúCreative‚Äù or ‚ÄúPrecise‚Äù mode. Of course feel free to explore and play around. I will use the terms Microsoft Copilot or Bing AI/Chat interchangably. I‚Äôm sure all these names will change soon again anyway.\n\n\nGitHub Copilot\nGitHub Copilot is currently free for students with the GitHub student developer pack I asked you to sign up for. Set it up by following the Quickstart guide.\nOnce you have Copilot activated, you can use it in R Studio. (It is also possible to use it with other popular editors, such as VS Code, but for this course I will focus on R Studio. If you prefer to code in VS Code, you can install the Copilot plugin).\nTo get Copilot working in R Studio, follow these instructions. You might want to leave Copilot indexing unchecked for now. The instruction guide explains what this is. Basically, if you check this, copilot can access all files that are part of your current project. This can lead to confidentiality issues. Therefore, only check it if you are sure that all files in your project are ok being shared with the AI (and possibly make their way to Github/Microsoft/OpenAI servers).\nOnce you have Copilot up and running, you should see it show up in the bottom of your R Studio input window (the top right window where you write code). It will start suggesting things as you type, once you stop for a few seconds. You can set that time in the options.\nAn introduction to using Copilot can be found in this video by Thomas Mock from Posit/RStudio. In general, if you want bigger chunks of code, it seems currently better to go straight to ChatGPT. Copilot seems most useful to help you write bits of code as you go along. It can also be useful to help you remember the names of functions and arguments. This blog post provides some additional useful information on how to best use Copilot.\nIn general, I recommend you turn off Copilot in RStudio when you are not actively using it. And before you turn it on, make sure the document you have open, and any other files it might have access to, do not contain information you want to keep private.\n\n\nClaude/Anthropic\nAnthropic is another company developing LLM AI tools. Their main AI is called Claude. As for ChatGPT, you can sign up to get a free account. It is apparently not as powerful as ChatGPT. However, it has one nice feature that the free versions of ChatGPT or Bing do not have, namely it lets you upload files. This is useful for data analysis tasks. I‚Äôll show an example of that in the Explore data with AI unit.\n\n\nOther LLM AI\nOther companies, such as Google, are also busy creating LLM AIs. I haven‚Äôt tried them yet. Once I do, I‚Äôll add my impressions. If you are using any other LLM for coding or data analysis tasks and find it useful, please let me know.\n\n\nOther tools\nIt is quite likely that in the future, there will be several LLM AI tools that are used by the majority of users, and then there will be lots of customizations running on top of those. There are already specialized LLM AI tools for lawyers and doctors and other professions that use ChatGPT under the hood. GitHub CoPilot is such a customization for coding. I expect further tools to keep coming out at a fairly rapid pace.\nFor instance, as you learned from Thomas Mock‚Äôs video above, the chattr R package seems like another great way to use ChatGPT from within R. Unfortunately, as of this writing, I haven‚Äôt been able to get it to work. It is not entirely clear to me based on the information provided on the chattr website, but I think this is because talking to ChatGPT via the API (which is what chattr does) requires a paid account.\nI‚Äôm also aware of the chatAI4R R package, but haven‚Äôt tried it yet. My guess is since it likely also talks to ChatGPT through their API, it also requires a paid account. If that‚Äôs not the case and you try it and find it useful, please report back!\nAs you are probably aware, AI tools are changing rapidly. I‚Äôm writing this in December 2023, you will read this a few months later. It could be that by this point, some new versions of AI tools have come out that are even better. Do explore and if you find some especially promising platform or package, let me know!\n\n\nSummary\nLLM AI tools are changing rapidly. Currently, ChatGPT seems to do a good job at helping with coding and data analysis related tasks. Claude seems a useful option too, since it allows upload of files (such as data). New tools keep showing up and are generally worth exploring.\n\n\nFurther Resources\nFor a list and description of some other AI tools (e.g., those that can produce images), see this guide on using AI to do stuff. Also see the AI resource unit."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html",
    "href": "content/module-coding-basics/assessment-coding.html",
    "title": "Assessment - R Coding Basics",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#group-setup",
    "href": "content/module-coding-basics/assessment-coding.html#group-setup",
    "title": "Assessment - R Coding Basics",
    "section": "Group setup",
    "text": "Group setup\nAs in the previous exercise, assign each member in your group an (arbitrary) number (I‚Äôm calling them M1, M2, ‚Ä¶). Make sure you are teamed up with a different person this time. For this exercise, everyone will first work on their own and finish this part by Wednesday. Then M1 will contribute to M2‚Äôs repository, M2 will work on M3, etc. The last person (M3/M4/M5, based on the number of people in your group), will work on M1‚Äôs repository. This way, everyone will work on their own and one group member‚Äôs repository. Details are given below."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#part-1",
    "href": "content/module-coding-basics/assessment-coding.html#part-1",
    "title": "Assessment - R Coding Basics",
    "section": "Part 1",
    "text": "Part 1\nPart 1 is due by Wednesday."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#getting-started",
    "href": "content/module-coding-basics/assessment-coding.html#getting-started",
    "title": "Assessment - R Coding Basics",
    "section": "Getting started",
    "text": "Getting started\nOpen your portfolio website project in RStudio. Then open the file for the coding exercise, coding-exercise.qmd.\n\nDocumenting well is very important! Add lots of comments to your code/file. I suggest that your code should be more than half comments. For every block of code, you have a few lines of comments at the beginning explaining what the code block does, and then each line of code gets its separate line of comment with more details. Comment on both the how and why of your code. This much commentary might seem overkill initially. But as your code gets more complex, it will be very useful. Both your collaborators, and your future self looking at the code you wrote several weeks ago will be incredibly thankful for your comments!\nIf you write R code, your comments will be lines that start with #. For Quarto or R Markdown files, you can either add comments as Markdown text above/below your code, and/or add comments inside your R code chunks. Both is ideal. R Studio allows you to quickly turn sections of a document into comments or un-comment them (In the Code section of the R Studio menu). That can be useful for turning on/off code during testing, or hiding some parts of text that‚Äôs just meant for you but not for the reader."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#loading-and-checking-data",
    "href": "content/module-coding-basics/assessment-coding.html#loading-and-checking-data",
    "title": "Assessment - R Coding Basics",
    "section": "Loading and checking data",
    "text": "Loading and checking data\nWe‚Äôll look at and play with some data from the dslabs package. Write a code chunk using the library() function that loads the package (install the dslabs package first if you don‚Äôt have it yet).\n\nIt is good practice to load all packages at the beginning of your code. So if you are using some R package, instead of loading it with library just before you use it, place all your library commands at the beginning of your R script or Quarto file. Also, add a short comment explaining why you are loading a certain package. For a complex project, it might even make sense to list all packages you use in a readme file. Even better is to use something like the renv package which keeps track of all your packages and makes sure someone running your project at a later time gets exactly the same packages you use. While I‚Äôm not requiring renv for the course, I encourage you to check it out and if you are up for it, use it for the class exercises and the project.\n\nAll the code you write for this (and any other) project should be written into an R or Quarto file, not in the R console. The reason for that is that you want a permanent record of what you did, and the ability to modify and re-run your analysis easily. For this exercise, you can either write the code directly into the Quarto document, or if you prefer the setup of a separate R script and pulling code into the Quarto document (see the data analysis template for an example), you can do it that way too.\nWe‚Äôll look at the gapminder dataset from dslabs. Once you have installed and loaded the dslabs package, the dataset is available. I.e., different than datasets you get from external sources, those that come with R packages are available right after you load the package. Write a code chunk using the help() function that pulls up the help page for the data to see what it contains. Then use the str() and summary() functions to take a look at the data. Use the class() function to check what type of object gapminder is.\nTo illustrate how that should look, you should have something like these lines of code and R output so far for this exercise.\n\n#load dslabs package\nlibrary(\"dslabs\")\n#look at help file for gapminder data\nhelp(gapminder)\n\nstarting httpd help server ... done\n\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\""
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#processing-data",
    "href": "content/module-coding-basics/assessment-coding.html#processing-data",
    "title": "Assessment - R Coding Basics",
    "section": "Processing data",
    "text": "Processing data\n\nYou can accomplish the next steps (and pretty much anything) with just basic R commands and not using extra functionality from packages. However, things are often easier with packages. For data processing tasks, the packages from the tidyverse are very useful. You can do the following tasks with any commands/packages you like.\n\nWrite code that assigns only the African countries to a new object/variable called africadata. Run str and summary on the new object you created. You should now have 2907 observations, down from 10545. Depending on how you do this, you might also notice that all the different categories are still kept in the continent (and other) variables, but show 0. R does not automatically remove categories of what in R is called a factor variable (a categorical variable) even if they are empty. We don‚Äôt have to worry about that just now, but something to keep in mind, it can sometimes lead to strange behavior.\nTake the africadata object and create two new objects (name them whatever you want), one that contains only infant_mortality and life_expectancy and one that contains only population and life_expectancy. You should have two new objects/variables with 2907 rows and two columns. Use the str, and summary commands to take a look at both. Make sure you add comments into your code to explain what each line of code is doing, and as needed, also add additional explanatory Markdown text to your Quarto file.\n\nI find it the least confusing to call things which store values in R objects (e.g., x is an object here: x &lt;- 2 + 2) and reserve the word variable for a data variable, i.e., usually a column. However, it is common in programming to also refer to an object as a variable. Because of that, I sometimes use that terminology (inadvertently) too. So if I talk about a variable, you need to determine from the context if I mean a certain variable in the data (e.g.¬†height or weight), or a variable in R (e.g.¬†x or result) that stores some content."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#plotting",
    "href": "content/module-coding-basics/assessment-coding.html#plotting",
    "title": "Assessment - R Coding Basics",
    "section": "Plotting",
    "text": "Plotting\nUsing the new variables you created, plot life expectancy as a function of infant mortality and as a function of population size. Make two separate plots. Plot the data as points. For the plot with population size on the x-axis, set the x-axis to a log scale.\nYou should see a negative correlation between infant mortality and life expectancy, which makes sense. You should also see a positive correlation between population size and life expectancy. In both plots, especially the second one, you will see ‚Äòstreaks‚Äô of data that seem to go together. Can you figure out what is going on here? Take another look at the africadata data we generated, which should give you a hint of what‚Äôs happening. Add descriptive text into your Quarto file to explain what you see and why."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#more-data-processing",
    "href": "content/module-coding-basics/assessment-coding.html#more-data-processing",
    "title": "Assessment - R Coding Basics",
    "section": "More data processing",
    "text": "More data processing\nI‚Äôm sure you realized that the pattern we see in the data is due to the fact that we have different years for individual countries, and that over time these countries increase in population size and also life expectancy. Let‚Äôs pick only one year and see what patterns we find. We want a year for which we have the most data. You might have noticed that in africadata, there are 226 NA (i.e., missing values) for infant mortality. Write code that figures out which years have missing data for infant mortality. You should find that there is missing up to 1981 and then again for 2016. So we‚Äôll avoid those years and go with 2000 instead. Create a new object by extracting only the data for the year 2000 from the africadata object. You should end up with 51 observations and 9 variables. Check it with str and summary."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#more-plotting",
    "href": "content/module-coding-basics/assessment-coding.html#more-plotting",
    "title": "Assessment - R Coding Basics",
    "section": "More plotting",
    "text": "More plotting\nLet‚Äôs make the same plots as above again, this time only for the year 2000. Based on those plots, there seems to still be a negative correlation between infant mortality and life expectancy, and no noticeable correlation between population size and life expectancy. Let‚Äôs apply some statistical model to this data."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#simple-model-fits",
    "href": "content/module-coding-basics/assessment-coding.html#simple-model-fits",
    "title": "Assessment - R Coding Basics",
    "section": "Simple model fits",
    "text": "Simple model fits\nUse the lm function and fit life expectancy as the outcome, and infant mortality as the predictor. Then repeat, now with the population size as the predictor variable. (Use the data from 2000 only.) Save the result from the two fits into two objects (e.g.¬†fit1 and fit2) and apply the summary command to both, which will print various fit results to the screen. Based on the p-values for each fit, what do you conclude? Add comments into your quarto file to explain what you did and found."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#sending-updates-to-github",
    "href": "content/module-coding-basics/assessment-coding.html#sending-updates-to-github",
    "title": "Assessment - R Coding Basics",
    "section": "Sending updates to Github",
    "text": "Sending updates to Github\nOnce you are done with your exercise, re-build your portfolio website. Make sure no error messages show up. A preview should show up, check that the page for this exercise looks the way you want it to. Once you are happy with how everything looks, close RStudio. Go to GitKraken, commit your changes, and push to the remote repository. Check your portfolio website online to make sure you can now see the newly created R exercise document (in addition to your previously created About page). Of course, at any point, feel free to make further enhancements to your portfolio website."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#part-2",
    "href": "content/module-coding-basics/assessment-coding.html#part-2",
    "title": "Assessment - R Coding Basics",
    "section": "Part 2",
    "text": "Part 2\nPart 2 is due by Thursday evening."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#adding-to-each-others-work",
    "href": "content/module-coding-basics/assessment-coding.html#adding-to-each-others-work",
    "title": "Assessment - R Coding Basics",
    "section": "Adding to each other‚Äôs work",
    "text": "Adding to each other‚Äôs work\nOnce everyone has done their first part (by Wednesday at the latest), you‚Äôll contribute to another group member‚Äôs project. As previously, find your classmates‚Äô portfolio repository and fork it, then clone to your local computer.\nOpen the repository. Remember to always do that by clicking on the .Rproj file in the main folder, which should open the project/repository in RStudio. Once you have the repository open, find the file for this exercise coding-exercise.qmd. Open it, make sure it runs/renders. Then add your part at the bottom of the file.\nStart off by adding a comment that says something like This section contributed by YOURNAME. This needs to be there for me to be able to grade your contribution."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#more-data-exploration",
    "href": "content/module-coding-basics/assessment-coding.html#more-data-exploration",
    "title": "Assessment - R Coding Basics",
    "section": "More data exploration",
    "text": "More data exploration\nPick another dataset from dslabs, whichever one you want. Unfortunately, the dslabs package doesn‚Äôt have a nice website. But you can go to its offical CRAN entry and click on Reference Manual. The pdf lists the various other datasets and for each provides a brief explanation.\nOnce you chose one of the datasets, write R code to go through similar steps as above. Specifically, do the following:\n\nExplore the dataset.\nDo any processing/cleaning you want to do.\nMake a few exploratory figures. Optionally, also some tables.\nRun some simple statistical model(s). Your choice.\nReport the results from the model(s).\nFor each step, add plenty comments to the code and explanatory Markdown text to the Quarto file."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#sending-a-pull-request-pr",
    "href": "content/module-coding-basics/assessment-coding.html#sending-a-pull-request-pr",
    "title": "Assessment - R Coding Basics",
    "section": "Sending a pull request (PR)",
    "text": "Sending a pull request (PR)\nOnce you are done with your additions, make sure the whole website renders. Then push your updates to your fork on GitHub. Then follow the instructions from last week and issue a pull request to the original owner of the repository. They should get a notification, but you can also let them know that your updates are ready."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#part-3",
    "href": "content/module-coding-basics/assessment-coding.html#part-3",
    "title": "Assessment - R Coding Basics",
    "section": "Part 3",
    "text": "Part 3\nPart 3 is due by Friday."
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#merging-the-pr",
    "href": "content/module-coding-basics/assessment-coding.html#merging-the-pr",
    "title": "Assessment - R Coding Basics",
    "section": "Merging the PR",
    "text": "Merging the PR\nOnce you received the pull request, check it. Hopefully all is ok and merge it. If something isn‚Äôt right, tell the person who sent the PR to fix what needs fixing and send another one.\nOnce you merged the PR, pull the updates to your local computer. Make sure everything looks fine and everything runs. Make any further needed changes/updates. Once everything is good, push your updated portfolio repository to GitHub. Check the website URL to make sure everything looks good. I‚Äôll be looking at the content shown on the website for assessment/grading purposes.\n\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it."
  },
  {
    "objectID": "content/module-coding-basics/rcoding-tidyverse.html",
    "href": "content/module-coding-basics/rcoding-tidyverse.html",
    "title": "Tidyverse and Friends",
    "section": "",
    "text": "One of the strengths of R (and also a source of confusion) is that it is very flexible and almost always lets you do things in more than one way. R itself comes with some functionality. This is often referred to as base R. Even with just this basic functionality, there are often many ways to accomplish a task. But the real power of R comes from its many packages. Packages (also called libraries in some other programming languages) contain additional functions and functionality that lets you fairly easily do things that would require a ton of coding effort if you tried to do it yourself. Someone basically wrote the functionality for you, and you can use it.\nWhile there are tons of packages available, some are very commonly used. For data analysis tasks, the folks from R Studio have developed many packages that are very useful. One such set of packages, the most widely used set, is called the tidyverse. By using those packages, a lot of coding applied to data analysis becomes easier, more readable, and more powerful. We will use the tidyverse packages and their functionality a lot. That said, knowing some base R is very useful. In general, you can fairly easily mix and match."
  },
  {
    "objectID": "content/module-coding-basics/rcoding-tidyverse.html#tidyverse-concepts",
    "href": "content/module-coding-basics/rcoding-tidyverse.html#tidyverse-concepts",
    "title": "Tidyverse and Friends",
    "section": "Tidyverse concepts",
    "text": "Tidyverse concepts\nAs you‚Äôll find out shortly, one feature of R code written in the tidyverse style is the heavy use of the pipe operator. The original pipe operator, the %&gt;% symbol, was introduced in the magrittr package. Since then, base R got its own pipe operator, which is the symbol |&gt;.\nFor instance, this is the kind of code that you might see when doing data wrangling (note that this code doesn‚Äôt work since it only shows the chain of functions, for it to work there would need to be arguments provided to each function, i.e.¬†there needs to be something inside the ()):\n\ndata %&gt;% filter() %&gt;% \n         select() %&gt;%\n         fct_lump() %&gt;%\n         mutate() %&gt;% ...\n\nThe idea is that you pipe the results from one operation into the next, and thus potentially build a long chain of commands. That style of coding makes it often quite easy to understand what the code is doing. For instance in the example code above, you first filter the data based on some row values, then select some columns, then combine some factor variable, then mutate a variable into a new one, and so on.\nThe problem, especially when you start out, is that things can (and will) go wrong at some of those steps, and it‚Äôs difficulty to figure out where the problem is. At least when starting out, it is in my opinion often better to save the result of some cleaning operation as a new intermediate variable. That lets you more easily check for bugs, and to see how the data changed from step to step and if it does what you think it should. So instead of using a long chain of pipes, you can write the code like so:\n\ndat_small1 &lt;- data %&gt;% filter()\ndat_small2 &lt;- dat_small1 %&gt;% select()\ndat_small3 &lt;- dat_small2 %&gt;% fct_lump()\n...\n\nThis code is not quite as easy to read, and it creates all these additional variables that you might not want or need. But I think at least as you are learning the different tidyverse functions, it often helps to be able to inspect what happens at each step, and thus more easily spot when things go wrong. Once you get more comfortable with cleaning steps and coding in general, and make few mistakes, you can start chaining things together, and make your chains longer. But if you start out writing code that way, it can be harder to follow along and find bugs.\nA nice introduction to pipes is this section of the R4DS book.\nThere is also a nice project called Tidy Data Tutor that allows you to visualize the different steps in a data analysis chain of commands. Once you get used to chaining commands together in a long pipeline, or try to inspect someone else‚Äôs code, using that tool to look at each step can be quite useful. Give it a try."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html",
    "href": "content/module-data-analysis/data-analysis-motivation.html",
    "title": "Motivating Data Analysis Examples",
    "section": "",
    "text": "We‚Äôll get into the details of what data analysis is and how to do it very soon. But to start out, I want to provide a few hopefully inspiring and motivating examples of data analysis and use. You‚Äôll be doing similar projects yourself soon! In fact, you will be reproducing several analyses below."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-1",
    "href": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-1",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 1",
    "text": "TED talk 1\nArguably one of the best communicators of public health data was the late Hans Rosling. He has several great entertaining and enlightening TED talks. Here is one of them: A statistics talk that has been viewed millions of times!\n\n\n\n\nYou can watch more TED talks by Hans Rosling. The software he uses to illustrate data is called Gapminder and available online on the Gapminder website."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-2",
    "href": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-2",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 2",
    "text": "TED talk 2\nIn this entertaining and empowering talk, Talithia Williams argues for the utility of careful assessment of data in your personal life."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-3",
    "href": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-3",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 3",
    "text": "TED talk 3\nIn this talk, Sebastian Wernicke gives some cautionary suggestions about trusting data too much and argues for critical assessment as an important part of any data analysis and decision process."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#more-ted-talks",
    "href": "content/module-data-analysis/data-analysis-motivation.html#more-ted-talks",
    "title": "Motivating Data Analysis Examples",
    "section": "More TED talks",
    "text": "More TED talks\nThe TED website has a large number of data related talks. Feel free to explore as much as you like. If you find especially good ones, post them on the class discussion board."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#tidytuesday",
    "href": "content/module-data-analysis/data-analysis-motivation.html#tidytuesday",
    "title": "Motivating Data Analysis Examples",
    "section": "TidyTuesday",
    "text": "TidyTuesday\nAn interesting initiative started by Thomas Mock is TidyTuesday, where every Tuesday a new dataset is released and individuals are encouraged to analyze it. Later in this course, you will be participating in some TidyTuesdays yourself. Tidy Tuesday has by now been running for many years.\nIndividuals who participate in TidyTuesday often post their projects online and link to it on social media. We will look at an example by David Robinson. David does screencast recordings showing him analyze the data.\nYou don‚Äôt need to watch all of the almost 2 hour long recording üòÑ, but do watch the first few minutes to see him coding/analyzing in real-time. You might just get hooked and keep watching. You might not be familiar with some or most of the R coding, but you will still get the overall idea of how he does the analysis. And his thinking out loud is informative as well. See for yourself:\n\n\n\n\nIf you like the ability to look over his shoulder while he does data analysis, he has a bunch more of those screencast recordings.\nThere are others who live-stream data analysis, including competitions, on platforms such as Twitch. Check out Jesse Mostipak, aka Kiersi or Nick Wan if interested.\nDavid does his work reproducibly and shares the code as RMarkdown files. For specific files, see the links provided in the video description. For convenience, download the code for his analysis. Just save the file after it opens in your browser. It should be named 2021_05_18_salary_survey.Rmd. Click on it, and it should open in RStudio.\nOnce the file is open in RStudio, you might see at the top of the file a message suggesting that you need to install several packages to run the file. Do so. If you don‚Äôt get that auto-suggestion, you need to install the packages by hand. Every package that is called with the library() command needs to be installed first.\nThis is a good example of an almost works reproducible example. It‚Äôs quite rare that things work completely without adjustments on different machines, and how to do that best is a huge topic in data analysis (e.g., for drug companies that need full reproducibility for licensing). In this example, I had to make the following adjustments:\n\nChange output: html_output to output: html_document in the YAML section.\nInstall the ranger package, which is not listed under the library statements but is needed.\n\nWith those changes, I was able to run the whole script by hitting the Knit button in RStudio. If you are still missing a package, you will get an error message. If things work out, the whole script will run a while (depending on the speed of your computer), and you should get a document/report which reproduces the complete analysis he did in the video! If things don‚Äôt work out, post your error message to Slack so someone can help troubleshoot. One potential problem could arise from the fact that David uses parallel computing to make things run faster. If you get an error message, you can try commenting out or deleting the line of code that says doParallel::registerDoParallel(cores = 4) and see if it works without it. It worked ok with that line of code on my windows machine, but might not always work. (If on Windows, you might be asked about firewall network permissions. That‚Äôs a quirk of how parallel computing works on a single machine. Just say ‚Äúyes‚Äù to both public and private networks.)\nAgain, most or all of the R code might not make sense to you just yet. That‚Äôs ok. The main point is to show you a nice example of a data analysis done in a way that others (you) can reproduce with just a few clicks. As part of this class, you will be producing those kinds of reproducible R Markdown files (well, we‚Äôll be doing Quarto files, which is the improved successor of R Markdown)."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#research-project",
    "href": "content/module-data-analysis/data-analysis-motivation.html#research-project",
    "title": "Motivating Data Analysis Examples",
    "section": "Research Project",
    "text": "Research Project\nFully open and reproducible analyses are also becoming common in academic research, with authors publishing everything needed to reproduce their paper and results. In my research group, we try to adhere to this approach as much as possible. As an example, I‚Äôm sharing a project/paper by Dr.¬†Brian McKay, a former grad student in my group. You can read the published paper. All materials to reproduce the full project and all results are available as supplementary material. Take a look at the Usage Notes section on the website.\nNext, click on Download Dataset, and unzip the file. You should end up with a folder called ProcSocB Supplemental Material.\nInside that folder you can see subfolders containing all the data and code to fully reproduce the analysis and manuscript. Find and click on Virulence_Trade-off.Rproj. This should open in RStudio. To reproduce the manuscript, go into the Manuscript folder, open Manuscript.Rmd. You‚Äôll see some code at the top which loads various datasets containing results. Further down, you‚Äôll find the text of the manuscript, combined with simple R commands that dynamically place numbers and figures into the text. Re-create the manuscript by clicking on the knit button. There might be error messages related to missing packages. If that happens, install those packages (e.g.¬†‚Äòbookdown‚Äô). If you get stuck, post your problem to the discussion boards.\nFeel free to explore further by opening various R scripts and running them. The Usage Notes section on the website where you downloaded the zip file, or the Supplementary-Material.docx document explain what the different pieces do and how to use them.\nAs you can tell, this is more complex and complete compared to the single R Markdown file example above. The details on how you structure your project and folders/files are up to you, but they should provide a way to organize things so it‚Äôs easy for you and others to understand what is going on where.\nThis example hopefully gives you an idea how nice and easy it is to write a whole paper with just a few clicks of a button. It is a combination of R scripts and R Markdown files (in this case a version called bookdown), which allow a fully automated generation of a complete data analysis and manuscript, including all figures, tables, references, etc. This is an incredibly efficient workflow and one you‚Äôll learn and use in this class. For instance if there was a mistake somewhere in the data, instead of going through the manuscript and manually changing numbers, one could just re-run the whole code, and it will produce new results based on the updated data in a fully automated manner. It‚Äôs a bit more work upfront until you get the hang of it, but once you have it set up, it can save an enormous amount of time and avoid potential copy & paste mistakes."
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-structure.html",
    "href": "content/module-data-analysis/data-analysis-structure.html",
    "title": "Structure of a Data Analysis",
    "section": "",
    "text": "Overview\nIn this unit, we‚Äôll discuss how to structure a data analysis to make it as efficient as possible.\n\n\nLearning Objectives\n\nBe able to set up a structure for an efficient and reproducible analysis.\n\n\n\nOverall project structure\nYou want to set up your analysis in such a way that it makes sense to you and others and allows for a good and efficient workflow. The main components of your analysis will be data, code, results (tables, figures, etc.) and products (reports, interactive apps, slides, etc.). It is a good idea to have separate folders for each of those elements inside your main project folder. Your project folder could be a Github repository (a good idea) or not. Since you might not want to share your analysis publicly, sometimes using a private repository is useful. (Though don‚Äôt worry about getting scooped. Everyone is so busy with their own stuff, nobody really cares what you are working on üòÅ. In general, I recommend private repositories if you have data that can‚Äôt be made public.) Starting your project as an R project is also a good idea.\nInside your folders, you can have subfolders, e.g., separate folders for figures and tables. Or you could have subfolders for different types of analyses. There is no one correct way to set up things, but you should think of a logical and consistent structure before you start your project.\n\n\nHow to structure your analysis\nYou will want different R and/or Quarto scripts for the cleaning/wrangling/exploring part and the analysis part. The number of scripts depends on your project and your preference. In general, keeping things modular is useful. If you had one file that did the analysis and created a report, that would be ok for a small project. But then if you wanted to make a set of slides based on your results, you‚Äôll have to find a way to include the code in those slides. It would be easier to have code produce and save results such as figures, which can then be included in both a report/manuscript and slides.\nAnother consideration is computational time. For simple projects, your code will likely run fast. Once your analysis or data become large, parts of your code might run very long. You will then want to structure your scripts such that the computationally-intensive part is only run when absolutely needed. You definitely want to avoid a scenario where you have to wait minutes or hours as you play around with a figure to make it look the way you want.\nAs an example, and hopefully useful guide for your class project, I created a public Github repository called dataanalyis-template which is meant as a template for doing a data analysis project. It has different folders for organizing your project. Various README files are provided to explain what each folder should contain. The template also contains several example files to show how the whole project workflow (or any data analysis workflow for that matter) can work. This is, of course, only one way to structure things. You are welcome to figure out your own setup and structure. Overall, figure out what setup works best for you, while keeping in mind that it should be easily understandable/reproducible for a reader (or your colleague, PI,‚Ä¶). You‚Äôll be exploring this template as part of the exercise.\n\n\nSetting paths\nOne problem that I encounter every time I teach a course like this is the use of paths that are specific only to the user‚Äôs computer and do not work on someone else‚Äôs machine. Do not set paths or load files from paths that only exist on your machine! Instead, you should only use relative paths. A relative path is a file path that is relative to some directory. So what should not be part of your code is the command setwd() (because your working directory is likely different from everyone else) or anything that involves a full path name (e.g.¬†C:/myusername/mydesktop/myfolder/) since nobody you share the code with will have that folder.\nFor a short video tutorial, see this video on relative paths. Note that the video is several years old and he doesn‚Äôt use R Projects. I strongly recommend using R Projects. By having any project you work on as an R project, relative paths will always be relative to the main directory in which your .Rproj file is located. As long as someone loads the project by clicking on the .Rproj file, and you only use relative paths, things should work well on any computer. While R projects solve most of the problem, some issues remain, e.g.¬†differences of how relative paths are treated in Rmd versus R files.\nTo solve those issues, you should use the here package, together with R Projects. It helps solve issues with relative paths being dealt with differently in an Rmd file versus an R file versus the console. Malcolm Barrett‚Äôs blog post is a great, short explanation of why one should use the here package and how to do it.\nHowever you do things, make sure that for your exercises and especially final project, someone else can clone your repository (or otherwise copy your project if it‚Äôs not on Github) to their computer and run everything, without having to have exactly your setup of folders.\n\nAnd one more time: Do NOT use absolute paths when you load/save files. For R, use R projects and the here package to load/save files relative to the main project directory (which is the folder that contains the .Rproj file.)\nAnd beware of having R projects inside projects. That‚Äôs a bad idea and can lead to problems.\n\n\n\nFurther Resources\nSeveral efforts to develop further tools to help improve reproducible research within the R system exist. The few I know about are listed below. I have not tried to use any of them, but feel free to try/use them as part of this class.\n\nFor some more useful information on structuring your project such that it has a good, reproducible workflow, see chapters 1-3 in this collection of (incomplete) notes by Jenny Bryan and Jim Hester.\nThe projects R package - meant to provide a framework for rather sophisticated projects.\nR Open Science Reproducibility Discussion - website with links to various resources related to reproducibility in R.\n\n\nWhile there is no substitute for clear thinking and being careful in your analysis, things such as having a clear structure can help with this. The importance of good structure to help you achieve optimal results is well appreciated in many contexts. For a short discussion of this, see Seth Godin‚Äôs article. For a more in-depth discussion of this idea, the Checklist Manifesto by Atul Gawande is a great read."
  },
  {
    "objectID": "content/module-data-exploration/data-exploration.html",
    "href": "content/module-data-exploration/data-exploration.html",
    "title": "Exploring Data",
    "section": "",
    "text": "Overview\nIn this unit, we will continue the topic of dealing with data prior to the actual statistical analysis.\nWhile there were separate modules for finding and wrangling and visualizing data, I want to remind you of this diagram:\n\n\n\nData analysis workflow\n\n\nDifferent data-analysis related tasks rarely happen in a linear sequence. Instead there is a general stage before statistical analysis where you mess with your data (that‚Äôs a technical term üòÅ) to get ready for the main formal analysis. And often, as you start that formal analysis, you‚Äôll have to go back and do some more cleaning/exploration.\n\n\nLearning Objectives\n\nLearn how to explore your data\nBecome familiar with preliminary analyses\n\n\n\nDescriptive analysis\nBefore any statistical analysis, you should summarize and describe your data. Only some of that will end up in a finished product (e.g., a paper or a report), but it‚Äôs important that you do it thoroughly and exhaustively to ensure you fully understand your data and thus know what statistical approaches are and are not appropriate.\nDescriptive analyses generally consist of making tables and figures that explore and summarize the data. Generally, you start with one variable at a time, and likely focus on the most important ones first. That means first you look at your outcomes of interest, then your main predictors/exposures of interest, then as much as feasible at all other variables. For continuous variables, you can use summary statistics (e.g., the summary command in R) or histograms (e.g.¬†the hist command). For categorical variables, you can look at tables (e.g.¬†the table command) or barplots (e.g.¬†the barplot command). The idea is to see if anything unusual or interesting is going on. You might notice problems with the data (e.g., a person with a negative weight) that need cleaning. You might find that you have a categorical variable with a lot of categories that contain only a few entries and only a few categories with a lot of entries (e.g., religion could be such a variable). You might want to flag that for further processing (we‚Äôll talk about that in the next unit).\nOnce you‚Äôve explored each variable by itself, you can start looking at combinations. For instance plots with each outcome of interest on the y-axis and each main predictor on the x-axis could be of interest. Depending on the type of variable for the outcome or predictor (continuous, categorical, etc.), you might want different types of plots, as discussed in the visualization module. Tables are possible too.\nFurther figures of interest might be correlation plots which show correlations between all predictors in your data and might indicate potential issues you need to address before fitting statistical models. For instance if you have height, weight, and BMI in your data, you know that BMI does not have independent information (it‚Äôs a combination of height and weight), and thus these variables are perfectly correlated. Many ‚Äì though not all ‚Äì statistical models don‚Äôt like such strongly correlated variables, so you might need to decide to remove some strongly correlated data beforehand. We‚Äôll talk about that more.\n\n\nPreliminary/exploratory statistical analysis\nSometimes, it is useful to start running some simple models on your data. Sometimes, such models will become part of the main analysis, but often they are just used to help explore the data. For instance if you have two continuous variables and make a scatterplot, adding a linear regression line through the data might be helpful to see if there is a pattern. That is easily done if you make your figure with ggplot2, as you can just add a stats geom to it.\nYou could also explore your data by running simple bivariate statistical models, e.g.¬†by fitting each predictor individually to the outcome of interest. We won‚Äôt talk much about standard regression approaches in this class, but I‚Äôm sure you are familiar with standard statistical tests that can be applied to two variables, depending on the types of variables (linear regression, t-test, etc.).\nA note of caution: It is ok (in my opinion) to use standard statistical tests and readouts (e.g., p-values) to guide main model choices and as suitable, to report them in a final product. It is however only ok to interpret p-values in the hypothesis testing sense if you predefined the hypothesis and the full data generation and analysis protocol. Outside a clinical trial, that‚Äôs almost never done, thus pretty much any reported p-value you see in the literature is improperly used/interpreted. Everyone does it, and pretty much everyone does it wrong. The state of affairs when it comes to statistical literacy, even among PhD holders, is still sad‚Ä¶\n\n\nFurther reading\n(Re)read the Exploratory Data Analysis and Using Models to Explore Your Data chapters (4 and 5) in The Art of Data science (ADS) book. We looked at that book previously. You don‚Äôt need to work/code along. But if you want to, see the Some practice section of the Data Analysis Overview page for some details on how to do it.\nAlso go through the rather short EDA tutorial which is part of the Visualize Data Posit Recipes.\nThe whole R4DS book focuses on the early stages of data analysis, including exploration. There is whole section called Explore and a chapter called Exploratory Data Analysis (EDA). I like this sentence from the beginning of their chapter: EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. That‚Äôs a very good way to put it. Skim through that chapter of R4DS (it‚Äôs very similar to the Posit EDA Recipe). And of course the rest of the book also provides lots of useful information on that topic."
  },
  {
    "objectID": "content/module-data-intro/data-finding.html",
    "href": "content/module-data-intro/data-finding.html",
    "title": "Finding and Getting Data",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss where to find data and how to get different types of data into R.\n\n\nObjectives\n\nKnow ways of acquiring data\nBe familiar with some important online sources for data\nUnderstand how to get data into R\n\n\n\nData Sources\nAn obvious way to get data to answer whatever question you have is to collect it yourself. This can lead to high-quality data tailored to the question you have. It is also likely expensive and time-consuming to acquire. If you do decide to collect data, it is a good idea to sketch out planned analyses as carefully as possible. It would be annoying if you forgot to obtain a piece of information that you later decided was crucial. The gold standard of data collection in human research are clinical trials that want FDA approval. These trials have to pre-register the question(s), the analysis plan, and the data collection plan. This approach means everything is run at a high level of rigor and quality. Even if you do not try to get FDA approval with your study, precisely specifying the analyses you plan to run is the best practice and will minimize p-hacking and other sources of bias (if you are unfamiliar with the term p-hacking, see e.g., the Pitfalls section on the General Resources page). Unfortunately, pre-registration is still very uncommon in most areas of research, and often not feasible.\nIf you cannot or do not want to collect data yourself, you could instead team up with someone who does. The advantage of working directly with a data collector is that you have a subject matter expert as a collaborator, and you can ask them questions.\nData that is publicly available, or that you can get after requesting it and being approved by the data owner, constitutes the largest pool of data these days. Going this route gives you access to a lot of different datasets. The drawback is that the data was not collected to answer your specific question, and there is usually nobody you can ask for clarification. Also, the quality of such data varies. As more and more data is collected on almost every aspect of our world, these sources of data keep increasing. It is hard to keep track of places to get (good) data. I have been trying to collect a few sources of data on this website I maintain with various links to resources, among them is a list with links to various data sources. There are likely tons of other good data sources. The tricky bit is sometimes getting the data and understanding enough about what the data is and how it was collected to allow for reasonable analysis.\nYou might have already, or will soon notice that there are datasets that come with R, and even more with R packages. For instance, this page lists what is likely only a small fraction. There is even a Reddit group dedicated to R datasets. The good and the bad about datasets that come with R packages is that they are often fairly clean/tidy. That‚Äôs unfortunately not how most ‚Äúreal world‚Äù datasets look like. Getting dirty and messy datasets and wrangling them into a form that is suitable for statistical analysis is part of most workflows and often takes a lot of time. If you are lucky, you might get or find a dataset that is already fairly clean and use it to answer an actual question of interest. Most of the time however, this does not happen and you need to spend a good bit of time getting your data into the right shape.\n\n\nData to-dos\nNo matter the source, you should try to get the data as raw as possible so that you can be in control of as many cleaning and processing steps as possible. That is (a bit) more work for you, but it gives you more flexibility to decide what to do with the data as you process it. If for instance, you get data with age in years, you can leave it as is, or decide to categorize it as young/old. Categorizing is generally a bad idea, but we‚Äôll talk about that later. If you get data in an already processed form and someone already did this categorization for you, you cannot choose.\nWherever you get data from, document it as much as possible. When did you get it? Where and how? Did it come with other meta-information, e.g., a codebook? Where is it? Are there other things about the data that one should know about? Write down everything in some document.\nTreat the raw data as you would a very fragile object. Ideally, do not touch it. Do not edit it. You want to only read the data into R, even if it is in a bad format, and then apply fixes/cleaning in R. If this is not possible, and you need to make edits to the data in whatever format you got it (e.g., Excel, SAS), make a copy of the data files and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down the edits you made.\nA large portion of data is entered and stored in spreadsheets, such as Excel. If you collaborate with others who produce and enter the data, especially if it is a frequent collaboration, having a discussion with them about best practices for data entry and storage could potentially save you a lot of time. If this is a situation you find yourself in, I recommend you read, and then ask your collaborators to read and implement the excellent paper Data Organization in Spreadsheets by Broman and Woo.\n\n\nData import in R\nSeveral functions in base R can be used for data import. (For example, read.table() or its variants is enough for many projects.) Some packages further expand the functionality for importing data into R. Since data come in so many types and forms, we will not be able to cover all of it. We will only look at a few examples. I am providing links to resources to get you started importing different kinds of data. If you have data of different type, Google is your friend üòÑ.\nHere are some of the most prominent ones:\n\nThe readr package is great first choice for importing CSV and similar spreadsheet type data. For some more introductory material on the readr package, see chapter 11 of R4DS. It can often guess the right way to import data, even if it is slightly messy. But always check! It‚Äôs a good first choice for CSV/TSV/text style data.\nreadr is not suitable for data in Excel format, for that use the readxl package.\nFor dealing with Google sheets, the googlesheets4 package can be used.\nThe haven package is good for dealing with SPSS, SAS and Stata data.\nFor reading in various online data forms, e.g., XML and JSON, see the Import section on the tidyverse packages site.\nThe folks from Posit/RStudio but together a useful data import cheat sheet, which is worth checking out.\n\nThe packages I listed above are all from the tidyverse. There are many more packages with additional functionality, e.g., for reading in large data fast.\nTo learn some more, you can check out the Importing Data chapter in IDS. The Web Scraping chapter in IDS provides additional information on how to pull online data into R.\n\n\nChecking the data\nIf you are lucky, your data reads in properly on the 1st try. Often, that is not the case. Getting data into R in such a form that you can start using it often requires a few tries.\nSometimes, the data might not load at all. You will get an error message and then have to figure out what to do to get the data into R. In some cases, you might not be able to get the data into R without editing it in its native format. For instance, reading proprietary data (e.g., SAS or Excel) doesn‚Äôt always work too well. In those instances, it is sometimes better to export the data from those programs as a comma-separated or tab-separated values file (CSV/TSV) and then read it into R.\nAnother example is if you get an Excel spreadsheet where the person used color to code some information. You might need to fix that and recode the color in the spreadsheet before reading it in. If you need to, make edits on the copies of the raw data until it is in a form that you can load into R.\nIf the data loads, it might do so with or without error or warning messages. In either case, you will want to look at the data to make sure what you expected to be there is there. Check if the data has the right number of rows and columns. Do other quick checks to ensure things look good enough to start working with the data. The str command is handy for that, as is glimpse from the dplyr package. As long as everything is there, no matter how messy, you are ok and can now use R to clean up and explore the data, which we will cover next.\n\n\nExamples of data import\nIf you want to see a few examples of live-coding which show how to read data into R, you can check out these videos by Jeff Leek."
  },
  {
    "objectID": "content/module-data-intro/data-types.html",
    "href": "content/module-data-intro/data-types.html",
    "title": "Types of Data",
    "section": "",
    "text": "For this unit, we will discuss different types of data and how data type influences the analysis approach.\n\n\n\nYou might not want to get yourself that kind of data. Source: dilbert.com."
  },
  {
    "objectID": "content/module-data-intro/data-types.html#basic-data-types",
    "href": "content/module-data-intro/data-types.html#basic-data-types",
    "title": "Types of Data",
    "section": "Basic data types",
    "text": "Basic data types\nCharacters/strings: A string is a collection of characters. You will often hear the labels ‚Äúcharacter‚Äù and ‚Äústring‚Äù used interchangeably, though strictly speaking, a string is a collection of characters. Everything can be encoded as a character string. Unfortunately, you cannot do a lot of analysis with strings. For instance, if you have the numbers 2 and 3, you can add and subtract. But if you code those numbers as characters ‚Äú2‚Äù and ‚Äú3‚Äù, you can‚Äôt do much with them. Thus, transforming characters into other, more useful categories (if applicable) is a common task. It is common that you read in some data and there is a variable which should be numeric, but some entries are not (e.g., the original spreadsheet shows something like ‚Äú&lt;10‚Äù). In this case, R reads all of these variables as characters. You then have to go in, clean the ‚Äú&lt;10‚Äù value, and convert the rest to numeric. Sometimes you do want to work with strings directly. There are many tools and packages in R that are helpful, including base R command. The stringr package is particularly useful.\nIt is quite likely that you will need to work with strings at some point during a data analysis, even if it is only to find specific values, clean up variable names, etc. Thus, learning more about this topic is a good idea. A very powerful, and also very confusing way to deal with strings is to use what are called regular expressions (or regex for short). This concept applies to any programming language, not just R. Being at least somewhat familiar with the concept of regular expressions is useful.\nIf you have no experience manipulating strings, I suggest you work through the Strings chapter (14) of R4DS, and do the exercises. The string processing chapter (25) of IDS contains further good material that is worth working through. Another good source is the Character Vectors chapter in the STAT 545 book by Jenny Bryan. Take a look at those various sources, decide which one is right for your level and go through some of them. And/or consult them as needed.\nFactors: That‚Äôs what R calls categorical variables. They can be ordered/ordinal or not. You need to make sure variables that should be coded as a factor are, and that those that shouldn‚Äôt be aren‚Äôt. For instance, you might have a variable with entries of 0, 1, and 2. Those could be numeric values, e.g., the number of siblings a person has. Or it could be a factor coding for 3 types of ethnicity (unordered), or 3 levels of socioeconomic status (ordered). You need to make sure it is coded as factor or numeric, based on what you know about the variable. An excellent package to work with factors is the forcats package.\nTo learn some more about factors, you might want to go through the Factors chapter of R4DS, and do the exercises.\nLogical: You can think of a logical variable as a type of categorical variable with 2 categories, TRUE and FALSE. Alternatively, in R, 0 is interpreted as FALSE and 1 as TRUE (and vice versa). You will use those logical values often when checking your data, e.g., if you want to see if your variable x is greater than 5, then the R command x &gt; 5 will return either TRUE or FALSE, based on the value of x.\nNumeric (double/integer): Numeric values that are either integers or any other numeric value (double). You generally do not need to care too much how exactly your numeric values are coded. Often, you can treat integers as general numeric value. (In R, a general numeric variable is called double.) You might rarely come across a case where some analytic method or other bits of code requires integers to be specified as such. In R, you can use the as.integer() function to convert general numeric values to integers. You don‚Äôt really need any other special packages in R to deal with numeric values. Note that when you type an integer value, e.g.¬†x &lt;- 2, into R, this is considered numeric by default. If you want to make sure it is treated as integer, add an L, e.g.¬†x &lt;- 2L.\nDate/time: While dates are a type of continuous numeric variable, you should assign the date class explicitly in R, which allows you to do more with them. Dates are quite difficult to work with in base R (which usually calls them POSIX variables). The lubridate package is a good package to work with dates, and is more user friendly. Others exist.\nTo learn some more about dates and times in R, check out the Dates and times chapter of R4DS as well as the Parsing Dates and Times chapter of IDS."
  },
  {
    "objectID": "content/module-data-intro/data-types.html#data-structures-in-r",
    "href": "content/module-data-intro/data-types.html#data-structures-in-r",
    "title": "Types of Data",
    "section": "Data structures in R",
    "text": "Data structures in R\nThe basic data types in R are usually combined into larger objects. The main ones in R are described in the following.\nVectors: vectors are a simple collection of elements in a single row or column. In R, the easiest way to create vectors is with the concatenate command, c(). An example is x1 &lt;- c(3,12,5). A single vector can contain only one element type (e.g., all characters or all numeric). If you try to mix and match, everything ends up as a character. Type the command for x1 and x2 &lt;- c(6, 5, 'h') into R and apply the class() command to both x1 and x2 and note the difference.\nMatrices: A matrix is a collection of elements in rows and columns. A matrix can contain only one element type. You can think of a matrix as a collection of horizontal vectors stacked on top of each other or vertical vectors next to each other.\nData frames: A data frame has the same shape as a matrix, i.e., it is a collection of elements in rows and columns. The critical difference is that each column of a data frame can contain elements of different types. This makes it ideal for storing data, with each row and observation and each variable in a column, and different columns potentially with different data types. E.g., column 1 could be age and numeric, and column 2 could be gender and be categorical, etc.\nA list: Lists are the most flexible data types in R. You can combine different elements as in data frames. Further, each element can be of varying length. For instance, you could have the first list element contain a person‚Äôs name, the second list element their age, the third their address. You can even have other elements inside lists, for instance, you could have a data frame as a list element containing the names and ages of the person‚Äôs parents. Lists are very flexible, and if you get deeper into data analysis, you‚Äôll be working with them. The downside is that because they are more flexible, they can also be a bit more confusing to work with. With enough practice, you‚Äôll figure it out. Also note that almost every function in R that returns something a bit more complicated to you (e.g., the result from a linear fit), returns it as a list.\nOther types of data structures exist; they are often introduced by specific R packages. An important one to know is the tibble which is a type of data frame used in the tidyverse. It is similar, but not exactly like a data frame. You can read more about tibbles on its package website.\nFor some more information, you can check out this video by Jeff Leek where he talks about the types of data and structures I described above. He also shows some R/coding examples and discusses the important concept of missing values and NA."
  },
  {
    "objectID": "content/module-data-intro/data-types.html#non-rectangular-data",
    "href": "content/module-data-intro/data-types.html#non-rectangular-data",
    "title": "Types of Data",
    "section": "Non-rectangular data",
    "text": "Non-rectangular data\nWe don‚Äôt have the time to cover analysis of all the variety of data that exists. As stated before, our focus here was on the analysis of what is sometimes called rectangular data. We have observations (individual units of data) generally as rows, and variables (outcome and predictors) as columns. This is still the most common type of data, especially in public health and more generally the life sciences. However, more complex types of data are rapidly increasing in frequency and importance. If the data has a different structure, it will usually require different, specialized analysis approaches. The good news is that most of what we cover in this course still applies. The difference is often in the processing of the data, and the exact way data and models are combined.\nAt the end of the course, there is a page that briefly goes into other types of data (time-series, text, and some others) and provides very brief pointers for further resources, should you be interested in data of that type."
  },
  {
    "objectID": "content/module-data-intro/data-types.html#footnotes",
    "href": "content/module-data-intro/data-types.html#footnotes",
    "title": "Types of Data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLogistic regression, which you might be familiar, is used for classification. However, the underlying model predicts a quantitative outcome (a value between 0 and 1 usually interpreted as a probability), which is then binned to make categorical predictions.‚Ü©Ô∏é"
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html",
    "href": "content/module-data-presentation/assessment-presenting-results.html",
    "title": "Assessment - Presenting Results",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#find-a-graph-to-reproduce",
    "href": "content/module-data-presentation/assessment-presenting-results.html#find-a-graph-to-reproduce",
    "title": "Assessment - Presenting Results",
    "section": "Find a graph to reproduce",
    "text": "Find a graph to reproduce\nFind an interesting graph from a news website. The main requirements are that the original graph must be interesting and good looking (a basic scatter plot is not enough), AND that the graph is freely available online, not behind a pay-wall. Ideally, the data that went into making the graph should be available too. If it is not, you will have to go through the extra step of extracting the data from the graph using tools like DataThief or WebPlotDigitizer.\nOne suggestion is to look at graphs from FiveThirtyEight. They are known to produce high-quality graphs, and for some of their stories and figures, they also provide the original data source.\nGood starting points for a chart from FiveThirtyEight might be their annual summaries of weirdest charts, which you can find here for 2015, 2016, 2018, 2019, 2020,\n2021,\n2022. (No, I don‚Äôt know why there‚Äôs no 2017.)\nOr you can go straight to their data, and see what graphs they produced with it and find a good one.\nIf you can‚Äôt find a graph on FiveThirtyEight that you like and would like to reproduce, you are also allowed to check out other major news outlets (USA Today, NY Times, Wall Street Journal, The Guardian, etc.)."
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#get-the-data",
    "href": "content/module-data-presentation/assessment-presenting-results.html#get-the-data",
    "title": "Assessment - Presenting Results",
    "section": "Get the data",
    "text": "Get the data\nAs mentioned, you either need to have the data in a file, or you need to extract it from the graph and place it into a file. I recommend a CSV file."
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#ask-ai-to-re-create-the-original-graph",
    "href": "content/module-data-presentation/assessment-presenting-results.html#ask-ai-to-re-create-the-original-graph",
    "title": "Assessment - Presenting Results",
    "section": "Ask AI to re-create the original graph",
    "text": "Ask AI to re-create the original graph\nAsk an AI of your choice (or try several) to re-create the original graph. I‚Äôm not sure what the best prompt is. You probably want to feed it the data and instructions on what kind of plot to make. I‚Äôm not sure if giving it the original plot, either as uploaded file or as a URL, will help. You can try. You will likely have to go through multiple iterations of asking the AI to update the code/figure.\nYou will likely want to use ggplot2 & friends, but if you want to use base R, lattice or another plotting approach that‚Äôs ok too, as long as everything happens with R code.\n\nIf you already feel comfortable making high-quality graphs with ggplot2 and want to try something different, feel free to create an interactive graph using one of the many options for that (e.g., Shiny, plotly). You can also recreate a previous static graph and make it interactive (e.g., allowing people to turn on/off specific parts).\n\nDocument your AI prompts in the visualization-exercise.qmd Quarto file. Also show the code that the AI produced. Depending on how many iterations you do, you don‚Äôt need to show the code for all intermediate steps.\nAs needed, manually edit the code if the AI gets stuck. Try to get as close as possible to the original graph. You should end up with a Quarto file that shows your AI prompts, possibly intermediate code that the AI returned, and finally a fully working bit of code that loads/takes the data and re-produces the original plot. Also display the original plot next/below your reproduction for comparison."
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#create-a-publication-quality-table",
    "href": "content/module-data-presentation/assessment-presenting-results.html#create-a-publication-quality-table",
    "title": "Assessment - Presenting Results",
    "section": "Create a publication quality table",
    "text": "Create a publication quality table\nFor this part, the easiest approach is to use the same data you used for the graph (or a subset of the data) and create a nice looking table.\nAlternatively, find a good-looking table somewhere online and try to recreate it. Of course, you need all data necessary to re-create the table. Often (but not always) that‚Äôs just the numbers in the table.\nAgain, start with AI. You can let it decide which R table package to use, or you can specify that as part of your prompt. Use whichever R table package you like. This tutorial shows how to make nice tables with the gt package. Our group uses flextable a lot.\nNo matter what R package you use to make the table, it should look professional, visually pleasing, and nicely convey the information. For inspiration, you can for instance check out this R table gallery. The R graph gallery also has a section on tables.\nAs for the graphs, feel free to make your table interactive.\nAdd the code and explanatory text (e.g., your AI prompts) into a new section at the end of your Quarto document. Once this part is finished, you should have working code that generates a nice-looking table.\nOnce all done, re-build your website and make sure that your data visualization exercise shows up nicely. Then, post the link to your online page showing your visualization into the discussion channel for this module. Do that by Wednesday so your classmates can take a look and comment by Friday."
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r.html",
    "href": "content/module-data-presentation/tables-in-r.html",
    "title": "Tables in R",
    "section": "",
    "text": "Unit to come.\n\nOverview\n\n\nLearning Objectives\n\n\nIntroduction\n\n\nFurther resources"
  },
  {
    "objectID": "content/module-data-presentation/visualization-overview.html",
    "href": "content/module-data-presentation/visualization-overview.html",
    "title": "Visualization Overview",
    "section": "",
    "text": "Overview\nIn this unit, we will discuss visualization (a.k.a. making figures), what they are good for, and how to do them in R.\n\n\n\nFlowcharts are sometimes, but not always good visualization tools. Source: xkcd.com.\n\n\n\n\nObjectives\n\nUnderstand when and how to use graphs.\nBecome familiar with major types of graphs and when to use them.\nLearn how to implement high-quality graphs in R.\n\n\n\nMotivating Examples\nThe topic of visualization is enormous, and impossible to cover thoroughly in a week (like pretty much any topic we talk about in this course üòÉ). To get you started, the two following TED talks provide some examples of interesting and inspiring data visualizations.\nIn the first video Mona Chalabi talks about good ways to present data and statistics and how good graphs can help in understanding.\n\n\n\n\nIn the second video Chris Jordan discusses how one can take abstract facts and make them much more tangible through visualization.\n\n\n\n\nIf you are interested, you can find more interesting talks on visualization in this TED topics site and this TED Studies collection.\n\n\nVisualization uses\nFigures and graphs are useful at any stage of your analysis. Use them early to explore your data, to get an idea for what you have, to figure out what cleaning you might need to do, and to check what analysis approaches might be useful. Use visualizations once you have done your analysis to check your results and interpret them. Use them at the end to communicate your results. While tables can be useful tools of communication as well, they tend to be much harder to understand and are ‚Äì in my opinion ‚Äì overused in science. Good figures are, most of the time, easier to understand and a more effective way to look at data.\n\n\nTypes of graphs\nI am sure you are familiar with several common types of graphs, such as scatter plots, boxplots, and bar charts. Many more chart types exist, some are much better at conveying potentially complex information than the simple standard ones. An excellent resources is the From Data to Viz website. It lists a lot of different graphs based on the kind of data you want to plot, and for each graph it gives a brief description and often a link to the R Graph Gallery which shows examples with code. Sometimes you start with data and are looking for the best way to plot it. Other times you see a great graph and want to know what it is called. These 2 websites are great resources for either situation.\nAnother great source that provides an overview of different graphs is the Directory of Visualizations chapter of the excellent Fundamentals of Data Visualization book by Claus Wilke. Further chapters in his book describe specific types of plots in more detail, and also provide general best practices for making plots. If you are interested in specific types of plots and general concepts of visualization, skim through his book.\n\n\nFurther visualization resources\nSee the General Resources page for some additional sources."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html",
    "href": "content/module-intro-tools/introduction-course.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to the class Modern Applied Data Analysis (MADA)!\nThe course is listed as EPID8060E/BIOS8060E. It doesn‚Äôt matter if you are enrolled under the EPID or BIOS label. I will generally refer to the course as MADA."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#introduction",
    "href": "content/module-intro-tools/introduction-course.html#introduction",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to the class Modern Applied Data Analysis (MADA)!\nThe course is listed as EPID8060E/BIOS8060E. It doesn‚Äôt matter if you are enrolled under the EPID or BIOS label. I will generally refer to the course as MADA."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#module-overview",
    "href": "content/module-intro-tools/introduction-course.html#module-overview",
    "title": "Course Introduction",
    "section": "Module Overview",
    "text": "Module Overview\nThis first module provides a brief introduction to the course, the tools we will be using, and the topic."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#learning-objectives",
    "href": "content/module-intro-tools/introduction-course.html#learning-objectives",
    "title": "Course Introduction",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe specific learning objectives for this unit are:\n\nKnow what this course is all about.\nKnow how this course is set up and what you are expected to do."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-goals",
    "href": "content/module-intro-tools/introduction-course.html#course-goals",
    "title": "Course Introduction",
    "section": "Course Goals",
    "text": "Course Goals\nThe main goal for this course is for you to learn the whole process of performing a data analysis project. The focus is on applied analysis of real world, messy data.\nA second goal is to introduce you to some modern analysis approaches commonly referred to as Machine Learning.\nA related goal is to introduce you to a set of tools that allow for a modern, reproducible workflow of your analysis.\nFor more detailed learning objectives, see the Syllabus.\n\n\n\nIn this course, I randomly switch back and forth between singular and plural. Source: phdcomics.com."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-philosophy",
    "href": "content/module-intro-tools/introduction-course.html#course-philosophy",
    "title": "Course Introduction",
    "section": "Course Philosophy",
    "text": "Course Philosophy\nHere are my goals, promises, and expectations for this course.\n\nI expect you to be self-motivated and committed to learning the material by putting in the effort needed to succeed.\nI will try to maximize the rewards you get by hopefully teaching methods that are useful to you, and I will try to provide as much help as needed to maximize your learning.\nThis class strives to be challenging but non-threatening. As such, I‚Äôll make you work hard, and expect you to do the assigned tasks by the deadlines, but in the end, I usually don‚Äôt grade hard - unless you fail to keep up your end of the agreement and don‚Äôt put in the work.\nThis class is open everything. You can use the internet, AI, ask your classmates, myself and others, get help from wherever you can. I trust you will find the right balance of getting help when you need it while still putting in enough effort to experience real learning.\nI will not perform any policing to try to prevent you from taking shortcuts (i.e., not doing work yourself). The class contains graded assessments with deadlines, but those are meant to help you stay on track. If you somehow cheat - and cheating will be easy - you are mainly cheating yourself out of learning.\n\nOverall, I hope this course is going to be useful, interesting, challenging and also interactive. Online courses are always a bit tricky with interaction/participation. I hope we can create something online that feels like a classroom. Please participate, ask questions, etc. The more you engage in the course, the more you‚Äôll get out of it."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-setup",
    "href": "content/module-intro-tools/introduction-course.html#course-setup",
    "title": "Course Introduction",
    "section": "Course Setup",
    "text": "Course Setup\n\nThe course is split into modules. Each module will usually be covered in a week. The Schedule document gives an outline. The schedule is not fully finalized and will change, so check frequently.\nEach module consists of one or several units/documents containing a mix of things I wrote, and writings or videos by others. They are listed in the order you should go through them.\nAll material for the course can be accessed through this course website. Some material might not be available yet and will be unlocked as the course proceeds.\nThere is generally a lot of material for each unit. You are expected to go through the main components at a level that allows you to get the big picture and be able to answer the quiz questions. Once you get the overall idea, consider the materials to be resources you can visit on demand, e.g., when you are working on the exercises.\nYou will be placed in groups throughout the course. The hope is that this will create a support group of classmates where you can help each other. Assignments often require interaction among group members (like in a real-world team), and I hope you will interact with your group members outside the assignments as well. You are welcome and encouraged to interact with anyone else in the course."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-tools",
    "href": "content/module-intro-tools/introduction-course.html#course-tools",
    "title": "Course Introduction",
    "section": "Course Tools",
    "text": "Course Tools\nThis is a brief overview of the tools we will be using for this class. The next module, which you can and should start right after finishing this one, describes all these tools in more detail and gives instructions on how to set them all up.\n\nThis website is the main resource for all materials.\nWe will use Discord for discussions and help.\nWe will use an online system for quizzes.\nWe will use the R software for writing code.\nWe will use RStudio (a graphical frontend to R). You can use a different editor if you want, but all instructions assume you are using RStudio.\nWe will use Quarto, which nicely plays with R/Rstudio to let you create easy, automated workflows.\nWe will use Git/GitHub for exercises and the project.\nWe will use GitKraken as Git/GitHub frontend. You can use a different software, but all instructions assume you are using GitKraken."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#assessments",
    "href": "content/module-intro-tools/introduction-course.html#assessments",
    "title": "Course Introduction",
    "section": "Assessments",
    "text": "Assessments\nFor details on assessments, see the Assessments page."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-resources",
    "href": "content/module-intro-tools/introduction-course.html#course-resources",
    "title": "Course Introduction",
    "section": "Course Resources",
    "text": "Course Resources\nWe‚Äôll be drawing on a lot of different resources. I compiled a list with the ones we‚Äôll use and others you might find helpful in the Resources section of the class website."
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#getting-help",
    "href": "content/module-intro-tools/introduction-course.html#getting-help",
    "title": "Course Introduction",
    "section": "Getting help",
    "text": "Getting help\nI do not expect you to figure it all out yourselves. You will get stuck and are encouraged to seek help. You can ask for help from your classmates or your instructor. Also, use the wider community online. For specific places to get help, see the Resources pages."
  },
  {
    "objectID": "content/module-intro-tools/tools-github-introduction.html",
    "href": "content/module-intro-tools/tools-github-introduction.html",
    "title": "Git/GitHub Introduction",
    "section": "",
    "text": "Overview\nThis document gives a brief explanation of Git, GitHub and related tools, and describes why and how we will use them for this course.\n\n\nLearning Objectives\n\nKnow what Git and GitHub are.\nKnow why one might want to use them.\nCreate and set up a GitHub account.\nSet up a Git/GitHub workflow using GitKraken or an equivalent setup.\n\n\n\nWhat are Git and GitHub\nGit is software that implements what is called a version control system. The main idea is that as you (and your collaborators) work on a project, the software tracks, and records any changes made by anyone. GitHub is distinct from Git. GitHub is in some sense on online interface/website, and Git the underlying engine (a bit like RStudio and R). Since we will only be using Git through GitHub, I tend to not distinguish between the two. Throughout the course, I will usually refer to all of it as just GitHub. Note that other interfaces/platforms for Git exist, e.g., GitLab or Bitbucket.\n\n\nWhy use Git/GitHub\nYou want to use GitHub to avoid this:\n\n\n\nSource: phdcomics.com\n\n\nGitHub gives you an organized way to track your projects. It is very well suited for collaborative work. Historically, version control was used for software development. It has since become broader and is now used for many types of projects, including data science projects. If you want to learn a bit more about Git/GitHub and why it is a great tool for data analysis, check out this article by Jenny Bryan.\n\n\nWhat to (not) use Git/GitHub for\nGitHub is ideal if you have a project with (possibly many) smallish files, and most of those files are text files (such as R code, LaTeX, Quarto/Markdown, etc.) and different people work on different parts of the project.\nGitHub is less useful if you have a lot of non-text files (e.g.¬†Word or PowerPoint) and different team members might want to edit the same document at the same time. In that instance, a solution like Google Docs, Word+Dropbox, Word+Onedrive, etc. might be better.\nGitHub also has a problem with large files. Anything above around 100MB can lead to very slow syncing and sometimes outright failure. Unfortunately, once you have a failed attempt to sync a large file, it can be quite tricky to fix the repository to work again. Therefore keep large (&gt;100MB) files out of your GitHub repositories. If you have to work with such files, try to reduce them first before placing into the GitHub repository. Or as alternative, place those files in another sync service (e.g.¬†Dropbox, OneDrive, GoogleDrive) and load them from there.\nFinally, if you have data, you need to be careful since by default, GitHub repositories (the GitHub name for your projects) are public and everyone can see them. You can set them private, but you need to be careful that you don‚Äôt accidentally expose confidential data to the public. It is in general not a good idea to have confidential data on GitHub. First anonymize your data enough that it is not at risk of identifiability, then you can place it in a private repository. If you put it public, be very careful that you only make things public that are ok to be made public.\n\n\nGetting Git/GitHub\nGit is a piece of software. It can be installed from the Git website. If you use GitKraken as described below, you don‚Äôt need to install Git separately. However, it might still be worth doing so, in case at some point you need to do some Git operation outside of GitKraken (e.g, through RStudio or a terminal).\nGitHub is an online platform. You don‚Äôt need to install anything, but if you don‚Äôt have a GitHub account yet, you need to create one on GitHub.com. Note that GitHub is widely used professionally. You might use it beyond this class and for your future career. You might also want to allow other people to see your GitHub presence. You should therefore use a future-proof, professional sounding user name for your GitHub account. You can use your UGA or a different email for GitHub. Whichever email you use is where you will receive GitHub activity related notifications (including our GitHub class activities).\n\n\nHow to use Git/GitHub\nThe most powerful and flexible way of using Git/GitHub is to open a command-line terminal (e.g., the one that comes with the Git install) and type a bunch of commands. This is often very confusing and we will not be using this approach.\nThe approach I recommend for this class, and the one I assume you will be using, is to interact with GitHub through a graphical interface. Specifically, for this class I recommend a software called GitKraken.\nIf you are already an experienced GitHub user and have your own preferred setup (e.g.¬†the command line or some other 3rd party software), or for some other reason you don‚Äôt want to use GitKraken, you can instead use another graphical client. GitHub itself provides a graphical interface with basic functionality. RStudio also has Git/GitHub integration, for R projects. Other graphical interfaces exist. I assume if you decide to do it your way, you are experienced enough to translate my GitKraken-centric instructions to your setup. While my instructions throughout this course assume you use GitKraken, you can do everything without using it. But then you have to figure out on your own how to do it üòÅ.\n\n\nGitKraken\nDownload and install GitKraken. The free version can do pretty much everything we need. One important limitation is that the free version does not allow you to access private repositories. As student, you can (and should) upgrade to the Pro version for free, see the GitHub developer pack section below on how to do it. By using GitKraken for your GitHub workflow, you do not need to go through the command line (unless things get so bad that you can‚Äôt fix them inside GitKraken). You also do not need to install Git separately, you get it with GitKraken. For this course, I assume you will be using the Pro version of GitKraken.\nOnce you have your GitHub account set up and GitKraken installed, make sure you connect the two.\n\n\nGitHub developer pack\nPlease get the GitHub student developer pack. This pack gives you a lot of nice free goodies that you might be interested in. I encourage you to explore all the different products and deals. For this class, we most want the following:\nGitHub Pro account: This will give you your own free private repositories. We will mostly use public repositories, but there might be times you want to have a private repository, e.g.¬†for data you don‚Äôt want to be public. Normally, if you want private repositories, you have to pay. But with the student pack (or as an academic), you get them for free.\nGitKraken Pro: Free GitKraken Pro access. While the free version of GitKraken might work ok for this course, you can‚Äôt access private repositories with it. Often, being able to use private repositories is useful.\nGitHub Copilot: Which gives you AI support for writing code.\nThere are a lot of other goodies that come with the student pack. We won‚Äôt use them in the course, but some might be of interest to you.\n\n\nGit/GitHub terminology\nGit/GitHub has a lot of specialized terminology that takes getting used to. The GitHub folks posted a handy page with short descriptions of some of the important terms. Some of the terms you will encounter soon are Repository (also often called repo), User, Organization, Branch, Fork, Push, Pull, Commit and Stage. For some reason, the last term is not explained in the list linked to above. Staging is the step you need to do after you made changes and before you commit them. For pretty much all of my work, I find this a not too helpful extra step, but you need to know what it means and you need to do/use it as part of the GitHub workflow.\nEach repository can have multiple branches (think of it as alternative versions of all files, allowing different people to work on different aspects of a project with minimal interference). In this course, we won‚Äôt really use branches. We will mainly just work with a single branch. The name of the main branch can be anything, but these days the default is main. Until not too long ago, the main branch was referred to as master. This has mostly been discontinued, but you might still see occasionally some Github repositories where the main branch is called master. It might happen that in these course documents, I still occasionally refer to the main branch by master (let me know if you spot the old terminology so I can update). Again, the name can be in principle anything. Just be aware that master and main usually refer to the main branch.\n\n\nGit/GitHub and other cloud based sync options\nIn some way, you can think of GitHub as a more controlled (and more complex) alternative to systems such as Dropbox, OneDrive, GoogleDrive, etc. With the GitHub workflow, you can work locally on different machines, and everything is backed up in the cloud (on github.com). The (sometimes annoying) difference to Dropbox & Co is that the syncing between your local computer and github.com is not automatic. So don‚Äôt forget to pull before you start work on a repository and push once you are done!\nI am a heavy user of both GitHub and Dropbox. I use Dropbox for a lot of regular files, e.g.¬†MS Office documents, pdfs, images, etc. Some of my research projects, especially those where I collaborate with people that don‚Äôt use GitHub, are run through Dropbox. Dropbox or a similar service are convenient since all the syncing happens automatically. For anything where I want a more structured and organized approach, e.g.¬†coding projects, some research projects or course materials like this site, I use GitHub.\nYou can do any setup you want. However, don‚Äôt mix and match. Because GitHub works similar to Dropbox, it is a bad idea to locally store your GitHub repositories in a folder that is synced by Dropbox. If you do, the Dropbox sync process and the GitHub sync process can conflict, potentially leading to syncing mess. For any GitHub repository, store it locally in a folder that is not synced across your computers by another software (such as Dropbox, OneDrive, Box, etc.)\nSimilarly, don‚Äôt store a GitHub repository inside another repository. While this is technically allowed, it can easily lead to a mess. Each repository should be separate and not connected to any other syncing service.\n\n\nGitHub for this course\nWe will make heavy use of GitHub for this course. Instead, everyone will just be working through their own GitHub accounts. The advantage of this setup is that once the course ends, everything you did for the course automatically stays with you.\nPrivacy note: As mentioned on the Course Syllabus page, this course is set up such that you work ‚Äúin the open‚Äù through public GitHub repositories. Please make sure to read the Privacy/FERPA section of the syllabus and let me know if you have any concerns. If I don‚Äôt hear from you, I consider this as you consenting with producing public materials as part of this course, and thus make your participation in this course public.\nFrustration note: Git/GitHub can be finicky, even for seasoned users. Sooner or later, you‚Äôll run into some issues and strange error messages. If that happens, don‚Äôt panic. You‚Äôll figure out how to fix things. The internet and Jenny Bryan‚Äôs book happygitwithr are great resources. And/or ask on Slack.\n\n\nFurther Resources and help\nGitHub can be confusing. Start slow. Using the graphical interface (GitKraken) makes getting started fairly easy. I‚Äôm also trying to give specific instructions whenever we do something new with GitHub. Both the GitHub documentation and GitKraken support page are good resources for information.\nIf the quick install instructions above are not detailed enough, follow these in Jenny‚Äôs paper or her great online book happygitwithr.\nAnother source worth looking at for GitHub information is this chapter of IDS. There is also a lot of good beginner material online."
  },
  {
    "objectID": "content/module-intro-tools/tools-randrstudio.html",
    "href": "content/module-intro-tools/tools-randrstudio.html",
    "title": "R and RStudio",
    "section": "",
    "text": "Overview\nThis is a very quick introduction to R and RStudio, to get you set up and running. We‚Äôll go deeper into R and coding later.\n\n\nLearning Objectives\n\nKnow some of the strengths and weaknesses of R.\nHave R and Rstudio up and running on your computer.\nKnow how to install and load R packages.\n\n\n\nAbout R\nLike every programming language, R has its advantages and disadvantages. Feel free to do a web search on that topic, and you will encounter tons of people with tons of opinions. Some of the features that are useful to us are:\n\nR is Open Source, FREE, and cross-platform.\nR is a ‚Äúhigh-level‚Äù programming language, relatively easy to learn (compared to Fortran, C, etc.).\nR comes with many integrated functions.\nR is great for statistics, data analysis, and graphics.\nThe R Community is very dynamic, helpful and welcoming.\nThrough R packages, it is easy to get lots of state-of-the-art algorithms.\nDocumentation and help files for R are generally good.\n\nWhile we use R in this course, it is not the only option to analyze data. Maybe the most similar to R, and widely used, is Python, which is also free. There is also commercial software that can be used to analyze data (e.g., Matlab, Mathematica, Tableau, SAS, SPSS). Other more general programming languages are suitable for certain types of analyses as well (e.g., C, Fortran, Perl, Java, Julia). Depending on your future needs or jobs, you might have to learn one or several of those additional languages. The good news is that even though those languages are all different, they all share general ways of thinking and structuring code. So once you understand a specific concept (e.g., variables, loops, branching statements or functions), it applies to all those languages. Thus, learning a new programming language is much easier once you already know one. And R is a good one to get started with.\n\n\nAbout RStudio\nRStudio is an integrated development environment (IDE) made by the folks from Posit (formerly the company was also named RStudio, but that changed recently). RStudio is separate from the R programming language. It basically wraps some useful tools around R to making writing and running R code much easier. RStudio is by far the most commonly used environment for folks who write R code, but it is not the only option. There are other IDEs available for R, most notably maybe VS Code. In this course, I assume you are using RStudio and instructions are specific for it. But if for some reason you don‚Äôt want to, you can use a different way to write and run R code. You just have to figure out some things on your own.\n\n\nInstalling R and RStudio\n\nIf you haven‚Äôt already, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version, at least version 4.1 or newer. I recommend you upgrade to the most recent version (as of this writing 4.2.2, but if there is a newer one available, use that one).\nOnce you have R installed, install the free version of RStudio Desktop. Get the most recent version. If you have an older one already installed, update it.\n\nInstalling R and RStudio should be fairly straightforward. If you want detailed instructions, go through this chapter of IDS. If things don‚Äôt work, ask for help on the discussion boards.\nI personally only have experience with Windows (and a little bit of Mac), but everything should work on all the standard operating systems (Windows, Mac, and even Linux).\n\n\nInstalling and loading R packages\nMost of the functionality and features in R come in the form of add-on packages. There are tens of thousands of packages available, some big, some small, some well documented, some not. We‚Äôll be using many different packages in this course. Of course, you are free to install and use any package you come across for any of the assignments.\nThe ‚Äúofficial‚Äù place for packages is the CRAN website. If you are interested in packages on a specific topic, the CRAN task views provide curated descriptions of packages sorted by topic.\nTo install a package from CRAN, go to the R prompt at the bottom left of your RStudio session and type install.packages(\"PACKAGENAME\"). The figure shows an example where I installed a package called learnr. Often, a package needs other packages to work (called dependencies), and they are installed automatically. It usually doesn‚Äôt matter if you use a single or double quotation mark around the name of the package. Note that R cares about capitalization, so you need to get the upper and lower case exactly right. Otherwise, it won‚Äôt work.\n\n\n\nInstalling an R package. Click on the image to enlarge it.\n\n\nTry installing a package yourself. Open RStudio. Then go to the R prompt (the &gt; symbol) in the lower-left corner and type\n\ninstall.packages('DSAIDE')\n\nThis installs a package that gives you access to various infectious disease simulation models. We won‚Äôt do anything with that package in this course, we just install it for practice. If you want to learn more about DSAIDE, take a look at the package website.\nIf this is the first time you are installing packages, you‚Äôll see that a lot of other packages are installed, too. You might get a message about Installing from source packages that need compilation. You should generally say No to this. If you are on a Windows computer, compilation requires you to have Rtools installed. It‚Äôs not a bad idea to install Rtools (if you do, make sure you pick the version that matches your R version.) But even then, or if you use a Mac or Linux (which have the equivalent of Rtools already pre-installed) sometimes the compilation doesn‚Äôt work. So if you have a choice, say No. (On some Mac/Linux setups, things happen automatically, then just let it run.)\nTo see which packages are needed by a specific package, e.g.¬†DSAIDE, and thus are being installed if not present, type tools::package_dependencies(\"DSAIDE\") into the R console. Of course it can be that those packages depend on other packages, so you end up installing even more. At some point, you‚Äôll have the most common packages all installed and installing new packages will lead to less overall installing. The package install process generally works well.\nIn RStudio, you can also install (and update/remove) packages by clicking on the Packages tab in the bottom right window.\nIt is very common these days for packages to be developed on GitHub. It is possible to install packages from Github directly. Those usually contain the latest version of the package, with features that might not be available yet on the CRAN website. Sometimes, in early development stages, a package is only on Github until the developer(s) feel it‚Äôs good enough for CRAN submission. So installing from Github gives you the latest. The downside is that packages under development can often be buggy and not working right. To install packages from Github, you need to install the remotes package and then use the install_github function. We won‚Äôt do that now, but it‚Äôs quite likely that at one point later in this course we will.\nYou only need to install a package once, unless you upgrade/re-install R. Once installed, you still need to load the package before you can use it. That has to happen every time you start a new R session. You do that using the library() command (an alternative is require() but I recommend library()). For instance to load the DSAIDE package, type\n\nlibrary('DSAIDE')\n\nYou should see a short message on the screen. Some packages show messages when you load them, and others don‚Äôt. In this case, the package tells you that you can start using it by typing dsaidemenu() into the R console. DSAIDE is a package I wrote that allows you to explore infectious disease models. We won‚Äôt use it in this class. I‚Äôm just using it as an example here since you can use the package without having to write code. Try it briefly, by typing the code below into the R console\n\ndsaidemenu()\n\nA menu should open in your browser, from which you can explore different models/apps. Once you are done with DSAIDE, close it.\nThis was a quick overview of R packages. We‚Äôll use a lot of them, so you‚Äôll get used to them rather quickly.\n\n\nNote about R packages\nThe quality of R packages varies. In general, if they are on CRAN or bioconductor, they passed some quality checks. That does however not mean that the functions do the right thing, just that they run. Other packages might be more experimental, and while they might work well, there might also be bugs. In general, packages that are used by many people, packages that involve people who work at R-centric companies (e.g., Posit), and packages that have many developers/contributors and are actively maintained are good signs that it‚Äôs a stable and reliable package. That said, there are many packages that are developed by a single person and are only available from GitHub, and they are still very good packages. Ideally, for a new package, test it and see if it does things stably and correctly. If yes, you can start using it. Just always carefully inspect the results you get to make sure things are reliable. If at some point, you work with R in a non-academic setting and you might use R packages for jobs that need to run reliably for many years to come, choosing packages might be a bit more tricky and require more thought. For an academic/research setting, it‚Äôs usually ok to use almost any package, as long as it seems reliable and works.\n\n\nLearning R Studio\nIf you are new to R and RStudio and want to learn a bit more, I suggest you skim through this chapter of IDS.\nWhile one can use R and do pretty much every task, including all the ones we cover in this class, without using RStudio. However, RStudio is very useful; it has a lot of features that make your R coding life easier. It has become pretty much the default integrated development environment (IDE) for R. Since RStudio has lots of features, it takes time to learn them. A good resource to learn more about RStudio are the RStudio Essentials collection of videos."
  },
  {
    "objectID": "content/module-intro-tools/tools-reproducibility.html",
    "href": "content/module-intro-tools/tools-reproducibility.html",
    "title": "Reproducibility, Quarto & Co",
    "section": "",
    "text": "Overview\nIn this unit, you will learn the concept of reproducible research, why it is important and helpful to build your analyses in a reproducible manner, and what tools you can use to implement an automated, reproducible workflow.\n\n\n\nNot a good example of reproducibility.\n\n\n\n\nLearning Objectives\n\nKnow the concept and importance of reproducible research\nKnow about a method to implement reproducible research with R\nBe familiar with Quarto/Markdown and start using it\n\n\n\nReproducibility in Science\nA hallmark of proper scientific research is that findings can be replicated/reproduced. Unfortunately, it is often the case these days that results can‚Äôt be replicated/reproduced by independent investigators/labs. Sometimes, even the same labs can‚Äôt reproduce their previous findings. You have probably heard about the (supposed) Reproducibility Crisis in science. If not, do a quick online search, you‚Äôll find lots of articles saying there is an increasing problem, while others saying that it‚Äôs not getting worse, we are just detecting more. While sometimes there is fraud, most often there are more benign reasons preventing reproducibility.\nThis video provides a short discussion of some of the current problems with reproducibility in science:\n\n\n\n\n\n\nReproducibility of Analyses\nIt‚Äôs hard and expensive to replicate/reproduce a full study, including all data collection, thus not routinely possible. It is easier to make sure the analysis part can be reproduced. Making the analysis easily reproducible doesn‚Äôt ensure the analysis is correct. However, it allows others to take a look at analyses, redo them, and thus more quickly spot and correct potential problems.\nTo make a fully reproducible analysis, you have to provide all the data and code, and the generation of results (figures and tables) needs to be fully automated. The scientific community is moving toward more reproducibility and transparency (e.g., mandating public access to data, computer code, etc.). An increasing number of funding agencies and journals require access to data and code.\nWhile there is a strong movement toward Open Access, providing all the data is not always possible. However, the full automation of data processing, analysis, and result generation is always possible, and we will use this approach.\nIn this video, Roger Peng goes into some more details of the concept of reproducible research:\n\n\n\n\nRoger Peng has additional videos related to reproducible research in this playlist of videos\n\n\nHow to do a reproducible analysis\nMost importantly and fundamentally, document everything.\nDo all processing and analysis in a scripted and well-documented manner. That means no Excel, no manual copy & paste, and no manual figure or table generation. All of these actions are not scripted or documented and as such, are not reproducible.\nSome further things to pay attention are the use of open standards (open data standards, open-source software), recording of software versions used, time-stamping data, and setting a random number seed in your code.\nA reproducible analysis should also be practically reproducible, not just theoretically. By that I mean if you provide code, but the code only runs on some specific computer system you used, then it‚Äôs not reproducible for others. Providing all data and code is a good first step, but your goal should also be to make reproducibility easy. This is beneficial for both the original producer of the results and the persons trying to reproduce it.\n\n\nWhy reproducibility is good for you\nA reproducible analysis is generally automated. That can save you a lot of time. Initially, it seems that doing your analysis in a reproducible and automated manner takes more time and is more difficult because you have to learn some new tools. That is true. However, once you are used to it, you will be much more efficient.\nConsider the case where you had some data in Excel, moved it into SAS to do an analysis, and make some raw figures, opened them in Photoshop and spend a few hours making them look good. Then you or your collaborators realize that some of the data that should have been included in the analysis was not (or some data should not have been included). You need to re-create the raw figure and re-work it in Photoshop, likely spending a good bit of time.\nIf you had an automated analysis, you could just press one (or a few) buttons and re-run everything. Also, automation makes it less likely that errors occur from copying data and intermediate results between programs. The case-study in the introductory unit is such an example, where everything was fully automated.\nMaking an analysis reproducible also means you to document all your steps and what you do well. So it not only helps others, but future you will be very thankful. The importance of documenting the process increases, as analyses get more complex.\n\n\nQuarto and R Markdown for reproducible analysis\nCreating a reproducible and automated analysis used to be a good bit of extra work, but not anymore. A large number of software tools are available that make it fairly easy to set up a reproducible workflow.\nWhile there are different tools and programming languages that allow reproducible research (e.g.¬†Jupyter notebooks in Python, Mathematica notebooks, Sweave, Latex), we will focus on one tool, namely the Quarto system. (GitHub is also part of this reproducible workflow, but we discuss that separately).\nThe folks from Posit (formerly Rstudio) previously developed R Markdown. This allowed you to combine R code with Markdown text. More recently, they developed Quarto, which is an improved version of the whole system that allows using different programming languages and creating many different types of outputs (papers, presentations, websites, etc.).\nR Markdown is still around and functional. For instance, this course website is written in R Markdown and lives on GitHub. But in my opinion, Quarto is the future. We‚Äôll therefore focus on using it. Eventually I‚Äôll switch all my materials over to Quarto. The good thing is that the transition is easy.\nThe idea behind Quarto is that you have a system that allows you to combine code with text and simple formatting to easily and reproducibly turn them into a lot of different output formats, e.g., HTML, PDF, Word, or slides. You can apply layout and styling to those documents, which is done separately from the content. This means you can quickly switch between outputs. Quarto calls various other pieces of software to - almost magically - turn text and code input into a variety of different output formats. The good thing is, you generally don‚Äôt need to care what goes on under the hood, it all happens (almost always) smoothly behind the scenes.\nThe input (for us) into a Quarto document are R code and Markdown text. Markdown is a simple way of formatting text to get some reasonably nice layouts with minimal effort. If you have no experience with Markdown, I suggest you go through this nice, short interactive tutorial. A good reference to look up formatting for Markdown until you have it memorized is this online cheat sheet.\nQuarto and R Markdown Note: Quarto is fairly new. It didn‚Äôt exist yet last time I taught the course. I‚Äôm switching to Quarto because I think it‚Äôs the future. But you might still find me talking about R Markdown in some places. If you do, let me know so I can update. In general, either R Markdown or Quarto will do the job, but Quarto is the future, so it‚Äôs best to use/learn it.\n\n\nGetting started with Quarto\nThe whole Quarto system is already incredibly flexible and powerful (and continues to grow). Because it is very feature rich and you can do a ton, trying to read all about it does not make much sense. The best is a learn it as you need it approach. The Quarto website has great documentation, you‚Äôll likely be going there often.\nTo get started, follow the get started guide on the Quarto website. Install Quarto and - if you haven‚Äôt already - R Studio. Then go through the 3 starter tutorials. There is already a lot going on in those tutorials, and there is no need to memorize it. The point is to get an idea of what you can do and play around a bit. You‚Äôll repeatedly be coming back to the Quarto documentation as you try to figure out how to do specific things. So if some of the material in the tutorials is not quite clear on your first pass-through, that‚Äôs ok. Just get an overall idea and play around so you feel comfortable. You‚Äôll be using Quarto in the exercise (and pretty much all the time from now on).\n\n\n\n\n\n\nOther reproducibility tips\nWe‚Äôll cover that more later, but it is important to not do things that only work on your computer but not on others. The most important and frequent mistake I see is someone using a local path. If you load data from C:\\Users\\myname\\mydata then it won‚Äôt work for me. Always load/save data relative to the project folder. The here R package and its associated here() command in R is very useful. You‚Äôll see it show up and you should use it.\nAnother good idea is to start your R code with use_blank_slate() from the usethis R package.\nFor some more background on this, see the tidyverse article by Jenny Bryan, co-author of both the here and usethis packages."
  },
  {
    "objectID": "content/module-model-basics/assessment-fitting-basic-models.html",
    "href": "content/module-model-basics/assessment-fitting-basic-models.html",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-model-basics/assessment-fitting-basic-models.html#setup",
    "href": "content/module-model-basics/assessment-fitting-basic-models.html#setup",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Setup",
    "text": "Setup\nThis will be part of your portfolio site.\nWe are going to use the data from Brian McKay‚Äôs paper you saw at the beginning of the course. Download all his files and make sure to unzip. Find the SympAct_Any_Pos.Rda file, which is the one we‚Äôll work with.\nMake a folder inside your portfolio repository, call it fluanalysis. Make a data sub-folder and place the the SympAct_Any_Pos.Rda file into that folder. You can just use a single data folder for this exercise, but if you want, you can also make separate raw_data and processed_data folders and place his .Rda file into the raw_data folder. (I know he already processed it some, but we use it as our start, so for us that is the raw material.)"
  },
  {
    "objectID": "content/module-model-basics/assessment-fitting-basic-models.html#wrangling-revisited",
    "href": "content/module-model-basics/assessment-fitting-basic-models.html#wrangling-revisited",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Wrangling revisited",
    "text": "Wrangling revisited\nWe already spent time on the important steps of data wrangling/cleaning and exploring/visualizing. But practice makes perfect üòÑ so we‚Äôll talk a bit more and do a bit more of it here.\nBy now you will have figured out that the whole wrangling/exploring process is iterative. Without exploring your data, you don‚Äôt know how to clean/wrangle it. Without at least some cleaning, exploration often doesn‚Äôt work (e.g.¬†if you have missing values, some R summary functions don‚Äôt work properly).\nFor this exercise, create a single code folder inside the fluanalysis folder and place a Quarto file (either as stand-alone Quarto or Quarto+R) called wrangling.qmd into that folder. Add code that further processes and cleans the data as follows:\n\nLoad the data. Note that this is actually an RDS file/object, not an RData/RDA file, despite the somewhat misleading .Rda file name ending. So you need to use the right function to load RDS files into R.\nRemove all variables that have Score or Total or FluA or FluB or Dxname or Activity in their name. Don‚Äôt do this manually one by one, figure out how to use R commands that let you remove things in an efficient manner, using e.g., contains() or starts_with() from dplyr/tidyselect.\nAlso remove the variable Unique.Visit. You should be left with 32 variables coding for presence or absence of some symptom. Only one, temperature, is continuous. A few have multiple categories.\nRemove any NA observations, there aren‚Äôt many.\n\nMake sure to add plenty of comments and text explaining each step. State briefly why you are making a decision, and then if the code is not obvious, explain the how.\nAt the end of your (fairly short) cleaning process, you should end up with 730 observations and 32 variables. Save the new data set as RDS file into the appropriate folder (either data or if you made it, your clean data folder). (For single objects RDS is better than RData/RDA. )\nFor our further analysis, we decide (well, I‚Äôm deciding that for you üòÄ) that our main continuous outcome of interest is Body temperature, and our main categorical outcome is Nausea. We want to see if the other symptoms are correlated with (predict) those outcomes."
  },
  {
    "objectID": "content/module-model-basics/assessment-fitting-basic-models.html#eda-revisited",
    "href": "content/module-model-basics/assessment-fitting-basic-models.html#eda-revisited",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "EDA revisited",
    "text": "EDA revisited\nAs part of the data wrangling/cleaning, and also to some extent as stand-alone exploration to prepare you for the formal statistical fitting, you need to explore and understand your data. Add a Quarto file (call it exploration.qmd) into the code folder. Write code that does a bit of exploration, produces a few figures and tables.\nAt minimum, your exploration code should do the following:\n\nFor each (important) variable, produce and print some numerical output (e.g.¬†a table or some summary statistics numbers).\nFor each (important) continuous variable, create a histogram or density plot.\nCreate scatterplots or boxplots or similar plots for the variable you decided is your main outcome of interest and the most important (or all depending on number of variables) independent variables/predictors. For this dataset, you can pick and choose a few predictor variables.\nIf applicable to your data, make some pairwise correlation plots. (Not applicable here)\nIf needed for your data, explore the pattern of missing values. (Not applicable here)\nAny other exploration steps that might be useful.\n\nIn general, if you only have a limited number of variables, say less than 10, you can usually explore them all. If you have a lot, focus your exploration on the ones you think are most interesting and important, which of course is the outcome and the main predictors of interest. You can decide on the predictors to explore. I determined the outcomes of interest for you above. Of course, if there are variables you know are of no interest, you can remove them during the cleaning/wrangling stage and don‚Äôt need to explore them."
  },
  {
    "objectID": "content/module-model-basics/assessment-fitting-basic-models.html#general-wrangling-and-eda-comments",
    "href": "content/module-model-basics/assessment-fitting-basic-models.html#general-wrangling-and-eda-comments",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "General wrangling and EDA comments",
    "text": "General wrangling and EDA comments\nAlways make sure you document well. Prior to each code piece that produces some output, you should add text describing what you are about to do and why. After you produced the result, add some text that comments on what you see and what it means. E.g. you could write something like Histogram for height shows two persons at 20in, everyone else else is above 50in. Checked to see if those are wrong entries or not. Decided to remove those observations.\nOnce you‚Äôve come this far, you should generally have at a minimum code that explores and cleans the data, produces some figures and tables, and saves the cleaned data set to an Rds file. You should also have documentation, as comments in the code and as text in a Quarto document, as well as some notes in README file(s), that explain what the different parts of data and code are/do, what variables the data contain, what each variable means, which are the main outcome(s) and main predictor(s), etc. Providing all this meta-information is important so that someone else (or your future self) can easily understand what is going on.\n\nFor this exercise, it isn‚Äôt so important that the outcomes and predictors are of actual scientific interest. We won‚Äôt do exciting science here, just explore the analysis workflow. However, I want to remind you that having good/interesting question to answer (and data you can use to answer it) is the most important part of any research project. Rather a great question and basic stats than really fancy stats answering a question nobody cares about!\n\n\nAny good data collection and analysis requires that data is documented with meta-data to describe what it contains, what units things are in, what variables are allowed, etc. Without good meta-data, analyses often go wrong. There are famous examples in the literature where the coding 0/1 was assumed to mean the opposite of what it was, so all conclusions were wrong. To guard against this, careful documentation is crucial. Also, giving variables easy to understand names and values is best. E.g. instead of coding sex as 0/1/99, code it as Male/Female/Other, or something along those lines. Factors in R allow you to do this very easily, so you may as well!"
  },
  {
    "objectID": "content/module-model-basics/model-fitting-r.html",
    "href": "content/module-model-basics/model-fitting-r.html",
    "title": "Fitting (Simple) Statistical Models in R",
    "section": "",
    "text": "Overview\nIn this unit, we will discuss common approaches and packages that are useful for fitting statistical models in R.\n\n\nLearning Objectives\n\nLearn about different packages in R that allow model fitting.\n\n\n\nIntroduction\nR has a few statistical model fitting routines built in, e.g., the lm() and glm() functions. Those are well-tested and reliable, but often do not allow for fitting more advanced models. To fit more advanced models, there are many (100+) different R packages that provide additional functionality. The variety can be bewildering. Often it is hard to decide which package to use. Also, many packages do things slightly differently, which can be confusing and can make coding tedious. At some point in your analysis career, you‚Äôll likely have to interact with packages specific to your data and questions and have to learn their syntax. Initially, to make things easier, several groups have undertaken efforts to create packages that allow a unified approach to fitting a lot of different models. We will focus on those packages here.\n\n\nThe tidymodels set of packages\nYou‚Äôve learned about the tidyverse already. A more recent effort by the folks from RStudio ‚Äì and many other contributors - is a set of packages called tidymodels. The idea is that similar to various packages in the tidyverse you can use for data wrangling, tidymodels provides a set of packages that help you with the code related to fitting models. For this course, we will focus on the tidymodels set of packages.\nYou can use tidymodels for pretty much any part of the modeling workflow (e.g., pre-processing, model evaluation, tuning). We have not yet covered most of those steps, but will do so shortly. The goal for this unit is to start exploring the tidymodels workflow ‚Äì for now we‚Äôll ignore a lot of the additional features, but we‚Äôll discuss shortly.\nThe tidymodels suite is a relatively recent addition to the R universe. One of the main persons behind tidymodels, Max Kuhn previously wrote the package caret. This was ‚Äì and still is ‚Äì a nice and comprehensive package. I used it in previous versions of this course. However, at this point, all effort by Max and his team is put into tidymodels. Thus, for this iteration of the course, we will focus on those newer packages. If you ever end up working with the caret package, you might want to check out the caret chapter in IDS ‚Äì caret has existed for a while so you can find all kinds of resources online as well.\n\n\nOther comprehensive packages\nWhile the whole data exploring, cleaning, and wrangling part in R is strongly dominated by the various tidyverse packages, tidymodel packages do not (yet) dominate the fitting part as much. And it is always good to have options.\nAnother great set of packages for model fitting is Machine Learning in R (mlr3). The goal of the various packages which are part of mlr3 is similar to those of tidymodels. While there is overlap, each set of tools can do certain things the other cannot do. For instance, in my experience, mlr3 has more options for parameter tuning, though tidymodels is catching up.\nThe main reason why we focus on tidymodels in this course is that the coding style is very similar to the tidyverse coding style, e.g., heavy use of pipes. Thus, in my opinion it is easier to learn. The mlr3 package has its own syntax. It is of course still R, but things look and operate quite a bit differently, which means one needs more time to get used to the code. Thus, to keep things as simple as possible on the coding side, we are not looking at mlr3 in this course. If, however, you ever end up trying to do a fitting/machine learning operation that you can‚Äôt do with the tidymodels set of packages, checking out mlr3 is certainly a good option.\nNote that similar to tidymodels and caret, mlr3 had a predecessor called mlr (I don‚Äôt know if there ever was mlr2 ü§∑). mlr still exists, but all new development occurs in mlr3.\n\n\nDirect interaction with statistical fitting packages\nThe idea behind tidymodels and mlr3 is that you write code that allows you to easily switch between the underlying model and algorithm you want to apply to your data, without having to write separate code each time. That often works rather well. Occasionally, you might need direct access to and interaction with a package. Say you want to fit some mixed-effects/multilevel/hierarchical models using a package that is not yet supported by tidymodels or mlr3. In that case, you will have to write code using the syntax your specific package needs. You might still be able to use, say, tidymodels to do a lot of the processing before and after fitting. It is generally good to try and start with a framework that tries to make your life easier, such as tidymodels. Once you realize you can‚Äôt get what you need through those packages, you can add custom code.\n\n\nFurther reading\nOne nice feature about tidymodels is that the developers are placing a lot of emphasis not only on implementing new features, but also on providing good documentation. The tidymodels website is your best starting point. It has several sections that contain documentation and help resources. I recommend you visit and browse through regularly.\nMax Kuhn and tidymodels co-maintainer Julia Silge also have on online book called Tidy Modeling with R which discusses both the general concepts and the specific details of some of the tidymodels packages.\nThere are lots of good tutorials and walk-throughs both on the tidymodels website and other places. I‚Äôll give you some more links soon. But since most of those discuss the full workflow, and we haven‚Äôt gotten there quite yet, we‚Äôll save most of those for later."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-diagnostics.html",
    "href": "content/module-model-evaluation/model-evaluation-diagnostics.html",
    "title": "Model Diagnostics",
    "section": "",
    "text": "Introduction\nIn the previous units, we discussed a way to evaluate model quality by looking at performance based on some cost function/metric we defined. While this is an important component of the model building and choosing process, one needs to go further.\nThe absolute value of a model‚Äôs performance is not always very meaningful. Is an RMSE of 3.2 good? Is 90% accuracy a good model? We can‚Äôt say without knowing something about the system and looking more closely.\nPerforming a detailed model diagnosis often increases our understanding of the data/model/analysis, and can help us figure out how to fix or improve the model. The following sections briefly describe some checks you can do to evaluate the quality of your model further.\n\n\nNull model comparisons\nYou should always compare your models to a ‚Äúdumb‚Äù null/baseline model. A null model for a continuous outcome would be one that always predicts the mean. That is it uses no information from the predictor variables. For categorical outcomes, a null model would always predict the most common category, without using any information about predictor variables. If your more complicated model can‚Äôt beat such a null model (based on the performance metric you chose), you need to question what your model is good for. Remember that the metric is important. A null model with, e.g., high accuracy might not be as good as a different model with lower accuracy. Recall the example of the brain cancer.\nIn tidymodels, you can use the null_model() function to compute the performance for a null/base model.\n\n\nSingle predictor comparisons\nBefore building and fitting a big model, it is useful to look at the different predictors individually. (Or if you have too many, at least at some you think are important.)\nTo that end, you can perform bi-variable analyses, fitting models that each only contain a single predictor and evaluate the model performance (ideally using cross-validation) of such single-predictor models. You should definitely do that for the predictor(s) that are of main interest.\nOnce you start fitting your larger models, you can compare those to your single predictor models. Conceptually, if you only look at the performance on the data used to build the model, your multi-predictor model always performs better or at least as good as your single predictor models, which in turn should perform at least as well as your null model. If that is not the case, it means something went wrong in the analysis. Of course, now that you are aware of overfitting, you know that if you evaluate your models through cross-validation, the bigger multi-variable model does not always perform better.\nAs an example, if you want to predict mortality and you have a model (say a logistic model with outcome of 5-year mortality yes/no) that includes as predictors diet, exercise, BMI, gender and age, and such a model performs as well as a model with only age, it means that including those additional variables does not add to model performance.\n\n\nObserved versus predicted values\nOnce you have a model with good performance, you want to inspect their actual predictions. For continuous outcomes, you can plot observed (data) versus predicted (model) outcomes. For a (hypothetical) perfect model, all points are along the 45-degree line. You don‚Äôt actually want them all on the line since this suggests overfitting. Some scatter along the line is expected and ‚Äúhealthy‚Äù. However, you want to look for systematic deviations from this line, as it suggests potential problems, i.e., it likely means your model is biased and is not flexible enough to capture important patterns still found in the data (i.e., your model is underfitting). In that case, you will want to try different models. Similarly, for categorical outcomes, you can look at the confusion matrix to see if there are lots of FN or FP, which might suggest the model gets certain categories systematically wrong.\n\n\nResiduals\nInstead of (or in addition to) plotting observed versus predicted for continuous outcomes, you can plot the difference between the two. These differences are called the residuals. What you are looking for is a cloud of points with no discernible pattern. If there is a pattern (e.g., an overall skew, or more points above the 0 y-axes than below), it again suggests that there is still some pattern/signal in the data that the model didn‚Äôt capture.\n\n\nData simulation\nOne of the best general approaches toward testing models is to simulate data and fit the model to the simulated data. You know what you used to generate the fake data, so you know what the model should return. For instance if you simulate data for the linear model \\[Y=b_0 + b_1X_1 + b_2X_2\\] (and add a bit of noise on top), and you chose b0=b1=b2=2, then when you fit such a linear model to this simulated data, those are the values for the coefficients you expect to get back (not quite, since you added a bit of noise, but they shouldn‚Äôt be far off). If you can‚Äôt get out what you stuck in, you have a problem. Most likely, it means you are over-fitting, and more than one combination of parameter values is giving almost the same performance measure, and the model can‚Äôt differentiate. You should then either get more data or make your model simpler.\nThis approach of simulating data is a very useful general approach. You should always consider if it is an option for your specific data and question and use it if possible. The more complex your model becomes, the more useful this type of diagnosis is. It can however at times be difficult if you use a model for which it is not clear how the mapping of inputs to outcomes works (e.g.¬†a complex machine learning model). Even then, if you make some data where say one predictor is strongly correlated with the outcome, and another one is only noise, then your model should properly predict that this is the case.\n\n\nSensitivity analysis\nSometimes you have observations that might have a strong impact on the model, i.e., without those observations, the best fitting model would look quite different. If you decided that those points are real (i.e., not data entry or other mistakes), you might want to run the model both with those data points present and absent to investigate how results might change. Similarly, you might make decisions along the data cleaning path (e.g., to remove all observations with missing values, or instead remove some variables with lots of missing) which could affect results. Performing analyses in multiple ways to see how these decisions affect outcomes is useful. If you find that the overall results stay the same, it instills confidence in the robustness of your findings. If in contrast, different decisions lead to different answers, it might point to something worth investigating further.\nSince you are learning to set up your analysis in a fully automated way, doing such additional analyses is fairly painless. Often it just requires a small adjustment in code and waiting for the additional analysis to run.\nNote that sometimes the term sensitivity analysis is used to imply in a more limited approach, namely just the exploration of the impact of model parameters on the outcomes. However, in the more broader sense of the word, it is the exploration how changes in the analysis pipeline (different subsets of data, different modeling assumptions, etc.) might or might not impact the results.\n\n\nModel Interpretation\nA nice feature of using subset selection or LASSO with GLMs, or fitting a single tree, is that the algorithm decides which predictor variables are important, and throws out the remaining ones. For the ones that stay in the model, we can look at the coefficients in front of each predictor variable, or look at the final tree/decision diagram, to assess the importance of individual predictor variables on the outcome. This provides an easy way to understand how specific variables influence the outcome.\nThose simple approaches are not available anymore for more complex models. With complex models, we generally give up interpretability in exchange for better performance. However, we ideally want both. The last several years have seen a good amount of development to come up with methods that allow one to peek inside the black box, i.e.¬†to understand why and how a complex model makes its predictions, and what the role of specific predictor variables is on the outcome. This area is generally called interpretable machine learning.\nThere‚Äôs no point in me repeating what others have already said (much better than I could) üòÉ, therefore, instead of me writing more on this, please take a look at chapters 2 and 3 of the Interpretable Machine Learning (IML) book, which gives a very nice introduction on this topic. As you can tell by the title, the whole book is about interpretation of machine learning methods, and is a great resource.\nFor a shorter, but also great resource, check out chapter 16 of HMLR, which provides both a quick introduction and overview to the topic, and lists and illustrates the use of several R packages to do various interpretation tasks.\nAnother good introduction is chapter 18 of Tidy Modeling with R.\nAs models get more complex, making sense of them will increase in importance. Even if you are mainly interested in predictive performance and less on understanding your system, it is a good idea to investigate your model in some detail. You might often figure out things that can help further increase model performance. Also, without understanding at least to some extent how complex models make their predictions, the potential for unnoticed bias increases. A good example of this can be found in this very recent Science paper which describes racial bias in a machine learning model that tries to predict healthcare needs of patients. See also Ruha Benjamin‚Äôs commentary on the article. Thus, this area of interpreting results from complex models will likely see much development in the near future, hopefully leading to models that are both powerful (and thus likely complex) and interpretable.\nGiven how easy it is to apply some of these methods to your models, I recommend that if you decide to use a somewhat complex model (or even a not-so complex one), you should always do at least some analyses that probe the model, e.g.¬†perform a variable importance analysis and related investigations as described in the references above.\n\n\nOther checks\nMany fitting functions return useful information as output (which you can read with the summary or similar commands). Take a close look. If you thought you had 200 data points but your fitting function result states that N=180, it means the function might have dropped observations with missing values without warning you (R unfortunately does things like that at times). By carefully reading/plotting model returns, you can diagnose your model and catch problems.\nIf you get strange results (either unexpectedly bad or unexpectedly good), look carefully. As you know, most often things that are too good to be true are in fact not true. Bugs (either of the coding or thinking type) in any step of your analysis can lead to strange results.\nIf at any time during your analysis, you get warning messages, you need to investigate them carefully. Sometimes it is ok to ignore warning messages in R but only after you know precisely what they mean and why they happen!. Frequently, warning messages indicate you are doing something you shouldn‚Äôt be doing and can lead to wrong results.\n\n\nFurther learning\nIn addition to the sources mentioned above, other good reads are the Judging Model Effectiveness and When Should You Trust Your Predictions? chapters of TMWR."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-overview.html",
    "href": "content/module-model-evaluation/model-evaluation-overview.html",
    "title": "Model Evaluation Overview",
    "section": "",
    "text": "Overview\nIn this module, we will discuss how one can assess the quality of different models, and what aspects of model quality one needs to consider.\n\n\n\n\n\n\n\nLearning Objectives\n\nLearn how to determine different aspects of model quality\nBecome familiar with different types of model performance metrics\nBecome able to choose the right assessment metric for your problem\nLearn what model uncertainty is and how to compute it\nBecome familiar with model diagnostics\n\n\n\nIntroduction\nIf we fit any kind of statistical model to data, we need to determine the quality of a fit somehow. How do we know one model is better than another? How do we know any given model is good? These questions, i.e., what model is better among a group of models, and if a model is good (even the best among a group of awful models can be a bad model!) have multiple parts to them, only some of which can be answered by statistics. Others are judgment calls.\nThere is no single way to define quality. Instead, multiple factors should be considered. Here are some important aspects to think about:\n\nModel performance: How well do the predictions from the model compare to the actual data?\nModel complexity: How easy or hard is it to understand the model? How fast does it run? How robust is it to violations of the assumptions you made?\nModel uncertainty: How much uncertainty is associated with the predictions made by the model?\nModel misspecification: How much systematic deviation is there between model predictions and actual data?\n\nWe will discuss these different ways one can assess model quality in this module."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-uncertainty.html",
    "href": "content/module-model-evaluation/model-evaluation-uncertainty.html",
    "title": "Evaluating Model Uncertainty",
    "section": "",
    "text": "A study with n=1 has large uncertainty. Source: SMBC."
  },
  {
    "objectID": "content/module-model-evaluation/model-evaluation-uncertainty.html#bootstrapping",
    "href": "content/module-model-evaluation/model-evaluation-uncertainty.html#bootstrapping",
    "title": "Evaluating Model Uncertainty",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nThe idea for bootstrapping is fairly straightforward. Bootstrapping tries to imitate a scenario in which you repeated your study and collected new data. Instead of having actual new data, the idea is that the existing data is sampled to form a new ‚Äúdataset‚Äù, which is then fit. Sampling is performed with replacement to obtain a sample the size of the original dataset. Some observations now show up more than once, and some do not show up. For each such sample, you fit your model and estimate parameters. You will thus get a distribution of parameter estimates. From those distributions, you can compute confidence intervals, e.g., the usual 95% interval. For each fit, you can also predict outcomes and thus obtain a distribution of prediction outcomes (see next).\nLike cross-validation, the bootstrap method is very general and can be applied to pretty much any problem. If your data has a specific structure, you can adjust the sampling approach (e.g., if you have observations from different locations, you might want to sample with replacement for each location.) Limitations for the bootstrap are that you need a decent amount of data for it to work well, and since you are repeating this sampling procedure many times, the procedure can take a while to run."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-data-processing.html",
    "href": "content/module-model-improvement/model-improvement-data-processing.html",
    "title": "Some Common Data Processing Tasks",
    "section": "",
    "text": "This unit provides a quick description of some common data processing tasks that one often needs to do before starting the actual fitting part."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-data-processing.html#variablepredictor-removal",
    "href": "content/module-model-improvement/model-improvement-data-processing.html#variablepredictor-removal",
    "title": "Some Common Data Processing Tasks",
    "section": "Variable/Predictor Removal",
    "text": "Variable/Predictor Removal\nIt is often the case that we have more data than we need to answer a scientific question. For almost any analysis, one therefore needs to remove some variables before starting the statistical model fitting process. A simple example is an identifier (ID) variable or the names of subjects. Often this information is not needed for the modeling process and can thus be removed before fitting. Other examples are instances where the data was collected for some purpose other than the planned analysis. In this case, it is likely that there are variables in the data which are irrelevant for our analysis.\nSuch removal of variables is done on scientific grounds, based on expert opinion. Ideally, you should report in enough detail which parts of the data you included and excluded in that way to allow the reader to make an informed decision if they agree with what you did. And of course, you should ‚Äì as much as possible ‚Äì also provide the raw data and the R scripts which do and document removal of specific variables ‚Äì such that someone who doesn‚Äôt agree with your choice could re-run the analysis with different inclusion/exclusion criteria.\nSometimes, you might have variables that could, in principle, be useful, but the reported values show little diversity and thus contain little information. For instance, if you had a sample of several hundred individuals and only 3 of them were smokers, then it might not be useful to include the smoking variable for the analysis of this dataset, even if, in general, it might be worth considering. Such variables that do not contain much information are called ‚Äúnear-zero variance‚Äù variables. Some models perform better if those variables are removed. Other modeling approaches do not care since they have built-in mechanisms to remove variables that are not useful in predicting the outcome.\nAnother instance where removing variables might be useful is if predictors are strongly collinear/correlated. A trivial example is if you have height reported as both inches and centimeters in your data. Obviously, one of them should be removed. Other variables might not be so obviously containing the same information, but might be related enough (collinear) that including both makes the model performance worse. An example might be age and height among children.\nAnother example we already disused are variables with missing values. In this case, you might want to remove the variable, or the observations, or a mix of them.\nIn R, removing variables or observations is easily done with the select and filter functions from dplyr. There are a variety of good packages for dealing with missing/NA values. The naniar package is one of them.\nAll of these inclusion/exclusion decisions are based on expert (that‚Äôs you!) judgment. There is no magic recipe you can follow. Some statistical methods can help (see below), but they are only so useful. If someone on the internet tries to sell a magical way of knowing how to process your data before fitting, run away!"
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-data-processing.html#variable-transformation",
    "href": "content/module-model-improvement/model-improvement-data-processing.html#variable-transformation",
    "title": "Some Common Data Processing Tasks",
    "section": "Variable transformation",
    "text": "Variable transformation\nIt is often a good idea to transform variables. For instance if you have a model with continuous and categorical predictors, and you want to compare the coefficients returned from some model, you need to have the predictors on the same scales, which you can accomplish by standardizing predictors through centering (transforming the data so the mean is a specific value) and scaling (transforming the data so that it falls within a specific range). The most common method of centering is subtracting the mean, and the most common method of scaling is dividing by the standard deviation. The drawback of such a transformation is that the model results might be harder to interpret and might have to be ‚Äòback-transformed‚Äô to have biological meaning.\nOther common used transformations are converting to log units, since often the log scale is the biologically relevant one. This also reduces the impact of very high and very low values.\nIt is also often the case that some numerical methods your statistical model uses work better when you transform variables. It is always good to check the manual for any statistical model (and its numerical implementation) to see what is described and recommended there.\nWhenever you do some kind of normalization or transformation, check the result of such a transformation to ensure nothing went wrong (e.g., you didn‚Äôt accidentally divide by 0 or take the log of a negative number).\nThere are many ways to do variable transformation in R. In this course we will focus for our model fitting approaches on the tidymodels set of packages. Within that set of packages, the recipes package is used for variable transformation."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-data-processing.html#feature-engineering-variable-creation",
    "href": "content/module-model-improvement/model-improvement-data-processing.html#feature-engineering-variable-creation",
    "title": "Some Common Data Processing Tasks",
    "section": "Feature engineering (variable creation)",
    "text": "Feature engineering (variable creation)\nThe term feature engineering is often used in machine learning. It basically means creating new variables from the existing ones. The hope is that the new variables are better at describing/predicting/correlating with the outcome. (Feature is another word used for predictor or independent variable. The term feature is especially common in the machine learning literature). For instance, if we want to assess the impact of weight on diabetes, using weight itself might not be the best option since it is confounded by height. A 200lb person who‚Äôs 5 feet tall is likely at greater risk than someone who is 6 feet tall. We can instead use weight and height and compute BMI and then include that variable/feature in our model instead. In the ML terminology, we just engineered the feature of BMI from the features weight and height. Or in more everyday wording we created a new variable BMI from the two existing variables weight and height.\nCreating the right features that are most meaningful for a model needs to be guided by expert knowledge, and often differentiates the models with good predictive performance from those that are not so good. There are whole books focused on feature engineering (e.g.¬†Feature Engineering and Selection: A Practical Approach for Predictive Models ). The recipes package mentioned above helps with some feature engineering tasks. We won‚Äôt use it too much in the course exercises, but keep in mind that it is always an option to create new variables from your existing ones. (One could think of variable transformation described in the previous section as a very simple form of feature engineering, i.e., we create a new variable/feature which is basically just the old one but somehow transformed.)"
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-data-processing.html#feature-reduction-variable-reduction",
    "href": "content/module-model-improvement/model-improvement-data-processing.html#feature-reduction-variable-reduction",
    "title": "Some Common Data Processing Tasks",
    "section": "Feature Reduction (variable reduction)",
    "text": "Feature Reduction (variable reduction)\nThis could be considered one type of feature engineering. In some areas of science, this is a step that‚Äôs almost always required. For instance, with -omics type data (proteomics, transcriptomics, metabolomics, etc.), it is not uncommon to measure 1000s of variables (e.g., gene expression levels) for one individual person. Thus those datasets often have many more predictors than observations (individuals/samples in the study). This means some models might not work at all (e.g.¬†a standard linear or logistic model) and other models might work but take way too long to run.\nIt is therefore often necessary to reduce the number of predictors. Manual removal of predictors based on biological/expert knowledge, as described above, is one option. But with thousands of predictors, and often no clear a priori idea of which ones are biologically meaningful, this quickly becomes unfeasible. Another option is to use a statistical approach with the goal to find a set of new predictors of size \\(m\\), made up of combinations of the \\(p\\) old ones, such that \\(m \\ll p\\) (\\(m\\) is much less than \\(p\\)). An approach called Principal Component Analysis (PCA) can be used to find such a smaller set of new predictors that capture most of the information contained in the larger set. One drawback of PCA is that it ignores the relationship between predictors and outcome. That means it tries to reduce the dimension/number of predictors by looking for correlations among them, but without paying attention to the potential correlations between predictors and outcome. This can at times lead to sub-optimal performance of the model. To keep predictors that are most associated with the outcome in the model, one can use a method called Partial Least Squares (PLS). Other approaches exist. The overall problem is that the new set of predictors is harder to interpret, and thus insights gained from the model are somewhat reduced. We won‚Äôt have time to look in much detail at any of those approaches, but the resources we have been using for this class cover some of the approaches, so check there if interested."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-regularization.html",
    "href": "content/module-model-improvement/model-improvement-regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "In this unit, we will cover the concept of regularization. We‚Äôll also briefly mention a few related approaches."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-regularization.html#ridge-regression",
    "href": "content/module-model-improvement/model-improvement-regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\nOne way to choose the function that penalizes the predictors is to weigh each predictor by the predictor‚Äôs coefficient squared. Choosing the penalty term as the square of the coefficient is called ridge regression (AKA L2 regularization, Tikhonov regularization, weight decay, and potentially lots of other names). This leads to the cost function:\n\\[C = SSR + \\lambda \\sum_j^p b_j^2.\\]\nThe parameter \\(\\lambda\\) decides the balance between the goodness of fit (low SSR) and the penalty for having large coefficients. Instead of trying different subsets as above and picking the best based on lowest CV performance, we now try different values of \\(\\lambda\\) and pick the model with the lowest (cross-validated) value for our performance measure, C. The parameter \\(\\lambda\\) is often referred to as the tuning parameter or the penalty. Sometimes \\(\\lambda\\) is also called a hyperparameter of the model, which just means that the best value of \\(\\lambda\\) cannot be found by fitting the model one time only."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-regularization.html#lasso",
    "href": "content/module-model-improvement/model-improvement-regularization.html#lasso",
    "title": "Regularization",
    "section": "LASSO",
    "text": "LASSO\nAn alternative is to penalize the coefficients by their absolute value, namely using this cost function:\n\\[C = SSR + \\lambda \\sum_j^p |b_j| \\]\nThis method is called L1 regularization or the Least Absolute Shrinkage and Selection Operator (LASSO). One nice feature of LASSO (which ridge regression does not have) is that coefficients may go to 0. That means the predictor has been dropped from the model, similar to the subset selection approach described previously. One can think of the LASSO as an efficient approach for performing subset selection. It is not quite equivalent though, since, in the LASSO, the predictors that remain might have been shrunk in their impact due to the regularization penalty."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-regularization.html#elastic-net",
    "href": "content/module-model-improvement/model-improvement-regularization.html#elastic-net",
    "title": "Regularization",
    "section": "Elastic net",
    "text": "Elastic net\nOne can also combine ridge regression and LASSO into an approach called elastic net, which has a cost function that is the combination of the previous two, namely:\n\\[ C = SSR + \\lambda \\left( (1-\\alpha) \\sum_j^p b_j^2 + \\alpha \\sum_j^p |b_j|\\right)\\]\nNow one needs to try different values for \\(\\lambda\\) (called the penalty parameter) and \\(\\alpha\\) (called the mixture parameter) to determine the model with the best (cross-validated) performance. \\(\\lambda\\) determines the overall weight given to the penalty factor, while \\(\\alpha\\) determines how the penalty should be distributed between the 2 alternative terms. There are also a few variants of this method, such as relaxed elastic net or adaptive elastic net which you can look into if you are interested but we won‚Äôt discuss here."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-regularization.html#tuning-a-regularization-model",
    "href": "content/module-model-improvement/model-improvement-regularization.html#tuning-a-regularization-model",
    "title": "Regularization",
    "section": "Tuning a regularization model",
    "text": "Tuning a regularization model\nDepending on the kind of regularization model you fit, you have to determine 1 or 2 extra parameters (\\(\\lambda\\) and \\(\\alpha\\)). These parameters are called tuning parameters (or hyperparameters) and it is the first time we see a model that has them. Most complex machine learning models have such tuning parameters, and determining good values for those is part of the model fitting/training process. We‚Äôll talk about that in the next unit."
  },
  {
    "objectID": "content/module-model-improvement/model-improvement-tuning.html",
    "href": "content/module-model-improvement/model-improvement-tuning.html",
    "title": "Model Tuning",
    "section": "",
    "text": "Overview\nIn this unit, we will cover the concept of parameter/model tuning and training.\n\n\nLearning Objectives\n\nLearn what parameter tuning/model training are.\nBe able to implement training/tuning in R.\n\n\n\nIntroduction\nWe discussed at length that one (but not the only) goal of model fitting is to find a model that has good performance when applied to new/different data.\nFor linear, logistic, and similar models (generalized linear models), we discussed the idea of subset/feature selection that can help determine a good model. For different sub-models, we fitted the model to some of the data and evaluated model performance on another part of the data using cross-validation. The model with the best cross-validated performance across all sub-models is then designated as the best (at least with regard to our chosen performance metric).\nWe then discussed regularization, which tries to solve a problem similar to subset selection, namely preventing a model that is too complex and thus overfits. In the regularization approach, one does not compare sub-models with different predictor variables. Instead, all predictors are present, and one (or two) parameters (the regularization or penalty parameters, We called them \\(\\lambda\\) and \\(\\alpha\\)) are varied to influence model complexity. For each value of that parameter, the model is evaluated through cross-validation, and the \\(\\lambda\\) which produces the model with the best performance is chosen (or sometimes one picks a somewhat larger \\(\\lambda\\) to further prevent potential overfitting).\nThis approach of taking a model parameter and evaluating models for different parameter values is called model/parameter tuning. These model-specific tuning parameters are often also referred to as hyperparameters. Simple models, like linear or logistic regression, do not have any parameters that can be tuned. However, more complicated models, which we will discuss soon, generally have one or more tuning parameters. Very flexible models, such as neural nets used in artificial intelligence tasks, can have thousands or even more parameters that need tuning. For any models with tuning parameters, it is essential to tune the parameters/train the model. Without it, the model will likely not perform very well.\n\n\nModel training/parameter tuning recipe\nTraining the model by tuning its parameters follows a general approach that is conceptually the same for all models. You need to go through these steps:\n\nSelect some values for your tuning parameters.\nUsing cross-validation, fit model to a part of the data (the analysis portion), evaluate model performance on the remainder of data (the assessment portion).\nSelect new values for your tuning parameters, and repeat step 2.\nKeep going until you hit some stopping criterion, e.g., you tried all parameter combinations you wanted to try, you hit the maximum number of tries, or you hit the maximum amount of time you allocated for this parameter tuning. (Or you found the absolute best tuning parameter values, but that‚Äôs only likely for simple models with few tuning parameters).\nPick your best model as the one with the parameter values that produced the overall best model performance.\n\nYour final model consists of both the type of model and the values of the parameters.\n\nYou might (or might not) have wondered if this procedure of repeatedly refitting the model for different values of the tuning parameters (hyperparameters) and re-fitting the model for different values of the model parameters (e.g., for the coefficients \\(b_i\\) for a linear or generalized linear model) isn‚Äôt more or less the same. Both conceptually and in practice, those approaches are quite similar. For GLM, there are differences in how things happen, namely the \\(b_i\\) can be determined in a single step, without the need for trial and error. But for other models, such as some of the ML models we‚Äôll explore, tuning parameters and internal parameters associated with the model might both need to be determined by iterative procedures. Thus, while one can try to distinguish between model and tuning parameters (see e.g., Jason Brownlee‚Äôs blog post, which discusses that point a bit more), this is often fuzzy. And the usual caveat applies: The terminology is not consistent, and what some people might call a model parameter might be called a tuning parameter by others. The good news is that in practice it doesn‚Äôt matter much what you call a specific parameter. Some can be tuned and you can chose to do so (or not and fix it), others cannot be tuned and are determined internally.\n\n\n\nSearching the tuning parameter space\nThe problem of trying a lot of different tuning parameter values to find the ones that lead to the best performance is very similar to the problem of trying to test a lot of different sub-models during subset selection to find the best model. Not surprisingly then, the procedures to perform the search over parameter space are similar to the ones one can use to search over subset/sub-model space. The most basic one for subset selection was to try every possible model (exhaustive search). This works in principle for tuning parameters as well, but only if the tuning parameters are discrete. For continuous tuning parameters (e.g., \\(\\lambda\\) in regularization), it is impossible to try all values. One instead chooses discrete values for the parameter between some lower and upper limits and then searches that grid of parameters. This is called grid search. The advantage of a grid search is that you know you tried every combination of parameters in your grid.\nFor instance, if you have 2 continuous tuning parameters and 1 categorical tuning parameter with 3 categories, if you chose 10 discrete values for the 2 continuous parameters each, your grid would be 10x10x3. In this case, you would need to evaluate the model for 300 different tuning parameter values. That‚Äôs not too bad. But you can see that one problem with this approach is that as the number of tuning parameters increases, or if you want to try many different discrete values (e.g., 100 instead of 10 for each continuous parameter), the number of times you need to run the model increases rapidly. That‚Äôs the same problem as the exhaustive search for subset selection.\nTo solve this problem, there are a few major methods:\n\nOne can efficiently choose parameter values to estimate ‚Äì instead of searching the entire grid, you can use a sampling method to find a smaller grid that covers the same space in the most efficient way. The most well-known algorithm for this is probably Latin hypercube sampling.\nMethods similar to those mentioned for subset selection can be applied, e.g., one can use Genetic Algorithms, Simulated Annealing, Racing Methods, or many other optimizer routines for tuning. While it is, in principle, possible to write your own code that implements the tuning procedure with whatever method you want to use, for most instances, it is easier to use pre-existing methods.\n\n\n\nTuning with R\ntidymodels, and its package tune currently have a few different algorithms for searching the tuning parameter space implemented. Grid search is the main one, but the package also implements Iterative Bayesian optimization. The finetune which is not yet on CRAN implements Simulated Annealing and Racing Methods. To learn more about those, see chapters 12-14 in Tidy Modeling With R.\nThe mlr3/mlr package I keep mentioning also has algorithms to tune parameters, including some that are not available (yet) in tidymodels. For tuning in mlr see e.g.¬†tuning tutorial and advanced tuning tutorial. For this course, we‚Äôll focus on what‚Äôs available in tidymodels, but if you ever need to do some major parameter tuning/model optimization, checking out mlr might be worth it (or implementing your own with tidymodels, which is possible.)\n\n\nFurther comments\nOnly if your model has very few tuning parameters and your data is manageable in size can you find the absolute best parameter values in a reasonable amount of time. More likely, you‚Äôll find parameter values that give you a close-to-optimal model.\nAt times, cross-validation might take too much time, and you might have to use a computationally faster method, such as AIC or similar, to try to estimate model performance on future data. That‚Äôs not ideal, you might want to consider other approaches first (fewer parameter evaluations, faster computer, running things in parallel‚Ä¶).\nAny process that adjusts the model repeatedly based on prior fits to data has the danger of overfitting, even if you try to guard against this using approaches such as CV. Thus, sometimes less tuning might actually give you a more robust/generalizable model.\nThe more tuning parameters in your model, the more data you need to be able to train the model properly. If you have a mismatch between the amount of data and model complexity, you are likely going to overfit. This is why complex models such as neural nets need vast amounts of data (millions or billions of observations).\n\n\nFurther information\nMost relevant and maybe good to visit next are chapters 12-14 in Tidy Modeling With R, which discuss the general tuning process, and then explain how to do grid search and iterative search using tidymodels.\nSection 2.5.3 of HMLR provides a very short section on tuning. ISLR mentions tuning in various places but doesn‚Äôt describe it in a dedicated section. IDS mentions it in the Machine Learning chapters but also does not have a dedicated section on the topic."
  },
  {
    "objectID": "content/module-model-ml-1/decision-trees.html",
    "href": "content/module-model-ml-1/decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Overview\nIn this unit, we will cover a class of models generally referred to as decision trees.\n\n\n\nAn interesting classification tree. Source: xkcd.\n\n\n\n\nLearning Objectives\n\nUnderstand what tree-based methods are and how to use them\nKnow the difference between regression and classification trees\nKnow when it might be good to use a tree-based method\nUnderstand the advantages and disadvantages of single tree model\n\n\n\nIntroduction\nSimple GLMs have the main limitation of capturing only overall smooth patterns relating inputs and outcomes. Higher order terms or interactions must be manually specified. The polynomial and spline models we discussed allow for more flexible patterns, at the cost of more potential overfitting and also reduced interpretability. Another class of models that allow non-linear relation between inputs and outcomes are based on decision trees. In general, a decision tree is a tree-like structure that helps you make decisions. In machine learning, there are specific methods to build those trees. Decision trees can be used for continuous outcomes, in which case they are called regression trees or for categorical outcomes, in which case they are called classification trees. The umbrella term Classification and regression trees - or CART for short is also often used to mean the same as decision trees.\nBasic tree models have the advantage of being fairly easy to understand and use, even by non-statisticians. Trees can deal with nonlinear relations between predictors and outcome, they easily accomodate more than two categories for classification, and they can handle predictors that have missing values.\n\n\nTree Examples\nThe following shows a tree from Afonso et al.¬†2012, which is a study led by one of my colleagues Mark Ebell to determine if there are simple decision rules that can determine if a person has an influenza infection or not.\n\n\n\nSource: Alfonso et al.¬†2012\n\n\nAs you can see, this is a very simple model that is very intuitive and could be easily understood by physicians and other laypeople. The tree is used here for classification, with a binary outcome (presence/absence of flu).\nThe next example is a tree used for a regression problem to predict/model a continuous outcome.\n\n\n\nSource: Hess et al.¬†1999\n\n\nIn this study, the authors tried to determine the survival time of patients who had cancers with certain characteristics. The tree was used to group individuals, and for each group the tree/model predicts median survival.\n\n\nBuilding trees\nThe way one computes a tree is a little bit similar in concept to forward selection in a linear model. We start by looking at each predictor. We take the tree, which, when the outcome is split at some value of that predictor, leads to the best (cross-validated) performance increase in the model (e.g., lowest SSR/RMSE or highest Accuracy/F1 score). Let‚Äôs say we want to predict BMI, and our performance measure is RMSE of predicted value from the true outcome. The null model using no predictors gives us some value for the RMSE. We then try each predictor (let‚Äôs say we have exercise, calories, age, sex) and find that if we use exercise and assign everyone who exercises more than 23min per day one BMI value, and everyone who exercises less than 23min another BMI value, we get the best reduction in RMSE we could get (across all predictors and possible cut-off values). This is our first split using the exercise predictor at 23min. We now have a tree with one root and 2 nodes (in this case, they are leaves, which is the name for the terminal nodes).\nWe now take each leaf and run through all predictors again including the one we just used and see which predictor split at some value will further improve model performance. Let‚Äôs say we find that for those exercising more than 23min, we can split by sex and get 2 new BMI values, one for males exercising &gt;23min and one for females exercising &gt;23min, which gives the maximum reduction in RMSE. Similarly, we find that for the &lt;23min exercise people, splitting at 1500 calories a day is best. We now have a tree with a root, 2 internal nodes, and 4 terminal nodes.\nWe keep building the tree in this way until some criterion is met (e.g., no more than n observations in a given leaf, no further increase in performance). Note that in this procedure, some predictors may never enter the model. In that way, a tree performs an automatic subset selection, i.e., the algorithm might decide to not include predictor variables it doesn‚Äôt find useful. Also, any predictor could be used more than once.\nAlgorithms that implement the tree building routine differ in their details, which is why you will see many different options in both mlr and caret. In general, if you have a good signal in your data, any algorithm should pick up on it.\nSince trees have a tendency to overfit, it is common to regularize them by including a penalty for tree size. For instance, if we were to minimize SSR/RMSE, we would add a penalty to the cost function, C, to get\n\\[C = SSR + \\lambda T, \\]\nwhere T is the number of leaves of the tree. More leaves means a larger tree, which is being penalized. The tuning parameter \\(\\lambda\\) needs to be determined using parameter tuning or model training. Reducing a tree in this way is also called pruning.\n\n\nFast and frugal trees\nWhile trees are fairly simple to understand, sometimes, especially if there are many branching points, a complex decision tree might still not be suitable in practice. Further, even with regularization, trees might overfit. There is a type of tree, called fast and frugal tree (FFT), which can potentially help with both aspects. The difference between a regular tree and a FFT is that for the FFT, at each split/decision point, at least one of the splits needs to end in a terminal node. Check e.g.¬†the example diagrams shown in the FFT Wikipedia article to see what this means. This constraint makes the trees often simpler and thus easier to implement in practice (e.g.¬†by a doctor) and they might also be more robust, i.e.¬†their performance on future data might be better than a larger tree. Disadvantages of FFT are that sometimes they might be too simple and thus not perform as well as a full tree, and they only work for binary outcomes.\nA very nice R package called FFTrees implements FFT (and automatically compares their performance to regular trees and some of the tree based methods discussed below). You can find more about this R package in the FFTrees documentation.\n\n\nAdvantages and disadvantages of tree models\nA great feature of trees is that they are relatively quick and easy to build and especially easy to understand. They can easily be communicated to and used by non-experts (e.g., doctors, other decision-makers). As such, trees (sometimes called decision diagrams or other names) are common in many fields. As mentioned, trees are also able to handle missing data in predictors, and they are often reasonably robust in the presence of collinear or near-zero-variance predictors since trees tend to use one of the variables and ignore the others. Tree models also tend to excel at inferring the presence of nonlinear interactions between variables. Often trees don‚Äôt need predictors to be standardized either.\nThe main disadvantage of trees is that they usually have reduced performance compared to other models. Thus, if a simple, interpretable model is the primary goal, trees are ideal. If instead, a high-performance predictive model is the goal, trees are rarely the best choice.\n\n\nFurther information\nR2D3‚Äôs excellent interactive tutorial gives a very nice, visual introduction to machine learning in general and trees in particular. It covers and nicely illustrates some of the topics discussed here, as well as topics discussed in previous units. In part 2 of the tutorial the authors discuss overfitting. I strongly recommend you check it out, even if you just skim through it. It is fun and informative!\nFor more on tree models, see the first part Tree-based Methods chapter of ISLR, the Decision Trees chapter of HMLR."
  },
  {
    "objectID": "content/module-model-ml-1/ml-models-overview.html",
    "href": "content/module-model-ml-1/ml-models-overview.html",
    "title": "Machine Learning Models - Overview",
    "section": "",
    "text": "Overview\nIn this brief unit, I make a few general comments about machine learning models, before we look at several types of those models.\n\n\nLearning Objectives\n\nKnow what ML models are.\n\n\n\nMachine Learning Model Zoo\nBy now, I am sure you have picked up on the fact that there is a whole zoo of different statistical and machine learning models. You have also likely picked up that the terminology is fuzzy. We could call a standard linear or logistic regression model a machine learning model, and people do. Or we can reserve the term ML for more complex models - however fuzzily we define ‚Äúcomplex‚Äù. In the end, getting hung up about terminology is not that useful and important.\nWhat is important is that you are at least a bit familiar with some of the more common models, how they work and what they are good for, such that when you either read someone else‚Äôs modeling results, or try your own, you have an idea of what‚Äôs going in. All we can do in this course is to take an introductory glance at some ML models. If you end up becoming a ‚Äúmachine learning practitioner‚Äù, you will want to learn more about specific models. The materials I link to hopefully give you places to look.\nML models fall in broad classes, and within each class, there are many different variants. It is impossible to cover them all, but in this module and the next, we‚Äôll briefly go over a few of the more common ones.\n\n\nGeneral ML considerations\nI want to make an important point here: The statistics/model fitting topics we have discussed so far, namely pre-processing data, choosing the right performance metric, using methods like train/test and cross-validation to minimize overfitting, and carefully evaluating your model results always apply, no matter if you fit a simple linear model or a complicated neural net.\nSome problems become more acute for certain types of models, e.g., the more complex the model, the more likely the chance for overfitting. But the general concepts always apply. Model tuning also applies to most ML models, it is just different model parts that are being tuned. Once you dig deeper, it is useful to understand enough about the model to get an idea of what tuning a specific parameter for a given model actually means. However, in a first pass, you can often do those tuning operations without knowing much about the models (which is what we‚Äôll do in this class). Nevertheless, always critically evaluate what your models return. Just because complex models are often black boxes and it‚Äôs hard to understand everything that goes on inside doesn‚Äôt mean you can skip your critical thinking and accept as reasonable whatever the model gives you back.\nI think by now you have also picked up on the idea that there is no recipe for choosing a specific machine learning/statistical modeling approach. It depends on the data, the question, the overall goal, what others in the field are using, and potentially further factors. In general, the most thorough approach is to try both simple and complex models, and then decide based on model performance and other considerations such as simplicity, speed, scalability, etc. on a specific model. There is always a level of subjectivity involved, i.e.¬†different analysts might favor different models. As long as the thought process behind choosing a specific model is well explained and justified, you should be allowed to choose the model that you think is overall best for a given situation. Since it is very easy to fit multiple different models and compare results, it is not a bad idea to do that. You can report the results form the main model you chose as the main findings, with results from other models as supplementary material.\nAnd with those general points out of the way, we‚Äôll look at several different statistical/ML models in the following units.\n\n\nFurther resources\nThe books we have been using throughout this course all cover many aspects of ML. Especially ISL, IDS and HMLR are very good starting points for learning more about different machine learning methods. I will point to specific chapters when we discuss specific models.\nThere are also tons of online resources on machine learning models, the quality varies widely, but it might be worth looking around a bit. TheInsaneApp.com‚Äôs 10 Most Used Machine Learning Algorithms in Python with Code is a nice overview. Note that at the time I‚Äôm writing this, when I did a brief read-through I noticed some inaccuracies. E.g., they claim that a logistic regression model predicts 0/1, which is not quite right, it predicts probability which then is usually converted to 0/1 by defining a threshold. In general, when you look at resources like that, they are rarely completely wrong but might occasionally not be fully accurate (I‚Äôm sure my course website is no different üôÑ). So it‚Äôs often good to cross-check with resources that are fairly certain to be right (e.g., textbooks or Wikipedia).\nIf you want to practice some more ML modeling using the tidymodels framework, check out this free online course by Julia Silge, one of the main tidymodels maintainers. It consists of 4 case studies that teach you both general ML ideas and how to do them with tidymodels. Another online course focusing on tidymodels is Allison Hill‚Äôs course."
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html",
    "title": "Assessment - Machine Learning Models 2",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline."
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html#setup",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html#setup",
    "title": "Assessment - Machine Learning Models 2",
    "section": "Setup",
    "text": "Setup\nMake sure your portfolio repository is fully up-to-date and synced. Open the project in RStudio."
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html#connecting-website-and-file-repository",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html#connecting-website-and-file-repository",
    "title": "Assessment - Machine Learning Models 2",
    "section": "Connecting website and file repository",
    "text": "Connecting website and file repository\nFirst, let‚Äôs make sure it‚Äôs easy for people to go from your portfolio website (the github.io location) to your file repository (the github.com location) and back. To that end, open the _quarto.yml file. At the bottom, it says URL-TO-THIS-REPOSITORY-HERE. Replace that with the URL to your Github repository. As an example, for the MADA course, the URL one would put in there is https://github.com/andreashandel/MADAcourse (while the website lives at https://andreashandel.github.io/MADAcourse/).\nOnce you done that, when people now are on your website, they can click on the Github icon in the top right corner and be taken to your file repository. Try to make sure it works by rebuilding your website and pushing to Github. It might take a minute or so and you might need to hit refresh before it shows up on the website.\nNow, let‚Äôs connect the two in reverse. Open Readme.md and update the text. This is what people see when they come to your github repository for your portfolio (as opposed to the website). I recommend adding a little bit of text and a link pointing users to the actual website. You can look at the Readme.md for this course as an example. Edit yours as you want.\nYou might also want to point to your website in the top right area of your repository on Github.com. If you go to your repository on GitHub, you should see an About section in the top right. If you click on the gear symbol, there is a field in which you can enter the URL for your website (the github.io location). It might already show as pre-populated, but you have to actually manually enter it before it will show. You can again see how that looks on the repository for this course.\nWith this, it is easy for anyone (including yourself) to quickly switch between the website and the file repository."
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html#creating-new-content",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html#creating-new-content",
    "title": "Assessment - Machine Learning Models 2",
    "section": "Creating new content",
    "text": "Creating new content\nIn preparation for next week, make a copy of the tidytuesday_exercise.qmd file, name it tidytuesday_exercise2.qmd. Open it, delete everything apart from the YAML header (the stuff between the ---). Add a sentence that says ‚ÄúTo be filled‚Äù or something like that. You will fill it next week.\nTo see the new file and access it from the website, we also have to include it in the website navigation bar. Open the _quarto.yml file and add a menu entry called Tidy Tuesday 2 Exercise and link it to ./tidytuesday_exercise2.html.\nMake sure your website rebuilds ok. You should now see another menu entry for Tidy Tuesday 2 in your Projects section and when you click on it, the essentially empty file we created should show up."
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html#adding-more-content",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html#adding-more-content",
    "title": "Assessment - Machine Learning Models 2",
    "section": "Adding more content",
    "text": "Adding more content\nWe‚Äôve added most content of interest to the portfolio website. If you have created other noteworthy products, either as part of this course or outside, feel free to add them to your website. By now you should know how to do that. For instance, if you want to add your project, either now or once its done, feel free to do so. It probably doesn‚Äôt make sense to put the whole project content inside the portfolio. Instead, take the main product (e.g., the manuscript file), render it to html, and add it to the website. Or create a new project.qmd file where you briefly describe what you did and show a few highlights, and then provide a link to your main project repository for those interested in looking at the whole thing."
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html#cleanup-and-styling",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html#cleanup-and-styling",
    "title": "Assessment - Machine Learning Models 2",
    "section": "Cleanup and Styling",
    "text": "Cleanup and Styling\nAt this point, it might be worth revisiting your already posted pages and making sure everything looks as nice and professional as possible.\nWhile the way we built the website has only limited ways of styling things (unless you want to start changing CSS and HTML code), you can still customize some. Feel free to play around and customize the look. You can find a good bit of information in the Quarto documentation. Some additional information (maybe slightly outdated since it‚Äôs Rmarkdown, but most should still work) is in the Bookdown section on RMarkdown sites. If you look into the repository for the MADA course, you will also see that I‚Äôm using my own CSS file (called MADAstyle.css). CSS lets you style websites. It‚Äôs not hard to write CSS code, but it is its own thing. I usually just search online to find what I‚Äôm looking for üòÅ. You certainly don‚Äôt have to, but if you want to further customize the look of your website, you can add your own CSS file and style it how you like."
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html#more-comments",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html#more-comments",
    "title": "Assessment - Machine Learning Models 2",
    "section": "More Comments",
    "text": "More Comments\nFuture employers really do look at portfolios like this, so being able to showcase something nice and polished is useful. It is also part of having a good online presence. I think for (future) professionals like yourself, a solid online presence is vital. I discuss this with our grad students in another class, if you want to see my thoughts on that, you can check out my presentation on building your brand ‚Äì which is of course made with R Markdown and posted to a Quarto based website üòÑ."
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html#submission",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html#submission",
    "title": "Assessment - Machine Learning Models 2",
    "section": "Submission",
    "text": "Submission\nI know where to find your portfolio, so you don‚Äôt need to submit anything. Just make sure you push a fully updated and re-built portfolio to Github and that the website looks right. I‚Äôll take a look at the website after the deadline."
  },
  {
    "objectID": "content/module-model-ml-2/more-ml-models.html",
    "href": "content/module-model-ml-2/more-ml-models.html",
    "title": "More Machine Learning Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover some further points related to ML that didn‚Äôt really fit into any other sections.\n\n\nLearning Objectives\n\nBe familiar with the idea of ensemble models.\nKnow about several unsupervised modeling approaches.\n\n\n\nEnsemble methods/models\nYou learned previously that one can combine many trees into a larger model (e.g., a random forest or a boosted regression trees model), and that those models often have better performance. In the many-tree units, I briefly mentioned that this is an example of an ensemble model.\nInstead of combining just the same kind of model (e.g., multiple trees), it is possible to build models which are combinations of different types of base models, e.g., combine a tree-based model with an SVM. Those approaches are known variously as ensemble methods/models or super learners or stacked methods/models. By combining different models in a smart way, it is often possible to increase performance beyond what can be achieved from any one individual model. The disadvantage is that fitting the model is more complex, takes more time, and results are even less interpretable (more black box) than any single model. And since each model has parameters that need tuning, more parameters means more data is needed for robust results. Nevertheless, if the only aspect that counts is predictive performance, and plenty of data is available, ensemble methods are often good choices.\nProperly fitting ensemble models is not easy, and requires usually a lot of data. In fact, so far I have never tried to fit an ensemble model for any of my research projects. Nevertheless, it is useful to know about them if you encounter them in the literature or if you have a problem/data where you think they might be helpful. For more on those models, check out the Stacked Models chapter of HMLR, the Ensembe Models chapter in TMWR and the stacks package, which integrates well with tidymodels.\n\n\nUnsupervised learning\nWhile I previously mentioned unsupervised learning here and there, we haven‚Äôt focused much on it in this course. The reason is that most data analysis problems deal with supervised learning, i.e.¬†with data that has a specific outcome of interest. However, there are situations where data without a given outcome needs to be analyzed. For instance images or customers might need to be sorted into categories. This analysis approach is also called clustering. Sometimes, unsupervised learning is also used as a preparatory step in a supervised learning setting. For instance it can be used to reduce the number of predictors. This is called dimension reduction. It is common in areas where one measures lots of variables but the observations are small, e.g.¬†genetic information on a few hundred individuals, with 1000s of genetic markers measured for each person. In such a case, one can reduce the number of predictor variables into a set of combinations of the original predictors such that the new set contains the most important information. Then one can use that reduced set to perform supervised learning.\nMethods like K-means clustering or Hierarchical clustering are ‚Äì as the name suggests ‚Äì used for clustering of unlabeled data. Partial least squares (PLS) and Principal component analysis (PCA) are methods for dimension reduction. Since for unsupervised learning, a performance measure like RMSE or Accuracy does not exist, other metrics are used to define the quality of model results. Different algorithms use different ways to perform the unsupervised learning task.\nThe Unsupervised Learning chapter of ISL discusses several unsupervised learning methods. So do the Dimension Reduction and Clustering sections of HMLR and the Clustering chapter of IDS. For an R implementation, check out the tidyclust package.\n\n\nFurther reading\nSee the references provided in the sections above, as well as the general references provided in the ML Introduction page."
  },
  {
    "objectID": "content/module-stats-overview/assessment-statistical-analysis.html",
    "href": "content/module-stats-overview/assessment-statistical-analysis.html",
    "title": "Assessment - Statistical Analysis Overview",
    "section": "",
    "text": "Quiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\n\n\nExercise\nThere is no exercise this week. Focus on part 2 of the project.\n\n\nProject\nFinish (and make sure to push) part 2 of the project. There is no need to submit a link. Unless you renamed your repository, I know where to find it based on your part 1 submission.\n\n\nDiscussion\nFind some paper online that does a data analysis. Explain what types of analyses the authors did (based on the categories described in this module). Also discuss what the (stated or not stated) objective of the paper/study was, if the authors used the right analysis, and if they interpreted/presented their results properly. If you think they didn‚Äôt do their analysis right or present their results correctly, make suggestions for what they should have done.\nYou don‚Äôt need to go into modeling details, a big picture description is enough. For instance if the authors clearly did a secondary analysis collected for a different purpose, but present their findings as if they showed causal relations, point out this mis-representation (there‚Äôs no shortage of studies like that).\nPost your findings and commentary by Wednesday. Then read through and comment on some of the studies your classmates posted."
  },
  {
    "objectID": "content/module-stats-overview/statistical-terminology.html",
    "href": "content/module-stats-overview/statistical-terminology.html",
    "title": "Statistical Modeling Terminology",
    "section": "",
    "text": "Overview\nThe field of data analysis has, in recent years, seen a lot of progress and changes. While Data Science or Data analysis was long synonymous with Statistics, in past years, other fields such as Computer Science, Engineering, and Business have all started to contribute toward the overall progress in data analytics. With a lot of old and new players engaged in this topic, a lot of new terminologies have emerged. Some of this terminology can be quite confusing, especially if different terms are used for the same concept. Throughout this course, I try to mention alternative names for any concept whenever I introduce it. The following is a brief discussion of the main names given to the task of analyzing data.\nOne of the hot ‚Äì and also confusing ‚Äì terms in recent years is Machine Learning (and the related terms of Data Mining, Deep Learning, and Artificial Intelligence), and how they relate to Statistics. There is a lot of debate about what exactly those different areas are. To get a bit of an idea of what people are talking about, skim through Frank Harrell‚Äôs blog post. As you can see from the post and the many other sources he cites, there is no real agreement on what exactly these terms mean.\nIn my view, it is not worth spending too much time trying to come up with a clear definition. But it‚Äôs good to have some frame of reference so when you see all these terms, you know what they mean. So here are my ‚Äì arguably fuzzy, but hopefully still somewhat useful ‚Äì thoughts on how to distinguish those topics. While making those distinctions can be at times useful, the reality is that the terminology is not clearly defined and all over the place.\nFor additional attempts at defining terms related to data analysis, see the Glossary page ‚Äì and certainly feel free to contribute!\n\n\nStatistics, Machine Learning and Artificial Intelligence\nStatistics is the classical machinery driving data analysis. Depending on the type of data, many different approaches have been developed (parametric vs.¬†non-parametric methods, longitudinal analysis, time-series analysis, and many more). Models are, in general, simple and interpretable, and the goal is to understand how inputs (predictors) relate to outcomes. Statistics was developed when data was sparse, computers didn‚Äôt exist, and mainly scientists interested in a deep understanding of their data used it. Because of this, statistical models tend to be simple and work well on small datasets. Most of classical statistics focuses on associative/exploratory/inferential analysis types.\nIn Machine Learning (ML), the models tend to be more complex, and the goal is mainly on getting a ‚Äúpowerful‚Äù model, i.e., a model that is good at prediction. Understanding how different inputs lead to different outcomes is of secondary importance. Data is often abundant, so more complex models can be used.\nArtificial intelligence (AI) can be considered a type of machine learning. The types of complex models that dominate AI these days are generally based on neural nets. Neural nets sound fancy, and they are certainly quite complex, but technically speaking, you can think of them as a large collection of logistic regression type models combined together.\nAs mentioned before, the terminology is fuzzy. Thus, a fairly simple model like a linear or logistic regression model could be considered a ‚Äúclassical‚Äù statistical model, while a more complex support vector machine or random forest (we‚Äôll visit those later) are generally considered machine learning models. However, the terminology is poorly-defined, and you will see pretty much any approach can be given any label. Often it makes sense to think of the two terms as describing more or less the same thing, and that is applying some kind of mathematical or computational model to gain insight from the data. For some similar ideas, and a bit more details, read Joshua Ebner‚Äôs blog post.\n\n\nRelated Terms\nThe term Data Mining is often used interchangeably with Machine Learning. It might sometimes indicate a ‚Äúfishing‚Äù approach of combing through data to look for patterns, without pre-defined hypotheses to be tested. As such, results from Data Mining explorations need to be confirmed with independent data. Exploratory Data Analysis or Secondary Data Analysis mean pretty much the same thing, though the latter are often done using smaller (Statistics) models as opposed to larger (Machine Learning) models. Again, terminology and use of the words is fuzzy.\nThe term Deep Learning is generally used when one class of machine learning models, namely neural nets, are applied to a data analysis problem. The ‚Äúdeep‚Äù part comes from the fact that the neural net models often have many stacked layers (it has nothing to do with deep as in especially insightful).\nThe use of deep learning and related approaches applied to ‚Äúcomplex‚Äù problems is often labeled Artificial Intelligence.\nNeural nets (and similar complex methods) usually need a lot of data to perform well. Thus, the term Big Data often shows up together with these other terms.\nIn general, there is currently a lot of hype around these topics, and people ‚Äì especially in the business, but also the research community ‚Äì use the words Deep Learning and Artificial Intelligence quite liberally, even if all they do is fit a linear model to data.\nWe‚Äôll discuss these topics a bit more at the end of the course, but since most scientific inquiry focuses on understanding patterns (i.e., questions tend to be inferential/causal/mechanistic) and amounts of data ‚Äì while growing ‚Äì still tend to be on the small side. So we won‚Äôt spend much time on it.\n\n\nTerminology abuse\nIn the last several years, there has been a tremendous interest among industry in anything related to Data Science. Most notable, the huge successes of AI in some areas and for some companies (Google, Amazon, Uber, ‚Ä¶) has meant that suddenly everyone wants AI and everyone sells AI, with a lot of players not having a clue what it is and can/can‚Äôt do. This has led to a lot of marketing hype and another host of acronyms. (For instance, I was recently at an online industry workshop with the title (quoting from memory) ‚ÄúFrom AI over BI to CI‚Äù - which apparently stand for Artificial Intelligence, Business Intelligence and Continuous Intelligence.) Most often, these terms can mean whatever you want them to mean. So some companies who have been doing data analysis by running simple linear or logistic regression models are now calling what they are doing AI. In the end, the only way to know what anyone is actually doing is to look at the detailed description of their methods. And if those are not provided, be very skeptical. Most of all, don‚Äôt be unduly impressed by all these big words. There is good data analysis, and there is bad data analysis, and just because someone uses a deep neural net AI model doesn‚Äôt mean what they are doing is any good. Critical and careful thinking about your data and the question you want to answer always trumps any new fancy modeling approach.\n\n\nFurther reading\nThis course has an optional module on Deep Learning, AI and Big Data. This is a very brief introduction you can check out if you are interested. For a broader introduction, the (non-free) book Artificial Intelligence: A Guide for Thinking Humans is a very good non-technical overview book. And of course, topics like ML and AI are so ‚Äúhot‚Äù these days, tons of free resources are available online‚Äìthough be sure to look at the writer‚Äôs credentials and think critically when reading online blogs."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html",
    "title": "Scrambling existing data - R",
    "section": "",
    "text": "In this tutorial, we discuss how to scramble existing data to make it ‚Äúnew‚Äù."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#ai-help",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#ai-help",
    "title": "Scrambling existing data - R",
    "section": "AI help",
    "text": "AI help\nSince you are working with the real data, you probably don‚Äôt want to use AI for this, unless your AI tool operates in a secure environment (e.g., fully on your companies‚Äô servers)."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#example",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#example",
    "title": "Scrambling existing data - R",
    "section": "Example",
    "text": "Example\nTime for a simple example. You can find the code shown below in this file.\n\nSetup\nFirst, we do the usual setup steps of package loading and other housekeeping steps.\n\nlibrary('dplyr')\nlibrary('ggplot2')\nlibrary('here')\n\n\n# setting a random number seed for reproducibility\nset.seed(123)\n\n\n\nData loading and exploring\nWe‚Äôll look at some real data from this paper. As is good habit (and should be the standard), the authors (which includes some of us) supplied the data as part of the supplementary materials, which can be found here.\nIf you want to work along, go ahead and download the supplement, which is a zip file. Inside the zip file, find the Clean Data folder and the SympAct_Any_Pos.Rda file. Copy that file to the location where you‚Äôll be placing your R script.\nFirst, we load the data. Note that the authors (that would be us üòè) used the wrong file ending, they called it an .Rda file, even though it is an .Rds file (for a discussion of the differences, see e.g.¬†here).\n\n\nThe data\n\n#assuming your R script is in the same folder\n#rawdat &lt;- readRDS('SympAct_Any_Pos.Rda')\n# this is for my setup\nrawdat &lt;- readRDS(here::here('data','SympAct_Any_Pos.Rda'))\n\nNext, we take a peek.\n\ndim(rawdat)\n\n[1] 735  63\n\ndplyr::glimpse(rawdat)  \n\nRows: 735\nColumns: 63\n$ DxName1           &lt;fct&gt; \"Influenza like illness - Clinical Dx\", \"Acute tonsi‚Ä¶\n$ DxName2           &lt;fct&gt; NA, \"Influenza like illness - Clinical Dx\", \"Acute p‚Ä¶\n$ DxName3           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Fever, unspecified\"‚Ä¶\n$ DxName4           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Other fatigue\", NA,‚Ä¶\n$ DxName5           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Headache\", NA, NA, ‚Ä¶\n$ Unique.Visit      &lt;chr&gt; \"340_17632125\", \"340_17794836\", \"342_17737773\", \"342‚Ä¶\n$ ActivityLevel     &lt;int&gt; 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, ‚Ä¶\n$ ActivityLevelF    &lt;fct&gt; 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, ‚Ä¶\n$ SwollenLymphNodes &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y‚Ä¶\n$ ChestCongestion   &lt;fct&gt; No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ ChillsSweats      &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, ‚Ä¶\n$ NasalCongestion   &lt;fct&gt; No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ CoughYN           &lt;fct&gt; Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, ‚Ä¶\n$ Sneeze            &lt;fct&gt; No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, ‚Ä¶\n$ Fatigue           &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ SubjectiveFever   &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes‚Ä¶\n$ Headache          &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes‚Ä¶\n$ Weakness          &lt;fct&gt; Mild, Severe, Severe, Severe, Moderate, Moderate, Mi‚Ä¶\n$ WeaknessYN        &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ CoughIntensity    &lt;fct&gt; Severe, Severe, Mild, Moderate, None, Moderate, Seve‚Ä¶\n$ CoughYN2          &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes‚Ä¶\n$ Myalgia           &lt;fct&gt; Mild, Severe, Severe, Severe, Mild, Moderate, Mild, ‚Ä¶\n$ MyalgiaYN         &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ RunnyNose         &lt;fct&gt; No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No‚Ä¶\n$ AbPain            &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N‚Ä¶\n$ ChestPain         &lt;fct&gt; No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, ‚Ä¶\n$ Diarrhea          &lt;fct&gt; No, No, No, No, No, Yes, No, No, No, No, No, No, No,‚Ä¶\n$ EyePn             &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye‚Ä¶\n$ Insomnia          &lt;fct&gt; No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ ItchyEye          &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, Yes,‚Ä¶\n$ Nausea            &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y‚Ä¶\n$ EarPn             &lt;fct&gt; No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y‚Ä¶\n$ Hearing           &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, No, No, No, No,‚Ä¶\n$ Pharyngitis       &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, ‚Ä¶\n$ Breathless        &lt;fct&gt; No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, ‚Ä¶\n$ ToothPn           &lt;fct&gt; No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N‚Ä¶\n$ Vision            &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, ‚Ä¶\n$ Vomit             &lt;fct&gt; No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N‚Ä¶\n$ Wheeze            &lt;fct&gt; No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N‚Ä¶\n$ BodyTemp          &lt;dbl&gt; 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, ‚Ä¶\n$ RapidFluA         &lt;fct&gt; Presumptive Negative For Influenza A, NA, Presumptiv‚Ä¶\n$ RapidFluB         &lt;fct&gt; Presumptive Negative For Influenza B, NA, Presumptiv‚Ä¶\n$ PCRFluA           &lt;fct&gt; NA, NA, NA, NA, NA, NA,  Influenza A Not Detected, N‚Ä¶\n$ PCRFluB           &lt;fct&gt; NA, NA, NA, NA, NA, NA,  Influenza B Not Detected, N‚Ä¶\n$ TransScore1       &lt;dbl&gt; 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5‚Ä¶\n$ TransScore1F      &lt;fct&gt; 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5‚Ä¶\n$ TransScore2       &lt;dbl&gt; 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4‚Ä¶\n$ TransScore2F      &lt;fct&gt; 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4‚Ä¶\n$ TransScore3       &lt;dbl&gt; 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3‚Ä¶\n$ TransScore3F      &lt;fct&gt; 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3‚Ä¶\n$ TransScore4       &lt;dbl&gt; 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4‚Ä¶\n$ TransScore4F      &lt;fct&gt; 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4‚Ä¶\n$ ImpactScore       &lt;int&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ ImpactScore2      &lt;int&gt; 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,‚Ä¶\n$ ImpactScore3      &lt;int&gt; 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, ‚Ä¶\n$ ImpactScoreF      &lt;fct&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ ImpactScore2F     &lt;fct&gt; 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,‚Ä¶\n$ ImpactScore3F     &lt;fct&gt; 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, ‚Ä¶\n$ ImpactScoreFD     &lt;fct&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ TotalSymp1        &lt;dbl&gt; 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1‚Ä¶\n$ TotalSymp1F       &lt;fct&gt; 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1‚Ä¶\n$ TotalSymp2        &lt;dbl&gt; 8, 10, 17, 16, 11, 14, 10, 11, 13, 10, 14, 19, 13, 1‚Ä¶\n$ TotalSymp3        &lt;dbl&gt; 8, 9, 16, 15, 11, 14, 10, 10, 12, 9, 13, 18, 12, 16,‚Ä¶\n\n\nSo it looks like these are 735 individuals (rows) and 63 variables (columns). A lot of them have names of symptoms and are coded as Yes/No.¬†Some variables are harder to understand, for instance without some meta-data/explanation, it is impossible to guess what TransScore3F stands for. Hopefully, your data came with some codebook/data dictionary/information sheet that explains what exactly everything means. For this specific data set, you can look through the supplementary materials to learn more. We won‚Äôt delve into it now, and just pick out a few variables to illustrate the data scrambling process.\n\n\nData processing\nFor simplicity, let‚Äôs assume we are interested in just a few of these variables, namely ActivityLevel, Sneeze, Nausea, and Vomit. We‚Äôll select those and look at the first 10 entries.\n\ndat &lt;- rawdat |&gt; dplyr::select(\"ActivityLevel\",\"Sneeze\",\"Nausea\",\"Vomit\")\nhead(dat,10)\n\n   ActivityLevel Sneeze Nausea Vomit\n1             10     No     No    No\n2              6     No     No    No\n3              2    Yes    Yes    No\n4              2    Yes    Yes    No\n5              5     No    Yes    No\n6              3    Yes    Yes    No\n7              4     No     No   Yes\n8              0    Yes     No    No\n9              0     No    Yes    No\n10             5     No    Yes    No\n\n\n\n\nData Scrambling\nNow we‚Äôll scramble the data. I‚Äôm doing this here with a simple loop. I‚Äôm looping through each variable, and I sample from the old values without replacement, which basically just rearranges them. There are computationally faster and more concise ways of doing this, but the loop makes it hopefully very clear what‚Äôs going on.\n\n# define a new data frame that will contain scrambled values\ndat_sc &lt;- dat\nNobs = nrow(dat) #number of observations\n# loop over each variable, reshuffle entries\nfor (n in 1:ncol(dat))\n{\n  dat_sc[,n] &lt;- sample(dat[,n], size = Nobs, replace = FALSE)\n}\n\nhead(dat_sc,10)\n\n   ActivityLevel Sneeze Nausea Vomit\n1              5     No     No    No\n2              0     No     No    No\n3              3    Yes     No    No\n4              5     No    Yes    No\n5              5     No    Yes    No\n6              1    Yes     No    No\n7              3    Yes     No    No\n8              8    Yes     No    No\n9              6    Yes     No    No\n10             4    Yes    Yes    No\n\n\nThe first 10 entries look different, so that‚Äôs promising.\n\n\nComparing old and new data\nNow let‚Äôs see if things worked. First, we summarize both the old and the new data. We should see that they are the same, since we just re-arranged the values across individuals. This is indeed the case.\n\nsummary(dat)\n\n ActivityLevel    Sneeze    Nausea    Vomit    \n Min.   : 0.000   No :340   No :477   No :656  \n 1st Qu.: 3.000   Yes:395   Yes:258   Yes: 79  \n Median : 4.000                                \n Mean   : 4.463                                \n 3rd Qu.: 6.000                                \n Max.   :10.000                                \n\nsummary(dat_sc)\n\n ActivityLevel    Sneeze    Nausea    Vomit    \n Min.   : 0.000   No :340   No :477   No :656  \n 1st Qu.: 3.000   Yes:395   Yes:258   Yes: 79  \n Median : 4.000                                \n Mean   : 4.463                                \n 3rd Qu.: 6.000                                \n Max.   :10.000                                \n\n\nWe can also look at correlations between variables. Here is where we run into the above-mentioned problems. Correlations that might exist in the original data can be wiped out. We see that here. In the original data, more individuals (approximately 63% + 9%) reported either absence or presence of both nausea and vomiting. In the scrambled data, this dropped to around 58% + 4%. We would expect that these 2 symptoms are somewhat related, and the scrambling removed it. Similarily, the original data showed lower activity levels for those with vomit as symptom. This pattern is gone in the scrambled data.\n\n# cross-tabulation of 2 symptoms\ntb1=table(dat$Nausea,dat$Vomit)\nprop.table(tb1)*100 #as percentage\n\n     \n             No       Yes\n  No  62.993197  1.904762\n  Yes 26.258503  8.843537\n\ntb2=table(dat_sc$Nausea,dat_sc$Vomit)\nprop.table(tb2)*100\n\n     \n             No       Yes\n  No  58.095238  6.802721\n  Yes 31.156463  3.945578\n\n# looking at possible correlation between activity level and Vomit\np1 &lt;- dat |&gt; ggplot(aes(x=Vomit,y=ActivityLevel)) + geom_boxplot()\nplot(p1)\n\n\n\n\n\n\n\np2 &lt;- dat_sc |&gt; ggplot(aes(x=Vomit,y=ActivityLevel)) + geom_boxplot()\nplot(p2)\n\n\n\n\n\n\n\n\nThat means any statistical conclusions based on the scrambled data are not valid. This kind of data is just useful at testing the overall workflow and making sure everything can run, but one can‚Äôt conclude anything from it.\nIt is of course possible to try to scramble while preserving potential correlations, but that gets tricky and at this stage one might maybe just re-create the data based on some of the concepts discussed in the previous unit."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-model-testing-r/synthetic-data-model-testing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-model-testing-r/synthetic-data-model-testing-r.html",
    "title": "Use synthetic data to test models",
    "section": "",
    "text": "Overview\nIn this unit, we explore the use of synthetic data to test your models.\n\n\nLearning Objectives\n\nUnderstand the use of synthetic data for testing models.\nBe able to implement model testing on synthetic data in R.\n\n\n\nIntroduction\nAt the beginning of this module, I mentioned that one great use-case of synthetic data is the ability to see if whatever patterns you stuck into the data when you generated it can be recovered by your analysis. If your analysis fails to recover what you built, it means your model is likely not suitable and you need to modify it.\nOf course, keep in mind that just because your model can recover the patterns/correlations you stuck into the data doesn‚Äôt mean it can do the same for the real data, or that any patterns in the real data are the same. However, if you can NOT recover the patterns for your simulated data, it‚Äôs a strong sign to stop and reconsider your analysis approach. It will very likely also not produce useful results for the real data (but you won‚Äôt know, since you don‚Äôt know what processes generated the real data).\n\n\nAn example\nWe will revisit the first example from the Generating synthetic data with R unit. If you don‚Äôt remember, take a look at the code shown in that unit to see that we created a variable called BloodPressure which was randomly distributed, and a variable called Cholesterol which varied by treatment group.\nIf we fit a model, we expect to recover these patterns. We can for instance fit a linear model with Cholesterol as the outcome and BloodPressure and TreatmentGroup as predictors.\nHere‚Äôs code to do that (you can again find the full R script here).\nSetting things up\n\n# Load the readr package for reading csv data\nlibrary(readr)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(parsnip)\nlibrary(here)\n\n# Read in the csv data \ndata &lt;- read_csv(here(\"data\",\"syn_dat.csv\"))\n\nWe‚Äôll do a little bit of processing before fitting\n\n# select variables of interest\n# not strictly needed, but can sometimes make for more robust code to only keep what's necessary\ndata &lt;- data %&gt;%  \n  select(Cholesterol, BloodPressure, TreatmentGroup)\n\n# Standardize BloodPressure and Cholesterol\n# Helps with interpretation of coefficients\ndata &lt;- data %&gt;% \n  mutate(\n    BloodPressure = scale(BloodPressure),\n    Cholesterol = scale(Cholesterol)\n  )\n# turn TreatmentGroup into a factor\ndata$TreatmentGroup &lt;- as.factor(data$TreatmentGroup)\n\n# check to make sure data looks ok before fitting\nsummary(data)\n\n    Cholesterol.V1      BloodPressure.V1   TreatmentGroup\n Min.   :-2.3886369   Min.   :-1.7201173   A      :43    \n 1st Qu.:-0.7631619   1st Qu.:-0.8109520   B      :30    \n Median : 0.0587370   Median : 0.1357530   Placebo:27    \n Mean   : 0.0000000   Mean   : 0.0000000                 \n 3rd Qu.: 0.7052625   3rd Qu.: 0.9111831                 \n Max.   : 2.5375210   Max.   : 1.4801446                 \n\n\nFitting the first model\n\n# Fit linear model \nmodel1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol ~ BloodPressure + TreatmentGroup, data = data)\ntidy(model1)\n\n# A tibble: 4 √ó 5\n  term                  estimate std.error statistic  p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)            -0.898     0.0822    -10.9  1.58e-18\n2 BloodPressure          -0.0822    0.0548     -1.50 1.37e- 1\n3 TreatmentGroupB         1.19      0.129       9.24 6.48e-15\n4 TreatmentGroupPlacebo   2.00      0.133      15.1  4.05e-27\n\nglance(model1)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.718         0.709 0.539      81.6 2.61e-26     3  -78.0  166.  179.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nWe find what we hope to find. The blood pressure variable shows no noticeable correlation with cholesterol, while treatment group does.\nWe can explore other models. Here is one with an interaction term between BloodPressure and TreatmentGroup. We know there is none, since we know how the data was generated. So to test our model, we fit it to confirm that this is what we get:\n\n# fit a model with interaction\nmodel2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol ~ BloodPressure + TreatmentGroup + BloodPressure*TreatmentGroup , data = data)\ntidy(model2)\n\n# A tibble: 6 √ó 5\n  term                                estimate std.error statistic  p.value\n  &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                          -0.901     0.0825   -10.9   2.15e-18\n2 BloodPressure                        -0.139     0.0766    -1.82  7.18e- 2\n3 TreatmentGroupB                       1.19      0.131      9.10  1.52e-14\n4 TreatmentGroupPlacebo                 2.02      0.134     15.1   8.90e-27\n5 BloodPressure:TreatmentGroupB         0.0856    0.129      0.665 5.08e- 1\n6 BloodPressure:TreatmentGroupPlacebo   0.163     0.144      1.13  2.60e- 1\n\nglance(model2)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.722         0.708 0.541      48.9 1.09e-24     5  -77.3  169.  187.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nAnd we do see no evidence for interaction being present. So for this simple example, the model works, namely it recovers the patterns we stuck into the data. This means it might be ok to use this model on our real data.\nLet‚Äôs now look at an example where the model does not properly recover the pattern in the data.\n\n\nA failing example\nWe‚Äôll start with the same data as before, but now mess around with it. Specifically, I‚Äôm creating a new variable called Drug and a new cholesterol variable that depends on drug concentration in a nonlinear manner.\n\n# Load the readr package for reading csv data\nlibrary(readr)\nlibrary(dplyr)\nlibrary(here)\nlibrary(tidymodels)\nlibrary(vip)\n\n# Read in the csv data \nset.seed(123)\ndata &lt;- read_csv(here(\"data\",\"syn_dat.csv\"))\n\n\n# Make Cholesterol depend on blood pressure in a nonlinear manner\ndata$Drug &lt;- runif(nrow(data),min = 100, max = 200)\n# making up Cholesterol value as function of Drug\n# this equation makes it such that cholesterol depends on drug in a nonlinear manner\ndata$Cholesterol2 &lt;- sqrt(100^2 + 10* (mean(data$Drug) -  data$Drug)^2)\n# plotting correlation between Drug and new cholesterol variables\nggplot(data,aes(x=Drug, y=Cholesterol2)) + geom_point()\n\n\n\n\n\n\n\n\n\n# select variables of interest\n# not strictly needed, but can sometimes make for more robust code to only keep what's necessary\ndata &lt;- data %&gt;%  \n  select(Cholesterol2, Drug, BloodPressure, TreatmentGroup)\n\n# Standardize continuous variables \n# Helps with interpretation of coefficients\ndata &lt;- data %&gt;% \n  mutate(\n    BloodPressure = scale(BloodPressure),\n    Drug = scale(Drug),\n    Cholesterol2 = scale(Cholesterol2)\n  )\n# turn TreatmentGroup into a factor\ndata$TreatmentGroup &lt;- as.factor(data$TreatmentGroup)\n\n# check to make sure data looks ok before fitting\nsummary(data)\n\n   Cholesterol2.V1          Drug.V1          BloodPressure.V1   TreatmentGroup\n Min.   :-1.2222251   Min.   :-1.7471666   Min.   :-1.7201173   A      :43    \n 1st Qu.:-0.9203239   1st Qu.:-0.8880440   1st Qu.:-0.8109520   B      :30    \n Median :-0.1257083   Median :-0.1129431   Median : 0.1357530   Placebo:27    \n Mean   : 0.0000000   Mean   : 0.0000000   Mean   : 0.0000000                 \n 3rd Qu.: 0.8741998   3rd Qu.: 0.9014615   3rd Qu.: 0.9111831                 \n Max.   : 2.1080599   Max.   : 1.7393649   Max.   : 1.4801446                 \n\n\nNow we‚Äôll fit another linear model, as before. There is a correlation between dose and cholesterol. But it‚Äôs not linear, and therefore the model doesn‚Äôt detect it.\n\n# Fit linear model \nmodel1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol2 ~ Drug + BloodPressure + TreatmentGroup, data = data)\nbroom::tidy(model1)\n\n# A tibble: 5 √ó 5\n  term                  estimate std.error statistic p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)             0.101      0.155     0.653   0.515\n2 Drug                    0.0353     0.103     0.343   0.732\n3 BloodPressure           0.0574     0.103     0.557   0.579\n4 TreatmentGroupB        -0.247      0.243    -1.02    0.311\n5 TreatmentGroupPlacebo  -0.0997     0.251    -0.398   0.692\n\nbroom::glance(model1)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0142       -0.0273  1.01     0.343   0.848     4  -141.  293.  309.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nIt is always possible to try a more complex model to see if there might be patterns that a linear or other simple model can‚Äôt detect. Here, we are trying a random forest model, which can detect more complicated correlations between predictor variables and output. Random forest models don‚Äôt produce the standard p-values, but one can look at something called variable importance to see which variables most impact the outcome. And we see that it correctly identifies drug as the most important variable. The other two should really be at zero importance, since they are not correlated with the outcome. But this model is flexible enough to fit to possibly spurious patterns.\n\n# fit a random forest model \n# use the workflow approach from tidymodels\nrf_mod &lt;- rand_forest(mode = \"regression\") %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\")\n# the recipe, i.e., the model to fit\nrf_recipe &lt;- \n  recipe(Cholesterol2 ~ Drug + BloodPressure + TreatmentGroup, data = data) \n\n# set up the workflow\nrf_workflow &lt;- \n  workflow() %&gt;% \n  add_model(rf_mod) %&gt;% \n  add_recipe(rf_recipe)\n\n# run the fit\nmodel2 &lt;- rf_workflow %&gt;% \n      fit(data)\n\n# get variable importance\nimp &lt;- model2 %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\nprint(imp)\n\n\n\n\n\n\n\n\nThe take-home message from this is that our simulated data showed us that a linear model can‚Äôt pick up the pattern, and we need a different model.\n\n\nSummary\nModel testing is one of the most important applications for synthetic data. Since you generated the data and know everything about it, you know exactly what the analysis of the data should find. If your models can‚Äôt find the right patterns, it means you need to modify your analysis.\n\n\nFurther Resources"
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html",
    "title": "Generating synthetic data with R",
    "section": "",
    "text": "In this unit, we look at a few examples of generating synthetic data with R."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup",
    "title": "Generating synthetic data with R",
    "section": "Setup",
    "text": "Setup\nWe‚Äôll start by loading the packages used in the code below. While we will not use packages that are specifically meant to generate synthetic data, we will still use several common packages that make data generation tasks easier. That said, you could also do all of this with base R and no additional packages.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(here)\n\n\n# Set a seed for reproducibility\nset.seed(123)\n# Define the number of observations (patients) to generate\nn_patients &lt;- 100"
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data",
    "title": "Generating synthetic data with R",
    "section": "Generating data",
    "text": "Generating data\n\n# Create an empty data frame with placeholders for variables\nsyn_dat &lt;- data.frame(\n  PatientID = numeric(n_patients),\n  Age = numeric(n_patients),\n  Gender = character(n_patients),\n  TreatmentGroup = character(n_patients),\n  EnrollmentDate = lubridate::as_date(character(n_patients)),\n  BloodPressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  AdverseEvent = integer(n_patients)\n)\n\n# Variable 1: Patient ID\nsyn_dat$PatientID &lt;- 1:n_patients\n\n# Variable 2: Age (numeric variable)\nsyn_dat$Age &lt;- round(rnorm(n_patients, mean = 45, sd = 10), 1)\n\n# Variable 3: Gender (categorical variable)\nsyn_dat$Gender &lt;- purrr::map_chr(sample(c(\"Male\", \"Female\"), n_patients, replace = TRUE), as.character)\n\n# Variable 4: Treatment Group (categorical variable)\nsyn_dat$TreatmentGroup &lt;- purrr::map_chr(sample(c(\"A\", \"B\", \"Placebo\"), n_patients, replace = TRUE), as.character)\n\n# Variable 5: Date of Enrollment (date variable)\nsyn_dat$EnrollmentDate &lt;- lubridate::as_date(sample(seq(from = lubridate::as_date(\"2022-01-01\"), to = lubridate::as_date(\"2022-12-31\"), by = \"days\"), n_patients, replace = TRUE))\n\n# Variable 6: Blood Pressure (numeric variable)\nsyn_dat$BloodPressure &lt;- round(runif(n_patients, min = 90, max = 160), 1)\n\n# Variable 7: Cholesterol Level (numeric variable)\n# Option 1: Cholesterol is independent of treatment\n#syn_dat$Cholesterol &lt;- round(rnorm(n_patients, mean = 200, sd = 30), 1)\n\n# Option 2: Cholesterol is dependent on treatment\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"A\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"A\"), mean = 160, sd = 10), 1)\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"B\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"B\"), mean = 180, sd = 10), 1)\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"Placebo\"), mean = 200, sd = 10), 1)\n\n# Variable 8: Adverse Event (binary variable, 0 = No, 1 = Yes)\n# Option 1: Adverse events are independent of treatment\n#syn_dat$AdverseEvent &lt;- purrr::map_int(sample(0:1, n_patients, replace = TRUE, prob = c(0.8, 0.2)), as.integer)\n\n# Option 2: Adverse events are influenced by treatment status\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"A\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"A\"), replace = TRUE, prob = c(0.5, 0.5)), as.integer)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"B\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"B\"), replace = TRUE, prob = c(0.7, 0.3)), as.integer)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"Placebo\"), replace = TRUE, prob = c(0.9, 0.1)), as.integer)\n\n\n# Print the first few rows of the generated data\nhead(syn_dat)\n\n  PatientID  Age Gender TreatmentGroup EnrollmentDate BloodPressure Cholesterol\n1         1 39.4 Female              B     2022-08-25         152.0       179.7\n2         2 42.7 Female              B     2022-06-14         128.7       192.2\n3         3 60.6 Female              A     2022-04-17         153.4       150.6\n4         4 45.7   Male              B     2022-02-02         131.1       171.7\n5         5 46.3 Female              A     2022-03-24         119.6       160.5\n6         6 62.2 Female              A     2022-12-20         156.5       154.3\n  AdverseEvent\n1            1\n2            0\n3            0\n4            0\n5            1\n6            1\n\n# Save the simulated data to a CSV and Rds file\nwrite.csv(syn_dat, here(\"data\",\"syn_dat.csv\"), row.names = FALSE)\n# if we wanted an RDS version\n#saveRDS(syn_dat, here(\"data\",\"syn_dat.Rds\"))"
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data",
    "title": "Generating synthetic data with R",
    "section": "Checking data",
    "text": "Checking data\nTake a peek at the generated data.\n\nsummary(syn_dat)\n\n   PatientID           Age           Gender          TreatmentGroup    \n Min.   :  1.00   Min.   :21.90   Length:100         Length:100        \n 1st Qu.: 25.75   1st Qu.:40.08   Class :character   Class :character  \n Median : 50.50   Median :45.60   Mode  :character   Mode  :character  \n Mean   : 50.50   Mean   :45.90                                        \n 3rd Qu.: 75.25   3rd Qu.:51.92                                        \n Max.   :100.00   Max.   :66.90                                        \n EnrollmentDate       BloodPressure    Cholesterol     AdverseEvent \n Min.   :2022-01-08   Min.   : 91.3   Min.   :129.6   Min.   :0.00  \n 1st Qu.:2022-04-06   1st Qu.:110.7   1st Qu.:160.7   1st Qu.:0.00  \n Median :2022-06-25   Median :130.8   Median :176.3   Median :0.00  \n Mean   :2022-06-30   Mean   :128.0   Mean   :175.2   Mean   :0.29  \n 3rd Qu.:2022-10-04   3rd Qu.:147.4   3rd Qu.:188.7   3rd Qu.:1.00  \n Max.   :2022-12-30   Max.   :159.5   Max.   :223.7   Max.   :1.00  \n\ndplyr::glimpse(syn_dat)  \n\nRows: 100\nColumns: 8\n$ PatientID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ Age            &lt;dbl&gt; 39.4, 42.7, 60.6, 45.7, 46.3, 62.2, 49.6, 32.3, 38.1, 4‚Ä¶\n$ Gender         &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Male\", \"Female\", \"Female‚Ä¶\n$ TreatmentGroup &lt;chr&gt; \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"Placebo\", \"A\", \"Pla‚Ä¶\n$ EnrollmentDate &lt;date&gt; 2022-08-25, 2022-06-14, 2022-04-17, 2022-02-02, 2022-0‚Ä¶\n$ BloodPressure  &lt;dbl&gt; 152.0, 128.7, 153.4, 131.1, 119.6, 156.5, 139.6, 118.9,‚Ä¶\n$ Cholesterol    &lt;dbl&gt; 179.7, 192.2, 150.6, 171.7, 160.5, 154.3, 172.8, 189.2,‚Ä¶\n$ AdverseEvent   &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1‚Ä¶\n\n# Frequency table for adverse events stratified by treatment\ntable(syn_dat$AdverseEvent,syn_dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 28 19      24\n  1 15 11       3\n\n# ggplot2 boxplot for cholesterol by treatment group\nggplot(syn_dat, aes(x = TreatmentGroup, y = Cholesterol)) +\n  geom_boxplot() +\n  labs(x = \"Treatment Group\", y = \"Cholesterol Level\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis concludes our first example. This was a very simple setup, with rectangular data. You often encounter these kind of data in clinical trials. As you generate your data, you can build in any dependencies between variables you want to explore. Then later, when you use the data to test your analysis code, you can see if your analysis code can detect the dependencies you built in. We‚Äôll come back to that.\nYou could also add further complexities into your synthetic data, for instance you could set some values to be missing, or you could add some outliers. The goal is to generate data that has as the important features of your real dataset to allow you to test your analysis approach on data where you now the truth (since you generated it). If your analysis works on your generated data, there is hope it might also work on the real data (for which of course you don‚Äôt know the truth). We‚Äôll define in a later unit what we mean by ‚Äúyour analysis works‚Äù. But basically, you want to be able to recover the patterns/dependencies you built into your data with your analysis methods."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup-1",
    "title": "Generating synthetic data with R",
    "section": "Setup",
    "text": "Setup\nThe usual setup steps.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(here)\n\n\n# Set seed for reproducibility\nset.seed(123)\n# Number of patients in each treatment group\nnum_patients &lt;- 20\n# Number of days and samples per patient\nnum_days &lt;- 7\nnum_samples_per_day &lt;- 1"
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data-1",
    "title": "Generating synthetic data with R",
    "section": "Generating data",
    "text": "Generating data\n\n# Treatment group levels\ntreatment_groups &lt;- c(\"Low Dose\", \"High Dose\")\n\n# Generate patient IDs\npatient_ids &lt;- rep(1:num_patients, each = num_days)\n\n# Generate treatment group assignments for each patient\ntreatment_assignments &lt;- rep(sample(treatment_groups, num_patients, replace = TRUE), \n                             each = num_days)\n\n# Generate day IDs for each patient\nday_ids &lt;- rep(1:num_days, times = num_patients)\n\n# Function to generate drug concentrations with variability\ngenerate_drug_concentrations &lt;- function(day, dose_group, patient_id) {\n  baseline_concentration &lt;- ifelse(dose_group == \"Low Dose\", 8, 15)\n  patient_variation &lt;- rnorm(1, mean = 0, sd = 1)\n  time_variation &lt;- exp(-0.1*day)\n  baseline_concentration * time_variation + patient_variation \n}\n\n\n# Generate drug concentrations for each sample\ndrug_concentrations &lt;- mapply(generate_drug_concentrations, \n                              day = rep(day_ids, each = num_samples_per_day), \n                              dose_group = treatment_assignments,\n                              patient_id = rep(1:num_patients, each = num_days))\n\n\n# Flatten the matrix to a vector\ndrug_concentrations &lt;- as.vector(drug_concentrations)\n\n# Generate cholesterol levels for each sample \n# (assuming a positive correlation with drug concentration)\ncholesterol_levels &lt;- drug_concentrations + \n  rnorm(num_patients * num_days * num_samples_per_day, mean = 0, sd = 5)\n\n# Generate adverse events based on drug concentration \n# (assuming a higher chance of adverse events with higher concentration)\n# Sigmoid function to map concentrations to probabilities\nadverse_events_prob &lt;- plogis(drug_concentrations / 10) \nadverse_events &lt;- rbinom(num_patients * num_days * num_samples_per_day, \n                         size = 1, prob = adverse_events_prob)\n\n# Create a data frame\nsyn_dat2 &lt;- data.frame(\n  PatientID = rep(patient_ids, each = num_samples_per_day),\n  TreatmentGroup = rep(treatment_assignments, each = num_samples_per_day),\n  Day = rep(day_ids, each = num_samples_per_day),\n  DrugConcentration = drug_concentrations,\n  CholesterolLevel = cholesterol_levels,\n  AdverseEvent = adverse_events\n)"
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data-1",
    "title": "Generating synthetic data with R",
    "section": "Checking data",
    "text": "Checking data\nTake a peek at the generated data.\n\n# Print the first few rows of the generated dataset\nprint(head(syn_dat2))\n\n  PatientID TreatmentGroup Day DrugConcentration CholesterolLevel AdverseEvent\n1         1       Low Dose   1          8.462781        12.401475            0\n2         1       Low Dose   2          6.909660        10.754871            0\n3         1       Low Dose   3          6.327317         7.988330            0\n4         1       Low Dose   4          5.473243         0.431360            1\n5         1       Low Dose   5          4.296404         3.699141            0\n6         1       Low Dose   6          6.177406         4.775430            1\n\nsummary(syn_dat2)\n\n   PatientID     TreatmentGroup          Day    DrugConcentration\n Min.   : 1.00   Length:140         Min.   :1   Min.   : 2.081   \n 1st Qu.: 5.75   Class :character   1st Qu.:2   1st Qu.: 5.174   \n Median :10.50   Mode  :character   Median :4   Median : 7.056   \n Mean   :10.50                      Mean   :4   Mean   : 7.593   \n 3rd Qu.:15.25                      3rd Qu.:6   3rd Qu.: 9.738   \n Max.   :20.00                      Max.   :7   Max.   :14.933   \n CholesterolLevel  AdverseEvent   \n Min.   :-4.494   Min.   :0.0000  \n 1st Qu.: 3.844   1st Qu.:0.0000  \n Median : 7.234   Median :1.0000  \n Mean   : 7.855   Mean   :0.6571  \n 3rd Qu.:11.259   3rd Qu.:1.0000  \n Max.   :24.422   Max.   :1.0000  \n\ndplyr::glimpse(syn_dat2)  \n\nRows: 140\nColumns: 6\n$ PatientID         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3‚Ä¶\n$ TreatmentGroup    &lt;chr&gt; \"Low Dose\", \"Low Dose\", \"Low Dose\", \"Low Dose\", \"Low‚Ä¶\n$ Day               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4‚Ä¶\n$ DrugConcentration &lt;dbl&gt; 8.462781, 6.909660, 6.327317, 5.473243, 4.296404, 6.‚Ä¶\n$ CholesterolLevel  &lt;dbl&gt; 12.4014754, 10.7548711, 7.9883301, 0.4313600, 3.6991‚Ä¶\n$ AdverseEvent      &lt;int&gt; 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1‚Ä¶"
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#exploratory-plot",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#exploratory-plot",
    "title": "Generating synthetic data with R",
    "section": "Exploratory plot",
    "text": "Exploratory plot\n\n# Plot drug concentrations over time for each individual using ggplot2\np1 &lt;- ggplot(syn_dat2, aes(x = Day, y = DrugConcentration, \n                      group = as.factor(PatientID), color = TreatmentGroup)) +\n  geom_line() +\n  labs(title = \"Drug Concentrations Over Time\",\n       x = \"Day\",\n       y = \"Drug Concentration\",\n       color = \"TreatmentGroup\") +\n  theme_minimal()\nplot(p1)\n\n\n\n\n\n\n\np2 &lt;- ggplot(syn_dat2, aes(x = as.factor(AdverseEvent), y = DrugConcentration, \n                           fill = TreatmentGroup)) +\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8), color = \"black\") +\n  geom_point(aes(color = TreatmentGroup), position = position_dodge(width = 0.8), \n             size = 3, shape = 16) +  # Overlay raw data points\n  labs(\n    x = \"Adverse Events\",\n    y = \"Drug Concentration\",\n    title = \"Boxplot of Drug Concentration by Adverse Events and Treatment\"\n  ) +\n  scale_color_manual(values = c(\"A\" = \"blue\", \"B\" = \"red\")) +  # Customize color for each treatment\n  theme_minimal() +\n  theme(legend.position = \"top\")\nplot(p2)\n\n\n\n\n\n\n\n\nThis dataset has a bit more complicated structure than the previous one, but it isn‚Äôt much harder to generate it with code. You‚Äôll want to generate your data such that it mimics the real data you plan on analyzing."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#save-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#save-data",
    "title": "Generating synthetic data with R",
    "section": "Save data",
    "text": "Save data\n\n# Save the simulated data to a CSV or Rds file\nwrite.csv(syn_dat2, here(\"data\",\"syn_dat2.csv\"), row.names = FALSE)\n# if we wanted an RDS version\n#saveRDS(syn_dat2, here(\"data\",\"syn_dat2.Rds\"))\n\nWe are saving the data as CSV and Rds files. CSV files are readable with pretty much any software and thus very portable. Rds files are R-specific, thus not as flexible. The advantage of Rds files is that they are generally smaller, and they retain information about the variables, e.g.¬†if they are factor or numeric variables. Either format works, sometimes one or the other might be better."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html",
    "title": "R packages to generate synthetic data",
    "section": "",
    "text": "This unit provides brief introductions of different R packages that can be used to generate synthetic/simulated/artificial/fake data."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#r-packages-to-generate-synthetic-data",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#r-packages-to-generate-synthetic-data",
    "title": "R packages to generate synthetic data",
    "section": "R packages to generate synthetic data",
    "text": "R packages to generate synthetic data\n\nThe coxed package allows you - among other things - to generate longitudinal survival (time-to-event) data using its sim.survdata() function.\nThe admiral package, which is part of the pharmaverse, allows users to generate data in ADaM format. ADaM is a common data format in the pharmaceutical area.\nThe simstudy package allows users to define variables and relations between them, and then have synthetic data generated based on those specifications. It can also create data with potentially complex structures, such as longitudinal or hierarchical data.\nThe synthpop package can produce synthetic data that is very similar to the original data, including potential patterns/correlations.\nThe mice package is most often used to impute missing data. But one can also use it to generate synthetic data. This tutorial provides a good introduction."
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#some-r-helper-packages",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#some-r-helper-packages",
    "title": "R packages to generate synthetic data",
    "section": "Some R helper packages",
    "text": "Some R helper packages\n\nThe CRAN Task View: Probability Distributions lists packages that can be used to generate data following various probability distributions. This can be useful if you want to generate data that has a certain shape/distribution."
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html",
    "href": "content/module-workflow/complete-data-analysis-workflow.html",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "",
    "text": "This very short unit re-visits all of the data analysis concepts and components we discussed so far, and provides a summary and big-picture discussion, meant to wrap up things."
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-question",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-question",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The question",
    "text": "The question\nHaving a good question (hypothesis) that is interesting, important, new, and can be answered with the resources you have (data, skills, computing power) is the most crucial part of any project. You can do an analysis that is technically perfect, but if you don‚Äôt answer an interesting and relevant question, nobody will care. While I think one should use state-of-the art analysis approaches as much as possible, it is in my opinion more important to answer a good question. I believe that an important question analyzed with a simple model is almost always better than using a complicated model to answer a question that nobody cares about. Of course, the simple model still needs to be reasonable. If one uses a completely wrong model or performs a faulty analysis, the whole project/paper might also be meaningless."
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-setup",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-setup",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The setup",
    "text": "The setup\nYou should do your whole analysis as automated, reproducible, and as well-structured and well-documented as possible. Your colleagues, your readers and your future self will thank you for it. We have used tools in this course (R/R Markdown/GitHub) which help in performing an analysis in such a way. Many other tools are available. While some tools are worse than others (e.g., Excel), in the end it doesn‚Äôt matter too much what tools you use, as long as you can do things automated, reproducible, and well-documented."
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-wrangling",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-wrangling",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The wrangling",
    "text": "The wrangling\nAs you will likely appreciate by now, getting the data into a shape that can be analyzed is ‚Äì for almost any dataset ‚Äì time consuming and also incredibly important. Lots of mistakes happen at that stage. For a recent prominent example where things went wrong, see e.g.¬†Aboumatar and Wise 2018 published to JAMA, where a mis-coding of a factor variable led to one conclusion, and upon fixing it, the conclusion changed in the other direction, leading to retraction of the original study (and republication of the corrected study). This is an example where an error was found and fixed. Unfortunately, there are probably a lot of studies in the literature where mistakes during the wrangling process were made, wrong results published, and nobody noticed. It is impossible to fully prevent making mistakes. But there are ways to try and minimize those problems. To do so, follow these rules:\n\nDocument everything very well. Every step in the wrangling/cleaning process should be explained and justified (e.g.¬†if you drop observations with NA, what does it mean and why do you think it‚Äôs ok to do).\nAutomate things as much as possible. Manual tasks often introduces errors.\nMake everything reproducible. That helps you and others spot mistakes faster.\nCritically evaluate every step you take. If something is happening that doesn‚Äôt look quite right, or you get warning messages in your code, stop and figure out what is going on. Only proceed once you know exactly what is happening and are ok with it.\nTry different alternatives. For instance if you are unclear if you should remove missing observations, or remove a variable that has a lot of missing, or use imputation, why not try it all 3 ways? It usually doesn‚Äôt take much extra work to do a few alternatives. If each version of doing things gives you more or less the same results, it helps convince yourself and the readers that your finding might be robust to the details of the analysis. If different reasonable ways of doing the analysis lead to different results, you have learned something too, and it might be worth digging deeper to understand why results differ. You might find some new, unexpected and interesting bit of science lurking. It is important to report an any analysis you did, even if just briefly in the supplement. You are not allowed to run multiple analyses and then just report the one that gives you the answers you want (likely happens often, see p-hacking above.)"
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-analysis",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-analysis",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The analysis",
    "text": "The analysis\nYou have learned that there are a lot of different analysis approaches out there, and which one to choose depends on many factors, such as the question (e.g.¬†do you care more about interpretability or performance), available resources, and many other considerations. All the rules listed above for wrangling hold for the analysis bit too. Make it reproducible, well documented, well explained and justified. Make sure you understand results at each step. If possible, try different alternative approaches. Some additional, analysis-specific considerations are the following:\n\nThink carefully about the performance measure you want to fit to. While the ‚Äòstandard‚Äô ones, like RMSE/SSR for continuous outcomes and accuracy for categorical outcomes are at times ok, often other measures might be more meaningful. E.g. for continuous outcomes, you might want to do RMSE not on the outcome but the log of the outcome. Or you might want to penalize with least absolute difference to better deal with outliers. Similarly, for categorical outcomes, especially when there is imbalance in the data and you have much fewer of one category than of others, using accuracy might not be best. Some other metric such as F1 score, or a custom performance measure might be better. Spend some time thinking about the best performance measure before you do all your fitting.\nOnce you picked your performance measure and are ready to fit/train your model, make sure to not evaluate performance on the data used for building the model. More complex models can always give improved performance on the data used to build the model, thus this metric is not meaningful! Instead, to evaluate model performance, ideally use some version of cross-validation, i.e.¬†fitting the model to some of the data and evaluating model performance on a part of the data that was not used for fitting. If this is not possible, e.g. because you don‚Äôt have much data or it takes too long to run, us AIC & Co.¬†as a backup option to determine model quality.\nCompare your model to baseline/null models and simple single-predictor models to get an idea for the improvement you can get. Try a complex model to estimate the upper bound of model performance. Then try a few reasonable models ‚Äúin between‚Äù the null model and the really complex model, and pick the one that works overall best for your purpose. That last step is subjective. That is ok, as long as you can explain and justify why you ended up going with the model you chose.\nOnce you have chosen your best model (or even before, for the purpose of picking your final model), perform model assessment. Look at uncertainty, investigate residuals, look at variable importance, etc. Poke your model in as many ways as possible to understand how it works and what its limitations might be.\nIf you have enough data, set some aside at the beginning (test data), and apply your model to that data at the very end. This gives you the most honest assessment of your model performance for new/unseen data."
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-reporting",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-reporting",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The reporting",
    "text": "The reporting\nYou almost never do an analysis just for fun and for yourself. Usually, there are other reasons. For instance in academia/science, we analyze data to better understand our system, to test hypotheses, to answer questions. Similarly in industry and other applied settings, we analyze data to come to actionable conclusions (e.g.¬†we determine which images show a likely cancer and therefore which patients need further tests or surgery). In all those situations, we want to communicate our findings to others. That can be through the peer-reviewed literature, in a meeting with our colleagues and bosses, as a report for patients, etc. Being able to report findings from a potentially complicated analysis in a way that it has an impact, and is appropriate for the right audience, is not easy. Some of the ideas listed above, as well as others are worth keeping in mind:\n\nPresent your findings in such a way that people can (but don‚Äôt have to) go deeper easily. Start with a short summary (often called executive summary in industry and abstract in academia). This short write-up should summarize your findings understandably and honestly. Do not spin/hype things that are not supported by your actual analysis. Also, focus on the main important finding(s) and their implications. The main deliverable (usually some form of written report or a paper), should present all the main findings and steps you took, nicely explained. Then provide additional information (e.g.¬†supplement, appendices) with more details. Finally, provide all the raw materials, i.e.¬†data and well-documented code, for others to look at. By layering content, different audiences can go into your findings in as much or little detail as they want.\nExplain and justify everything. It‚Äôs unlikely that everyone would have made exactly the same decisions you did during your analysis. But by explaining your rationale, readers can decide if they find what you did reasonable, and thus make an informed decision as to how much they trust your findings.\nReport results from multiple approaches: If you show how certain decisions during the analysis do or don‚Äôt affect the results, it makes things more transparent and can instill greater confidence in readers.\nAutomate things. As much of your final products as possible should be automated. That means that figures and tables should not be created by hand. This way, if you want or have to change things upstream (e.g.¬†you noticed a mistake in some analysis step or reviewers/your boss request changes), you can update everything as quickly, automated and seamless as possible.\nUse modern ways to report and disseminate your findings. The standard academic way is still to write peer-reviewed papers, or in industry, prepare a report. However, such documents are generally not too widely read and at times have only limited impact. As appropriate for your project, consider other sources of dissemination. For instance, make a website for your analysis. Turn it into a blog post. Tweet about it. Use interactive tools (e.g. the R Shiny package) to allow the audience to interact with your results. Be creative and think about the best ways to reach your intended audience."
  },
  {
    "objectID": "content/module-workflow/open-science.html",
    "href": "content/module-workflow/open-science.html",
    "title": "Open Science and Reproducible Research",
    "section": "",
    "text": "Overview\nIn this unit, we‚Äôll talk a bit more about reproducible research, and how that fits into the much larger concept of ‚Äúopen science.‚Äù\n\n\nLearning Objectives\n\nUnderstand the general idea of open science\nExplain what makes research reproducible and/or replicable\nKnow what FAIR data standards are\n\n\n\nIntroduction\nWay back at the beginning of the course, we briefly discussed reproducibility. Throughout the course, we emphasized the importance to do analyses in a reproducible (and thus generally automated/scripted) manner. In this module, we‚Äôll discuss a bit more what reproducible research means and entails, current trends towards open science, and why you pretty much need to have a solid grasp on reproducible research strategies in the future.\n\n\nFederal open science requirements\nWhile the concepts of reproducible research and open science have been around for a while, they have recently increased in prominence. Several US government organizations have said that 2023 is the Year of Open Science, and a recent Office of Science and Technology Policy memo has recommended that all federal agencies take serious steps towards open science practices (more on what exactly this means later).\nFederal agencies like the NIH have been requiring data sharing and public releases of papers (on PMC) for some time now, but these recommended measures are even stronger, and in my opinion, a very positive direction for science. It is quite likely that if you work with federal funding in the near future, you will be required to share your data and code publicly as soon as you decide to publish. The exact implementation of these protocols hasn‚Äôt been determined yet (nor has the degree to which these recommendations will be formally implemented), but the writing is on the wall, so to speak.\nIt is easier for you to get comfortable with reproducible research methods and open science now, rather than scrambling to learn it in the future!\n\n\nThe general idea of open science\nThere are many ways of defining and describing ‚Äúopen science‚Äù.\nIf you search online for ‚Äúwhat is open science,‚Äù you will likely find various conceptual diagrams and descriptions. The papers Open science saves lives: lessons from the COVID-19 pandemic and From Open Data to Open Science provide pretty good conceptual discussions of open science, but I think the best of schematic summary is from NASA.\n\n\n\nSource: NASA\n\n\nNASA defines three components that are central to open science.\n\nAccessible ‚Äì your research process and results need to be transparent. Open access, FAIR data, and code sharing all fall into this.\nReproducible ‚Äì other people should be able to get the same results as you, and you should strive to make reusable products.\nInclusive ‚Äì if your research is only accessible and reproducible for relatively well-off scientists at major institutions in the USA, it‚Äôs still not really open üôÇ.\n\nIf you want a formal definition of ‚Äúopen science,‚Äù here‚Äôs the one from the OSTP, which is pretty comprehensive:\n\n‚ÄúThe principle and practice of making research products and processes available to all, while respecting diverse cultures, maintaining security and privacy, and fostering collaborations, reproducibility, and equity.‚Äù\n\nOpen science is like spaghetti sauce‚Äìit has a lot of ingredients that have to be cooked together to get the best flavor. Let‚Äôs go into a bit more detail on each of these topics.\n\n\nAccessibility and FAIR data\nOpen access publishing is a big topic, but for the purposes of this class I will just say that I think you should publish Open Access whenever possible. In the future, it seems likely that any federally funded projects in the US will require an open access fund as part of the budget, as publishing OA can often be prohibitively expensive. If you want to learn more, check out the Wikipedia page on Open Access, or if you really want to learn more, see Peter Suber‚Äôs book on OA.\nThe main purpose of this section is to talk about data sharing. Data sharing used to be a pretty contentious topic, but fortunately it is more normalized now than it has ever been before, and thanks to NIH (and other agency mandates), it will likely be a normal part of science before too long.\nOf course just dumping messy data without any documentation into a depository is not great for accessibility and reproducibility. As some of you noticed during this course, even data from the CDC or other reputable organizations is often very poorly formatted and does not have a lot of metadata or documentation to help you out. To somewhat solve this issue, the FAIR data standards was created. FAIR stands for:\n\nFindable: when someone goes to your paper or project, they should be able to easily get to the data source.\nAccessible: users need to be able to access the data, and data are stored in a way that users can feasibly get them.\nInteroperable: data should be in standardized formats, using standard vocabulary. You should be specific if your data is derived from another dataset or if the user will need other related datasets.\nReusable: data should be documented and version-controlled when you release it, with clear descriptions of what information is contained in the data, who can use the data, and for what purposes.\n\nThe FAIR standard was originally described in this 2016 publication. Another great resource is the GO FAIR initiative. It is worth taking a short look at both these resources.\nOf course dealing with things like data use agreements and individual privacy is paramount, so 100% achieving these goals all the time is not feasible. Sometimes you might need to use synthetic data based on your real data, or remove certain information from your data to protect privacy, or require a data use agreement before you can distribute the data to individuals. We should strive for data to be as FAIR as possible, but protecting individual privacy is equally important!\nIn general, a lot of authors still hide behind ‚ÄúI can‚Äôt share my data, it‚Äôs confidential‚Äù as an excuse for not having to share. Most of the time, it is possible to share de-identified data without confidentiality problems. Hopefully, the near future will make data sharing much more common and also easier.\n\n\nReproducibility (and replicability too)\nWe discussed reproducibility in our previous reading, revisit it if you need a refresher. In one sentence, your research is reproducible if another scientist can repeat your methods on your data and get the same results. That is, someone else can take your code and data, rerun all your code, and get the same answers that you got in your paper.\nAn important and related issue is replicability ‚Äî- you‚Äôve probably heard of the ‚Äúreplication crisis‚Äù in science. If not, see for instance this Wikipedia page. In one sentence, your research is replicable if someone else can follow your methods and materials to collect new data, analyze their data, and get similar results. Of course in social and semi-social sciences (like epidemiology), when your results often depend on the target population of interest and are expected to vary over time, replicability can be a bit more nuanced than in an experimental field, where experiments should pretty much control for anything that you aren‚Äôt interested in.\nNote that replicability is somewhat related to the analysis topic of generalizability/portability we discussed. The difference is that we focused on analysis approaches that would lead to replicable modeling/analysis results using other data sources. More broadly, one wants to be able to replicate overall findings if one were to repeat the full study, including collection of new data. This is important, but for purposes of analysis, the data is considered given, and the focus is on reproducibility/replicability of the data analysis part.\n\n\nInclusivity\nThe topic of inclusivity in data science is heavily tied into the idea of data ethics ‚Äì critical thinking about the potential results and consequences of the products we create is crucial in data science. Many of you have some public health training, and are therefore likely familiar with the Belmont Report.\nRegardless of your scientific background, when working with data (especially human subjects data), the guiding principles of respect for persons, beneficence, and justice should be critical considerations for all of our research.\nFor some additional reading on inclusivity and data science ethics, see for instance these resources:\n\nElaine O Nsoesie, Sandro Galea, Towards better Data Science to address racial bias and health equity, PNAS Nexus, Volume 1, Issue 3, July 2022, pgac120, https://doi.org/10.1093/pnasnexus/pgac120\nO‚ÄôNeil, Cathy. Weapons of math destruction. Crown, 2016. ISBN: 978-0553418811.\nNoble, Safiya Umoja. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press, 2018. ISBN: 978-1479837243.\nhttps://datapractices.org/manifesto/\nFloridi L, Taddeo M. What is data ethics?. Philos Trans A Math Phys Eng Sci. 2016;374(2083):20160360. doi:10.1098/rsta.2016.0360\n\n\n\nPractical considerations\nIt is all nice and good to tell people that their data needs to be FAIR, and that their science needs to be open. But the question is, why do it? Most people might agree that there are societal benefits. For instance having a resource like GenBank for genetic sequences has allowed many scientists beyond those who created the original sequence to use the data and answer important scientific questions. However, there generally need to also be benefits for individuals to entice them to spend time following Open Science standards. This is happening increasingly. On one hand, tools to do research in an Open Science framework are getting increasingly better. For instance the whole R + Quarto framework we‚Äôve been using in class makes it rather easy to do things automated and reproducible. Many other similar tools and resources are becoming available. Another potential benefit of Open Science is increased visibility. If others can use your data and models, they will likely cite your work. They might even want to collaborate with you. It seems for those individuals who are good at sharing, the benefits outweigh the perceived risks (such as ‚Äúbeing scooped‚Äù). And the final, ever increasing ‚Äúbenefit‚Äù is that publishers and funding agencies increasingly require it. So if you don‚Äôt follow Open Science standards, it might soon get hard to publish work or get funding. It‚Äôs less of a ‚Äúbenefit‚Äù and more of an ‚Äúavoid the penalties‚Äù thing, but regardless, it is an important practical consideration.\n\n\nFurther reading\nEverything we discussed throughout this course, and this short introduction, hopefully conveyed the message that Open Science is an important topic to think about and strive toward. The importance of the topic has been increasing in recent years. It is useful to stay somewhat informed. Here are a few places you can find more open science resources.\n\nThe Center for Open Science, a nonprofit dedicated to promoting open science practices and community change, maintains a blog with several thought provoking readings.\nStatistician Andrew Gelman sometimes writes about open science, mostly from a statistical perspective and has some nice comments on replication. (1) (2)\nFOSTER is a European organization with a set of free courses on open science principles."
  },
  {
    "objectID": "courseinfo/course-communication.html",
    "href": "courseinfo/course-communication.html",
    "title": "Course Communications",
    "section": "",
    "text": "The main course content can be found on this website.\nWe‚Äôll mostly communicate and discuss through Discord.\nThere will be completely optional Zoom sessions (aka office hours).\nSome communication/interaction will happen via GitHub. More details are provided in the relevant sections.\nQuizzes will be available from this website and submitted to an online system, instructions are provided.\n\n\n\n\nThis is the first time I‚Äôm using Discord for the class, previous classes used Slack. I decided to switch since the free version of Slack is very limited. If you find anywhere on this website a mention of Slack, just assume that I mean Discord. Also please let me know so I can fix it. (Or contribute the fix yourself, you‚Äôll soon learn how to do that through GitHub).\n\nThe main mode of communication will be through Discord. You should get an email invite to the Discord server for this class. If you haven‚Äôt received an email by Friday before classes start or can‚Äôt find the Discord invite link on eLC, let me know. We won‚Äôt use eLC, everything is going to happen on Discord or this website.\nIf you are unfamiliar with Discord, it is essentially a chat/discussion tool (similar to Slack or Teams or‚Ä¶). The main area for the class is called a server in Discord. Ours is called MADA-Spring-2024. If you use Discord for other purposes, you might be a member of additional servers. Each server has channels. Each channel corresponds to a topic. The names and descriptions of the different channels should be self-explanatory.\nIf you have requests for specific channels to be created, let me know.\nIf a channel is public, you can see it and participate in the discussion. For private channels you need an invite to see it and participate. For simplicity, all channels are public, though there will be channels which likely won‚Äôt be of interest to you. Specifically, you‚Äôll be placed into groups during the course and will likely only be interested in the channel for your group.\nInside a channel, you can write messages. To respond to someone‚Äôs message, you can either respond as thread or type your message just below the previous one. The good thing with responding as new message is this is that everyone sees it easily. The bad thing is that if there are a good number of messages being typed, it can be hard to figure out which ones go together. An alternative, and maybe better way is to reply in a thread.\nYou can also use Discord to send one or more individuals direct messages (DM). You can use that to communicate with me or each other.\nOverall, Discord is fairly intuitive, and hopefully much easier to use than the rather clunky discussion feature of eLC. But some features might need getting used to. If you have no previous Discord experience, I suggest you browse through their very comprehensive documentation to pick up a few more tips and tricks.\nOne useful thing to know is that people will get a specific notification if you either send them a direct message or use their handle in our message, e.g.¬†by writing @personname can you take a look at the figure below‚Ä¶. Specifically, if you want to get my attention, you need to do it that way. I have too many things pinging me all the time üòÅ. While I try to stay on top of all the messages in our class, I likely won‚Äôt. To ensure I don‚Äôt miss something directed at me, use the @ notation (or DM).\nI will use the announcements channel to post any important course related announcements. Check regularly.\nUse Discord widely to ask questions, to answer others‚Äô questions, to notify me and others of things that are unclear/wrong, to post links to interesting resources, etc. I hope we can build a community of learners where you help and support each other. So please don‚Äôt always wait for me to answer questions or provide feedback! Engage with each other. I will regularly visit and read the discussions and participate, but I hope that I will end up as one participant of many, not the only person providing answers.\nI prefer that you use Discord instead of email to ask questions. The reason is that if you have a question, it is likely that others have it too. By asking and answering online, everyone can see it. That said, there might be instances where the question only relates to you. In such cases, feel free to either send me a direct message on Discord or email me.\nYou might have already a Dischord user name or decide to choose a user name which is not your real name. In that case, please update your Discord profile for our server to your real name (and if you want, preferred pronouns and other information). Also, to make things more personal, add an image of yourself to the profile. It will help make things a bit more social, even if it‚Äôs not quite the same as in-person. You can do all that if you right-click your user name and choose ‚ÄúEdit Server Profile‚Äù.\n\n\n\nI will offer weekly synchronous meetings through Zoom. Those are completely optional. Whoever wants to join can do so, and we will use the meeting for discussions, answering questions, and whatever else you want to talk about. Think of those meetings a bit like virtual office hours or virtual in-class discussions. To ensure everyone will be able to join at least occasionally, I plan to alternate between two times each week.\nI want to emphasize that there is nothing covered in the meetings other than me addressing questions from individuals. I will not record these meetings to ensure students feel comfortable asking ‚Äúdumb‚Äù questions (they are rarely ‚Äúdumb‚Äù, but I don‚Äôt want anyone to feel limited out of fear of ‚Äúlooking stupid‚Äù in the recording). If any topic comes up that I think is worth sharing more widely, I will afterwards post a message and/or add content to the website. Therefore, if you can‚Äôt make these Zoom meetings, you are not missing anything (other than the option of asking me questions that might be tricky to discuss on Discord).\nTentative dates for Zoom meetings are listed in the Schedule document. Those are subject to change. If we need to move things around, I‚Äôll post an announcement and also update the Schedule document. At the designated times, I will start the meeting. I‚Äôll wait around for 15 minutes. If nobody connects, I‚Äôll end the meeting. If people connect, we‚Äôll talk until we covered everything, or until I or you need to move on to our next appointments. The information for the Zoom meeting login will be provided on Discord.\nAnd of course, if you have something you want to discuss outside of those scheduled times, send me a message and we can schedule a meeting.\n\n\n\nWe will also use Github for some specific communications, related to exercises and projects. More detailed instructions on that are provided at the time you are asked to use GitHub.\n\n\n\nWe will not be using eLC. I might occasionally use the email everyone feature in eLC to send out an email to the class in the first few weeks until everyone has settled into using Discord. After that, I do not plan on using eLC.\n\n\n\nIn some semesters, I have a TA for the class. If there is a TA the semester you are taking the class, they will be another point of contact for you to ask questions. The TA will be posting an introduction to Discord with details of their availability and preferred modes of communication (Discord/Zoom/etc.)."
  },
  {
    "objectID": "courseinfo/course-communication.html#discord",
    "href": "courseinfo/course-communication.html#discord",
    "title": "Course Communications",
    "section": "",
    "text": "This is the first time I‚Äôm using Discord for the class, previous classes used Slack. I decided to switch since the free version of Slack is very limited. If you find anywhere on this website a mention of Slack, just assume that I mean Discord. Also please let me know so I can fix it. (Or contribute the fix yourself, you‚Äôll soon learn how to do that through GitHub).\n\nThe main mode of communication will be through Discord. You should get an email invite to the Discord server for this class. If you haven‚Äôt received an email by Friday before classes start or can‚Äôt find the Discord invite link on eLC, let me know. We won‚Äôt use eLC, everything is going to happen on Discord or this website.\nIf you are unfamiliar with Discord, it is essentially a chat/discussion tool (similar to Slack or Teams or‚Ä¶). The main area for the class is called a server in Discord. Ours is called MADA-Spring-2024. If you use Discord for other purposes, you might be a member of additional servers. Each server has channels. Each channel corresponds to a topic. The names and descriptions of the different channels should be self-explanatory.\nIf you have requests for specific channels to be created, let me know.\nIf a channel is public, you can see it and participate in the discussion. For private channels you need an invite to see it and participate. For simplicity, all channels are public, though there will be channels which likely won‚Äôt be of interest to you. Specifically, you‚Äôll be placed into groups during the course and will likely only be interested in the channel for your group.\nInside a channel, you can write messages. To respond to someone‚Äôs message, you can either respond as thread or type your message just below the previous one. The good thing with responding as new message is this is that everyone sees it easily. The bad thing is that if there are a good number of messages being typed, it can be hard to figure out which ones go together. An alternative, and maybe better way is to reply in a thread.\nYou can also use Discord to send one or more individuals direct messages (DM). You can use that to communicate with me or each other.\nOverall, Discord is fairly intuitive, and hopefully much easier to use than the rather clunky discussion feature of eLC. But some features might need getting used to. If you have no previous Discord experience, I suggest you browse through their very comprehensive documentation to pick up a few more tips and tricks.\nOne useful thing to know is that people will get a specific notification if you either send them a direct message or use their handle in our message, e.g.¬†by writing @personname can you take a look at the figure below‚Ä¶. Specifically, if you want to get my attention, you need to do it that way. I have too many things pinging me all the time üòÅ. While I try to stay on top of all the messages in our class, I likely won‚Äôt. To ensure I don‚Äôt miss something directed at me, use the @ notation (or DM).\nI will use the announcements channel to post any important course related announcements. Check regularly.\nUse Discord widely to ask questions, to answer others‚Äô questions, to notify me and others of things that are unclear/wrong, to post links to interesting resources, etc. I hope we can build a community of learners where you help and support each other. So please don‚Äôt always wait for me to answer questions or provide feedback! Engage with each other. I will regularly visit and read the discussions and participate, but I hope that I will end up as one participant of many, not the only person providing answers.\nI prefer that you use Discord instead of email to ask questions. The reason is that if you have a question, it is likely that others have it too. By asking and answering online, everyone can see it. That said, there might be instances where the question only relates to you. In such cases, feel free to either send me a direct message on Discord or email me.\nYou might have already a Dischord user name or decide to choose a user name which is not your real name. In that case, please update your Discord profile for our server to your real name (and if you want, preferred pronouns and other information). Also, to make things more personal, add an image of yourself to the profile. It will help make things a bit more social, even if it‚Äôs not quite the same as in-person. You can do all that if you right-click your user name and choose ‚ÄúEdit Server Profile‚Äù."
  },
  {
    "objectID": "courseinfo/course-communication.html#zoom",
    "href": "courseinfo/course-communication.html#zoom",
    "title": "Course Communications",
    "section": "",
    "text": "I will offer weekly synchronous meetings through Zoom. Those are completely optional. Whoever wants to join can do so, and we will use the meeting for discussions, answering questions, and whatever else you want to talk about. Think of those meetings a bit like virtual office hours or virtual in-class discussions. To ensure everyone will be able to join at least occasionally, I plan to alternate between two times each week.\nI want to emphasize that there is nothing covered in the meetings other than me addressing questions from individuals. I will not record these meetings to ensure students feel comfortable asking ‚Äúdumb‚Äù questions (they are rarely ‚Äúdumb‚Äù, but I don‚Äôt want anyone to feel limited out of fear of ‚Äúlooking stupid‚Äù in the recording). If any topic comes up that I think is worth sharing more widely, I will afterwards post a message and/or add content to the website. Therefore, if you can‚Äôt make these Zoom meetings, you are not missing anything (other than the option of asking me questions that might be tricky to discuss on Discord).\nTentative dates for Zoom meetings are listed in the Schedule document. Those are subject to change. If we need to move things around, I‚Äôll post an announcement and also update the Schedule document. At the designated times, I will start the meeting. I‚Äôll wait around for 15 minutes. If nobody connects, I‚Äôll end the meeting. If people connect, we‚Äôll talk until we covered everything, or until I or you need to move on to our next appointments. The information for the Zoom meeting login will be provided on Discord.\nAnd of course, if you have something you want to discuss outside of those scheduled times, send me a message and we can schedule a meeting."
  },
  {
    "objectID": "courseinfo/course-communication.html#github",
    "href": "courseinfo/course-communication.html#github",
    "title": "Course Communications",
    "section": "",
    "text": "We will also use Github for some specific communications, related to exercises and projects. More detailed instructions on that are provided at the time you are asked to use GitHub."
  },
  {
    "objectID": "courseinfo/course-communication.html#elc",
    "href": "courseinfo/course-communication.html#elc",
    "title": "Course Communications",
    "section": "",
    "text": "We will not be using eLC. I might occasionally use the email everyone feature in eLC to send out an email to the class in the first few weeks until everyone has settled into using Discord. After that, I do not plan on using eLC."
  },
  {
    "objectID": "courseinfo/course-communication.html#course-teaching-assistant",
    "href": "courseinfo/course-communication.html#course-teaching-assistant",
    "title": "Course Communications",
    "section": "",
    "text": "In some semesters, I have a TA for the class. If there is a TA the semester you are taking the class, they will be another point of contact for you to ask questions. The TA will be posting an introduction to Discord with details of their availability and preferred modes of communication (Discord/Zoom/etc.)."
  },
  {
    "objectID": "courseinfo/course-schedule.html",
    "href": "courseinfo/course-schedule.html",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Schedule",
    "section": "",
    "text": "This is the current course schedule for the class. Things might change, so check regularly for updates.\nThe Assessments column lists items that need to be submitted and that will be graded. Those are described in more detail in an Assessments page and in the assessment document for each module.\nNote: Withdrawal deadline is Thursday 3/21."
  },
  {
    "objectID": "courseinfo/course-schedule.html#synchronous-zoom-meetings",
    "href": "courseinfo/course-schedule.html#synchronous-zoom-meetings",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Schedule",
    "section": "Synchronous Zoom meetings",
    "text": "Synchronous Zoom meetings\nI will offer a weekly Zoom meeting (aka office hour). See the Communication section for some more details. As described in the Communication document, if you don‚Äôt join the Zoom meetings you are not missing any important information, these are just completely optional opportunities to ask for help/feedback etc.\nYou can find the Zoom link on Discord. The plan is to have weekly alternating times for those zoom meetings such that everyone is able to make the meeting at least every 2 weeks.\nThe current plan is to have Zoom meetings for odd module numbers on Wednesdays at 8am, and for even module numbers on Tuesdays at 8am. The first Zoom session will be on Wednesday, 1/10 at 8am ET.\nHopefully, at least one of those two times works for you, allowing you to join at least every other time if wanted. If you already know that neither times work for you, and you are keen on joining the Zoom meetings, let me know and we can adjust."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Modern Applied Data Analysis (MADA)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course was developed and is maintained by Andreas Handel.\nThe following individuals have contributed to improving the course: Ian Bachli (@idbachli), Megan Beaudry (@MeganSarah), Zane Billings (@wzbillings), Amanda Blubaugh (@alblubaugh), Mia Carmichael (@mncarmic), Ellen Cheng (@ellenmonster), Paige Duffin (@paigeduffin), Robert Edenfield (@ClaytonEdenfield), Julia Frederick (@JuliaFrederick), Savannah Hammerton (@smhammerton), Megan Lott (@meganejlott), Yao Lu (@yl3362), Brian McKay (@bmckay), William Norfolk (@williamnorfolk), Trang Quach (@TrangQuach), Emily Rayens (@Rayense), Megan Robertson (@meganjro), Jadey Ryan (@jadeynryan), Amanda Skarlupka (@askarlupka), Jonathan Wilson (@JJWilson1991), Yang Zhang (@yz17970)\n\nThe course materials are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "project/project-examples.html",
    "href": "project/project-examples.html",
    "title": "Course Project Examples",
    "section": "",
    "text": "Overview\nTo showcase some of the work previous students have done, and as inspiration of future projects, below you can find links to a selection of past projects. Of course, all are posted with permission from the students.\n(I only decided to ask students if I can showcase some of their projects in 2021, so currently that‚Äôs the only year that has examples.)\n\n\n2021 Projects\n\nPlastic in the Urban Freshwater Environment by Amelia Foley - main report, supplement file, GitHub project repository.\nPredicting Federal Emergency Management Agency Funding for U.S. Disasters by Morgan Taylor - main report, supplement file, GitHub project repository."
  },
  {
    "objectID": "project/project-review.html",
    "href": "project/project-review.html",
    "title": "Project Review Information",
    "section": "",
    "text": "You will be asked to review the projects of a few of your classmates twice, once for part 3, and once at the end (part 5).\nI have several goals for these project reviews:\n\nYou get feedback that helps you improve your project.\nYou get experience with giving and receiving reviewer feedback, like you would when you submit a paper for publication or review someone‚Äôs submitted paper.\nYou practice being on the ‚Äòreceiving end‚Äô of reproducible research, i.e.¬†you will have to be able to reproduce someone else‚Äôs project so you can properly critique it."
  },
  {
    "objectID": "project/project-review.html#part-3-review-logistics",
    "href": "project/project-review.html#part-3-review-logistics",
    "title": "Project Review Information",
    "section": "Part 3 review logistics",
    "text": "Part 3 review logistics\n\nFind the projects you are assigned to in the announcments channel. If you have been asked to review a private project, make sure you are added as collaborator.\nFollow the instructions provided in the repository to run all code and reproduce everything.\nUse the simple grading rubric shown in the part 3 section of the Project Details document.\nSubmit your review by filing one (or several) GitHub Issues in the project repository. Provide any feedback you think your classmate(s) might find useful to help them further improve their project and ensure they are on the right track. Also provide your score as indicated in the rubric."
  },
  {
    "objectID": "project/project-review.html#part-5-review-logistics",
    "href": "project/project-review.html#part-5-review-logistics",
    "title": "Project Review Information",
    "section": "Part 5 review logistics",
    "text": "Part 5 review logistics\nFollow the instructions provided in the repository to run all code and reproduce everything. Review the whole project using this review template (right-click to download).\nWrite up a detailed review using and filling out the template. Once you are done, place your completed review document into the repository you are reviewing, and send a pull request (or do a direct push) for each project you review.\n\nAs you reproduce the project, you will likely make a lot of changes to the repository (e.g., by re-creating figures/tables). If you and another reviewer do that both, you might run into conflicts when sending a pull request. If you encounter problems like this, you can provide the project owner your completed review in some other manner (e.g.¬†through email or Slack) and they‚Äôll add it to the repository. The important part is that at the end of the review process, there should be (at least) 2 completed review documents in the project repository (either the main folder, or make a reviews sub-folder for them.)"
  },
  {
    "objectID": "project/project-review.html#review-assessment-rubric",
    "href": "project/project-review.html#review-assessment-rubric",
    "title": "Project Review Information",
    "section": "Review Assessment Rubric",
    "text": "Review Assessment Rubric\nOk, now it gets maybe a bit complicated. I will review and assess the quality of your reviews. To that end, I‚Äôll use a fairly simple rubric, similar to the ones for the previous project submissions.\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nReviews are complete or fairly complete\n3\n\n\nSomewhat insufficient\nReviews are somewhat incomplete, lack useful/detailed feedback\n2\n\n\nInsufficient\nOnly one review was submitted, or submissions were very incomplete\n1\n\n\nAbsent\nNo reviews were submitted\n0\n\n\n\nYou get a full week to do those reviews, so I expect thorough and good quality work!"
  },
  {
    "objectID": "project/project-review.html#finals-steps",
    "href": "project/project-review.html#finals-steps",
    "title": "Project Review Information",
    "section": "Finals steps",
    "text": "Finals steps\n\nUse the peer feedback/reviews from your classmates to further improve your project. You can make any further changes you want to make.\nPush your final project to your GitHub repository by the specified deadline for final grading.\nI will assess your final project using the same categories in the template used during peer review."
  },
  {
    "objectID": "resources/resources-course.html",
    "href": "resources/resources-course.html",
    "title": "Course Resources",
    "section": "",
    "text": "This page lists resources that this course draws on frequently or at least occasionally. For a more extensive list of data science resources, see the General Resources page.\nAll materials described below are (should be) freely available online. If you can‚Äôt get to them, let me know. Note that a lot of the listed resources are dynamic and ever changing. That means occasionally links might not work, sites go offline, chapters in online books get re-arranged, etc. If you notice anything that‚Äôs not quite right, please let me know.\n\nBooks and online tutorials\n\nIntroduction to Data Science (IDS is the book I‚Äôll refer to a good bit in the 1st part of the course.\nR for Data Science (R4DS) (2nd edition). This book will also be mentioned frequently. It is a very gentle and good introduction to data science in R. It has a lot of good exercises and solutions. Note that most of the course material was written when only the 1st edition existed. I‚Äôve tried to update references to specific chapters/pages of R4DS on this website, but it could be that there is still the occasional pointer to something in the 1st edition. If you find such outdated references/links, please let me know so I can fix.\nhe Art of Data Science (ADS) has some good big-picture, introductory level chapters on different aspects of a data analysis. It is a pay what you want book with a minimum price of zero, which means you can get it for free (note that the book + video bundle has a non-zero minimum price). (I heard that sometimes, once you register, the email with the book link goes to the spam folder, make sure to check there.)\nAn Introduction to Statistical Learning (with R) (ISL) is a good introduction to the statistical and machine learning (i.e.¬†model fitting) part of the data analysis workflow. I‚Äôll refer to it frequently in the second part of the course. While we won‚Äôt work through the exercises/labs that are part of ISL, if you are interested in trying out some of those, Emil Hvitfeldt‚Äôs website does the labs using the tidymodels set of packages, which we will also be using in this course.\nHands-on Machine Learning with R (HMLR) covers somewhat similar material as ISL, but with different emphasis and a different approach. I refer to it in several of the later course modules.\nFeature Engineering and Selection (FES) focuses on a specific aspect of the data analysis workflow and is a good resource for those topics.\nhappygitwitR (HGR) is a good resource to learn some of the basics of Git/Github with R.\nPosit Recipes (PR) are a great source for short code examples to do common tasks.\n\n\n\nTools\n\nWe‚Äôll be using R and R Studio Desktop. The R and R Studio document has details.\nWe will also be using Git and GitHub. You need to get that software and a GitHub user account. I also recommend/assume you are using the GitKraken client (but you don‚Äôt have to). See the GitHub introduction document for details.\n\n\n\nImportant notes\n\nThe company now called Posit was called R Studio. You will likely see the old label show up. The software editor we use for writing R code is still called R Studio.\nQuarto is fairly new, in the past we used R Markdown instead. You will likely see references to R Markdown. Just think of it as now being Quarto (i.e.¬†instead of .Rmd files we now work with .qmd files). They are very similar, Quarto is basically the newer version of R Markdown. R Markdown still works, but Quarto is more powerful, so it‚Äôs the better option to use/learn it.\n\n\n\nGeneral help\n\nThe main place to get any course specific help are our course discussion boards. Use them widely to ask questions, to answer others‚Äô questions, to post links to interesting resources, etc.\nMost questions you will have are likely not course specific, but will have to do with R/Github/RMarkdown/etc. For that, Google will be your best friend. Most of the time, someone had the same problem/question you do and someone else answered it. The only tricky thing is finding that post. Even after years of doing this, I probably google how to do something in R every day üòÉ.\nWhen you search online for help, quite often you land on some StackExchange site (often Stack Overflow). These are a collection of widely-used online question and answer sites covering all kinds of topics (including R, Github, Data Analysis, etc.). The majority of the time, you will find an answer, or at least something that gets you closer, on those sites. I rarely go to those sites directly, instead I type my query into Google and it often sends me to one of the StackExchange sites.\nThe Posit community is another good place to ask questions. It is not as widely used as StackExchange, but it seems to be more newcomer friendly. I haven‚Äôt used it much.\nAnother highly recommended resource is the R for Data Science learning community. They have a Slack workspace which you can join for free and ask questions about R, Data Science and related topics. They are very newbie-friendly. I have not used them much but they seem to respond fairly quickly and are helpful.\n\n\n\nMore resources\nThe General Resources page has a more extensive list of relevant resources."
  },
  {
    "objectID": "resources/resources-glossary.html",
    "href": "resources/resources-glossary.html",
    "title": "Course Glossary",
    "section": "",
    "text": "This page is work in progress. Feel free to contribute via a GitHub pull request.\nThere are lots of technical terms related to data and its analysis. In this course, I try to explain them whenever they show up first. However, I figured it might also be useful to have one central place for quick reference. Terms are ordered alphabetically. This is how I define them, others might have different definitions - and so might I in the future, I try to change/update my understanding of things every so often üòÅ.\nNote that in general, there is no regulatory terminology agency, so everyone can use these words to mean whatever they want. While some are fairly well defined, other terms are often used and abused in ways outside the definitions I provide. Always look carefully at what is described to decide what exactly is going on.\n\nNot surprisingly, I‚Äôm not the 1st one with the idea of compiling a list of data science related terms and definitions. After starting this, I found another one by Frank Harrell, which you can find on his website. I‚Äôm sure I‚Äôll find and list more alternatives soon.\n\nArtificial Intelligence: The use of Deep Learning and related approaches applied to ‚Äúcomplex‚Äù problems. Historically, it was used for trying to solve problems using modeling approaches that mimic in a simplified form a brain (i.e.¬†neural net models). Currently this term has become hot and is used more widely than it should.\nBig Data: Any dataset that doesn‚Äôt easily fit into the memory of a regular computer (or cluster) and thus needs to be analyzed using special tools. Alternatively, data that is so big that doing analysis on it takes too long using standard tools (e.g.¬†R on a regular computer) and instead requires special treatment. Of course this also depends on the type of analysis, not only the type of data. As computers keep getting faster and tools more flexible and integrated, the label big data is a moving target.\nBinary Variable: A categorical variable with 2 categories, e.g., yes/no, dead/alive, diseased/healthy.\nCategorical Variable: A variable that can take on discrete values. Those categories can be ordered (at which point it is an Ordinal variable - see there) or not. Examples of non-ordered variables are hair color or ethnicity. No ordering is possible. A special and common case of categorical variables are Binary variables.\nCausal Modeling: Analysis of data with the goal to make causal inferences between variables (e.g.¬†X (partially) caused Y).\nClassification: Analysis approaches for Categorical Variables.\nContinuous Variable: A variable that can take on any numeric value. E.g. weight. Note that in practice the values are often discrete, e.g. while age is in principle a continuous variable, it is usually expressed in some units (say years) and individual values are reported in discrete units (e.g.¬†whole years). For analysis purposes, we still generally treat it as if it could have been any value.\nData Mining: Often used interchangeably with Machine Learning. It might sometimes indicate specifically a ‚Äúfishing‚Äù approach of combing through data to look for patterns, without pre-defined hypotheses to be tested. I that sense it is similar to Exploratory Data Analysis or Secondary Data Analysis, though those two are often done using smaller (Statistical) as opposed to larger (Machine Learning) models.\nDeep Learning: Generally applied to a specific class of machine learning models, namely neural nets. The ‚Äúdeep‚Äù part comes from the fact that the neural net models generally have multiple layers stacked on top of it (it has nothing to do with deep as in especially insightful).\nDependent (variable): An alternative name for Outcome.\nDescriptive Analysis: Describing and presenting data in meaningful ways using tables and figures, without trying to perform statistical modeling, i.e.¬†without looking for correlations/patterns.\nExploratory Data Analysis: Looking for patterns in data, without a hypothesis specified before data collection (or at least before looking at the data). Very useful, but any result needs to be tested on independent data.\nExposure (variable): A name for a predictor variable of particular interest. For instance, if we wanted to study if the daily duration of exercise had an impact on BMI. We would consider BMI our outcome, exercise duration our exposure (our main predictor of interest), and any other variable we record (e.g., a person‚Äôs age and gender) as other predictors (sometimes called covariates). The term exposure is common in the biomedical and related disciplines, not so much in other areas.\nFeature (variable): An alternative name for Predictor often used in the machine learning literature.\nIndependent variable: An alternative name for predictor, most often used in the statistical literature.\nInterval scale variable: A numerical (quantitative) variable for which taking differences makes sense. Unlike ratio scale variables, the zero point for interval data may be arbitrary. Addition and subtraction are meaningful for interval data, but in general, taking ratios is less meaningful. For example, temperatures in Celsius are on the interval scale. If the temperature was 70 degrees yesterday and 65 degrees today, it makes perfect sense to say ‚Äúit is 5 degrees cooler than it was yesterday.‚Äù But a day that is 70 degrees is not exactly twice as warm than a day that is 35 degrees, so division does not make sense here. There are also temperatures lower than 0 degrees, so the zero point is arbitrary.\nLabeled data: If we have data for which there is a specific outcome of interest and we know it, it is called labeled data. For instance, if we had a lot of pictures of tissue samples, and someone had gone through them and labeled them as cancerous or not cancerous, it is labeled data. Labeled data (the most common type) is usually analyzed using Supervised Learning/Analysis approaches.\nMachine Learning: An approach to data analysis that tends to use more complex models. The goal is mainly to obtain a model that is good at prediction. Understanding how different inputs lead to different outcomes is of secondary importance. Data is often abundant, so more complex models can be used. Often this term and Statistics/Statistical Learning are used interchangeably (though some people try to distinguish them, see e.g.¬†Frank Harrell‚Äôs blog post).\nMechanistic Modeling: Building and using models that explicitly incorporate mechanisms and processes of the system under study to understand how things interact and lead to specific outcomes. Many models in the hard sciences are of this type. A common way to formulate such models is with differential equations.\nNominal variable: A qualitative measurement that can take on distinct categories, but there is no natural order to these categories. For example, apples can be red, yellow, or green, but there is no inherent way to order these colors, even if the data is useful.\nObservation: An observation is a recording of the different variables for a single unit of analysis. Usually an individual, e.g., a single human or animal, but it could also be a picture or video, a county, a city, or whatever our level of observation is. For each observation, values for the different variables should be available. In R, it is most common that each observation is stored as a row in a data frame.\nOrdinal variable: A variable that can take on distinct categories which can be ordered, but the difference between levels might not allow for mathematical operations. For instance, if a question asks a person to rank their level of a pain on a scale from 1-10, a 7 is clearly higher than a 6, and a 6 higher than a 5. But it‚Äôs unclear if the difference between 5 and 6 is the same as 6 and 7.\nOutcome (variable): The variable of main interest for our analysis. This can be a single outcome (most common) or multiple. For instance, the main outcome might be if an individual survives or dies. Or it could be their BMI, or it could be if a given picture contains a cat or not. Also called response (variable) or dependent (variable).\nPredictive Modeling: Using models with the main goal of predicting future outcomes, given a collection of predictors.\nPredictor (variable): All variables that are not the outcome, which we use to see if we can predict the outcome. For instance, if we wanted to predict the price of houses, we could use the square footage of each house and the school district as the predictors.\nRatio scale variable: Ratio data are a type of quantitative data that are continuous, have consistent differences, and have a true zero. Addition/subtraction and multiplication/division are all meaningful for ratio data. For example, distance is ratio scale because the difference between two distances is meaningful, and a distance of 0 is truly the lowest possible distance value. Ratios are meaningful for distance: a distance of 4 miles is exactly twice as long as a distance of 2 miles.\nRegression: A type of supervised learning/modeling, where the outcome of interest is quantitative (or can be treated as such).\nResponse (variable): See Outcome (variable).\nSecondary Data Analysis: Analysis of a dataset that was not specifically collected for the purpose of answering the question one wants to answer. With an increasing abundance and routine collection of data, such secondary analyses are becoming very common. If a clear hypothesis is formulated before one looks at the data, one might consider any results confirmatory, otherwise it is an Exploratory Analysis. Even just looking at the data a little bit during the cleaning process should move such a secondary analysis into the exploratory category.\nStatistics: The basic/classical machinery for data analysis. Depending on the type of data, many different approaches have been developed (parametric vs.¬†non-parametric methods, longitudinal analysis, time-series analysis, and many more). Models tend to be simple and interpretable, the goal is to understand how inputs (predictors) relate to outcomes. Statistics was developed when data was sparse, computers didn‚Äôt exist, and mainly scientists interested in a deep understanding of their data used it. Because of this, statistical models tend to be simple and work well on small datasets.\nStatistical Learning: A term that seems to become more widely used in recent years. While some people distinguish this term from Statistics and consider it a sub-field (see e.g.¬†Chapter 1 of Introduction to Statistical Learning), the two terms are often used interchangeably.\nSupervised Learning/Analysis: Fitting Labeled data. The two types of supervised learning are Regression and Classification.\nUnlabeled data: If we have data for which there is either no specific outcome variable of interest or we do not know it, it is called unlabeled data. For instance, if we had a lot of pictures of tissue samples, and we knew that some showed cancerous tissue and others not, but we didn‚Äôt know which are which, it is unlabeled data. Similarly, if we had pictures of different tissue samples (or say a number of gene sequences) and all we wanted to know is if some samples are more related to each other than others, but there is no main outcome, it is considered unlabeled data. Unlabeled data is usually analyzed using unsupervised analysis approaches.\nVariable: Any quantity that we record like height, weight, income, or species type. In R, it is most common, that each variable is stored as a column in a data frame. The column name should be the name of the variable."
  }
]