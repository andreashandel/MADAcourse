# ml 1 assignment notes



* Add that for reproducibility, rngseed needs to be set in the engine
arguments for the random forest.


* RF hyperparameters: since we're fitting to the same data we're evaluating
on, it's likely that we got these hyperparameters cause they allow more
overfitting to happen. as mtry increases, "better" splits are allowed to happen
in all trees, reducing the variance across trees, and as min_n goes down, each
tree is allowed to segment the data more each time. So both of these choices are
helping to induce overfitting in the model.

Once we tune it, a higher min_n is better, because this parameter reduces the
amount of bias each tree is allowed to have and essentially prevents each of the
trees in the RF from growing too complicated and overfitting to the training
data. However, the mtry is still best when it is high, likely because we don't
have many highly correlated features -- the primary purpose of mtry is to

* I get that the final best model is the plain linear regression. This
indicates that variable selection/shrinkage isn't helping, probably because we
are in the small n, small p case where shrinkage/selection is not really all
that useful. The random forest is also likely still overfitting a lot because
we have so few data points, so the simple model works best here.
