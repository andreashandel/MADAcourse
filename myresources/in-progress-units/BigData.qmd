---
title: Big Data 
subtitle: ""
---

# Overview
In this unit, we will **very** briefly talk about Big Data, and how R can be used for them. 


# Learning Objectives
* Know what Big Data are.
* Know how one can use R for such tasks.



# Big Data

You often hear the term *big data* when people talk about DL or AI. The reason for that is that complex models, such as DL models, need a lot of data to work well. Thus, DL models and big data often -- but not always -- go together. That said, you can analyze big data with any model you want, including simple GLM or similar such models.

While the term *big data* is a bit fuzzy, in general people mean any dataset that doesn't easily fit into the memory of a regular computer (or cluster) and thus needs to be analyzed using special tools. Alternatively, data that is so big that doing analysis on it takes too long using standard tools (e.g. R on a regular computer) and instead requires special treatment. Of course this also depends on the type of analysis, not only the type of data. As computers keep getting faster and tools more flexible and integrated, the label **big data** is a moving target.

Generally, big data is stored somewhere in a database. [SQL type databases](https://en.wikipedia.org/wiki/SQL) are most common. You then want to access that database in a form that allows you to perform your analysis. There are different ways of dealing with big data. Most methods are general and apply independent of the programming language you use. [Alex Gold's article](https://rviews.rstudio.com/2019/07/17/3-big-data-strategies-for-r/) describes a few general approaches and explains how they can be implemented in R. Watch the [webinar](https://rstudio.com/resources/webinars/working-with-big-data-in-r/) for a bit more information and a nice description of the overall setup for big data. As you learn in that tutorial, R is often used together with other software to analyze big data. A tool that is often used for big data analysis is [Spark](http://spark.apache.org/). For R, there is the [`sparklyr` package](https://spark.rstudio.com/), which allows one to nicely interface with Spark.
 
In general, when you work with big data, you will have to carefully look at the data, the type of database it is stored, and the analysis goals. Based on that, you should use a stack of tools that allows analysis. The [*Databases* task view](https://cran.r-project.org/web/views/Databases.html) gives a good overview of different R packages for specific types of databases. You will use R for your analysis, and R will then interface with other software. This interface is usually fairly seamless. 

# Computing resources

As you do DL/AI or use big data, you might exceed what your desktop can do in a reasonable amount of time. The tips described above (e.g. using a GPU for DL or sampling to reduce data size) often help. But at times, you need more powerful computing. Fortunately, this is increasingly available for cheap. You can often access academic computing resources, such as UGA's [Georgia Advanced Computing Resources Center (GACRC)](https://gacrc.uga.edu/) and use them for free. Commercial services also exist, such as [Amazon Web Services (AWS)](https://aws.amazon.com/) which allow you to tap into a vast amount of computing power. Of course, for AWS and similar services, you need to pay. At times, you might get a discount as a student/academic. You can find good information on such services online, for instance on the GACRC website.


# Further information


