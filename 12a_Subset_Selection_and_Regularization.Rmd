---
title: MADA Course - Subset Selection and Regularization 
subtitle: ""
author: Andreas Handel
institute: "University of Georgia"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc_depth: 3
---

```{r, include=FALSE}
#just so I can use emoticons
#devtools::install_github("hadley/emo")
library(emo)
#set some figure options
knitr::opts_chunk$set(out.width = '70%')
```



# Overview
In this module, we will cover the related ideas of subset selection and regularization. We'll also briefly mention a few related approaches.


# Learning Objectives
* Know what subset selection is and when to use it
* Know what regularization is and when to use it
* Be able to implement these approaches in R


# Introduction

In our previous discussion of model performance, you were introduced to the problem of **overfitting.** Here is a quick repeat. The idea is that you generally want to make inferences or predictions **not for the data you used to fit your model, but for new/different data.** 

If the goal was to fit the data you have as well as possible, you could always achieve that with a complex model. If you have N observations, a polynomial model with N-1 degrees of freedom can fit your data perfectly. However, as discussed when we talked about the **bias-variance trade-off**, a model that gets too close to the data used for fitting will generally not perform well when applied to new data.

You want a model that gets the trade-off between high bias (model not flexible enough to capture all the patterns seen in the data) and high variance (model too flexible, will capture noise as well) right.

I showed the figure below previously. I'm repeating it here for convenience. The yellow model, in this example, has too much bias. The model is too simple to capture all the important patterns, and it **underfits** the data. The green model has too much variance. It goes too close to the data used for fitting and doesn't generalize to new data, it **overfits** the data. In this example, the blue model has the optimal model complexity.

```{r trainfit,  fig.show='hold', fig.cap='Bias-variance tradeoff. Source: ISLR.', echo=FALSE, fig.align='center'}
knitr::include_graphics("./media/islr-fig29.jpg")
```

We talked about cross-validation and how it can be used to help you determine if you over-fit (or underfit). What we haven't discussed is how to find a model that gets the trade-off between bias and variance right. Two common techniques are subset selection and regularization.

The general idea for these strategies is that if you let the model use all the predictors you have (this is called a _full_ or _saturated_ model), it might be too big and overfit the data. You can get some hint of that if the cross-validated performance measure for a full model is not better than that for a null model or a single-predictor model. The latter ones are likely too simple and underfit (unless none or a single predictor describes your data), while the full model is likely too flexible and overfits. So you want to find a model that only uses the important/useful predictors.



# Subset selection

You might have already seen subset selection in another statistics class. In this case, this is a refresher. Though you might not have used the approaches I describe here to perform subset selection. Subset selection is also called _variable selection_ or _feature selection_. The overall idea is that you try every model with any combination of predictors, evaluate its performance (using cross-validation to get an honest estimate of model performance on new data), and pick the reduced/sub-model with the best performance. The performance here is whatever metric you decided to optimize on (e.g., SSR, RMSE, Accuracy, F1 score, etc.).

The approach of trying models with all combinations of predictors is called _exhaustive subset selection_ or _best subset selection_. If you have only a few predictors, you can try every combination of predictors. However, once you have more than 5-10 predictors (depending on the number of observations), trying every combination of sub-models would be too time-consuming. Therefore, most of the time, such an exhaustive feature selection is not feasible, and most of the time, one of the approaches described next is used.

## Forward selection
In forward selection, you start with the null-model, then evaluate all 1-predictor models. The best-performing 1-predictor model is chosen, and you build all 2-predictor models on top of that chosen 1-predictor model. You choose the best 2-preditor model, then go on to add a 3rd, and do that until adding any further predictors does not lead to a model that performs better (**again, measured by CV on the test set**) than the smaller model. E.g., you might find that adding any of the remaining 3rd predictors does not lead to a better-performing model than the 2-predictor model you already have. Then you stop here and pick the 2-predictor model as your best.

Let's consider an example. Say you want to predict `BMI` based on `age`, `sex`, `calorie intake`, and `exercise level`. Since BMI is continuous, we can consider a linear regression model, and we might try to minimize RMSE between model-predicted BMI and actual measurements. We start by computing RMSE for the null model. Then we compute RMSE for all 1-predictor models. Assume that all 1-predictor models have RMSE lower than the null model, and the lowest RMSE (all computed using cross-validation) of the 1-predictor models is the one that includes `calorie intake`. Next, you try all 2-predictor models that include `calorie intake`. Among those, a model with `calorie intake + exercise level` has the lowest RMSE, and it's also lower than the model with `calorie intake` only. Next, you try all 3-predictor models that include the 2 chosen predictors. In this example, only 2 of those 3-predictor models are possible, namely `calorie intake + exercise level + age` and `calorie intake + exercise level + sex`. Let's assume both of those models have RMSE that is larger than the 2 predictor model. Note, it is important to compute RMSE through cross-validation since the RMSE evaluated on the data used for fitting will always be lower for the bigger model. We thus found our best model, namely one that includes the 2 predictors `calorie intake + exercise level`.


## Backward selection
Backward selection is essentially the reverse to forward selection: You start with the model with all predictors, then evaluate all models with one predictor dropped. The smaller model with the best performance is your choice. You then drop each predictor at a time from that model. You continue until dropping predictors does not improve performance anymore. Then you keep the model with the best performance. Note that this method does not work on datasets that have more predictors than observations (e.g., many omics type data).

Let's consider the above example again. We now start with a model that includes all 4 predictors and compute cross-validated RMSE. Next, we drop each predictor and try all 3-predictor models. Let's say the one that dropped `exercise level` is the 3-predictor model with the lowest RMSE, lower than all other 3-predictor models, and lower than the full model. Next, we drop each variable from the current best candidate 3-predictor model, which is `age + sex + calorie intake`. All of the 2-predictor models have RMSE larger than the 3-predictor model. We thus found our final model, with the 3 predictors `age + sex + calorie intake`.

## Other selection methods

You might have noticed that in my example, forward selection and backward selection do not necessarily produce the same model. That is true in reality as well, even when applied to the same data, the two different approaches might lead to different final models. Also, as in my example, often (but not always), the final model determined through backward selection tends to include more predictors than a model determined through forward selection. In such a case, you have to decide which model to choose. Or, better yet, use one of the methods described next.

A problem with both forward- and backward selection is that they do not try all models. They use what is called a _greedy_ strategy. Once they included or excluded a predictor, it can never be dropped or included. Once you picked your 1-predictor model, all subsequent models will consist of that 1 predictor model - even if there might be a better performing 2-predictor model that doesn't include the 1st predictor you picked. The same holds for dropping predictors in backward selection. 

There have recently been a number of approaches that try to be somewhat smarter in finding the best model, without having to try every combination. The `caret` package has several methods, such as Genetic Algorithms and Simulated Annealing. Those methods often do a good job finding a very good sub-model in a reasonable amount of time. While the only guarantee that you found the single best model is to try them all using exhaustive subset selection, these modern approaches usually find a model that is either the best or at least close to the best while doing so rather efficiently. See [sections 18-22 of the caret manual](https://topepo.github.io/caret/) for more information. You don't need to read them now, but you'll likely need to refer to them when we practice feature selection. (The `mlr` package has similar selection methods.) 


## Further comments

Most commonly, subset selection is used by starting with a full model that includes each predictor/feature in linear form only. However, there is no reason why one couldn't build a full model that includes every predictor in more complicated forms. For instance, one could start with a model that includes each term linear and quadratic, e.g.

$$Y=b_0 + b_1X_1 + b_2X_1^2 + b_3 X_2 + b_4 X_2^2 + ...$$ 

One could then perform subset selection by removing each term (though if one removes the linear form, one also needs to remove higher orders). It is rarely done, and I don't think I've seen an example of this in the literature. If interactions or higher-order terms are suspected, one often uses a different model that performs some form of feature selection automatically. We'll visit some of those models soon.

You might have learned other approaches to compare model performance, e.g., using AIC or similar metrics or F-tests. As previously mentioned, I consider cross-validation the most robust and reliable method and, thus, do not discuss the others. AIC or similar might sometimes be necessary if the cross-validation approach takes too long to run, e.g., if you have a lot of data or a complicated model. I don't see a use for any of the other methods. (But I'm happy to change my mind if someone gives me some convincing reasons `r emo::ji('smiley')`.)

Sometimes, you have predictors of particular interest, e.g., your main exposure. What if the subset selection algorithm removes those variables? You have to decide. Most routines allow you to specify some variables that must be kept in the model so that you can perform subset selection on the others. It is also worth thinking about what the model is trying to tell you. If it throws out the variable you are interested in, it means it might not be as important in influencing the outcome as you thought. This is a situation that requires careful thinking and judgement and then based on your best judgment, you proceed.

For more details on subset selection, [see section 6.1. of ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) and [sections 18-22 of the caret manual](https://topepo.github.io/caret/). Those readings are optional.


# Regularization

The standard subset selection method is an "all or none" approach. Either the predictor is included, or it is not. Another approach to making models less "wiggly" and more "regular", i.e., reduce variance and thus prevent overfitting, is through an approach called _regularization_. In this approach, predictor variables and their coefficients are penalized in the cost function. It is easiest to explain with a specific example, so let's consider a linear model. Note, however, that the regularization concept and approach is general and applies to many models beyond linear ones.

Our model is given by 

$$Y=b_0 + b_1 X_1 + b_2 X_2 + ....$$. 

We might decide to minimize the SSR, i.e., we are minimizing a cost function 

$$C = SSR=\sum_i (Y_m^i - Y_d^i)^2$$. 

Now, if we use regularization, we are going to instead minimize

$$C = SSR + f(b_j) $$
where the function _f_ is some function of the model parameters. There are 3 main ways to choose that function _f_, described next.

## Ridge regression

One way to choose the function that penalizes the predictors is to weigh each predictor by the predictor's coefficient squared. Choosing the penalty term as the square of the coefficient is called _L2 regularization_ or _ridge regression_. This leads to the cost function:

$$C = SSR + \lambda \sum_j^p b_j^2 $$

The parameter $\lambda$ decides the balance between the goodness of fit (low SSR) and the penalty for having large coefficients. Instead of trying different subsets as above and picking the best based on lowest CV performance, we now try different values of $\lambda$ and pick the model with the lowest (cross-validated) value for _C_. The parameter $\lambda$ is often referred to as the _tuning parameter_. 

## LASSO
An alternative is to penalize the coefficients by their absolute value, namely using this cost function:

$$C = SSR + \lambda \sum_j^p |b_j| $$

This method is called _L1 regularization_ or _Least Absolute Shrinkage and Selection Operator (LASSO)._ One nice feature of LASSO (which ridge regression does not have) is that coefficients may go to 0. That means the predictor has been dropped from the model, similar to the subset selection above. One can think of the LASSO as an efficient approach for performing subset selection. It is not quite equivalent though, since, in the LASSO, the predictors that remain might have been shrunk in their impact due to the regularization penalty.

## Elastic net

One can also combine ridge regression and LASSO into an approach called _elastic net_, which has a cost function that is the combination of the previous two, namely:

$$ C = SSR + \lambda ( (1-\alpha) \sum_j^p b_j^2 + \alpha \sum_j^p |b_j|)$$

Now one needs to try different values for $\lambda$ and $\alpha$ to determine the model with the best (cross-validated) performance. $\lambda$ determines the overall weight given to the penalty factor, while $\alpha$ determines how the penalty should be distributed between the 2 alternative terms.


## Further comments

You might have noticed that the cost function in regularization looks a bit like the equations for AIC or similar quantities. That is no accident. Both try to penalize the model for being overly complicated and thus have equations that contain terms for both model performance and model complexity while trying to find the model with the best balance.

If LASSO has the nice feature of potentially removing variables and thus making the model simpler, why ever use ridge regression or the elastic net? It turns out that for some problems, those other methods perform better. [See section 6.2. of ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/) for more.

There is a lot of math behind the regularization concept. From an applied perspective, the focus is to understand the overall idea and how to implement them. For more on regularization [see section 6.2. of ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/), [chapter 6 of HMLR](https://bradleyboehmke.github.io/HOML/regularized-regression.html) and [section 34.9 of IDS](https://rafalab.github.io/dsbook/). I won't quiz you on those readings, but I strongly encourage you to check out the ISLR and HMLR readings and skim through them to get a better understanding of these widespread and powerful techniques. 


# Other methods

Subset selection and regularization are not the only methods that one can use to try and find the best performing model. Other methods that try to find the optimal set of predictors, often labeled _feature reduction methods_, can be used. Some of those are _Partial Least Squares_, _Principal Component Analysis (PCA)_, _Generalized Low Rank Models (GLRM)_, _Autoencoders_. 
Those methods are for instance discussed in [chapters 17-19 of HMLR](https://bradleyboehmke.github.io/HOML/) and [section 6.3. of ISLR](http://faculty.marshall.usc.edu/gareth-james/ISL/). We won't cover those, but if you are interested, take a look.


# Further comments

In addition to having potentially better performance, smaller models are also easier to interpret and potentially use. Having smaller and simpler models can sometimes be of little importance and other times of great importance.

For instance, if you want to build a model that allows doctors to predict the chance that a patient has a certain disease, you might want to have a model that only uses the fewest (or easiest/cheapest to measure) variables to obtain good performance. So if you collect a lot of data, some based on checking patient symptoms and some on bloodwork, you might not want to use all those variables in your model. In fact, **you might not even want the best performing model**. Let's say that you had data on 10 predictors, 5 for symptom variables (e.g., body temperature and similar), and 5 variables that each come from a different lab test. You'll perform subset selection (or LASSO) and find that the best performing model retains 3 symptom variables and 2 lab tests. Let's say its performance is 95% (I'm purposefully fuzzy about what that performance exactly is since it doesn't matter. It could be accuracy, or F1 score, or AUC, or...). But you also find that another model that contains 4 symptom variables and no lab tests has 85% performance. Which do you choose? That comes back to our previous discussion about assessing model quality: Performance is one and is an important measure, but it's not the only one. In this case, since you could get data on the 4 symptoms very quickly and cheaply, you might want to recommend that model for most doctors offices, and only use the better, but more time-consuming and expensive model with the 2 lab tests in settings such as high-risk populations in the hospital.

In contrast, if you are a bank that tries to predict fraud by having complicated models that constantly analyze various data streams, you might not care how complicated and big your model is, only that the performance in flagging fraudulent transactions is as good as possible.