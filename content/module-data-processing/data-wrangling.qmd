---
title: Wrangling Data  
subtitle: ""
authors: 
  - name: Andreas Handel
  - name: Megan Beaudry
    role: "Contributor"
---

# Overview

In this unit, we will discuss what data wrangling is and why it's a very important part of any analysis.

![How data used to be cleaned before the tidyverse.](../../media/cartoonscrubdata.jpg){width="80%" fig-alt="A cartoon showing two men with cleaning supplies standing in front of a woman, who is sitting at a desk. The caption on the cartoon reads \"We're ready to scrube the data. #betterdata\"."}

# Learning Objectives

-   Learn what data wrangling is
-   Know about specific important wrangling tasks

# Data wrangling overview

Once you have the data loaded into your analysis software, such as R, you almost always need to process it to get it into a format that suitable for statistical analysis. The terms often applied to this messing around with the data is called *wrangling* or *tidying*. *Cleaning*, *munging*, or *processing* are other terms that generally mean the same thing. To figure out how to get your data into the right shape and what cleaning steps you need to take, you also need to explore the data. R provides a lot of functionality for data wrangling and exploring. We focus on a specific approach which makes use of R packages collectively known as the `tidyverse`, which we'll cover next. We will then use them to practice some data wrangling.

You have already done some data wrangling as part of previous exercises, here we talk about it in some more detail.

**As with any part of the data analysis process, wrangling should be automated, reproducible, and well-documented. No "fixing by hand"!**

# Specific wrangling tasks

There are some types of problems that you encounter often. I briefly discuss a few important ones.

## Missing data


## Outliers

It is not uncommon to have values in your data that are *strange*. It could be that someone at data entry made a mistake. Or your codebook doesn't properly explain the entries. Other times, it could be *real* data that is just an outlier. It can be hard to decide if a value is *strange but real* or a mistake. Dealing with such entries is a judgment call. The best approach is to have a pre-written analysis plan that explains exactly what to do in any such cases. Outside of clinical trials seeking FDA approval, that pretty much never happens. We usually have to decide what to do with strange entries when we run into them. Some good rules are to be consistent, fully document and explain what you do, and if you are able, do it *both ways* (e.g. do the analysis with the values as they are, and then again with them removed). Of course *both ways* can quickly turn into *a million different ways* and at some point, you probably have to stop. However, trying it more than one way can be reassuring if you get pretty much the same answer each time. If you do not get similar results, then you have to be more careful and should describe in your report/paper in detail why and how different approaches to your data cleaning lead to different results.

## Censored data

Sometimes data are censored, i.e., you know that the value is "X or greater/less" but not the exact value. This is common in longitudinal (survival or similar) data. It is also common in experimental data, where you might have a measurement procedure with lower or upper limits of detection. The way to deal with censored data depends on what kind of censored data you have, and what you want to do with it. In R, packages that are used to analyse survival data, e.g. the [`survival` package](https://github.com/therneau/survival), can deal with some censored data. Otherwise, you will likely need to google around to figure out how you should deal with the censored data scenario you have and how to do it in R.

## Combining categories

At times, you might have categorical variables with many categories, and a lot of them have only a few observations. This can make the modeling process problematic. In that case, you might want to consider combining certain categories into larger ones. For instance, if you have a variable for jobs which has many different values, it might make sense to group the jobs into categories (e.g., manual labor, clerical,..). You need to report what you did so readers can decide if this is a reasonable approach. Sometimes you might also want to group all minor categories into an "other" category. For instance if you have a dataset of nicotine users, your main categories might be cigarettes, cigars, chewing, vaping and everything else (whatever that might be, I don't know) could be "other". Note that maybe the way I'm grouping things here is really dumb. That shows that I'm not an expert on smoking. You can let the data decide the grouping by looking at numbers in each category, but there is no substitute for some level of expert topical knowledge.

For tasks like combining categories, and other operations that have to do with categorical (factor) variables, the [`forcats` R package](https://forcats.tidyverse.org/) is a great tool.

## Merging data

It is not uncommon that your raw data comes in more than one file. It could be multiple spreadsheets of a single study, or it could be different datasets from different sources (e.g., a dataset on asthma cases combined with a different data source that records air pollution levels). In such cases, you will likely need to combine the data. That is often called *joining*. The `dplyr` package has a great set of `_join()` functions that let you do different types of joining of data. Other tools for R exists, e.g. the [`data.table` package](http://r-datatable.com).

# Data cleaning and exploratory analysis

As you do your data cleaning/wrangling, you need to explore your data to figure out what needs fixing. As such, certain aspects of exploratory analysis and cleaning go together. You need to explore your data to figure out how to clean it, and you need to clean it to allow further exploration. So remember that even though in a course like this we have to cover one topic at a time, in reality, a lot of these activities occur at the same time.

We will cover exploratory analysis in a separate, later module, and we will consider exploratory *statistical* analysis as part of the exploration process. There is no clear definition for exploratory analysis. Some people consider exploratory analysis only the parts up to and including a *descriptive* analysis. I consider very simple bi-variate statistical approaches also part of the exploration. To me, the exploratory part is anything that comes before you apply your full (usually multivariable) models to the data with the goal of producing "final" results. Just don't get too hung up on the terminology. In practice, there is no clear division, e.g., people often plot exploratory figures and add a simple linear regression or another simple statistical model to the figure as part of the exploration process.

# Further reading

The whole [Wrangle section of *R4DS*](http://r4ds.had.co.nz/index.html) shows a lot of the topics discussed here in action. For a more interactive (but less comprehensive) source of information, check out the *Work with Data* and *Tidy your Data* sections of the [Posit Recipes](https://posit.cloud/learn/recipes).

[Finney 2012](https://www.tandfonline.com/doi/pdf/10.1198/000313006X150182) provides a nice further discussion regarding outliers. It's very non-technical (no equations) and easy to read.
