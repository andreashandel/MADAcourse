---
title: Assessment - Machine Learning Models 1
subtitle: ""
---

# Quiz

Get the quiz sheet for this module from the general [**Assessments**](../../courseinfo/assessments-overview.qmd) page. Fill it in, then submit to the online grading system before the deadline.


# Discussion

We'll use this week's discussion to talk a bit more about the projects. Last week you received feedback on our project from a few classmates, this week I want everyone to have a chance to hear about and comment on all projects.

To that end, post a summary of your project to the discussion channel. Briefly describe your data, your question(s), your (planned) methods and (expected) results. Sort of like an abstract for a paper, though maybe with a bit more details. This way, everyone can get a quick glimpse as to what everyone else is doing. Also provide a link to the repo for easy access (for private repositories, you can skip this).

Also mention specific questions/struggles/concerns you might have regarding your project. It's quite likely that there is some overlap in approaches and questions among you and your classmates. Hopefully, through this discussion you can provide each other with some help and input.

Post by Wednesday. Then read the summaries of your classmates' projects. If you see a question/topic that you think you might be able to help with, either by answering a specific question, or by providing some general feedback, do so. Of course, you are also welcome to take inspiration from what you see others doing and integrate some of that into your own project.

Since you get feedback from me at the other submission points, I will stay out of this week's discussion üòÅ. So this is all you helping each other. Each of you has already seen a few projects and will be reviewing 2 projects in depth at the end, but hopefully with the discussion this week you get a bit of an idea of what everyone else is doing, and you can comment on any project you want.



# Exercise

For this exercise, we will implement some of the machine learning models covered in this module, and some of the general approaches we covered previously. This is a continuation of the exercise you've been working on over the last few weeks. 

**Note: This exercise is likely time-consuming and you might get stuck. Please plan accordingly. Start early and ask for help if needed.**


## Setup

This is a solo-exercise, and it's part of your portfolio.

Since the previous Quarto document was getting long, we'll set this up as a different one. Make a new folder in your portfolio repo and call it `ml-models-exercise`.

If you haven't done yet, update your previous code to save the clean data (with the variable `SEX` included) from the last exercise into an `Rds` file. Copy that `Rds` file into the newly created folder. Also in this new folder, start a new quarto file called `ml-models-exercise.qmd`. 

Open the new quarto file you just created, and we should be ready to go.


## Preliminaries

Write code to load your packages. You already know you'll need `tidymodels` and probably `ggplot2`. You can add other package loading commands later.

Load the data.


## More processing

Just to practice, we'll do a bit more data wrangling. 

Earlier, we dropped the variable `SEX`. Now we decide we want to keep it. Let's go on a 'treasure hunt'. Try to find out what the values that are used to encode race stand for. This is the kind of task one has to do at times for any real-world data analysis. Spend a few minutes to see if you can figure out the meaning of the numbers. 

If you can't find out the meaning, that's ok for the purpose of the exercise. What we'll do is to combine the two rare numbers into a single category, so that we are left with race 1,2, and 3, where the latter is a re-coding of everyone with a 7 and 88. 

::: instruction 
Write code that changes the `RACE` variable such that it combines categories 7 and 88 and calls them 3.
:::


## Pairwise correlations

Next, let's make a pairwise correlation plot for the continuous variables. If we were to find any very strong correlations, we might want to remove those. 

::: instruction 
Write code to make a correlation plot for the continuous variables. 
:::

You'll see that while some variables are reasonably strongly correlated, nothing seems excessive (e.g., above an absolute value 0.9). So looks like we won't have too much of a problem with collinearity.


<!-- ## Near-zero variance -->

<!-- As we discussed, sometimes there might be a variable that contains little information, e.g. if pretty much everyone belongs to a single category or has very similar numbers. For instance if we had a data set of individuals, and every individual had the same value for SEX or RACE, it wouldn't contain any useful information to predict the outcome (though it could be quite important for interpreting the results from the study!) In such a case, it is generally best to remove that variable before modeling. Similarly, if you had a data set where everyone had an age between 39 and 41 years, it's likely not informative. -->

<!-- The EDA of the data, where you plot histograms and boxplots and similar figures should have already alerted you to such potential problems. We can also test that more formally. -->

<!-- ::: instruction -->
<!-- Write code that looks at near-zero variance in the predictors. -->
<!-- ::: -->

<!-- It looks like no problems here either, so we can keep the variables. -->


### Feature engineering

As you learned, _feature engineering_ is a fancy word for creating new variables (or more broadly, doing stuff to existing variables, e.g., one could interpret our recoding of `RACE` above as feature engineering).

Let's become a feature engineer üòÅ. Our data contains height and weight for each individual (`HT` and `WT`). There's usually some correlation between those -- which you likely noticed above when you did the pairwise correlation. It's not too strong here, but maybe it would still be good to combine these variables into a new one. Let's do that by computing BMI. 

::: instruction
Look up the formula for BMI. Then add a new variable to the data frame called `BMI`, computed from the `HT` and `WT` variables. Be careful with the units! They are not given, so you have to guess.  
:::



## Model building

We are finally done with our additional processing bits and are ready to fit some models. Specifically, we will explore these 3 models: 

* For the first model, we'll revisit the one we had previously, namely a linear model with all predictors.

* For our second model, we'll do a LASSO.

* For our third model, we'll do a random forest.

Of course, we'll also have model "0" again, our null model.


We could do the train/test split, but for this exercise we'll skip that. We'll use cross-validation to try and get an "honest" assessment of the performance of our models. 

<!-- To mix things up, we'll know use a different metric, namely the mean absolute error (MAE), which -- as you learned -- can be better when dealing with outliers. In our dataset, there aren't really any outliers, so for a real problem I would likely stick with RMSE as metric. But this is an exercise, and I want you to see how easy it is to use different metrics (the hard part is making a choice based on scientific/expert reasoning). -->


::: instruction
Create `tidymodels` workflows/recipes and fit the 3 models specified above to the data. The outcome is `Y`, predictors are all other variables. Ignore any kind of tuning for 
:::

::: note
You'll likely have to install some further R packages to do LASSO and random forest, namely the `glmnet` and `ranger`` packages.
:::



-   We want to do 5-fold cross-validation, 5 times repeated. (There's no specific reason to do this 5x5 pattern, other than to show you that there are different ways to pick the sample, and that I want you to not use the default.) For the CV folds, we also want to stratify on `BodyTemp`, as we did for the main train/test split. Use the `vfold_cv()` function to create a resample object for the training data with these specifications.

-   Create a recipe for the data and fitting. You won't need to do much, just make sure you code the categorical variables as dummy variables, otherwise things might not work smoothly. For that, you want to use the `step_dummy()` function and pick all nominal predictor variables (which are actually all predictor variables here, since the only continuous variable is our outcome).

### Null model performance

Write some code to compute the performance of a null model, i.e. a "model" that doesn't use any predictor information. For a continuous outcome and RMSE as our metric, a null model is one that always predicts the mean of the outcome. Compute the RMSE for both training and test data for such a "model". We'll use that later to compare it to the performance of our real models. Of course, we expect/hope our real models that use predictor information to be better. If they aren't that means they are no good.

### Model tuning and fitting

We'll fit a tree, a LASSO model, and a random forest. I chose those because they are used in the tutorial on the `tidymodels` website. You can of course add further models. For the tree, see the *Tune model parameters* section of the [*Get Started* tutorial](https://www.tidymodels.org/start/tuning/). For LASSO and the random forest, check out the *Case Study* section of the [*Get Started* tutorial](https://www.tidymodels.org/start/case-study/). Note that you will need to adjust the code for our scenario since we have a continuous outcome.

If you follow the tutorial, you'll likely use the packages `rpart`, `glmnet` and `ranger` to fit those 3 models. Make sure they are installed and loaded.

I suggest you write code for each model separately. A lot of the code will look similar, so once you got the first one set up, the other two should be easier. They mainly differ in the commands specifying the tuning parameters and the tuning grid.

Each of these models requires some tuning. For the choices regarding the tuning parameters, you can follow the examples. Most of the models have more things that can be tuned, but for now you can stick to what they show in the tutorial. Follow the examples by setting up a workflow, set a tuning grid, and then use the `tune_grid()` function to tune the model using cross-validation.

**Note that the tuning part, i.e., calling `tune_grid()` might take a good bit of time to run (possibly minutes).**

The steps (block of code) you should have here are 1) model specification, 2) workflow definition, 3) tuning grid specification and 4) tuning using cross-validation and the `tune_grid()` function.

### Model evaluation

Once you have done the tuning, you can take a look at some diagnostics by sending your object returned from the `tune_grid()` function to `autoplot()`. For instance if you tuned the tree and saved the result as `tree_tune_res`, you can run `tree_tune_res %>% autoplot()`. Depending on the model, the plot will be different, but in general it shows you what happened during the tuning process.

Next, you want to get the model that the tuning process has determined is the best. You can get the best-fit model with `select_best()` and `finalize_workflow()` and then do one more fit to the **training data** with this final workflow using the `fit()` function. Follow the examples in the tutorial.

To evaluate the final fit for each model, do the following.

1)  Make two plots, one that shows model predictions from the tuned model versus actual outcomes, and one that plots residuals. The actual outcomes you get straight from the data, the predicted outcomes you can get by applying the `predict()` function to the final fit.

2)  Look at/print the model performance and compare it with the null model (still only on training data). Here, we want the performance of the tuned, best-fitting model on the CV dataset (we are not yet touching the test data). You can get that for instance with the `show_best()` function, which gives you the mean cross-validated performance for the best models. It also shows the standard deviation for the performance. Compare that model performance with the null model.

The mean and standard deviation of the performance give you a measure of overall performance and variability in that measure. The plots show you if there are any systematic deviations between model and data. Taken together, these can be compared for the different models and based on those (and as wanted, other considerations) a final model can be chosen.

**Implement the model tuning/fitting and evaluating steps for all 3 models.**

### Comments for specific models

Here are some more suggestions and hints. Most of this is optional but worth trying.

**For the tree model**, if you want to plot the tree, you can use the `rpart.plot` package and run this command `rpart.plot(extract_fit_parsnip(best_tree_fit)$fit)` (assuming your result from the final workflow fit is called `best_tree_fit`). You might get a warning message, but the tree will show. You will likely find when you look at the actual/predicted plot or the residual plot that the tree model does not perform very well, and the model only predicts a few discrete outcome values. That's also noticeable when you compare RMSE for the tree model and the null model, they are very similar.

**For the lasso model**, you will likely find that it performs a bit better than the tree, but not a lot. If you want to see a plot for how the number of predictors included in the LASSO model changes with the tuning parameter, you can extract the model from your final fit (say it's called `best_lasso_fit`) with `x <- best_lasso_fit$fit$fit$fit` and then `plot(x, "lambda")`. I know, this is awful code having to dig that deep into the `best_lasso_fit` object. You might also be able to use the `extract_fit_engine()` function to get the underlying fit object produced by LASSO, e.g., `x <- extract_fit_engine(best_lasso_fit)` might work (this is new in `tidymodels` and I haven't fully tried it yet).

**For the random forest model**, you will likely again find that it performs a bit better than the tree, but not a lot. The tuning setup might require some fiddling around, I had a few initial tries where the whole tuning failed. For a model like random forest, all variables stay in the model. There are ways to look at the variables that are most important. If you want to do that, you again need to pull out the fit object. Say it's called `best_rf_fit`, you can do that with `x <- best_rf_fit$fit$fit$fit` and then use the `vip()` function from the `vip` package to plot the importance of the variables. Alternatively `extract_fit_engine()` should also work.

### Model selection

Once you have implemented above steps for the 3 models, you should have a "best" fit for each one based on the tuning process. For each best model you should have performance, uncertainty around the performance measure, and some diagnostic plots. While for any real research project, you likely want to look deeper (e.g. at uncertainty in predictions instead of just overall performance), for now this is enough. Pick one of the three models. Explain why you pick it. There is no single answer that's correct, I just want you to practice reasoning for why you are doing something: in this case justify why you are picking the model you do.

### Final evaluation

Once you picked your final model, you are allowed to once -- **and only once** -- fit it to the test data and check how well it performs on that data. This gives you a somewhat honest estimate of how the model might perform for new, unseen data. You can do that using the `last_fit()` function applied to the model you end up choosing. For the final model applied to the test set, report performance and the diagnostic plots as above.

And that concludes what is likely a fairly long exercise. The code itself is not that long, but it will take you time to cobble it together from the `tidymodel` tutorial and possibly other sources.

## Test and website update

Make sure your analysis and all results are nicely documented and everything runs/renders correctly. Then, add the newly created Quarto document as an entry into your `_quarto.yml` file, as a sub-menu of `Flu fitting`. Call it `Machine Learning`. Recompile your portfolio website and make sure everything works and shows up as expected. Then commit and push.

**Since this is part of your portfolio site, you don't need to post anything, I know where to find it. Therefore there is no exercise Slack channel for this module.**






<!-- For these two models don't have a tuning parameter, so all we do is a single fit. In this instance, there's no reason to do anything like cross-validation during the "training" part, since we aren't really training/tuning/optimizing our model (other than finding the best-fitting parameters for the coefficients). -->

<!-- The last model we'll test is a penalized regression model with LASSO. The example in the [predictive modeling case study](https://www.tidymodels.org/start/case-study/) will be useful to look at. Set up a workflow that uses a linear model with the `glmnet` engine. We want to do a LASSO model (set the `mixture` parameter accordingly) and we want to tune the penalty parameter. -->

<!-- We want to use a grid search for the tuning parameter and use 5-fold cross-validation to evaluate the model performance during the tuning process. Note that for this you'll need to take snippets of code from the [Evaluate your model with resampling section](https://www.tidymodels.org/start/resampling/) of the tutorial. -->

<!-- You should end up with a workflow that uses a `glmnet` model, and does fitting while tuning the `penalty` parameter of the model with a grid search, optimized on RMSE as the metric. By combining bits of code in the sections mentioned above, you should be able to stich it together. -->

<!-- Once you re done with this part, you should end up with 3 fit objects, one for each model. In the following, I assume they are called `fit1`, `fit2` and `fit3` but you can give them any name. -->


<!-- ## Model assessment -->



<!-- Follow the example in the *Use a trained workflow to predict* section of the tutorial to look at the predictions, ROC curve and ROC-AUC for your data. Apply it to both the training and the test data. ROC curve analysis and ROC-AUC is another common performance measure/metric for categorical outcomes. If you are not familiar with it, you can read more about them by following the link in the tutorial. It's not too important to go into the details for now. The focus here is on getting the code to work. In general, a ROC-AUC of 0.5 means the model is no good, 1 is a perfect model. Generally, somewhere above 0.7 do people think the model might be useful. If your model has a ROC-AUC a lot less than 0.5, you likely have an issue with how your factors are coded or how `tidymodels` is interpreting them. -->

<!-- ## Alternative model -->

<!-- Let's re-do the fitting but now with a model that only fits the main predictor to the categorical outcome. You should notice that the only thing you have to change is to set up a new `recipe`, this time one that only has the name of the predictor of interest on the right side of the formula (instead of the `.` symbol, which is shorthand notation for "all predictors".) Then you can set up a new workflow with the new recipe, rerun the fit and evaluate performance using the same code as above. In general, if you do multiple models/recipes, you might want to write a loop to go over them, or parallelize/vectorize things. For now, just copying and pasting most of the code is ok. -->




