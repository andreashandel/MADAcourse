---
title: "Model diagnostics"
#subtitle: "More info here if needed"
date: 2024-01-25
date-modified: last-modified
---

# Overview

This unit discusses different diagnostics that are helpful to assess models.


# Learning Objectives

* 

# Introduction

# Observed versus predicted values

Once you have a model with good performance, you want to inspect their actual predictions. For continuous outcomes, you can plot observed (data) versus predicted (model) outcomes. For a (hypothetical) perfect model, all points are along the 45-degree line. You don't actually want them all on the line since this suggests overfitting. Some scatter along the line is expected and "healthy". However, you want to look for systematic deviations from this line, as it suggests potential problems, i.e., it likely means your model is *biased* and is not flexible enough to capture important patterns still found in the data (i.e., your model is underfitting). In that case, you will want to try different models. Similarly, for categorical outcomes, you can look at the confusion matrix to see if there are lots of FN or FP, which might suggest the model gets certain categories systematically wrong.

# Residuals

Instead of (or in addition to) plotting observed versus predicted for continuous outcomes, you can plot the difference between the two. These differences are called the residuals. What you are looking for is a cloud of points with no discernible pattern. If there is a pattern (e.g., an overall skew, or more points above the 0 y-axes than below), it again suggests that there is still some pattern/signal in the data that the model didn't capture.



# Algorithm performance

Convergence, frequentist and bayesian





# Summary

# Further Resources





