---
title: "Model diagnostics"
date: 2024-01-25
date-modified: last-modified
---

# Overview

This unit discusses different diagnostics that are helpful to assess models.


# Learning Objectives

* Be familiar with diagnostic approaches that can help assess model quality.


# Introduction

We already discussed several approaches of asssessing models. One was based on a broad comparison with the real world, the other one focused narrowly on model evaluation based on some performance metric. There are several other ways to assess models that are in some sense between the very broad and very narrow approaches we discussed.



# Algorithm assessment

These days, every model is implemented in and fit through some kind of numerical algorithm. It is important to ensure the algorithm worked. While this is not assessing the actual model, if you find that your model fitting algorithm didn't work, you can't use the results and will likely have to modify your model.

For simple models, such as GLM-type models, this is almost never a problem. But as models get more complex, things can go wrong. A common issue is algorithm convergence. What is meant by that is that your fitting routine can't find the value that optimizes your objective function/performance metric. This is often related to overfitting. You are likely trying to fit a model that's too complex given the data you have available. 

It is generally not necessary to understand the details of how the underlying algorithm works. However, if you get error or warning messages, or if there are diagnostic readouts that indicate a problem, then you'll have to first fix those before you can further consider the model. Often, the fix is to simplify the model.




# Observed versus predicted values

The model performance metric gives you a single quantity describing how well the model matches the data, given the metric you defined. That's useful, but you generally want to dig deeper and compare **model predictions** to the **observed data** for individual observations.

For continuous outcomes, you can plot those two quantities on the x- and y-axes. For a (hypothetical) perfect model, all points are along the 45-degree line. You don't actually want them all perfectly on the line since this suggests overfitting. Some scatter along the line is expected and healthy. However, you want to look for systematic deviations from this line, as it suggests potential problems, i.e., it likely means your model is *biased* and is not flexible enough to capture important patterns still found in the data (i.e., your model is underfitting). In that case, you will want to try different models. 

Similarly, for categorical outcomes, you can look at the confusion matrix to see if there are lots of FN or FP, which might suggest the model gets certain categories systematically wrong. 

In the common case that you only have 2 categories (e.g., yes/no) and you use a logistic model or some other model that predicts probabilities, you can plot those model-predicted probabilities together with the observed data (which will be just 0 or 1). You are again looking for any patterns that might indicate that there are systematic deviations between model predictions and data that could suggest that your model needs tweaking.



# Residuals

Instead of (or in addition to) plotting observed versus predicted for continuous outcomes, you can plot the difference between the two. These differences are called the residuals. What you are looking for is a cloud of points with no discernible pattern. If there is a pattern (e.g., an overall skew, or more points above the 0 y-axes than below), it again suggests that there is still some pattern/signal in the data that the model didn't capture.



# Summary

When assessing your models, it is important to go beyond the performance metric and look at individual model predictions and how closely they agree with the data. If you spot residual patterns, that might indicate that you could (but don't have to) make your model more complex/flexible to try and capture additional details of the data.


# Further Resources

None ATM.


# Test yourself

```{r}
#| echo: false
#| results: asis

# Define the questions
q1 <- "On a predicted versus observed plots, the points should cluster along a horizontal line to indicate a good model fit."
q2 <- "If your overall model metric (e.g. Accuracy) is very good, you don't need to look at individual predictions."
q3 <- "What's the term for the differences between data and model that you want to see distributed like a symmetric cloud?"

# Define the answer choices
q3_choices <- c("Overfitting", "Variance", answer= "Residuals",  "Bias")


cat(q1,webexercises::torf(FALSE),"<br>")
cat(q2,webexercises::torf(FALSE),"<br>")
cat(q3,webexercises::longmcq(q3_choices))
```



# Practice

* Revisit any of the papers you found in one of the previous exercises. See if the authors used any of the approaches discussed here to assess their model(s). Often, this kind of information would be in the supplement. At the minimum, you'd want the authors to mention that they did these checks. Unfortunately, you'll often see it missing. It seems that at times, authors/analysts don't want to look too closely, otherwise they would need to acknowledge that their cherished model is actually not that good üòÅ.


