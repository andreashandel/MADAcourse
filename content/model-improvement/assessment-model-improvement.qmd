---
title: Assessment - Improving Models
subtitle: ""
---

# Quiz

Get the quiz sheet for this module from the general [**Assessments**](../../courseinfo/assessments-overview.qmd) page. Fill it in, then submit to the online grading system before the deadline.

# Project

Review and provide feedback on part 3 of the projects you have been assigned to as described in the [ **Projects**](../../project/project-Overview.qmd) section.

# Discussion

Because you'll be doing the project reviews, there is no discussion assignment for this module.


# Exercise

We'll practice the topics covered in this and prior units by continuing with the exercise we started previously.


## Setup

You'll continue working on the exercise you started previously. We'll also do some group work again, using the -- by now familiar -- M1, M2, ... setup. Assign each other a number. As much as possible, do it such that you end up working with group members you have not (or not in a while) worked with. Use the 'circular' setup such that everyone will work on their own repository and on one other person's repository.

At this stage, I assume all the data wrangling and EDA code, as well as the model fitting code you worked on previously is present and fully functional. If there are still some issues that need to be resolved, go ahead and do so.

We'll continue this exercise using the previous Quarto `data-fitting.qmd` file.

::: note
This exercise is heavily inspired by the [Getting Started](https://www.tidymodels.org/start/) tidymodels tutorial. If you haven't yet, now would be a great time to at least skim through the tutorial and make sure you understand the overall steps. We'll draw on all 5 sections of the tutorial for this exercise.  
:::


## Data splitting

Write code that takes the data and splits it randomly into a 75% train and 25% test set, following for instance the example in the [Data Splitting section of the Get Started `tidymodels` tutorial](https://www.tidymodels.org/start/recipes/).

::: note
We only have 120 observations here. This is not much, so for a real project, it might not make a lot of sense to do a train/test split. Instead, one would probably use all data during the model fitting process, and then use cross-validation to try and estimate model performance on unseen data. But since this is an exercise, we'll do the train/test split.
:::

::: note
Also note that we are doing a random train/test split. But for a case like we have here, one might want to split differently. For instance we could split by using everyone who got the lower doses as training data, and then testing if we can predict the higher-dose outcomes. This is a common question of interest in drug development.
:::


## Model fitting



## Workflow creation and model fitting

Next, following the example in the ["Fit a model with a recipe" section of the Get Started `tidymodels` tutorial](https://www.tidymodels.org/start/recipes/), create a simple workflow and recipe that fits a linear model to our **continuous outcome of interest**, (here, 'Y`) using all predictors. For the fitting, you should only use the training data set.

Then update the workflow by changing the model to one where the only predictor is `DOSE` and fit again.

We'll start with 3 models that are being fit to the **continuous outcome of interest**, (here, 'Y`). The first model is really a null-model. Specifically, we just predict the mean outcome for each value. 




For these two models don't have a tuning parameter, so all we do is a single fit. In this instance, there's no reason to do anything like cross-validation during the "training" part, since we aren't really training/tuning/optimizing our model (other than finding the best-fitting parameters for the coefficients).

The last model we'll test is a penalized regression model with LASSO. The example in the [predictive modeling case study](https://www.tidymodels.org/start/case-study/) will be useful to look at. Set up a workflow that uses a linear model with the `glmnet` engine. We want to do a LASSO model (set the `mixture` parameter accordingly) and we want to tune the penalty parameter.

We want to use a grid search for the tuning parameter and use 5-fold cross-validation to evaluate the model performance during the tuning process. Note that for this you'll need to take snippets of code from the [Evaluate your model with resampling section](https://www.tidymodels.org/start/resampling/) of the tutorial.

You should end up with a workflow that uses a `glmnet` model, and does fitting while tuning the `penalty` parameter of the model with a grid search, optimized on RMSE as the metric. By combining bits of code in the sections mentioned above, you should be able to stich it together.

Once you re done with this part, you should end up with 3 fit objects, one for each model. In the following, I assume they are called `fit1`, `fit2` and `fit3` but you can give them any name.


## Model assessment



Follow the example in the *Use a trained workflow to predict* section of the tutorial to look at the predictions, ROC curve and ROC-AUC for your data. Apply it to both the training and the test data. ROC curve analysis and ROC-AUC is another common performance measure/metric for categorical outcomes. If you are not familiar with it, you can read more about them by following the link in the tutorial. It's not too important to go into the details for now. The focus here is on getting the code to work. In general, a ROC-AUC of 0.5 means the model is no good, 1 is a perfect model. Generally, somewhere above 0.7 do people think the model might be useful. If your model has a ROC-AUC a lot less than 0.5, you likely have an issue with how your factors are coded or how `tidymodels` is interpreting them.

## Alternative model

Let's re-do the fitting but now with a model that only fits the main predictor to the categorical outcome. You should notice that the only thing you have to change is to set up a new `recipe`, this time one that only has the name of the predictor of interest on the right side of the formula (instead of the `.` symbol, which is shorthand notation for "all predictors".) Then you can set up a new workflow with the new recipe, rerun the fit and evaluate performance using the same code as above. In general, if you do multiple models/recipes, you might want to write a loop to go over them, or parallelize/vectorize things. For now, just copying and pasting most of the code is ok.

## Wrap up part 1

Make sure everything runs and works as expected. Also make sure everything is well commented/documented/explained! Then commit, push and tell your classmate that they can take over. **Finish this by Wednesday.**

## Continuous outcome

Fork and clone (or if you are added as collaborator, clone directly) your classmate's repository. Open their `modeleval.qmd` file. **Add a heading at the bottom that says `# This section added by YOURFULLNAME`. I need this so I can grade accordingly.**

Add code that repeats the above steps, but now fits linear models to the continuous outcome. One model with all predictors, one model with just the main predictor. For that, you need to change your metric. RMSE is a good one to choose. You should find that a lot of the code your classmate wrote can copied/pasted/re-used with only minimal modifications. Make sure to add comments/documentation.

## Wrap up part 2

Make sure everything runs and works as expected. Then commit, push and if you forked the repo, initiate a pull request. Tell our classmate that its done.

## Test and website update

The original repository owner should make sure everything works. Then, add the newly created Quarto document as an entry into your `_quarto.yml` file, as a sub-menu of `Flu fitting`. Call it `Model Evaluation`. Recompile your portfolio website and make sure everything works and shows up as expected. Then commit and push.

**Since this is part of your portfolio site, you don't need to post anything, I know where to find it. Therefore there is no exercise Slack channel for this module.**

## Looking ahead

We also covered overfitting and strategies to minimize it (e.g., cross-validation), and further model assessment strategies in the materials. We'll practice those in future exercises. I figured the above will already take a good bit of getting used to, and we will be able to practice more when we look at further model types in the coming weeks.

