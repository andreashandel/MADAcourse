---
title: Assessment - Evaluating models
author: Andreas Handel
date: "`r file.mtime(knitr::current_input())`"
output: 
  html_document:
    toc_depth: 3
---


```{r, include = FALSE}
library(emoji)
```



# Quiz

Get the quiz sheet for this module from the general [__Assessments__](./Course_Assessments.html) page. Fill it in, then submit to the online grading system before the deadline. 


# Exercise

We'll practice the topics covered in this unit by continuing with the exercise from last week.


## Setup

For this week's exercise, you'll be working on it on your own. We'll again revisit and extend the project you started in the _wrangling_ module. Make sure you have the latest version of the repository on your local machine (it might not be, since last time your classmate contributed to your repository). As needed pull/push/merge to make sure remote and local repositories are in sync. Open the project in RStudio. 

At this stage, I assume all the data wrangling and EDA code, as well as the model fitting code you worked on is present and fully functional. If there are still some issues that need to be resolved, go ahead and do so.




## Data splitting


Start with a new/clean analysis script. You can either use an Rmd file, or you can do an R script. In either case, make sure you comment/document liberally.

Write code that takes the data and splits it randomly into a train and test that, following for instance the example in the [Data Splitting section of the Get Started tidymodels tutorial](https://www.tidymodels.org/start/recipes/). 



## Workflow creation and model fitting

Next, following the example in the [Create Recipes section of the Get Started tidymodels tutorial](https://www.tidymodels.org/start/recipes/), create a simple recipe that fits whatever your categorical outcome of interest is to all predictors (we'll start with categorical and all predictors since that's the closest to the shown example). For now, you can ignore the concept of `roles` and `features` they mention. 

Set a model as you did in the previous exercise, then use the `workflow()` package to create a simple workflow that fits a logistic model to all predictors using the `glm` function. To that end, follow the _Fit a model with a recipe_ section of the tutorial and adjust for your case. 

You should end up with a fit object similar to the one shown at the end of that section in the tutorial - of course, yours will look somewhat different since you are using a different dataset, but overall things should look similar.


## Model 1 evaluation

Follow the example in the _Use a trained workflow to predict_ section to look at the predictions, ROC and ROC-AUC for your data. Apply it to both the training and the test data. ROC and ROC-AUC is another common performance measure/metric for categorical outcomes. If you are not familiar with it, you can read more about them by following the link in the tutorial. It's not too important to go into the details for now. The focus here is on getting the code to work. In general, a ROC-AUC of 0.5 means the model is no good, 1 is a perfect model. Generally, somewhere above 0.7 do people think the model might be useful.


## Alternative model

Let's re-do the fitting but now with a model that only fits the main predictor to the categorical outcome. You should notice that the only thing you have to change is to set up a new `recipe`, this time one that only has the name of the predictor of interest on the right side of the formula (instead of the . symbol, which is shorthand notation for "all predictors".)  Then you can set up a new workflow with the new recipe, rerun the fit and evaluate performance using the same code as above. In general, if you do multiple models/recipes, you might want to write a loop to go over them, or parallelize/vectorize things. For now, just copying and pasting most of the code is ok.


## Optional

If you are up for it, add code that repeats the above steps, but now fits a linear model the the continuous outcome. For that, you also need to change your metric. RMSE might be a good one to choose. If you try this, you'll find that a lot of the code from above can be re-used with only minimal modifications.


## Looking ahead

We also covered overfitting and strategies to minimize it (e.g., cross-validation), and further model assessment strategies in the materials. We'll practice those in future exercises. I figured the above will already take a good bit of getting used to, and we will be able to practice more when we look at further model types in the coming weeks.


## Cleanup and submit

At this point, perform some cleanup in your repository. You will keep working on this exercise, and eventually add it to your online portfolio website, so it's a good idea to clean up a bit. Update readme files, remove files and folders that are not used, etc. At this point, you should have only materials in there that are relevant to the various exercise parts you (and your classmate contributors) did for this. Any leftover stuff from the original template should go or be updated.

Make sure everything looks good and runs, then update everything on Github. Since this is the same repository as the one you used previously, and you previously posted the link to the slack channel, you don't need to re-post (unless for some reason the repo changed). I know where to find the repository and will take a look and review/grade after the deadline.


# Discussion

Write a post in this week's discussion channel that answers this question:

Which of the concept(s) we covered in this module is/are the most interesting/surprising to you, and why? And which concept(s) or topic(s) from this module's materials do you find the most confusing, and why/how so? 

Post by Wednesday, then reply to each other by Friday. See if you can help each other reduce any existing confusion. I'll be participating too of course.

