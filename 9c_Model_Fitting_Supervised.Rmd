---
title: MADA Fall 2019 - Supervised Modeling
subtitle: ""
author: Andreas Handel
institute: "University of Georgia"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc_depth: 3
---

```{r, include=FALSE}
#just so I can use emoticons
#devtools::install_github("hadley/emo")
library(emo)
#set some figure options
knitr::opts_chunk$set(out.width = '70%')
```



# Supervised modeling details
Since supervised modeling is the most common data analysis approach, let's discuss it a bit more. While I try to minimize the math in this course, it can be helpful to express such models with equations, which is what I will do.

We will consider a scenario where we have a single outcome of interest, _Y_. If you have more than one outcome of interest, the most common and easiest approach is to analyze each outcome independently. We will not further dsicuss [multivariate analysis](https://en.wikipedia.org/wiki/Multivariate_statistics), i.e. the approach where you try to fit multiple outcomes at the same time. (Not to be confused with a scenario where you have a single outcome and multiple predictors, which is unfortunately also sometimes called _multivariate modeling/analysis_).

_Y_ stands for the vector of all outcomes, with each entry being an individual outcome value, $y_i$, for each observation (e.g., for each individual on whom you collected data). As a simple example, if you measured the heights of 5 people in centimeters and considered that your outcome of interest, you would have _i=1,..,N_ with _N=5_ being your _sample size_ (number of observations). And specific example for _Y_ could be something like (using R notation) `Y=c(175,180,166,165,172)`.

We usually have multiple _predictors_ (also called _inputs_ or _features_), which we label $X_1, X_2,...,X_M$, where $M$ indicates the total number of predictors you have (those that enter your model after cleaning/processing). Sometimes, one of the predictors is our main focus (usually called the _exposure_ in epidemiology and related areas), and others are often called _covariates_. At other times, all predictors are considered equally important. Each of the $X_j$ is again a vector with the same length _N_ as _Y_, and individual values of $X_j$ are labeled $x_i,j$. In other words, for each outcome value, you have a matching value for the predictor (of course that's the ideal case, you can certainly have missing values in practice).

We want to find a model or equation, __f__, that describes how (if at all) the inputs $X_j$ can be used to estimate the outcome, __Y__. In equation form, we have

$$Y=f(X_1,X_2,...)+e$$
where __e__ is some residual error not captured in our model.

If _Y_ is continuous, the simplest model is a linear model that includes all or some of the predictors, 

$$Y=b_0 + b_1X_1 + b_2X_2 + ...$$

One can include interactions and higher order terms in the predictors, for instance 

$$Y=b_0 + b_1X_1 + b_2X_2 + b_3 X_1^2 + b_4 X_2^2 + b_5 X_1X_2 + ...$$ 

Such models with higher order terms ($X^2$, $X^3$, ...) are often called polynomial models. Note that you need to always include the lower order terms in a model if you include higher order ones.

If the outcome _Y_ is categorical and binary, a common model is the logistic model, 

$$\log \left( \frac{Y}{1-Y} \right) = b_0 + b_1X_1 + b_2 X_2 + ...$$
In this model, __Y__ is the probability for the binary outcome. One can include higher order terms and interactions as for the linear model. There are other types of ways to connect the linear combinations of predictors on the right side of the equation to the outcome __Y__ on the left side. This class of models is called _generalized linear models_ and includes, for instance, Poisson regression for count outcomes.

One can go beyond the linear model by making __f__ more complicated. For instance, a generalized additive model (GAM) is one that allows each predictor/input to be related to __Y__ in a potentially complicated manner, while preserving the additivity of the predictors. Such a model is given by 

$$Y=b_0 + f_1(X_1) + f_2(X_2) + ...$$ 

For many other models, one cannot write down simple equations. Instead, those models are often defined algorithmically. For instance, in a regression or classification tree model (CART), there are consecutive splits along a tree based on which predictor improves model performance the most (we'll look in more details at such models later). 








