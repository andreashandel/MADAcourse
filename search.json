[
  {
    "objectID": "webexercises.html",
    "href": "webexercises.html",
    "title": "Webexercises",
    "section": "",
    "text": "This is a Web Exercise template created by the psychology teaching team at the University of Glasgow, based on ideas from Software Carpentry. This template shows how instructors can easily create interactive web documents that students can use in self-guided learning.\nThe {webexercises} package provides a number of functions that you use in inline R code or through code chunk options to create HTML widgets (text boxes, pull down menus, buttons that reveal hidden content). Examples are given below. Render this file to HTML to see how it works.\nNOTE: To use the widgets in the compiled HTML file, you need to have a JavaScript-enabled browser."
  },
  {
    "objectID": "webexercises.html#example-questions",
    "href": "webexercises.html#example-questions",
    "title": "Webexercises",
    "section": "Example Questions",
    "text": "Example Questions\n\nFill-In-The-Blanks (fitb())\nCreate fill-in-the-blank questions using fitb(), providing the answer as the first argument.\n\n2 + 2 is \n\nYou can also create these questions dynamically, using variables from your R session.\n\nThe square root of 16 is: \n\nThe blanks are case-sensitive; if you don‚Äôt care about case, use the argument ignore_case = TRUE.\n\nWhat is the letter after D? \n\nIf you want to ignore differences in whitespace use, use the argument ignore_ws = TRUE (which is the default) and include spaces in your answer anywhere they could be acceptable.\n\nHow do you load the tidyverse package? \n\nYou can set more than one possible correct answer by setting the answers as a vector.\n\nType a vowel: \n\nYou can use regular expressions to test answers against more complex rules.\n\nType any 3 letters: \n\n\n\nMultiple Choice (mcq())\n\n‚ÄúNever gonna give you up, never gonna: let you goturn you downrun awaylet you down‚Äù\n‚ÄúI bless the rainsguess it rainssense the rain down in Africa‚Äù -Toto\n\n\n\nTrue or False (torf())\n\nTrue or False? You can permute values in a vector using sample(). TRUEFALSE\n\n\n\nLonger MCQs (longmcq())\nWhen your answers are very long, sometimes a drop-down select box gets formatted oddly. You can use longmcq() to deal with this. Since the answers are long, It‚Äôs probably best to set up the options inside an R chunk with echo=FALSE.\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion\n\nWhat is true about a 95% confidence interval of the mean?\n\n 95% of the data fall within this range there is a 95% probability that the true mean lies within this range if you repeated the process many times, 95% of intervals calculated in this way contain the true mean"
  },
  {
    "objectID": "webexercises.html#checked-sections",
    "href": "webexercises.html#checked-sections",
    "title": "Webexercises",
    "section": "Checked sections",
    "text": "Checked sections\nCreate sections with the class webex-check to add a button that hides feedback until it is pressed. Add the class webex-box to draw a box around the section (or use your own styles).\n\nI am going to learn a lot: TRUEFALSE\nWhat is a p-value?\n\n the probability that the null hypothesis is true the probability of the observed, or more extreme, data, under the assumption that the null-hypothesis is true the probability of making an error in your conclusion"
  },
  {
    "objectID": "webexercises.html#hidden-solutions-and-hints",
    "href": "webexercises.html#hidden-solutions-and-hints",
    "title": "Webexercises",
    "section": "Hidden solutions and hints",
    "text": "Hidden solutions and hints\nYou can fence off a solution area that will be hidden behind a button using hide() before the solution and unhide() after, each as inline R code. Pass the text you want to appear on the button to the hide() function.\nIf the solution is a code chunk, instead of using hide() and unhide(), simply set the webex.hide chunk option to TRUE, or set it to the string you wish to display on the button.\nRecreate the scatterplot below, using the built-in cars dataset.\n\n\n\n\n\n\n\n\n\n\n\nI need a hint\n\nSee the documentation for plot() (?plot)\n\n\n\n\n\nClick here to see the solution\n\nplot(cars$speed, cars$dist)"
  },
  {
    "objectID": "resources/resources-glossary.html",
    "href": "resources/resources-glossary.html",
    "title": "Course Glossary",
    "section": "",
    "text": "This page is work in progress. Feel free to contribute via a GitHub pull request.\nThere are lots of technical terms related to data and its analysis. In this course, I try to explain them whenever they show up first. However, I figured it might also be useful to have one central place for quick reference. Terms are ordered alphabetically. This is how I define them, others might have different definitions - and so might I in the future, I try to change/update my understanding of things every so often üòÅ.\nNote that in general, there is no regulatory terminology agency, so everyone can use these words to mean whatever they want. While some are fairly well defined, other terms are often used and abused in ways outside the definitions I provide. Always look carefully at what is described to decide what exactly is going on.\n\nNot surprisingly, I‚Äôm not the 1st one with the idea of compiling a list of data science related terms and definitions. After starting this, I found another one by Frank Harrell, which you can find on his website. I‚Äôm sure I‚Äôll find and list more alternatives soon.\n\nArtificial Intelligence: The use of Deep Learning and related approaches applied to ‚Äúcomplex‚Äù problems. Historically, it was used for trying to solve problems using modeling approaches that mimic in a simplified form a brain (i.e.¬†neural net models). Currently this term has become hot and is used more widely than it should.\nBig Data: Any dataset that doesn‚Äôt easily fit into the memory of a regular computer (or cluster) and thus needs to be analyzed using special tools. Alternatively, data that is so big that doing analysis on it takes too long using standard tools (e.g.¬†R on a regular computer) and instead requires special treatment. Of course this also depends on the type of analysis, not only the type of data. As computers keep getting faster and tools more flexible and integrated, the label big data is a moving target.\nBinary Variable: A categorical variable with 2 categories, e.g., yes/no, dead/alive, diseased/healthy.\nCategorical Variable: A variable that can take on discrete values. Those categories can be ordered (at which point it is an Ordinal variable - see there) or not. Examples of non-ordered variables are hair color or ethnicity. No ordering is possible. A special and common case of categorical variables are Binary variables.\nCausal Modeling: Analysis of data with the goal to make causal inferences between variables (e.g.¬†X (partially) caused Y).\nClassification: Analysis approaches for Categorical Variables.\nContinuous Variable: A variable that can take on any numeric value. E.g. weight. Note that in practice the values are often discrete, e.g. while age is in principle a continuous variable, it is usually expressed in some units (say years) and individual values are reported in discrete units (e.g.¬†whole years). For analysis purposes, we still generally treat it as if it could have been any value.\nData Mining: Often used interchangeably with Machine Learning. It might sometimes indicate specifically a ‚Äúfishing‚Äù approach of combing through data to look for patterns, without pre-defined hypotheses to be tested. I that sense it is similar to Exploratory Data Analysis or Secondary Data Analysis, though those two are often done using smaller (Statistical) as opposed to larger (Machine Learning) models.\nDeep Learning: Generally applied to a specific class of machine learning models, namely neural nets. The ‚Äúdeep‚Äù part comes from the fact that the neural net models generally have multiple layers stacked on top of it (it has nothing to do with deep as in especially insightful).\nDependent (variable): An alternative name for Outcome.\nDescriptive Analysis: Describing and presenting data in meaningful ways using tables and figures, without trying to perform statistical modeling, i.e.¬†without looking for correlations/patterns.\nExploratory Data Analysis: Looking for patterns in data, without a hypothesis specified before data collection (or at least before looking at the data). Very useful, but any result needs to be tested on independent data.\nExposure (variable): A name for a predictor variable of particular interest. For instance, if we wanted to study if the daily duration of exercise had an impact on BMI. We would consider BMI our outcome, exercise duration our exposure (our main predictor of interest), and any other variable we record (e.g., a person‚Äôs age and gender) as other predictors (sometimes called covariates). The term exposure is common in the biomedical and related disciplines, not so much in other areas.\nFeature (variable): An alternative name for Predictor often used in the machine learning literature.\nIndependent variable: An alternative name for predictor, most often used in the statistical literature.\nInterval scale variable: A numerical (quantitative) variable for which taking differences makes sense. Unlike ratio scale variables, the zero point for interval data may be arbitrary. Addition and subtraction are meaningful for interval data, but in general, taking ratios is less meaningful. For example, temperatures in Celsius are on the interval scale. If the temperature was 70 degrees yesterday and 65 degrees today, it makes perfect sense to say ‚Äúit is 5 degrees cooler than it was yesterday.‚Äù But a day that is 70 degrees is not exactly twice as warm than a day that is 35 degrees, so division does not make sense here. There are also temperatures lower than 0 degrees, so the zero point is arbitrary.\nLabeled data: If we have data for which there is a specific outcome of interest and we know it, it is called labeled data. For instance, if we had a lot of pictures of tissue samples, and someone had gone through them and labeled them as cancerous or not cancerous, it is labeled data. Labeled data (the most common type) is usually analyzed using Supervised Learning/Analysis approaches.\nMachine Learning: An approach to data analysis that tends to use more complex models. The goal is mainly to obtain a model that is good at prediction. Understanding how different inputs lead to different outcomes is of secondary importance. Data is often abundant, so more complex models can be used. Often this term and Statistics/Statistical Learning are used interchangeably (though some people try to distinguish them, see e.g.¬†Frank Harrell‚Äôs blog post).\nMechanistic Modeling: Building and using models that explicitly incorporate mechanisms and processes of the system under study to understand how things interact and lead to specific outcomes. Many models in the hard sciences are of this type. A common way to formulate such models is with differential equations.\nNominal variable: A qualitative measurement that can take on distinct categories, but there is no natural order to these categories. For example, apples can be red, yellow, or green, but there is no inherent way to order these colors, even if the data is useful.\nObservation: An observation is a recording of the different variables for a single unit of analysis. Usually an individual, e.g., a single human or animal, but it could also be a picture or video, a county, a city, or whatever our level of observation is. For each observation, values for the different variables should be available. In R, it is most common that each observation is stored as a row in a data frame.\nOrdinal variable: A variable that can take on distinct categories which can be ordered, but the difference between levels might not allow for mathematical operations. For instance, if a question asks a person to rank their level of a pain on a scale from 1-10, a 7 is clearly higher than a 6, and a 6 higher than a 5. But it‚Äôs unclear if the difference between 5 and 6 is the same as 6 and 7.\nOutcome (variable): The variable of main interest for our analysis. This can be a single outcome (most common) or multiple. For instance, the main outcome might be if an individual survives or dies. Or it could be their BMI, or it could be if a given picture contains a cat or not. Also called response (variable) or dependent (variable).\nPredictive Modeling: Using models with the main goal of predicting future outcomes, given a collection of predictors.\nPredictor (variable): All variables that are not the outcome, which we use to see if we can predict the outcome. For instance, if we wanted to predict the price of houses, we could use the square footage of each house and the school district as the predictors.\nRatio scale variable: Ratio data are a type of quantitative data that are continuous, have consistent differences, and have a true zero. Addition/subtraction and multiplication/division are all meaningful for ratio data. For example, distance is ratio scale because the difference between two distances is meaningful, and a distance of 0 is truly the lowest possible distance value. Ratios are meaningful for distance: a distance of 4 miles is exactly twice as long as a distance of 2 miles.\nRegression: A type of supervised learning/modeling, where the outcome of interest is quantitative (or can be treated as such).\nResponse (variable): See Outcome (variable).\nSecondary Data Analysis: Analysis of a dataset that was not specifically collected for the purpose of answering the question one wants to answer. With an increasing abundance and routine collection of data, such secondary analyses are becoming very common. If a clear hypothesis is formulated before one looks at the data, one might consider any results confirmatory, otherwise it is an Exploratory Analysis. Even just looking at the data a little bit during the cleaning process should move such a secondary analysis into the exploratory category.\nStatistics: The basic/classical machinery for data analysis. Depending on the type of data, many different approaches have been developed (parametric vs.¬†non-parametric methods, longitudinal analysis, time-series analysis, and many more). Models tend to be simple and interpretable, the goal is to understand how inputs (predictors) relate to outcomes. Statistics was developed when data was sparse, computers didn‚Äôt exist, and mainly scientists interested in a deep understanding of their data used it. Because of this, statistical models tend to be simple and work well on small datasets.\nStatistical Learning: A term that seems to become more widely used in recent years. While some people distinguish this term from Statistics and consider it a sub-field (see e.g.¬†Chapter 1 of Introduction to Statistical Learning), the two terms are often used interchangeably.\nSupervised Learning/Analysis: Fitting Labeled data. The two types of supervised learning are Regression and Classification.\nUnlabeled data: If we have data for which there is either no specific outcome variable of interest or we do not know it, it is called unlabeled data. For instance, if we had a lot of pictures of tissue samples, and we knew that some showed cancerous tissue and others not, but we didn‚Äôt know which are which, it is unlabeled data. Similarly, if we had pictures of different tissue samples (or say a number of gene sequences) and all we wanted to know is if some samples are more related to each other than others, but there is no main outcome, it is considered unlabeled data. Unlabeled data is usually analyzed using unsupervised analysis approaches.\nVariable: Any quantity that we record like height, weight, income, or species type. In R, it is most common, that each variable is stored as a column in a data frame. The column name should be the name of the variable.",
    "crumbs": [
      "Resources",
      "Glossary"
    ]
  },
  {
    "objectID": "resources/resources-course.html",
    "href": "resources/resources-course.html",
    "title": "Course Resources",
    "section": "",
    "text": "This page lists resources that this course draws on frequently or at least occasionally. For a more extensive list of data science resources, see the General Resources page.\nAll materials described below are (should be) freely available online. If you can‚Äôt get to them, let me know. Note that a lot of the listed resources are dynamic and ever changing. That means occasionally links might not work, sites go offline, chapters in online books get re-arranged, etc. If you notice anything that‚Äôs not quite right, please let me know.\n\nBooks and online tutorials\n\nIntroduction to Data Science (IDS is the book I‚Äôll refer to a good bit in the 1st part of the course. The link above takes you to the 1st edition of the book. Mentions of specific chapters of IDS refer to the 1st edition. However, there is now a new version of the book, which splits the old version into 2, Introduction to Data Science and Advanced Data Science. I assume the newer versions are improved, so you might want to read them instead of version 1. I just haven‚Äôt gotten around to updating references to version 2.\nR for Data Science (R4DS) (2nd edition). This book will also be mentioned frequently. It is a very gentle and good introduction to data science in R. It has a lot of good exercises and solutions. Note that most of the course material was written when only the 1st edition existed. I‚Äôve tried to update references to specific chapters/pages of R4DS on this website, but it could be that there is still the occasional pointer to something in the 1st edition. If you find such outdated references/links, please let me know so I can fix.\nThe Art of Data Science (ADS) has some good big-picture, introductory level chapters on different aspects of a data analysis. It is a pay what you want book with a minimum price of zero, which means you can get it for free (note that the book + video bundle has a non-zero minimum price). (I heard that sometimes, once you register, the email with the book link goes to the spam folder, make sure to check there.)\nAn Introduction to Statistical Learning (with R) (ISL) is a good introduction to the statistical and machine learning (i.e.¬†model fitting) part of the data analysis workflow. I‚Äôll refer to it frequently in the second part of the course. While we won‚Äôt work through the exercises/labs that are part of ISL, if you are interested in trying out some of those, Emil Hvitfeldt‚Äôs website does the labs using the tidymodels set of packages, which we will also be using in this course.\nHands-on Machine Learning with R (HMLR) covers somewhat similar material as ISL, but with different emphasis and a different approach. I refer to it in several of the later course modules.\nFeature Engineering and Selection (FES) focuses on a specific aspect of the data analysis workflow and is a good resource for those topics.\nhappygitwitR (HGR) is a good resource to learn some of the basics of Git/Github with R.\nPosit Recipes (PR) are a great source for short code examples to do common tasks.\n\n\n\nTools\n\nWe‚Äôll be using R and R Studio Desktop. The R and R Studio document has details.\nWe will also be using Git and GitHub. You need to get that software and a GitHub user account. I also recommend/assume you are using the GitKraken client (but you don‚Äôt have to). See the GitHub introduction document for details.\n\n\n\nImportant notes\n\nThe company now called Posit was called R Studio. You will likely see the old label show up. The software editor we use for writing R code is still called R Studio.\nQuarto is fairly new, in the past we used R Markdown instead. You will likely see references to R Markdown. Just think of it as now being Quarto (i.e.¬†instead of .Rmd files we now work with .qmd files). They are very similar, Quarto is basically the newer version of R Markdown. R Markdown still works, but Quarto is more powerful, so it‚Äôs the better option to use/learn it.\n\n\n\nGeneral help\n\nThe main place to get any course specific help are our course discussion boards. Use them widely to ask questions, to answer others‚Äô questions, to post links to interesting resources, etc.\nMost questions you will have are likely not course specific, but will have to do with R/Github/RMarkdown/etc. For that, Google will be your best friend. Most of the time, someone had the same problem/question you do and someone else answered it. The only tricky thing is finding that post. Even after years of doing this, I probably google how to do something in R every day üòÉ.\nWhen you search online for help, quite often you land on some StackExchange site (often Stack Overflow). These are a collection of widely-used online question and answer sites covering all kinds of topics (including R, Github, Data Analysis, etc.). The majority of the time, you will find an answer, or at least something that gets you closer, on those sites. I rarely go to those sites directly, instead I type my query into Google and it often sends me to one of the StackExchange sites.\nThe Posit community is another good place to ask questions. It is not as widely used as StackExchange, but it seems to be more newcomer friendly. I haven‚Äôt used it much.\nAnother highly recommended resource is the R for Data Science learning community. They have a Slack workspace which you can join for free and ask questions about R, Data Science and related topics. They are very newbie-friendly. I have not used them much but they seem to respond fairly quickly and are helpful.\n\n\n\nMore resources\nThe General Resources page has a more extensive list of relevant resources.",
    "crumbs": [
      "Resources",
      "Course Resources"
    ]
  },
  {
    "objectID": "project/project-overview.html",
    "href": "project/project-overview.html",
    "title": "Class Project Overview",
    "section": "",
    "text": "Some of this will only fully make sense once we are a few weeks into the course. It‚Äôs nevertheless a good idea to read through it and get an overall idea before we have discussed all the different tools and details mentioned. Re-visit/re-read as needed.",
    "crumbs": [
      "Project",
      "Project Overview"
    ]
  },
  {
    "objectID": "project/project-overview.html#your-own-data",
    "href": "project/project-overview.html#your-own-data",
    "title": "Class Project Overview",
    "section": "Your ‚Äúown‚Äù data",
    "text": "Your ‚Äúown‚Äù data\nIf you have data that you are using for some research project(s) you are doing, you are welcome and encouraged to work on this as part of the class project. Of course, what you do for this class project needs to be new work, not a previously done and recycled analysis. Also, since the analysis needs to be fully reproducible, you need to provide the data at least within the class (no need to make it publicly available). I encourage you to use the class project toward helping you do an analysis and write a report that can help you with a project you want to publish as part of your research!",
    "crumbs": [
      "Project",
      "Project Overview"
    ]
  },
  {
    "objectID": "project/project-overview.html#publicly-available-data",
    "href": "project/project-overview.html#publicly-available-data",
    "title": "Class Project Overview",
    "section": "Publicly available data",
    "text": "Publicly available data\nYou can use any data you can get access to. Since it needs to be reproducible, you need to be able to share the data at least with me and the classmates who will review your project. If you need some ideas for data, check out this website I maintain with various links to resources, among them is a list with links to various data sources. Of course you are not limited to data listed there.\nOften, the most interesting questions can be asked by combining data from more than one source.",
    "crumbs": [
      "Project",
      "Project Overview"
    ]
  },
  {
    "objectID": "project/project-details.html",
    "href": "project/project-details.html",
    "title": "Class Project Details",
    "section": "",
    "text": "Overview\nThis documents provides more detailed instructions and grading rubrics for each part of the project.\nNote that you should not consider the scores below as corresponding to a letter grade. Instead, I use those to differentiate between different aspects of the project. Only at the end while I make actual grade-like scores, which will likely involve curving (up, never down).\nAt my discretion, I might consider assignments even if they are submitted after the deadline. In that case, I might take off a point for lateness.\n\n\nPart 1\nThe main objective for part 1 is to describe data and question in enough detail that I can assess if the planned project is feasible.\nUse the manuscript.qmd file. Remove or replace the template information. Add your proposed project title and your name(s).\nTo that end, provide the following pieces of information:\n\nBriefly describe what the data is, how it was collected, where you will get (or got) it from. How many observations do you have, what was measured? Anything else important to report about the data?\nAt this stage you are not required to already have and show the data, but if you do, even better. Then add a few lines of code which load the data and using some of the commands you learned about, provide summary descriptions of the data.\nExplain the question you want to answer using the data. What will be your outcome(s) of interest (if any)? What (if any) specific predictors will you focus on? What relations/patterns are you looking for in the data?\nAs much as you know, suggest how you will analyze it. At this stage in the course, we haven‚Äôt covered analysis approaches yet, so you can keep things vague and non-technical here.\nYou are allowed, but not yet required, to provide background information for the question you plan to answer. For instance you can describe why it‚Äôs an interesting question, who else has done similar analyses, how your analysis will be new/different, etc. Similar to what you read in an introduction to a research paper. For the final report, you‚Äôll need these parts. For part 1, they are not required, but you are welcome to already write down some of that.\nEventually, for your final report, what you write for this part will go into different sections of the full report. Some will go into the introduction, some in the methods section. You can already place these items there, or for now just write them as a single section.\n\nGrading for this part will follow the following rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete, contains enough information to allow assessment of feasibility of proposed project. Fully reproducible.\n3\n\n\nSomewhat insufficient\nMost components are present, but noticeable gaps exist, or existing materials can‚Äôt be fully reproduced.\n2\n\n\nNot sufficient\nSubmission is rather incomplete, has major missing parts which does not allow assessment of feasibility of proposed project\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\n\nPart 2\nThe main objective for part 2 is to have mostly completed the data loading/cleaning/wrangling/exploring part.\nTo that end, provide the following pieces of information:\n\nEverything from part 1. That doesn‚Äôt mean you need to keep what you wrote for part 1 frozen. Just that the description you provided there should be part of this submission. As appropriate, you can rewrite/reformat things to get it closer to a final report structure (e.g.¬†start moving some parts into a method section).\nA somewhat detailed description containing text and code showing your cleaning/wrangling/exploring steps. Place this in separate Quarto/R files inside the code folder or its sub-folders. Make sure it‚Äôs clear which files are the relevant ones, delete any non-relevant files.\nUpdate the main manuscript. Add a few of the exploratory results and any other content you think should be shown.\nRemoval or replacement of any left-over files and leftover text and code from the templates. Update all readme files, delete any files and folders that are not part of your project. Remove any comments and bits of code that are not relevant. At this stage, only information, code and files relevant to your project should be present, with appropriate documentation.\nThe main text should show plots or tables that explore the data, with a focus on the quantities of main interest (outcome, main predictor, co-variates of specific interest, etc.).\nIt is up to you how you structure things. You can use a combination of R or Quarto scripts. As long as things are well documented, reproducible and logical, the exact setup is your choice.\nEverything needs to be fully reproducible and you need to provide somewhere (e.g.¬†in the main text file or in the readme file in your repository) instructions on what one needs to do to completely reproduce everything.\nYour main article and - if applicable supplementary files - should knit into a word or pdf or html documents.\nIf you start including references, you should use a reference manager and a bibtex file from which you cite references in your manuscript. I recommend managing the bibtex file with the free Zotero reference manager, but if you have another reference manager that can handle bibtex files, you can use that too. Your bib file should be part of the project repository (for instance in the same folder as the manuscript). Feel free to pick any citation style you like (you can get CSL files from e.g.¬†Zotero‚Äôs style repository).\n\nGrading for this part will follow the following rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\n\nPart 3\nThe main objective for part 3 is to have started the analysis part of the project and continued to improve everything.\nTo that end, provide the following pieces of information:\n\nAll relevant files and documents needed to reproduce everything.\nAll non-relevant files (e.g., leftovers from the template) removed or updated.\nSome documentation (e.g.¬†a readme.md file) explaining how your project is set up and which scripts need to be run in what order to reproduce everything.\nFiles with well-documented code (either R scripts or Quarto files) that do all the previous tasks (cleaning/processing), as well as running a few analyses. You can start with simple bivariate ones, looking for patterns between your outcome(s) and individual predictors of interest. I suggest as much as possible you use the tidymodels framework. You can also try a few multivariable GLM. Results from those explorations should be saved in whatever form you consider most appropriate (figures or tables).\nA main article/manuscript file which contains the most pertinent results and findings from everything you have done so far. You can include more figures/tables here than you would in a regular manuscript. However, it should still be nicely readable and somewhat focused, so don‚Äôt produce page-long raw R output or a ton of exploratory figures or similar things. The main results of your analysis should be in this manuscript. Any further explorations and results (figures and tables) should go into a separate Quarto file that comprises the supplementary material.\n\nGrading for this part will follow this rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, minor parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\nThis part of your project will be assessed by some of your classmates. See the Project Review document for more details.\n\n\nPart 4\nThe objective is to have mostly completed implementation of analyses following the approaches covered in the course.\nTo that end, provide the following pieces of information:\n\nAll relevant files and documents needed to reproduce everything.\nDocumentation for everything you are doing. Any files/documentation not related to your project should be removed, only relevant information should be present.\nCode that performs statistical analyses of your data using the approaches we cover in class, such as: train/test split, cross-validation, trying different models, exploring model quality (performance, uncertainty, diagnostics, etc.). Depending on your data and question, not all approaches will make sense for your data. Choose the ones that make sense. E.g., if you happen to do an analysis of text or high-dimensional data, use methods/models appropriate for that data. The main point is that you should show you understand the main concepts regarding analysis and model evaluation and can apply them to your data with the tools we covered.\nUpdate your manuscript and supplementary files with the new results (figures/tables) from your analysis.\nAt this point, make sure you are also far along with your background section, including referencing (using a reference manager and bibtex, file, no manual references) and everything starts looking like a full analysis similar to what one could submit to a journal for publication.\n\nGrading for this part will follow this rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, minor parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\n\nPart 5\nThe objective for part 5 is to have a fully completed project, ready for peer review.\nTo that end, provide the following pieces of information:\n\nEvery file needed to reproduce your complete analysis and report in your project repository (that includes files such as readme, bibtex, style files, etc).\nDetailed instructions (e.g.¬†as a readme.md file) explaining how to reproduce all your results.\nA complete, nicely readable and well formatted report, written in structure like a research paper (Abstract/Introduction/Methods/Results/Discussion/Citations).\nWell documented supplementary material\nWell-documented code as Quarto files or R scripts that explain clearly all steps in your analysis (including the wrangling/exploring parts).\nMeta-data explaining your data as needed.\nEverything very well documented and polished as much as possible.\n\nCheck the Project Review Template file to see how this submission will be assessed by your peers. I will not grade this part but it will be assessed by your peers, as described in the Project Review document.\nA strong project would be at the level of one of the projects on the Project Examples page.\n\n\nPart 6\nThe main objective for part 6 is to have a finished project, ready for final grading.\nTo that end, provide the following pieces of information:\n\nA complete project, with as many further improvements as you want to implement, based on feedback from your classmates and any other improvements you can think of.\n\nI will grade the final project using the same criteria your classmates used for peer review. This will be graded on a 100 points scale and combined with the other project scores for a final overall project score.",
    "crumbs": [
      "Project",
      "Project Details"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html",
    "href": "courseinfo/course-syllabus.html",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "",
    "text": "This course provides a survey of modern statistical approaches to analyze data. We will cover a variety of modern approaches for analyzing and interpreting data commonly encountered in public health, biomedical sciences, and related areas. This is an applied, hands-on class. We will use real data (bring your own data if you have it) to learn different methods of analysis. We will discuss all the steps of a data analysis, including obtaining and cleaning data, exploratory and full analysis, and presentation of results.\nWe will discuss how to formulate scientifically solid questions for a given set of data, how to decide on the right method of analysis, how to implement the analysis in R, and how to present and communicate the results. We will cover statistical topics such as regression, tree based models, cross-validation, bootstrapping, and model selection.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#overview",
    "href": "courseinfo/course-syllabus.html#overview",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "",
    "text": "This course provides a survey of modern statistical approaches to analyze data. We will cover a variety of modern approaches for analyzing and interpreting data commonly encountered in public health, biomedical sciences, and related areas. This is an applied, hands-on class. We will use real data (bring your own data if you have it) to learn different methods of analysis. We will discuss all the steps of a data analysis, including obtaining and cleaning data, exploratory and full analysis, and presentation of results.\nWe will discuss how to formulate scientifically solid questions for a given set of data, how to decide on the right method of analysis, how to implement the analysis in R, and how to present and communicate the results. We will cover statistical topics such as regression, tree based models, cross-validation, bootstrapping, and model selection.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#learning-objectives",
    "href": "courseinfo/course-syllabus.html#learning-objectives",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe main goal for this course is for you to learn the whole process of performing a data analysis project. This starts with identifying a suitable question-data pair, proceeds to getting, cleaning and exploring the data, culminates with fitting statistical models and producing materials that communicate what you did and found. A second goal is to introduce you to some modern analysis approaches that these days often go by the name of ‚ÄòMachine Learning‚Äô. Finally, a related goal is to introduce you to a set of tools that allow for a modern, reproducible workflow of your analysis.\nThe specific learning objectives that I hope you will achieve by going through this course are:\nDefine meaningful data analysis questions and assess the feasibility of answering these questions with the available data.\n\nGiven a data set, define the questions that can be answered and formulate and implement suitable analytic approaches.\nGiven a data analysis question, determine the type of data and analytic approach needed to answer it.\n\nBe able to obtain, organize and process data for detailed analysis.\n\nKnow how to obtain data from a variety of different sources.\nBe knowledgeable about data types and standards and how to process them.\nBe able to organize and process data in a reproducible, automated and documented manner.\nBe able to thoughtfully and critically assess strengths and weaknesses of specific data sets and process the data appropriately.\n\nBe knowledgeable of different data analysis methods and select the appropriate approach for a given project based on data and question.\n\nCritically compare and evaluate the strengths and weaknesses of different data analysis approaches.\nJudge the appropriateness of different approaches for specific questions and data sets and know how to apply an appropriate analytic approach.\nDesign and implement successful data analyses using state-of-the-art analysis software to translate data to information and knowledge that leads to actionable insights.\n\nEfficiently communicate results from data analyses to a variety of stakeholders.\n\nSummarize analysis results in ways that provide actionable conclusions and that are easily understandable by different audiences, such as laypersons, decision makers, and expert colleagues.\nAssess the strengths and weaknesses of different formats for representing the results of data analyses.\n\nUse modern coding and analysis tools to implement automated, reproducible analysis and project management workflows.\n\nExplain the importance of workflow, project management, and reproducibility tools, and know how to use those tools.\nBe proficient in R coding to implement and execute a complete data analysis project in a reproducible and automated manner.\nBe comfortable using R and Github to do data analysis in a reproducible manner.\nBe able to quickly learn how to use new software and tools, figure out how to get help when stuck, and make it work for you.\n\nDevelop skills to critically assess your own and others‚Äô analyses and conclusions.\n\nJudge the usefulness and appropriateness of data analyses described in the primary research literature.\nLearn to look over your shoulder and critically assess what you are doing, what assumptions you make by doing certain things, and if and how you can justify these.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#topics",
    "href": "courseinfo/course-syllabus.html#topics",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Topics",
    "text": "Topics\nHere is a non-exhaustive lists of topics that this course does and does not cover.\n\nWe will cover\n\nHow to set up an analysis workflow that is as reproducible and automated as possible.\nGetting, cleaning and processing messy real-world data.\nData visualization.\nModern tools for data analysis (e.g., R, Quarto, Git/GitHub, AI systems).\nThe tidyverse for data processing and tidymodels for fitting models.\nAn introduction to some Machine Learning tools and techniques.\n\n\n\nWe will not (or barely) cover\n\nAdvanced visualization techniques using interactive tools such as R/Shiny.\nDealing with ‚Äúnon-rectangular‚Äù data, such as time-series data, images, audio, complex -omics data, etc. (we won‚Äôt cover it, but you can use such data for your class project).\nStatistical tests and basic statistical modeling (linear and generalized linear models). Some familiarity with those techniques is assumed and they do show up a few times, but won‚Äôt be covered in any depth.\n\nHow to code in R. We will use R, but this course doesn‚Äôt teach R. With enough effort, a student can learn both the subject matter and pick up enough R at the same time, but this will require extra effort. In general, some basic familiarity with R or another programming language will be assumed.\nAnything in depth. This is a survey course and covers a lot of material, thus we won‚Äôt be able to go into much depth for any topic. Resources are provided to allow anyone interested to go deeper on their own.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#prerequisites",
    "href": "courseinfo/course-syllabus.html#prerequisites",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nFormal requirement for the course is BIOS 7010. Knowledge of material from BIOS 7010 is assumed. If your lack pre-requisites prevent you from enrolling in this course, please contact me to get permission to enroll.\nThis is a quantitative course. We will not discuss the mathematical details of specific data analysis approaches, however some statistical background and being comfortable with quantitative thinking are needed. Knowledge of statistics at the level of fitting linear or logistic models to data (e.g., as obtained in our BIOS 7010 and 7020 courses) is assumed. Some R coding skills (e.g., as obtained in our EPID 7500 class) are also needed.\nIf you do not have any coding or statistics knowledge, you can still take the class, but you need to be prepared to spend extra time and effort to fill any gaps. This will be especially true for the R coding part. Some of you likely have previous R experience, while others might have little to none. If you are in the little to none category, expect to spend extra time getting up to speed. I believe it‚Äôs doable and worth the effort, but you need to be prepared for it. There will be plenty of help from myself, classmates, and the internet if you end up getting stuck with some of the coding, but your effort and commitment are still required.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#should-i-take-this-course",
    "href": "courseinfo/course-syllabus.html#should-i-take-this-course",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Should I take this course?",
    "text": "Should I take this course?\nHere are a few more pointers to what to expect from the course, and comments from previous students (so you can hear it from them, not just from me).\n\nThe course is a high-level survey course. We cover a lot of material.\nA lot of the content is fairly conceptual/broad/big-picture, we don‚Äôt go much into technical details on specific topics.\nThe pace of the course is fast and covers a lot of topics.\nThere is a good bit of hands-on work/exercises that you need to do each week, which can be quite time-consuming.\nThis course comes with a good bit of team work.\nYou will learn skills that are likely useful for your research/work career.\nYou will be using R, GitHub, Quarto and other useful tools.\n\n\nTo re-emphasize: This course requires a significant time commitment, especially if you have not used R or any other programming language before. Budget your time accordingly and plan ahead! If you do, I‚Äôm fairly certain you will find it worth it. If you are not able or willing to allot the time needed to learn enough R (and GitHub) to make things work, this course might not be ideal for you.\n\n\nSome feedback from past students\n‚ÄúI have found this course to be useful and relevant to my research interests. I think the mixture of the modules, additional resources, and coding exercises really helped strengthen my understanding of important data analysis concepts.‚Äù\n‚ÄúI have thoroughly enjoyed this class from start to finish, and I have gained a lot of knowledge, starting from Github to the entire data analysis workflow. However, I feel that we have covered a lot of material in just one semester.‚Äù\n‚ÄúI have learned so much from this class in what feels like such a short amount of time, and it has gotten me a lot more comfortable working with and interpreting my data and I now feel that I have a strong grasp on data analysis workflows. However, sometimes I feel like I‚Äôve learned about so many new things that I feel dumb all over again because there is so much to take in when it comes to machine learning and I want the opportunity to explore each facet in greater detail.‚Äù\n‚ÄúThe last half of the semester has seemed like a whirlwind. We have reviewed everything from modeling to training and even machine learning.‚Äù",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#course-setup",
    "href": "courseinfo/course-syllabus.html#course-setup",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Course Setup",
    "text": "Course Setup\nThis course is a fully online, asynchronous, cohort-based course. That means there are weekly deadlines, but other than the fixed deadlines, you can do the work whenever it is convenient for you.\nAll course materials are freely available online. We will make use of several freely available textbooks and other materials. All course materials are listed on the course website. We will use the R software for data analysis. We will also use a few other software tools. All are freely available.\nThis course is very hands-on. The weekly exercises (aka homework) are usually quite in-depth and also often time-consuming. Plan accordingly. For each exercise, I will provide detailed instructions that hopefully make it clear what you need to do. The materials provided on the course website are not meant to be memorized, but to be used to be able to do the hands-on activities, such as the exercises and the class project.\nFor more details on course logistics, see the other information in the General Information section of the course website.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#privacyferpa",
    "href": "courseinfo/course-syllabus.html#privacyferpa",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Privacy/FERPA",
    "text": "Privacy/FERPA\nThe course is set up such that you work ‚Äúin the open‚Äù by having a GitHub repository and accompanying website that showcases the work you are doing in this class as an online portfolio. This is a great way to showcase your work to potential employers. However, it also means that your work is publicly visible.\nAs a student, under the Family Educational Rights and Privacy Act (FERPA) you have the right to have all your student-related information kept private, including the fact that you are taking this class. Keeping everything private can be done using private GitHub repositories. But it means you won‚Äôt be able to create a public website/portfolio, and you can‚Äôt follow the general instructions in the exercises. Instead I‚Äôll need to give you special instructions. If for some reason you want to keep the fact that you are taking this class a secret, and thus do not want to use public GitHub repositories/create a publicly accessible online portfolio please let me know. If you take this course and do not inform me that you want your attendance in this course to be kept private, I consider this as you consenting with producing public materials as part of this course, and thus make your participation in this course public. If you have any concerns about this, please contact me to discuss.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#grading",
    "href": "courseinfo/course-syllabus.html#grading",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Grading",
    "text": "Grading\nThe grade will be made up as follows:\n\n20% quizzes\n30% exercises/homework\n10% participation/discussions\n40% a course long project, broken up into pieces.\n\nThe following grading scale will be used, final grades might be curved (upward, never down): A 93-100, A- 90-93, B+ 87-90, B 83-87, B- 80-83, C+ 77-80, C 73-77, C- 70-73, D 60-70, F &lt; 60\nMore detailed descriptions of the different assessments is provided on the Assessments page.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#class-attendance-make-up-policy",
    "href": "courseinfo/course-syllabus.html#class-attendance-make-up-policy",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Class Attendance, Make-up Policy",
    "text": "Class Attendance, Make-up Policy\nThis class is online. You are expected to submit all assignments by their due dates. Excused misses of due dates are only provided by prior agreement with the instructor or for special reasons (medical, etc.).",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#getting-help",
    "href": "courseinfo/course-syllabus.html#getting-help",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Getting Help",
    "text": "Getting Help\nIf you have questions about any aspect of the course, please do not hesitate to ask for help. The course materials describe in detail the ways you can ask for help.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#university-honor-code-and-academic-honesty-policy",
    "href": "courseinfo/course-syllabus.html#university-honor-code-and-academic-honesty-policy",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "University Honor Code and Academic Honesty Policy",
    "text": "University Honor Code and Academic Honesty Policy\nAll academic work must meet the standards contained in A Culture of Honesty. All students are responsible to inform themselves about those standards before performing any academic work. More detailed information about academic honesty can be found on UGA‚Äôs honesty webpage.\nDiscussions with your classmates and the instructor are encouraged. However, the final work should be your own.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#students-with-disabilities",
    "href": "courseinfo/course-syllabus.html#students-with-disabilities",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Students with Disabilities",
    "text": "Students with Disabilities\nStudents with disabilities who require reasonable accommodations in order to participate in course activities or meet course requirements should contact the instructor.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#general-disclaimers",
    "href": "courseinfo/course-syllabus.html#general-disclaimers",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "General Disclaimers",
    "text": "General Disclaimers\nThis syllabus is a general plan, deviations announced to the class by the instructor may be necessary.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#course-outline",
    "href": "courseinfo/course-syllabus.html#course-outline",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "Course Outline",
    "text": "Course Outline\nFor an outline of the course, please see the Course Schedule document.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#more-details",
    "href": "courseinfo/course-syllabus.html#more-details",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Syllabus",
    "section": "More Details",
    "text": "More Details\nThe General Information section and the introductory unit of this course contains all the logistic details you need to know.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-overview.html",
    "href": "courseinfo/course-overview.html",
    "title": "Course Information Overview",
    "section": "",
    "text": "This section contains most of the logistic information. The Syllabus page explains all the general logistic parts, the Schedule page lists what content is covered and what assessments you need to do each week, and the Communication page provides details on how we‚Äôll interact throughout this course.\nA few more logistic details can be found in other places, e.g., the introductory unit of this course. You‚Äôll be pointed to the sources.",
    "crumbs": [
      "General Information",
      "Course Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html",
    "href": "courseinfo/assessments-overview.html",
    "title": "Course Assessments - Overview",
    "section": "",
    "text": "Each module comes with assessments. Those are things you need to do that will be graded. In general, expect that each module contains a Quiz, an Exercise, and a Discussion. Sometimes there are additional/alternative components, such as submission of project parts.\nDue dates are usually Friday 5 pm the week of the module. Some might be at other times, e.g., the beginning or middle of the week. See the schedule or the module specific assessment document. If unclear, ask.\nAll assessments are located in the Assessments section of the website. Their labeling matches that of the module, so it should be clear for each module which assessment to complete.\nThe assessments, especially the exercises, are the main way of learning the material. They are therefore meant as a help for you, giving you incentives to work through and learn the course material. It is very easy to take shortcuts (cheat) for any of the assessments. Of course, our academic honesty policy prohibits you from doing so. But more importantly, I assume you want to learn the material. And if you take shortcuts by getting answers from a classmate, you are only hurting your own learning. That said, I encourage getting help from any source you can, including your classmates. I let you decide how much helping each other is acceptable and at what point it turns into having someone else do the work for you that you should be doing yourself.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#assessments-overview",
    "href": "courseinfo/assessments-overview.html#assessments-overview",
    "title": "Course Assessments - Overview",
    "section": "",
    "text": "Each module comes with assessments. Those are things you need to do that will be graded. In general, expect that each module contains a Quiz, an Exercise, and a Discussion. Sometimes there are additional/alternative components, such as submission of project parts.\nDue dates are usually Friday 5 pm the week of the module. Some might be at other times, e.g., the beginning or middle of the week. See the schedule or the module specific assessment document. If unclear, ask.\nAll assessments are located in the Assessments section of the website. Their labeling matches that of the module, so it should be clear for each module which assessment to complete.\nThe assessments, especially the exercises, are the main way of learning the material. They are therefore meant as a help for you, giving you incentives to work through and learn the course material. It is very easy to take shortcuts (cheat) for any of the assessments. Of course, our academic honesty policy prohibits you from doing so. But more importantly, I assume you want to learn the material. And if you take shortcuts by getting answers from a classmate, you are only hurting your own learning. That said, I encourage getting help from any source you can, including your classmates. I let you decide how much helping each other is acceptable and at what point it turns into having someone else do the work for you that you should be doing yourself.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#quizzes",
    "href": "courseinfo/assessments-overview.html#quizzes",
    "title": "Course Assessments - Overview",
    "section": "Quizzes",
    "text": "Quizzes\nThe quizzes cover the basics and ensure that you have gone through some of the material. Each quiz consists of around 10-20 short questions. Not all answers to the quiz questions are directly found in the materials, sometimes you need to do a - hopefully straightforward - online search.\nQuizzes are auto-graded on submission based on a simple counting of the right answers. At the end, I‚Äôll average across all quizzes. Note that since there are &gt;10 quizzes and all quizzes together count for only a fraction of the total grade, each quiz counts for a small part of your grade. So try to get them as correct and complete as possible, but don‚Äôt stress out about missing a question here and there.\n\nThe link to the website for quiz submission is in the important-information Discord channel. Before submitting any quizzes, you need to set up a user name and password by filling the Google Form that was shared with you in the welcome email. The link to the Google Form can also be found in the important-information Discord channel. The submission system becomes available once everyone has submitted their user names/passwords.\nUnless otherwise specified, you can submit each quiz once, and it needs to be submitted by the deadline (generally Fridays), otherwise the system will not take your submission.\n\n\nQuiz sheets\nFor each module, you are asked to fill out an Excel sheet with questions related to the course material. Download the quiz files from the links below, fill in the Answer column, and submit to the online quiz submission system by the deadline specified in the Schedule document.\n\nYou can expand and resize the columns/rows of the Excel file so you can make sure to see all content. Other than that, please only fill in the Answer column and don‚Äôt make other changes (e.g.¬†don‚Äôt rename or delete rows/columns). If you do make changes, submission might fail. If that happens and you get an error message during submission, simply come back here, download a fresh sheet and copy and paste your answers into it, making sure to not change any other part of the sheet.\n\nQuizzes will become available here as the course progresses. Once a quiz is available, you should be able to submit it at any time before the deadline.\n\nModule 1 ‚Äì Course and tools introduction - you have 2 attempts available for this one.\nModule 2 ‚Äì Data analysis introduction\nModule 3 ‚Äì R coding basics\nModule 4 ‚Äì All about data\nModule 5 ‚Äì Data Processing\nModule 6 ‚Äì Data presentation\nModule 7 ‚Äì Statistical Analysis Overview\nModule 8 ‚Äì Fitting basic statistical models\nModule 9 ‚Äì Evaluating your model fits",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#exercises",
    "href": "courseinfo/assessments-overview.html#exercises",
    "title": "Course Assessments - Overview",
    "section": "Exercises",
    "text": "Exercises\nThe exercises ‚Äì together with the project ‚Äì are the most important component of the course! The material we cover in this class can only be learned by doing it yourself. As such, the focus is really on the exercises (i.e.¬†the homework), with the readings and videos mainly meant to help you find and learn the material you need to do the exercises.\nBecause the exercises are so important, they are usually quite in-depth and also often time-consuming. Plan accordingly. For each exercise, I will provide detailed instructions that hopefully make it clear what you need to do.\nIf you get stuck during the exercises (quite possible), feel free to ask for help. Use the various course materials, the course discussion board as well as outside resources. But do try to do as much as possible yourself (of course with the help of Google).\nUnless otherwise specified, exercises are due by Friday. Quite often, some part of the exercise needs to be done before that deadline to allow for group work.\n\nExercise grading\nI grade the exercises based on 4 categories:\n\n3 points = complete or only minor omissions/mistakes.\n2 points = some noticeable bits are missing or wrong.\n1 points = some parts are present, but overall very incomplete.\n0 points = completely or almost completely missing.\n\nAt my discretion, I might consider assignments even if they are submitted after the deadline. In that case, I might take off a point for lateness.\nDon‚Äôt interpret these scores as grades, I just use them to differentiate, then I‚Äôll adjust at the end when I compute the overall project score.\nFor exercises that include multiple parts (e.g., you start something, then your colleague takes over), each part will be assigned a score.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#discussions",
    "href": "courseinfo/assessments-overview.html#discussions",
    "title": "Course Assessments - Overview",
    "section": "Discussions",
    "text": "Discussions\nWe can‚Äôt have in-class discussions in an online course, but hopefully we can still have robust discussions as part of the course. I hope that lively online discussions will allow us to create a sense of class community that usually comes more naturally in an in-class course. I want everyone to feel like they are part of a class, a community of students learning together, instead of just being on their own interacting with the course in front of a computer screen. To achieve that, having a lot of discussion activity is vital.\nYou will be asked to participate in discussions throughout the course, and discussion participation is part of the grade. The discussions are meant to connect the module content to the broader context and hopefully will spark interesting further thoughts and insights.\nUnless otherwise specified, the first posting of the discussion is due by Wednesday, the discussion replies are due by Friday. Post to the discussion channel for the applicable module (e.g.¬†module2_discussion, module3_discussion).\n\nDiscussion guidelines\nTo make sure discussions work well for everyone, here are some general guidelines.\n\nYou can and will be asked to both start new threads/posts or reply to others. If you start a new topic, make sure you are in the right channel. (If needed, you can also move posts to other channels later).\nYou are encouraged to include links to additional material that you find online, including but not limited to research papers, or sources like Wikipedia, and other useful resources. There are no rules on formatting other than that it should be readable.\nYou don‚Äôt always have to provide references/links to resources, nor do you need to be sure that ‚Äòyou got it right‚Äô. It‚Äôs ok to post speculations/ruminations/thoughts occasionally.\nMake meaningful contributions. You are welcome to react to each others‚Äô posts with üòÉ or üëç or other such emotions, but that does not replace more substantial responses. Give reasons why you think something is one way or another.\nFor the purpose of assessment, you need to participate in the discussions at the level specified in each module‚Äôs assessment document. I encourage you to engage in further discussions. Those further posts won‚Äôt be counted toward the requirement for a given module but will factor into the overall participation portion of the grade.\nIt should be obvious that any posts should be helpful and civilized. I will remove any posts that are demeaning or otherwise inappropriate. Please let me know if you see such posts.\nDiscussions will remain available for the whole course, and you can keep contributing at any time.\n\n\n\nDiscussion grading\nFor most modules, you‚Äôll have to do the following (unless specifically directed otherwise):\n\nYou need to write a post (generally a few hundred words) as instructed. Post it to the appropriate channel by Wednesday evening.\nYou need to write at least 3 replies to posts from classmates. Those can be replies to 3 different posts or multiple back-and-forth (of substance) for the same post/thread. You must post your replies by Friday evening.\nFor each module (i.e.¬†each week) you need to post a minimum of 4 posts (1 initial, 3 replies) with around 500 meaningful words total by the deadlines. I won‚Äôt count words (unless it looks really short), and probably also won‚Äôt count the number of posts, unless both seem well below the minimum.\nI will use the same 0-3 point score for each discussion as for the quizzes.\nGoing above the minimum (more posts, longer contributions, especially thoughtful, etc.) will count positively toward the participation portion of the final grade.\nFor each module, the assessment document contains specific discussion instructions. If the specific discussion instructions are different than the general ones here, you should follow the specific ones.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#projects",
    "href": "courseinfo/assessments-overview.html#projects",
    "title": "Course Assessments - Overview",
    "section": "Projects",
    "text": "Projects\nSee the Projects section on the website for more information.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html",
    "href": "content/module-workflow/assessment-complete-workflow.html",
    "title": "Assessment - Complete Workflow",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#overview",
    "href": "content/module-workflow/assessment-complete-workflow.html#overview",
    "title": "Assessment - Complete Workflow",
    "section": "Overview",
    "text": "Overview\nFor this exercise, we will join the R data science community and participate in Tidy Tuesday, a weekly exercise that allows you to practice playing with data. The focus for Tidy Tuesday is on getting, cleaning (i.e., tidying) and exploring a data set, though people sometimes do statistical analyses as well. We‚Äôll do a ‚Äúfull‚Äù analysis workflow.\nThis is a solo exercise and will go into your portfolio.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#what-is-tidy-tuesday",
    "href": "content/module-workflow/assessment-complete-workflow.html#what-is-tidy-tuesday",
    "title": "Assessment - Complete Workflow",
    "section": "What is Tidy Tuesday",
    "text": "What is Tidy Tuesday\nYou briefly encountered Tidy Tuesday before in the Data Analysis Motivation unit. If you can‚Äôt remember, revisit that unit and re-read that section. Also read the information at the provided links.\nIf you want to see another Tidy Tuesday example, see one I did, as part of a previous version of MADA.\nFor even more examples of previous analyses, see this Shiny app which scans twitter for posts mentioning #tidytuesday. Unfortunately, it doesn‚Äôt seem to be updated/maintained anymore. But you can still find people on social media posting links to their Tidy Tuesday creations (feel free to share yours as well).\n\nApparently, there is now also a Tidy Tuesday podcast. I‚Äôm not familiar with it, so I‚Äôm not sure what it‚Äôs all about and if it is worth checking out. It probably is worth giving it a try, since it‚Äôs (almost) always good to check out new things üòÅ and those are very short recordings, so I think they just introduce the data of the week. If you end up listening to some, let me know what you think.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#your-tidy-tuesday-exercise",
    "href": "content/module-workflow/assessment-complete-workflow.html#your-tidy-tuesday-exercise",
    "title": "Assessment - Complete Workflow",
    "section": "Your Tidy Tuesday Exercise",
    "text": "Your Tidy Tuesday Exercise\nYour assignment is to participate in Tidy Tuesday by analyzing this week‚Äôs dataset. You can start as soon as the dataset is posted, which is Mondays. The datasets are released on GitHub. As I‚Äôm writing this, I have no idea what the data will be the week this exercise happens. That‚Äôs part of the fun of it üòÑ.\nHere are some more detailed instructions:\nUse your portfolio website. Make sure it‚Äôs up to date and fully synced. Open it in R Studio. Then open the tidytuesday-exercise.qmd file. That‚Äôs where you‚Äôll write your Tidy Tuesday analysis.\nGo to the TidyTuesday Github repository. Look for the dataset for this week, and read the instructions on how to get the data. You will also be provided with a data dictionary. If the data is available for download, place it somewhere in your portfolio repository (e.g., in a new folder called data). Remember the data limit for Git/GitHub, don‚Äôt place files that are too large into your repo!",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#what-you-need-to-include",
    "href": "content/module-workflow/assessment-complete-workflow.html#what-you-need-to-include",
    "title": "Assessment - Complete Workflow",
    "section": "What you need to include",
    "text": "What you need to include\nWrite a Quarto file (with R code as part of the Quarto file or in a separate file) that loads the data, performs any needed cleaning and wrangling, does some EDA, and fits some models. Here are the details:\n\nLoad, wrangle and explore the data. The EDA part involves making tables/figures to get an idea what your data is about. By now you know this is an iterative procedure, so it‚Äôs ok to have these parts of the process/code intertwined.\nOnce you understand the data sufficiently, formulate a question/hypothesis. This will determine your outcome of interest and, if applicable, main predictor(s) of interest. Since we don‚Äôt know the data yet, it might be that the question is a bit contrived and not actually too interesting, but I‚Äôm sure there will be more than one potentially reasonable question one can ask, no matter what the data will be.\nOnce you determine the question and thus your outcome and main predictors, further pre-process and clean the data as needed. Then split into train/test. (It might be that the data is too small for this split to make sense in real life but for this exercise, we‚Äôll just do it.)\nFit at least 3 different model types to the data using the tidymodels framework we practiced. Use the CV approach for model training/fitting. Explore the quality of each model by looking at performance, residuals, uncertainty, etc. All of this should still be evaluated using the training/CV data. You can of course recycle code from previous exercises, but I also encourage you to explore further, e.g.¬†try different ML models or use different metrics. You might have to do that anyway, depending on the data and your question/outcome.\nBased on the model evaluations and your scientific question/hypothesis, decide on one model you think is overall best. Explain why. It doesn‚Äôt have to be the model with the best performance. You make the choice, just explain why you picked the one you picked.\nAs a final, somewhat honest assessment of the quality of the model you chose, evaluate it (performance, residuals, uncertainty, etc.) on the test data. This is the only time you are allowed to touch the test data, and only once. Report model performance on the test data.\nSummarize everything you did and found in a discussion. Make sure you discuss your findings with regard to your original question/hypothesis. What did you learn? If feasible, show a summary figure or table that illustrates your main scientific finding from this analysis.\n\n\nRemember to add ample information/documentation in the form of both code comments and explanatory text You should provide a running commentary on what you do, why you do it, and how your R code accomplishes that (so that if you stare at it in a few weeks, you can remember what in the world you did back then üòÑ).",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#finish-up",
    "href": "content/module-workflow/assessment-complete-workflow.html#finish-up",
    "title": "Assessment - Complete Workflow",
    "section": "Finish up",
    "text": "Finish up\nOnce done with your Tidy Tuesday analysis, rebuild your portfolio site to make sure everything works and looks good, that all the links work, etc. Then push to GitHub by the deadline. Since this will be on your portfolio website, and I know where to find it, there is no need to post any link.\nAnd you just completed all the exercises for MADA! Now would also be a good time to check all of your portfolio page and further clean up/improve. This website is a nice showcase of what you‚Äôve done and learned in this course, and something you can show to others (future employers, etc.). So I suggest you make sure it looks reasonably good and presentable. Here are some specific steps you should check.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#connecting-website-and-file-repository",
    "href": "content/module-workflow/assessment-complete-workflow.html#connecting-website-and-file-repository",
    "title": "Assessment - Complete Workflow",
    "section": "Connecting website and file repository",
    "text": "Connecting website and file repository\nFirst, let‚Äôs make sure it‚Äôs easy for people to go from your portfolio website (the github.io location) to your file repository (the github.com location) and back. To that end, open the _quarto.yml file. At the bottom, it says URL-TO-THIS-REPOSITORY-HERE. Replace that with the URL to your Github repository. As an example, for the MADA course, the URL one would put in there is https://github.com/andreashandel/MADAcourse (while the website lives at https://andreashandel.github.io/MADAcourse/).\nOnce you done that, when people now are on your website, they can click on the Github icon in the top right corner and be taken to your file repository. Try to make sure it works by rebuilding your website and pushing to Github. It might take a minute or so and you might need to hit refresh before it shows up on the website.\nNow, let‚Äôs connect the two in reverse. Open Readme.md and update the text. This is what people see when they come to your github repository for your portfolio (as opposed to the website). I recommend adding a little bit of text and a link pointing users to the actual website. You can look at the Readme.md for this course as an example. Edit yours as you want.\nYou might also want to point to your website in the top right area of your repository on Github.com. If you go to your repository on GitHub, you should see an About section in the top right. If you click on the gear symbol, there is a field in which you can enter the URL for your website (the github.io location). It might already show as pre-populated, but you have to actually manually enter it before it will show. You can again see how that looks on the repository for this course.\nWith this, it is easy for anyone (including yourself) to quickly switch between the website and the file repository.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#adding-more-content",
    "href": "content/module-workflow/assessment-complete-workflow.html#adding-more-content",
    "title": "Assessment - Complete Workflow",
    "section": "Adding more content",
    "text": "Adding more content\nYou‚Äôve added most ‚Äúproducts‚Äù you generated as part of htis class to the portfolio website. If you have created other noteworthy products, either as part of this course or outside, feel free to add them to your website. By now you should know how to do that. For instance, if you want to add your project, either now or once its done, feel free to do so. It probably doesn‚Äôt make sense to put the whole project content inside the portfolio. Instead, take the main product (e.g., the manuscript file), render it to html, and add it to the website. Or create a new project.qmd file where you briefly describe what you did and show a few highlights, and then provide a link to your main project repository for those interested in looking at the whole thing.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#cleanup-and-styling",
    "href": "content/module-workflow/assessment-complete-workflow.html#cleanup-and-styling",
    "title": "Assessment - Complete Workflow",
    "section": "Cleanup and Styling",
    "text": "Cleanup and Styling\nAt this point, it might be worth revisiting your already posted pages and making sure everything looks as nice and professional as possible.\nWhile the way we built the website has only limited ways of styling things (unless you want to start changing CSS and HTML code), you can still customize some. Feel free to play around and customize the look. You can find a good bit of information in the Quarto documentation. If you look into the repository for the MADA course, you will also see that I‚Äôm using my own CSS file (called MADAstyle.scss). SCSS/CSS lets you style websites. It‚Äôs not hard to write CSS code, but it is its own thing. I usually just search online to find what I‚Äôm looking for üòÅ. You certainly don‚Äôt have to, but if you want to further customize the look of your website, you can add your own CSS file and style it how you like.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#more-comments",
    "href": "content/module-workflow/assessment-complete-workflow.html#more-comments",
    "title": "Assessment - Complete Workflow",
    "section": "More Comments",
    "text": "More Comments\nFuture employers really do look at portfolios like this, so being able to showcase something nice and polished is useful. It is also part of having a good online presence. I think for (future) professionals like yourself, a solid online presence is vital. I have in the past discussed this with our grad students in another class, if you want to see my thoughts on that, you can check out my presentation on building your brand ‚Äì which is of course made with R Markdown and posted to a Quarto based website üòÑ.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-overview.html",
    "href": "content/module-synthetic-data/synthetic-data-overview.html",
    "title": "Overview of the synthetic data content",
    "section": "",
    "text": "Overview\nThis module covers why and how to generate and use synthetic (also known as simulated/artificial/fake) data.\n\n\nLearning Objectives\n\nKnow about reasons why one might want to use simulated data.\nBe familiar with ways to generate synthetic data.\nBe aware of several R packages that can be used to generate synthetic data.\nKnow how to use synthetic data for testing.\nKnow how to use synthetic data with AI tools.\n\n\n\nContent\nThe module consists of the following units:\n\nIntroduction and Motivation\nCreating new data\nCreating new data based on existing data\nScrambling existing data\nR packages for data creation\nUse synthetic data with AI tools\nUse synthetic data to test models",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Synthetic Data Overview"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html",
    "title": "Generating synthetic data based on existing data",
    "section": "",
    "text": "In this unit, we discuss how to generate synthetic data based on existing data.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data based on existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#ai-help",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#ai-help",
    "title": "Generating synthetic data based on existing data",
    "section": "AI help",
    "text": "AI help\nYou probably don‚Äôt want to feed your real data to the AI. So the first part of summarizing existing data has to be done in a safe/secure space. Once you have good data summaries, you can use AI to generate synthetic data and code.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data based on existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#example",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#example",
    "title": "Generating synthetic data based on existing data",
    "section": "Example",
    "text": "Example\nWe‚Äôll use the data we generated in example 1 of the previous tutorial and now assume that this is ‚Äúreal‚Äù data and that we want to generate synthetic data that‚Äôs similar to the real data.\n\nSetup\nFirst, we do the usual setup steps of package loading and other housekeeping steps.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(here)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(gtsummary)\n\n\n# Set a seed for reproducibility\nset.seed(123)\n# Define the number of observations (patients) to generate\nn_patients &lt;- 100\n\n\n\nLoad and explore data\nIn a first step, we want to understand how each variable in the real data set is distributed, so we can create synthetic data that looks very similar.\nWe can use various helper functions from different packages to get good descriptive summaries of the data and variables. This code below shows two such helper functions.\nNote that we added more information to the table output. The table summary function treats the date variable and patient ID variable as numeric, so the output is somewhat nonsensical. But this is just for internal ‚Äúquick and dirty‚Äù use, so we don‚Äôt need to make things pretty. Of course you could if you needed/wanted.\n\ndat &lt;- readRDS(here::here(\"content/module-synthetic-data/synthetic-data-new-existing-r/syn_dat.rds\"))\nskimr::skim(dat)\n\n\nData summary\n\n\nName\ndat\n\n\nNumber of rows\n100\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n4\n6\n0\n2\n0\n\n\nTreatmentGroup\n0\n1\n1\n7\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nEnrollmentDate\n0\n1\n2022-01-08\n2022-12-30\n2022-06-25\n86\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPatientID\n0\n1\n50.50\n29.01\n1.0\n25.75\n50.50\n75.25\n100.0\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nAge\n0\n1\n45.90\n9.14\n21.9\n40.08\n45.60\n51.92\n66.9\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÇ\n\n\nBloodPressure\n0\n1\n127.96\n21.31\n91.3\n110.67\n130.85\n147.38\n159.5\n‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá\n\n\nCholesterol\n0\n1\n174.67\n32.58\n88.9\n153.23\n171.85\n196.62\n271.0\n‚ñÅ‚ñÜ‚ñá‚ñÉ‚ñÅ\n\n\nAdverseEvent\n0\n1\n0.29\n0.46\n0.0\n0.00\n0.00\n1.00\n1.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n\n\n\n\ngtsummary::tbl_summary(dat, statistic = list(\n  all_continuous() ~ \"{mean}/{median}/{min}/{max}/{sd}\",\n  all_categorical() ~ \"{n} / {N} ({p}%)\"\n),)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1001\n\n\n\n\nPatientID\n51/51/1/100/29\n\n\nAge\n46/46/22/67/9\n\n\nGender\n\n\n\n\n¬†¬†¬†¬†Female\n49 / 100 (49%)\n\n\n¬†¬†¬†¬†Male\n51 / 100 (51%)\n\n\nTreatmentGroup\n\n\n\n\n¬†¬†¬†¬†A\n43 / 100 (43%)\n\n\n¬†¬†¬†¬†B\n30 / 100 (30%)\n\n\n¬†¬†¬†¬†Placebo\n27 / 100 (27%)\n\n\nEnrollmentDate\n2022-06-30/2022-06-25/2022-01-08/2022-12-30/107.06744593852\n\n\nBloodPressure\n128/131/91/160/21\n\n\nCholesterol\n175/172/89/271/33\n\n\nAdverseEvent\n29 / 100 (29%)\n\n\n\n1 Mean/Median/Minimum/Maximum/SD; n / N (%)\n\n\n\n\n\n\n\n\nWe can also look at the distribution of the different variables individually, using e.g., base R commands (or any other package of your choice).\n\n# using base R to explore variable distributions\ntable(dat$Gender)\n\n\nFemale   Male \n    49     51 \n\ntable(dat$TreatmentGroup)\n\n\n      A       B Placebo \n     43      30      27 \n\ntable(dat$AdverseEvent)\n\n\n 0  1 \n71 29 \n\nhist(dat$Age)\n\n\n\n\n\n\n\nhist(dat$BloodPressure)\n\n\n\n\n\n\n\nhist(dat$Cholesterol)\n\n\n\n\n\n\n\n\nLooks like a normal distribution for age and uniform distribution for blood pressure should work well. That‚Äôs of course not surprising since we produced the data that way in an earlier tutorial. But for real data you don‚Äôt know what process produced it, you just want to see how things are distributed so you can recreate it that way.\nFor Cholesterol, the distribution doesn‚Äôt look quite normal. That‚Äôs because in the original data generating process, we made it dependent on treatment.\nIf you want to see if there are correlations in the data that you might want to also have in your synthetic data, you can explore those with tables or plots like these.\n\n# explore some correlations between variables\ntable(dat$AdverseEvent, dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 28 19      24\n  1 15 11       3\n\nplot(dat$Age, dat$BloodPressure)\n\n\n\n\n\n\n\nggplot(dat) + geom_histogram(aes(x = Cholesterol, fill = TreatmentGroup)) \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAt this stage, it is up to you to decide if you want to try to include correlations between variables that might or might not exist in the real data, or if you just want to give each variable an independent distribution.\n\n\nGenerate data\nNow we‚Äôll create synthetic data that is similar to the real data. In this code example, we pull directly from the actual data stored in dat. However, you can also save that information into an intermediary object or file (e.g., save the mean and standard deviation of age) and then just use those summary statistics to generate the synthetic data. This prevents for instance issues with confidentiality if you use AI to help write the synthetic data code.\n\n# Create an empty data frame with placeholders for variables\nsyn_dat &lt;- data.frame(\n  PatientID = numeric(n_patients),\n  Age = numeric(n_patients),\n  Gender = character(n_patients),\n  TreatmentGroup = character(n_patients),\n  EnrollmentDate = lubridate::as_date(character(n_patients)),\n  BloodPressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  AdverseEvent = integer(n_patients)\n)\n\n# Variable 1: Patient ID\n# can be exactly the same as the original\nsyn_dat$PatientID &lt;- 1:n_patients\n\n# Variable 2: Age (numeric variable)\n# creating normally distributed values with the mean and SD taken \n# from the real data\nsyn_dat$Age &lt;- round(rnorm(n_patients, mean = mean(dat$Age), sd = sd(dat$Age)), 1)\n\n# Variable 3: Gender (categorical variable)\n# create with probabilities based on real data distribution\nsyn_dat$Gender &lt;- sample(c(\"Male\", \"Female\"), \n                         n_patients, replace = TRUE, \n                         prob = as.numeric(table(dat$Gender)/100))\n\n# Variable 4: Treatment Group (categorical variable)\n# create with probabilities based on real data distribution\nsyn_dat$TreatmentGroup &lt;- sample(c(\"A\", \"B\", \"Placebo\"), \n                                 n_patients, \n                                 replace = TRUE,\n                                 prob = as.numeric(table(dat$TreatmentGroup)/100))\n\n# Variable 5: Date of Enrollment (date variable)\n# use same start and end dates as real data\nsyn_dat$EnrollmentDate &lt;- lubridate::as_date(sample(seq(from = min(dat$EnrollmentDate), \n                                                        to = max(dat$EnrollmentDate), \n                                                        by = \"days\"), n_patients, replace = TRUE))\n\n# Variable 6: Blood Pressure (numeric variable)\n# use uniform distribution as indicated by histogram of real data\n# use same min and max values as real data\nsyn_dat$BloodPressure &lt;- round(runif(n_patients, \n                                     min = min(dat$BloodPressure), \n                                     max = max(dat$BloodPressure)), 1)\n\n# Variable 7: Cholesterol Level (numeric variable)\n# here, we re-create it based on the overall data distribution pattern\n# since the data didn't quite look like a normal distribution, \n# here we'll just use it as its own distribution and sample right from the data\n# note that this breaks the association with treatment group \n# for real data, we wouldn't know if there is any, but if we suspect, we could\n# generate data with and without such associations and explore its impact on model performance\nsyn_dat$Cholesterol &lt;- sample(dat$Cholesterol, \n                                    size = n_patients, \n                                    replace = TRUE)\n\n\n# Variable 8: Adverse Event (binary variable, 0 = No, 1 = Yes)\n# we implement this variable by taking into account different probabilities stratified by treatment\nprobA = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,1])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,1])\nprobB = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,2])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,2]) \nprobP = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,3])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,3])\n\n# this re-creates the correlation we find between those two variables\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"A\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"A\"), replace = TRUE, prob = probA)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"B\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"B\"), replace = TRUE, prob = probB)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"Placebo\"), replace = TRUE, prob = probP)\n\nYou can always make your synthetic data partially different from the real data for important quantities (e.g., your main input/exposure or outcome of interest) to explore different implications and model performance.\n\n\nCheck and save data\nQuick peek at generated data to make sure things look ok, then we can save it.\n\n# Print the first few rows of the generated data\nhead(syn_dat)\n\n  PatientID  Age Gender TreatmentGroup EnrollmentDate BloodPressure Cholesterol\n1         1 36.0   Male              A     2022-06-26         114.3       151.7\n2         2 45.1 Female        Placebo     2022-04-13         152.3       120.1\n3         3 55.7   Male              A     2022-02-02          93.5       183.8\n4         4 44.6   Male              A     2022-08-25         107.5       148.7\n5         5 35.3   Male              B     2022-06-04         138.1       190.3\n6         6 38.4 Female              A     2022-12-23         106.7       142.6\n  AdverseEvent\n1            0\n2            0\n3            0\n4            1\n5            0\n6            1\n\n# quick summaries\nsummary(syn_dat)\n\n   PatientID           Age           Gender          TreatmentGroup    \n Min.   :  1.00   Min.   :16.40   Length:100         Length:100        \n 1st Qu.: 25.75   1st Qu.:40.10   Class :character   Class :character  \n Median : 50.50   Median :45.10   Mode  :character   Mode  :character  \n Mean   : 50.50   Mean   :45.83                                        \n 3rd Qu.: 75.25   3rd Qu.:50.33                                        \n Max.   :100.00   Max.   :65.50                                        \n EnrollmentDate       BloodPressure    Cholesterol     AdverseEvent \n Min.   :2022-01-15   Min.   : 92.7   Min.   : 97.6   Min.   :0.00  \n 1st Qu.:2022-04-12   1st Qu.:107.5   1st Qu.:158.0   1st Qu.:0.00  \n Median :2022-06-28   Median :126.9   Median :170.6   Median :0.00  \n Mean   :2022-06-27   Mean   :125.8   Mean   :175.4   Mean   :0.31  \n 3rd Qu.:2022-09-23   3rd Qu.:144.3   3rd Qu.:196.1   3rd Qu.:1.00  \n Max.   :2022-12-26   Max.   :159.0   Max.   :248.7   Max.   :1.00  \n\ndplyr::glimpse(syn_dat)  \n\nRows: 100\nColumns: 8\n$ PatientID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ Age            &lt;dbl&gt; 36.0, 45.1, 55.7, 44.6, 35.3, 38.4, 52.2, 43.0, 33.9, 4‚Ä¶\n$ Gender         &lt;chr&gt; \"Male\", \"Female\", \"Male\", \"Male\", \"Male\", \"Female\", \"Fe‚Ä¶\n$ TreatmentGroup &lt;chr&gt; \"A\", \"Placebo\", \"A\", \"A\", \"B\", \"A\", \"Placebo\", \"B\", \"A\"‚Ä¶\n$ EnrollmentDate &lt;date&gt; 2022-06-26, 2022-04-13, 2022-02-02, 2022-08-25, 2022-0‚Ä¶\n$ BloodPressure  &lt;dbl&gt; 114.3, 152.3, 93.5, 107.5, 138.1, 106.7, 113.0, 103.2, ‚Ä¶\n$ Cholesterol    &lt;dbl&gt; 151.7, 120.1, 183.8, 148.7, 190.3, 142.6, 151.7, 212.1,‚Ä¶\n$ AdverseEvent   &lt;int&gt; 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0‚Ä¶\n\n# Frequency table for adverse events stratified by treatment\ntable(syn_dat$AdverseEvent,syn_dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 34 12      23\n  1 16 12       3\n\n# ggplot2 boxplot for cholesterol by treatment group\nggplot(syn_dat, aes(x = TreatmentGroup, y = Cholesterol)) +\n  geom_boxplot() +\n  labs(x = \"Treatment Group\", y = \"Cholesterol Level\") +\n  theme_bw()\n\n\n\n\n\n\n\n# Save the simulated data to a CSV file\nwrite.csv(syn_dat, \"syn_dat_new.csv\", row.names = FALSE)",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data based on existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#summary",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#summary",
    "title": "Generating synthetic data based on existing data",
    "section": "Summary",
    "text": "Summary\nThe process of generating synthetic data based on existing data is fairly straightforward:\n\nLoad existing data.\nLook at distributions/frequencies of variables of interest.\nGenerate new data with variables that are distributed like original data.\n\nSometimes, you can just look at your original data and make up new data that‚Äôs roughly similar. At other times you want to be very close, in that case you need to either draw new values from distributions that are well-describe the original data, or you need to re-sample the original data (with or without replacement).\nFortunately, most of the time it‚Äôs good enough to get your data somewhat similar to the original. You would hope/expect any kind of statistical method to work well on different sets of data that are roughly similar. If that‚Äôs not the case, your method might not be very robust and might require changes.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data based on existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html",
    "title": "Why create and use synthetic data?",
    "section": "",
    "text": "In this unit, we discuss why you might want to create synthetic (simulated/artificial/fake) data as part of your analysis and what general approaches there are for doing so.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#knowing-the-truth",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#knowing-the-truth",
    "title": "Why create and use synthetic data?",
    "section": "Knowing the truth",
    "text": "Knowing the truth\nFor your simulated data, you know exactly how it is created (because you do it). Let‚Äôs say for your data analysis, you want to look at the relation between the daily dose of some statin drug a person takes, and if that has some association with their LDL cholesterol. Depending on the exact structure of your data, you might fit various simple or complex models and get some answers. You can ‚Äî and should! ‚Äî look carefully at the results to see if they make sense. But you never know for sure if the model ‚Äúgot it right‚Äù since you don‚Äôt know what the true (whatever that might mean) answer is. In contrast, if you created the data, you know exactly what the true answer should be. This allows you to test your models and see if they can recover what you know the truth to be. For instance, if you set up the data such that there is a linear relation between drug dose and outcome of interest, and you set the slope of this relation to some value, then when you fit a linear model you should expect to get that value back.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#no-problems-with-sharing",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#no-problems-with-sharing",
    "title": "Why create and use synthetic data?",
    "section": "No problems with sharing",
    "text": "No problems with sharing\nThere are often limitations (real or perceived) for sharing the actual data. An important reason might be confidentiality. This not a problem with simulated data. So if you want to be able to share your analysis with collaborators or readers when you publish, and for some reason you can‚Äôt share the actual data, you can share simulated data. This allows others to run all your code and reproduce your analysis workflow, even if they won‚Äôt get quite the same results since they are using simulated and not the real data. (In this case, you should also share the results you got on your simulated data, in addition to your real results, as a basis for comparison.)\nAnother important aspect of being able to share is the increasing use of AI models to help with data analysis. You can upload data to AI tools and request them to perform certain analyses and return the code that does the analysis. This can be very powerful and time-saving. However, you probably often do not want to give the AI your real data, as whatever you upload is often stored and re-used by the companies owning the AI tool (this is part of what you agree to when you click that terms and conditions check box). What you can do instead is feed the AI synthetic data, ask it to write code to do the analyses you are interested in, then take the code and with probably only a few modifications, apply it off-line on your real dataset.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#easier-to-play-around-with",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#easier-to-play-around-with",
    "title": "Why create and use synthetic data?",
    "section": "Easier to play around with",
    "text": "Easier to play around with\nOften, real-world datasets are very messy and require a good bit of time to clean them up (wrangle them) to get data in a form that allows analysis. If you are not even sure if your idea/analysis makes sense, it would be a waste to spend a lot of time cleaning data that ends up not being useful. What you can do instead is simulate data that has similar structure and content to the real data, but since you create it, you can make sure it‚Äôs clean and easy to work with. Then you try your analysis ideas on that simulated data. If it works ok, there‚Äôs a chance (unfortunately no guarantee!) that it might also work for the real data. As you play around with your simulated data, you might realize that certain ideas/approaches you initially had won‚Äôt work, and you can modify accordingly. Once your explorations using the simulated data suggest that your analysis plan can work, then you can start the often arduous task of cleaning and wrangling your real data.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#make-it-all-up",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#make-it-all-up",
    "title": "Why create and use synthetic data?",
    "section": "Make it all up",
    "text": "Make it all up\nCompletely making up data is the most flexible approach. You can try to produce synthetic versions of the full dataset, or just parts that are of interest. Let‚Äôs say in the statin-cholesterol example above, you real data set also contains information on the age and sex of the individuals, and if they have any pre-existing heart condition. Maybe in a first pass, you don‚Äôt want to look at those and just explore the main statin-cholesterol relation. Then you can just simulate data for that part. You can add further information ‚Äî including simulated characteristics that are not in the original data!‚Äì - as needed.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#summarize-the-original-data-then-re-create",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#summarize-the-original-data-then-re-create",
    "title": "Why create and use synthetic data?",
    "section": "Summarize the original data, then re-create",
    "text": "Summarize the original data, then re-create\nEven if you fully make up your synthetic data, you want it to somewhat resemble the original one. The easiest way is to just glimpse at the variables in the original data and generate new data that looks approximately similar.\nIf you want to be a bit more rigorous and get closer to the original data, you can also statistically summarize the original data, then use those summary distributions to generate new data. For instance, let‚Äôs say you find that the cholesterol variable can be well approximated with a normal distribution. If you fit the normal distribution to the data, you can get the mean and standard deviation. Then you can generate synthetic cholesterol data coming from a normal distribution with those estimated mean and standard deviation values.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#scramble-the-data",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#scramble-the-data",
    "title": "Why create and use synthetic data?",
    "section": "Scramble the data",
    "text": "Scramble the data\nIf you already have the data in a clean form that you can work with, you can use that data and scramble things to make new data. Basically you can randomly re-arrange variable values for different individuals, such that the new data does not correspond to the real data anymore, but has the overall same structure, and the same values (just re-arranged between individuals).\nFor instance you can take the age of each person, and randomly re-assign it to someone else. Note that this breaks potential patterns. For instance if in the original data, there happen to be an association between cholesterol and age, once you re-shuffled age values, this pattern will change. So the results you get from scrambled data will possibly be different, but since it has exactly the same structure and the same values as the original data, your whole analysis pipeline should work on the scrambled data. Of course, since you didn‚Äôt create the data, you don‚Äôt know the ‚Äútruth‚Äù and as such can‚Äôt assess fully if your analysis gives you the right results.\nIf you have already done your analysis and want/need scrambled data for sharing, e.g.¬†as part of a submission to a journal where you can‚Äôt submit the original data, you can try to do the reshuffling in a way that preserves patterns you identified. For instance if you saw that age was correlated with LDL, you can do your reshuffle such that age and LDL pairs stay together, while other variables (e.g., sex) get randomly reassigned. Of course you need to change at least some values for each individual, you can‚Äôt just move all variables for an individual from one row to another, it will still be the same data for that person, thus associated with potential confidentiality issues.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#do-a-mix",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#do-a-mix",
    "title": "Why create and use synthetic data?",
    "section": "Do a mix",
    "text": "Do a mix\nIt is of course entirely possible to combine the two approaches above. For instance, you can start with the original data and do some scrambling if needed. Then you can replace some variables with the simulated data you generated. This allows you to test your analysis more thoroughly since now you know what you put in, so you can check if you get that out.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "",
    "text": "In this unit, we explore the use of LLM AI tools to generate synthetic data with R.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Use synthetic data with AI tools"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#a-simple-example",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#a-simple-example",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "A simple example",
    "text": "A simple example\nUsing AI to generate code that creates synthetic data can be very efficient. Here is an example. I typed the following prompt into ChatGPT 3.5.\n\nWrite R code that generates a dataset of 100 individuals with ages from 18 to 49, BMI values from 15 to 40 and smoking status as yes or no. Assume that age and BMI are uncorrelated. Assume that smokers have a somewhat lower BMI. Then use the patchwork R package to generate a panel of ggplot2 plots. The first panel should show a violin plot with BMI on the y-axis and smoking status on the x-axis. The second panel should show a scatterplot with age on the y-axis and BMI on the x-axis. Add thorough documentation to your code.\n\nOnly the first part is really the data generation bit, but since it‚Äôs so quick and easy, I wanted some code that also explores the data so I can see if what it produced is what I had in mind.\nThe result was this fully-working code.\nSince the AI systems are continuously updated, it is possible that if you type the same prompt into ChatGPT, your code might look slightly differently, but hopefully it is still working.\nIn my experience, you usually don‚Äôt quite get what you want on the first try. But it‚Äôs easy to tell the AI to update the code until it does what you want it to, or at least is close and then you can do the rest by hand.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Use synthetic data with AI tools"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#one-more-example",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#one-more-example",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "One more example",
    "text": "One more example\nIt is possible to ask the AI to create more complex data. You might not always get exactly what you want, but it‚Äôs often worth a try. I gave this prompt to Mcirosoft Bing/Copilot in precise mode.\n\nWrite R code that creates data for N=100 individuals. Individuals ages are randomly distributed between 18 and 50 years. Assume that individuals belong to 3 treatment groups: placebo, low dose, and high dose. Individuals in each group receive either no drug, or 100mg of drug or 200mg of drug every week. Drug concentration is measured every other day. Assume that drug concentration follows an exponential decline between doses, with the decay rate being the same for the low dose and high dose groups.\nIn addition to drug concentration, cholesterol levels are measured daily. Assume that higher drug concentrations correlate with lower cholesterol levels.\nCreate a data frame that contains Patient ID, treatment group, age, time, drug concentration, and cholesterol level.\nFor function calls, specify explicitly the package from which the function comes from.\nAdd thorough documentation to the code so it is clear what each line of code does.\n\nThe code I got back did not quite work, so I provided this prompt:\n\nThe code above does not work. It gives this error :\nError in map(): In index: 1. With name: ID. Caused by error in patient$ID:\nPlease fix the code. Also add code at the end that plots the raw data for drug concentration as a function of time for all patients, stratified by treatment status. Also plot the mean for each group.\n\nThis is the code I got back.\nThe code runs and does some of the things I asked for, but not all. I specified that the drug is given weekly, but the code only contains a single dose at the start. I forgot to tell the AI for how long the data should be simulated. It picked 13 days, which means there should have been 2 doses of drug.\nOf course I could keep talking to the AI, telling it to update things such that dosing is weekly, maybe explicitly say that I want to run for 28 days, fix whatever else might not be quite right (I didn‚Äôt check the code too carefully). It‚Äôs so quick and easy, there‚Äôs no reason not to do a few more iterations and see how close the AI can get to what I want. Eventually, I will likely intervene and finish things off manually.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Use synthetic data with AI tools"
    ]
  },
  {
    "objectID": "content/module-stats-overview/statistical-analysis-introduction.html",
    "href": "content/module-stats-overview/statistical-analysis-introduction.html",
    "title": "Statistical Analysis Overview",
    "section": "",
    "text": "Overview\nIn this Unit, we will take another look at the whole data analysis process. Here, we focus on the actual (statistical) analysis (which I also call model fitting) component of the data analysis process. A general conceptual understanding is useful before we jump into looking at and applying specific modeling/statistical approaches.\n\n\n\nSource: VALDO.com\n\n\n\n\nLearning Objectives\n\nBecome familiar with different types of analyses.\nLearn how the type of data determines the modeling approach and be able to choose the right methods.\nUnderstand the overall goals of fitting statistical models to data.\n\n\n\nIntroduction\nSo far in this course, you briefly encountered a few simple statistical models in some of the readings and exercises (e.g., applying a linear model), but we didn‚Äôt focus on it. Instead, we looked at a lot of other components that are important for a full data analysis project but are less often taught. You might have come across the statement (e.g., in R4DS) that 80% of the time spent on data analysis is in the non-modeling/non-statistical parts. From my experience, that is true. While the statistical modeling/analysis part is certainly important, it often takes a fairly small amount of the whole project time. And while it is certainly possible to use the wrong statistical model, it seems to me that the most important and consequential mistakes that might invalidate an analysis do not happen in the modeling part. Sure, people often use the wrong statistical method, but that has - in my experience - often surprisingly (and thankfully!) little impact on the results. Not properly cleaning and processing data (e.g., not realizing that weight is reported in pounds and not kilograms when computing BMI) often has a much bigger impact on results.\nNo matter what statistical analysis you end up using, you will need to do the steps of getting and cleaning/processing/wrangling the data. During that process, you also explore your data, e.g., through plots and tables. Doing those steps efficiently and accurately is crucial. After you are done with the preliminary steps of getting/cleaning/wrangling data, you can move on to your main goal, fitting models to data. Once you reach the stage where you are ready to fit models, most of the hard work is done.\nOnce you reach the state at which your data is ready for statistical analysis, you should have a good idea of the types of models that might be appropriate. The choice of model is partly driven by the data, and partly by the kind of question you want to answer. There are several reasons why you might want to apply statistical models to your data, we‚Äôll discuss them briefly.\n\n\nWhy fit models to data\nThere are several reasons why we might want to fit models. The following is a brief list. The next section goes into more detail.\n\nTo produce or test hypotheses (e.g., ‚Äúthere is a (linear) correlation between BMI and diabetes‚Äù)\nTo estimate parameters (e.g.¬†‚Äúa 1 unit increase of particulate matter leads to 2 more asthma attacks per person per year‚Äù)\nTo draw causal conclusions (e.g.¬†‚Äútaking statins causes reduction in cholesterol‚Äù)\nTo draw mechanistic conclusions (e.g.¬†‚Äúinterferon reduces HCV virus load by stopping production of new virus‚Äù)\nTo make predictions (e.g.¬†‚Äúexposure to N cholera bacteria leads to an infection with probability p‚Äù)\n\nDepending on the objective, you will likely be using different statistical approaches. Note however that there is no clear mapping. For instance you can use a linear model to come up with hypotheses (i.e., do an exploratory analysis), to test hypotheses (e.g., in a clinical trial), to estimate parameters, or make predictions. A lot of models can be used for different purposes. This might be initially confusing. Just keep in mind that while some models are better for some purposes (e.g.¬†a complex machine learning or artificial intelligence model might be good for predictions, but bad for causal conclusions), most models can be applied to more than one type of objective.\n\n\nData Analysis Types\nThe following way of categorizing types of data analyses follows The Art of Data Science (ADS) ‚Äì specifically, chapter 3. Now might be a good time to give that chapter another quick re-read. A very similar, shortened version of the ADA chapter 3 discussion can be found in Jeff Leek and Roger Peng‚Äôs article. The following figure from their paper gives a good summary of what follows.\n\n\n\nSource: Leek and Peng 2015\n\n\nThe most straightforward analysis is a descriptive one. At that stage, you process, summarize and present the data, and do not go further. You don‚Äôt need to fit any statistical models. A lot of data collected by surveillance systems or government agencies falls into the descriptive category. For most scientific projects, we often start with a descriptive presentation. E.g. Table 1 in a study often describes and summarizes the data source. Note that some authors have the bad habit of including model fitting quantities, such as p-values, in a descriptive table. Measures that involve fitting a model (such as p-values), go beyond a descriptive analysis and should therefore generally not be in a descriptive table.\nSometimes, a descriptive study is interesting and sufficient by itself. But often, we then want to go beyond the descriptive presentation of data. The most common analysis approach is associative. Here, we are looking for associations (i.e., patterns) in the data. We are interested in seeing if patterns exist (e.g., if there is a correlation between age and speed of solving mathematical problems) and what the shape of the pattern is (e.g., linearly increasing/decreasing or non-monotone). This is also called correlation analysis.\nDepending on the way we came up with our question and the data, the results from such an associative analysis can be interpreted as an exploratory or hypothesis-generating approach, or an inferential or hypothesis-supporting approach. In general, if you asked the question/posed the hypothesis first, then went out and collected the data and analyzed it to see if your hypothesis holds, you can interpret your findings as supporting or refuting your hypothesis. If you had data that were not collected to answer your question specifically, and you analyzed the data to see if you can find some interesting patterns, then your findings should be interpreted as hypothesis-generating.\nFor both exploratory and inferential settings, you are usually interested in understanding how specific inputs/predictors affect the outcome(s) of interest. For that reason, you generally want to keep your models fairly simple and easy to interpret.\nEssentially all standard statistical tests and approaches you are likely already familiar with (e.g., various statistical tests like t-tests, simple linear regression, or simple classification such as logistic regression) fall into these categories of associative, exploratory, or inferential. We will cover some of those modeling approaches in future units.\nOften, we would like to go from association to causation, i.e., we would like to say that not only does an increase in X correlate with an increase in Y, but that X causes that increase in Y. There are two ways of doing so. One is to collect the data in the right way, namely using a randomized clinical trial or equivalent lab science approach where all contributing factors but the input of interest, X, are controlled. This way, we can say that a change in X directly causes a change in Y. If the data does not come from such a study design, methods of causal inference (which we won‚Äôt discuss in this course) can sometimes help in trying to determine causality.\nClassical statistical models can get us as far as determining potential causal relations. If we want to go even further and not only try to answer if X causes Y, but how X causes Y, we will need to employ studies or models that are mechanistic. Such models explicitly include postulated mechanisms, and by comparing such models to data, one can often determine which mechanisms are more plausible. Mechanistic models are also beyond what we cover in this course (if you are interested in those, I teach two courses on mechanistic modeling in infectious diseases üòÉ).\nWhile the main goal of science is generally understanding a system as well as possible, outside of science, other goals are often more important. In applied/industry/commerce settings, one often does not care too much if or how exactly certain inputs influence outcomes of interest. Instead, the main purpose is to try and predict future outcomes given a set of inputs. In this case, the interpretability of your model is not that important. Instead, a predictive modeling framework is more important. That‚Äôs where complex models, such as machine learning and artificial intelligence (neural net) approaches come into play. Those models are too complex to allow much interpretation and understanding of the system, but often these kinds of models are very good at prediction and real world performance (e.g., differentiating cats from dogs in images).\n\n\nExamples of data analysis types\nAn example of an inferential analysis might be the question which, if any, immunological markers (e.g., cytokines in blood) are most influential for a given clinical outcome. Building a simple model here, e.g.¬†a linear model if the outcome is continuous (e.g., blood sugar level) or a logistic model if the outcome is binary (e.g., heart attack within 5 years or not), allows us to quickly and transparently investigate how each variable in our model affects the outcome and which variables (immunological markers) might be important to study further.\nA good example of a prediction model is the monitoring of credit cards by the issuing companies, who try to predict fraudulent transactions. To that end, they feed all the data they can get about you into their models, and if something happens that is unusual, you might get flagged, and your card denied, or you will have to call to confirm. In this case, the interest is not too much on how exactly all these data points about you and your behavior lead to the prediction of legitimate vs.¬†fraudulent, only that the accuracy of those predictions is high. Because of this, in situations where prediction is important, models tend to be more complex, and one is willing to trade simplicity and interpretability of a model for increased predictive performance.\n\n\nModeling methods based on data\nWhile the goal of the analysis will guide you toward choosing a general type of modeling approach, the data usually dictate in more detail what kinds of models are suitable. The main determinant of the model type to use is the outcome(s) one wants to analyze.\nFirst, is there even an outcome? While the majority of datasets have an outcome(s) of interest, that is not always the case. Data without a clear outcome are sometimes called unlabeled. For instance, we might have collections of images of cell types, and our question is if these images cluster into specific types of cells - without knowledge of what those types might be. Another example is a scenario where we might have a large dataset of customers and lots of information about each customer. We might want to know if those customers can somehow be grouped based on the data we have about them (with the goal to design a marketing campaign directed at specific groups). Methods for those tasks are generally called clustering methods or unsupervised learning methods. Examples are k-means clustering, principal component analysis, and neural networks. (Note that some of these methods can also be used with data that include outcomes.)\nThe more common data structure is one with a specific outcome(s) of interest. This is also referred to as labeled data. Since labeled data is the most common, we focus on it in this course. In this case, we use approaches referred to as supervised learning methods. Those can be further divided based on the type of outcome. If the outcome is continuous (or can be treated as such), we use regression approaches, or if the outcome is categorical, we use classification approaches.\nYou are likely already familiar with some of these approaches. Most basic statistical tests are simple models for regression or classification, i.e., they try to detect patterns in data with quantitative or categorical outcomes. Some other statistical methods are generalized linear models (which include the basic linear and logistic models), generalized additive models, trees, support vector machines, neural nets, k-nearest neighbors, linear discriminant analysis, and a lot of further methods, many of which are variants of others. Some, but not all, of the more complex methods can be applied to both quantitative and categorical outcomes. We will cover a few of these methods later in the course.\nNote that there is, unfortunately, no one method that is universally best for all data/questions. Both the type of question and the details of the data will influence the model choices. Often, there are several models that would be reasonable choices for a given setting, and in such instances it is often worthwhile (and fairly easy) to explore multiple alternative models.\n\n\nFurther reading\nFor some additional (as well as overlapping) information to what I wrote, read Chapters 1 and 2.1. An Introduction to Statistical Learning (ILS). You don‚Äôt need to work through it in detail and can skip over the math bits if you want to. But do try to get an overall idea of the concepts these chapters are trying to convey. Chapter 1 of HMLR is another good source that you should skim through. Again, try to get the main points (which will of course overlap with the text above and the other readings).\nThis recent paper A practical guide to selecting models for exploration, inference, and prediction in ecology provides a nice discussion of different modeling goals and what approaches one should use. We have not yet discussed some of these approaches, but will soon. You could skim through the paper now, then revisit later once we covered more of the topics discussed in there.\n\n\n\nSource: VALDO.com\nSource: Leek and Peng 2015",
    "crumbs": [
      "Content",
      "7. Statistical Analysis Overview",
      "Statistical Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-stats-overview/assessment-statistical-analysis.html",
    "href": "content/module-stats-overview/assessment-statistical-analysis.html",
    "title": "Assessment - Statistical Analysis Overview",
    "section": "",
    "text": "Quiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\n\n\nExercise\nThere is no exercise this week. Focus on part 2 of the project.\n\n\nProject\nFinish (and make sure to push) part 2 of the project. There is no need to submit a link. Unless you renamed your repository, I know where to find it based on your part 1 submission.\n\n\nDiscussion\nFind a scientific paper of your choice online that does a data analysis. Explain what types of analyses the authors did (based on the categories described in this module). Also discuss what the (stated or not stated) objective of the paper/study was. Comment on if you think the authors performed the right analysis given their goals, and state your opinion if they interpreted/presented their results properly. If you think they didn‚Äôt do their analysis right or didn‚Äôt present/interpret their results correctly, make suggestions for what they should have done.\nYou don‚Äôt need to go into modeling details, a big picture description is enough. For instance if the authors clearly did a secondary analysis collected for a different purpose, but present their findings as if they showed causal relations, point out this mis-representation (there‚Äôs no shortage of studies like that).\nPost your findings and commentary by Wednesday. Then read through and comment on some of the studies your classmates posted.",
    "crumbs": [
      "Assessments",
      "7. Statistical Analysis Overview"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-software-management/ready-software-management.html",
    "href": "content/module-ready-workflow/ready-software-management/ready-software-management.html",
    "title": "Software management",
    "section": "",
    "text": "For this unit, we will discuss ways to make sure your software is managed in such a way that it allows future reproducibility.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Software management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-software-management/ready-software-management.html#r-package-renv",
    "href": "content/module-ready-workflow/ready-software-management/ready-software-management.html#r-package-renv",
    "title": "Software management",
    "section": "R package renv",
    "text": "R package renv\nThe renv package (renv stands for reproducible environments) is a tool that allows you to install all packages you use for a given project as part of that project. It records the packages and exactly which version of the package is used. If you then give your project to someone else, they can install these exact versions of the packages on their machine, and therefore ensure that they are using the same versions you did.\nNote that this only holds for R packages, not R itself. But again, R doesn‚Äôt change that rapidly, so even if you run something under a newer or older version of R, most of the time it is not much of a problem.\nBecause reproducibility issues related to outdated packages are quite common, it is a good idea to use renv for all your data analysis projects. Once you got the hang of it, it is fairly easy to use. The renv website has a good tutorial.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Software management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-software-management/ready-software-management.html#renv-alternatives",
    "href": "content/module-ready-workflow/ready-software-management/ready-software-management.html#renv-alternatives",
    "title": "Software management",
    "section": "renv alternatives",
    "text": "renv alternatives\nThere is a package called packrat. It‚Äôs basically the older version of renv and not as good. There is also a package called capsule, which is supposed to be an renv alternative. I have no experience with it.\nPosit (formerly R Studio) has the Posit package manager, which is an alternative to renv. It‚Äôs more powerful, but also more complex and not free.\nThere are further, more specialized options. For instance the company Metrum Research Group has a tool called the Metrum Package Network. It is also fairly complex and not recommended for individual use.\nBasically, at the time of this writing, renv is pretty much the only useful option for individuals working on projects and wanting to manage R packages.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Software management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-overview.html",
    "href": "content/module-ready-workflow/ready-overview.html",
    "title": "Reproducible, efficient, automated, and documented (READy) workflows",
    "section": "",
    "text": "Overview\nThis module covers different aspects and topics related to data analysis projects done in a reproducible, efficient, automated, and documented (READy) manner.\n\n\nLearning Objectives\n\nKnow different components of a reproducible, efficient, automated, and documented (READy) workflow and approaches to implementing it.\n\n\n\nContent\nThe module consists of the following units:\n\nIntroduction to READy\nProject organization\nProject management\nSoftware management\nProject documentation\nTools for a READy workflow\nOpen research",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "READy workflows overview"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-open/ready-open.html",
    "href": "content/module-ready-workflow/ready-open/ready-open.html",
    "title": "Open Research",
    "section": "",
    "text": "Overview\nThis unit provides an introduction to the concept of open research/science.\n\n\nLearning Objectives\n\nUnderstand the general idea of open science\nExplain what makes research reproducible and/or replicable\nKnow what FAIR data standards are\n\n\n\nIntroduction\nWhile READy principles and openness are not necessarily the same, they are often connected.\nThe idea is that if you provide others access to your data, code and other components of analysis - the open part - it is generally only useful if this is done in a READy framework. Otherwise, it might become close to impossible for others to reproduce and further build on your work.\n\n\nThe general idea of open science\nThere are many ways of defining and describing ‚Äúopen science‚Äù.\nIf you search online for ‚Äúwhat is open science,‚Äù you will likely find various conceptual diagrams and descriptions. The papers Open science saves lives: lessons from the COVID-19 pandemic and From Open Data to Open Science provide pretty good conceptual discussions of open science, but I think the best of schematic summary is from NASA.\n\n\n\nSource: NASA\n\n\nNASA defines three components that are central to open science.\n\nAccessible ‚Äì your research process and results need to be transparent. Open access, FAIR data, and code sharing all fall into this.\nReproducible ‚Äì other people should be able to get the same results as you, and you should strive to make reusable products.\nInclusive ‚Äì if your research is only accessible and reproducible for relatively well-off scientists at major institutions in the USA, it‚Äôs still not really open üôÇ.\n\nIf you want a formal definition of ‚Äúopen science,‚Äù here‚Äôs the one from the OSTP, which is pretty comprehensive:\n\n‚ÄúThe principle and practice of making research products and processes available to all, while respecting diverse cultures, maintaining security and privacy, and fostering collaborations, reproducibility, and equity.‚Äù\n\n\n\nFederal open science requirements\nWhile the concepts of reproducible research and open science have been around for a while, they have recently increased in prominence. Several US government organizations have said that 2023 is the Year of Open Science, and a recent Office of Science and Technology Policy memo has recommended that all federal agencies take serious steps towards open science practices (more on what exactly this means later).\nFederal agencies like the NIH have been requiring data sharing and public releases of papers (on PMC) for some time now, but these recommended measures are even stronger, and in my opinion, a very positive direction for science. It is quite likely that if you work with federal funding in the near future, you will be required to share your data and code publicly as soon as you decide to publish. The exact implementation of these protocols hasn‚Äôt been determined yet (nor has the degree to which these recommendations will be formally implemented), but the writing is on the wall, so to speak.\nIt is easier for you to get comfortable with reproducible research methods and open science now, rather than scrambling to learn it in the future!\n\n\nAccessibility and FAIR data\nOpen access publishing is a big topic, but for the purposes of this class I will just say that I think you should publish Open Access whenever possible. In the future, it seems likely that any federally funded projects in the US will require an open access fund as part of the budget, as publishing OA can often be prohibitively expensive. If you want to learn more, check out the Wikipedia page on Open Access, or if you really want to learn more, see Peter Suber‚Äôs book on OA.\nThe main purpose of this section is to talk about data sharing. Data sharing used to be a pretty contentious topic, but fortunately it is more normalized now than it has ever been before, and thanks to NIH (and other agency mandates), it will likely be a normal part of science before too long.\nOf course just dumping messy data without any documentation into a depository is not great for accessibility and reproducibility. As some of you noticed during this course, even data from the CDC or other reputable organizations is often very poorly formatted and does not have a lot of metadata or documentation to help you out. To somewhat solve this issue, the FAIR data standards was created. FAIR stands for:\n\nFindable: when someone goes to your paper or project, they should be able to easily get to the data source.\nAccessible: users need to be able to access the data, and data are stored in a way that users can feasibly get them.\nInteroperable: data should be in standardized formats, using standard vocabulary. You should be specific if your data is derived from another dataset or if the user will need other related datasets.\nReusable: data should be documented and version-controlled when you release it, with clear descriptions of what information is contained in the data, who can use the data, and for what purposes.\n\nThe FAIR standard was originally described in this 2016 publication. Another great resource is the GO FAIR initiative. It is worth taking a short look at both these resources.\nOf course dealing with things like data use agreements and individual privacy is paramount, so 100% achieving these goals all the time is not feasible. Sometimes you might need to use synthetic data based on your real data, or remove certain information from your data to protect privacy, or require a data use agreement before you can distribute the data to individuals. We should strive for data to be as FAIR as possible, but protecting individual privacy is equally important!\nIn general, a lot of authors still hide behind ‚ÄúI can‚Äôt share my data, it‚Äôs confidential‚Äù as an excuse for not having to share. Most of the time, it is possible to share de-identified data without confidentiality problems. Hopefully, the near future will make data sharing much more common and also easier.\n\n\nInclusivity\nThe topic of inclusivity in data science is heavily tied into the idea of data ethics ‚Äì critical thinking about the potential results and consequences of the products we create is crucial in data science. Many of you have some public health training, and are therefore likely familiar with the Belmont Report.\nRegardless of your scientific background, when working with data (especially human subjects data), the guiding principles of respect for persons, beneficence, and justice should be critical considerations for all of our research.\nFor some additional reading on inclusivity and data science ethics, see for instance these resources:\n\nElaine O Nsoesie, Sandro Galea, Towards better Data Science to address racial bias and health equity, PNAS Nexus, Volume 1, Issue 3, July 2022, pgac120, https://doi.org/10.1093/pnasnexus/pgac120\nO‚ÄôNeil, Cathy. Weapons of math destruction. Crown, 2016. ISBN: 978-0553418811.\nNoble, Safiya Umoja. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press, 2018. ISBN: 978-1479837243.\nhttps://datapractices.org/manifesto/\nFloridi L, Taddeo M. What is data ethics?. Philos Trans A Math Phys Eng Sci. 2016;374(2083):20160360. doi:10.1098/rsta.2016.0360\n\n\n\nPractical considerations\nIt is all nice and good to tell people that their data needs to be FAIR, and that their science needs to be open. But the question is, why do it? Most people might agree that there are societal benefits. For instance having a resource like GenBank for genetic sequences has allowed many scientists beyond those who created the original sequence to use the data and answer important scientific questions. However, there generally need to also be benefits for individuals to entice them to spend time following Open Science standards. This is happening increasingly. On one hand, tools to do research in an Open Science framework are getting increasingly better. For instance the whole R + Quarto framework we‚Äôve been using in class makes it rather easy to do things automated and reproducible. Many other similar tools and resources are becoming available. Another potential benefit of Open Science is increased visibility. If others can use your data and models, they will likely cite your work. They might even want to collaborate with you. It seems for those individuals who are good at sharing, the benefits outweigh the perceived risks (such as ‚Äúbeing scooped‚Äù). And the final, ever increasing ‚Äúbenefit‚Äù is that publishers and funding agencies increasingly require it. So if you don‚Äôt follow Open Science standards, it might soon get hard to publish work or get funding. It‚Äôs less of a ‚Äúbenefit‚Äù and more of an ‚Äúavoid the penalties‚Äù thing, but regardless, it is an important practical consideration.\n\n\n\n\nSummary\nWhile open research is not the same as READy principles, there is a lot of overlap. With the increasing requirements for research (data as well as code) to be made openly accessible and FAIR, it is important to ensure your analyses are done in a way that it is easy to publicly share your results and for others to build on it. The READy principles will ensure this is the case.\n\n\nFurther Resources\n\nThe Center for Open Science, a nonprofit dedicated to promoting open science practices and community change, maintains a blog with several thought provoking readings.\nStatistician Andrew Gelman sometimes writes about open science, mostly from a statistical perspective and has some nice comments on replication. (1) (2)\nFOSTER is a European organization with a set of free courses on open science principles.\n\n\n\n\nSource: NASA",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Open research"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-documentation/ready-documentation.html",
    "href": "content/module-ready-workflow/ready-documentation/ready-documentation.html",
    "title": "Project documentation",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss why and how to document the different parts of your project.\n\n\nLearning Objectives\n\nKnow why documenting everything is essential.\nKnow ways to efficiently document different aspects of your project.\n\n\n\nIntroduction\nIf you do not explain the why, what and how of your project, it is essentially impossible for someone else to understand what‚Äôs going on. They might be able to technically reproduce what you did by running code in the order you specified, but they will unlikely be able to understand what‚Äô going on in enough detail to be useful to them.\nThus, any practically meaningful reproducibility attempt requires ample documentation.\n\n\nDocument your project, folder and files\nTogether with a good structure for your project, you should provide documentation to orient users. The easiest way is to have readme text files. Plain .txt text files or simple.md Markdown files are best. There should be one for the overall project, and then as suitable for individual folders. Those files don‚Äôt need to be too detailed, but should explain what is to be found where, and what different files contain/do.\nAn important bit is to keep those readme files updated. If your documentation talks about files and folders that don‚Äôt exist because you renamed or deleted them, it is confusing. So if you do major (or even minor) updates, take a few seconds to update your readme files too. Definitely make sure they are up to date before you share it with others, e.g.¬†collaborators or reviewers or readers.\n\n\nDocument your process\nIn addition to explaining your structure and the basics of your files, you should document what you are doing. This is in some sense the equivalent to a lab notebook. Some of this information might later make it into a finished product, but most of it might not. An ideal format for documenting is text that can be formatted somewhat, such as the markdown format. Quarto is excellent since it allows you to combine your code with documentation in a seamless manner.\nThis documentation should explain why you are doing what you are doing, and what you are thinking as you look at results, plots, etc.\n\n\nDocument your code\nThere is overlap between documenting your process and your code, but they are not the same. For instance you might write some commentary explaining that you deal with missing values by removing a few variables that contain mostly missing values, and for the remaining variables, remove any observations that has at least one missing. This would be the general explanation. Then you write code to do that. It is generally useful to document each line of code to explain what it does. This might seem like too much. But trust me, it helps. Code can get complicated, something that was obvious to you when you wrote it will not be in a few weeks, or will not be clear to someone else. Therefore, a good rule of thumb is that your code should be 50% or more comments. Each line of code can use a comment, and then have additional blocks of comment to explain what certain chunks of code are doing.\nThere is almost never too much documentation in code, but there certainly often is not enough.\nCommenting also helps you think clearly about what you are doing. If you end up with code that might not have enough comments, you could also try to ask some AI tool to add comments for you.\n\n\nSummary\nDocument everything. A lot. Others and your future self will thank you. Remember to documentation how your code works and also why you wrote the code you did.\n\n\nFurther Resources\n\nThe organization Write the Docs has a website with many resources, including a brief guide for beginners.\nGoogle‚Äôs developer documentation style guide is a very comprehensive free resource.\nYou might be interested in the idea of documentation driven development (DDD) (see i.a. this blog post), which is the idea that you should write down what your code will do, how a user will do each task, and how the code will internally do what it is supposed to before you even write the code down. It is a somewhat trendy philosophy that is maybe not 100% practical but will result in good (and often revised) documentation, by forcing you to do the thinking critically about how your code should work instead of just trying things.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project documentation"
    ]
  },
  {
    "objectID": "content/module-model-ml-2/more-ml-models.html",
    "href": "content/module-model-ml-2/more-ml-models.html",
    "title": "More Machine Learning Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover some further points related to ML that didn‚Äôt really fit into any other sections.\n\n\nLearning Objectives\n\nBe familiar with the idea of ensemble models.\nKnow about several unsupervised modeling approaches.\n\n\n\nEnsemble methods/models\nYou learned previously that one can combine many trees into a larger model (e.g., a random forest or a boosted regression trees model), and that those models often have better performance. In the many-tree units, I briefly mentioned that this is an example of an ensemble model.\nInstead of combining just the same kind of model (e.g., multiple trees), it is possible to build models which are combinations of different types of base models, e.g., combine a tree-based model with an SVM. Those approaches are known variously as ensemble methods/models or super learners or stacked methods/models. By combining different models in a smart way, it is often possible to increase performance beyond what can be achieved from any one individual model. The disadvantage is that fitting the model is more complex, takes more time, and results are even less interpretable (more black box) than any single model. And since each model has parameters that need tuning, more parameters means more data is needed for robust results. Nevertheless, if the only aspect that counts is predictive performance, and plenty of data is available, ensemble methods are often good choices.\nProperly fitting ensemble models is not easy, and requires usually a lot of data. In fact, so far I have never tried to fit an ensemble model for any of my research projects. Nevertheless, it is useful to know about them if you encounter them in the literature or if you have a problem/data where you think they might be helpful. For more on those models, check out the Stacked Models chapter of HMLR, the Ensembe Models chapter in TMWR and the stacks package, which integrates well with tidymodels.\n\n\nUnsupervised learning\nWhile I previously mentioned unsupervised learning here and there, we haven‚Äôt focused much on it in this course. The reason is that most data analysis problems deal with supervised learning, i.e.¬†with data that has a specific outcome of interest. However, there are situations where data without a given outcome needs to be analyzed. For instance images or customers might need to be sorted into categories. This analysis approach is also called clustering. Sometimes, unsupervised learning is also used as a preparatory step in a supervised learning setting. For instance it can be used to reduce the number of predictors. This is called dimension reduction. It is common in areas where one measures lots of variables but the observations are small, e.g.¬†genetic information on a few hundred individuals, with 1000s of genetic markers measured for each person. In such a case, one can reduce the number of predictor variables into a set of combinations of the original predictors such that the new set contains the most important information. Then one can use that reduced set to perform supervised learning.\nMethods like K-means clustering or Hierarchical clustering are ‚Äì as the name suggests ‚Äì used for clustering of unlabeled data. Partial least squares (PLS) and Principal component analysis (PCA) are methods for dimension reduction. Since for unsupervised learning, a performance measure like RMSE or Accuracy does not exist, other metrics are used to define the quality of model results. Different algorithms use different ways to perform the unsupervised learning task.\nThe Unsupervised Learning chapter of ISL discusses several unsupervised learning methods. So do the Dimension Reduction and Clustering sections of HMLR and the Clustering chapter of IDS. For an R implementation, check out the tidyclust package.\n\n\nFurther reading\nSee the references provided in the sections above, as well as the general references provided in the ML Introduction page.",
    "crumbs": [
      "Content",
      "12. Machine Learning Models II",
      "Some More ML models"
    ]
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html",
    "title": "Assessment - Machine Learning Models 2",
    "section": "",
    "text": "Quiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\n\n\nProject\nSubmission of part 4 is due. Make sure your updated project is on Github.\n\n\nDiscussion\nWrite a post in this week‚Äôs discussion channel that answers this question:\nWhich of the machine learning methods we covered (or any we didn‚Äôt cover but you are aware of) do you find most exciting/interesting and why? And which ML method seems the most confusing/pointless to you, and why? Are there any ML approaches you want to learn more about and need more resources beyond those provided on the website?\nPost by Wednesday, then reply to each other by Friday.\n\n\nExercise\nSince this week is another week with a project submission deadline, there‚Äôs no exercise. You can practice some of the models we covered this week in next week‚Äôs exercise.",
    "crumbs": [
      "Assessments",
      "12. Machine Learning Models II"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/ml-models-overview.html",
    "href": "content/module-model-ml-1/ml-models-overview.html",
    "title": "Machine Learning Models - Overview",
    "section": "",
    "text": "Overview\nIn this brief unit, I make a few general comments about machine learning models, before we look at several types of those models.\n\n\nLearning Objectives\n\nKnow what ML models are.\n\n\n\nIntroduction\nBy now, I am sure you have picked up on the fact that there is a whole zoo of different models out there one can use to analyze data. You have also likely picked up that the terminology is fuzzy. We could call a standard linear or logistic regression model a machine learning (ML) model, and people do. Or we can reserve the term ML for more complex models - however fuzzily we define ‚Äúcomplex‚Äù. In the end, getting hung up about terminology is not that useful and important.\nWhat is important is that you are at least a bit familiar with some of the more common models. Most of the time, you do not need to understand every detail of a model. You do however want to understand enough about how the model works, its strengths and limitations, to determine if a certain model is useful for a given situation or not. This way, when you read someone else‚Äôs modeling results, or try your own, you have an idea of what‚Äôs going in.\nWe will do a whirlwind tour of several classes of common models. There‚Äôs no way you can learn them in detail in the short amount of time we spend on them. However, you should be able to pick up enough about a type of model such that if you happen to work on a problem where you think a certain kind of model might be useful, you will then want to revisit that model and learn more.\n\n\nGeneral ML considerations\n\nI want to make an important point here: The statistics/model fitting topics we have discussed so far, namely pre-processing data, choosing the right performance metric, using methods like train/test and cross-validation to minimize overfitting, and carefully evaluating your model results apply to pretty much any data analysis project, no matter if you fit a simple linear model or a complicated neural net.\n\nMost models that are considered ML models tend to be more complex than the GLM type models we discussed so far. More complexity usually means more predictive performance, but also an increased chance for overfitting and more difficulty understanding what is going on inside the model and how to interpret model results.\nML models often have parameters that need to be tuned as part of the fitting process. In a first pass, you can often do those tuning operations without knowing much about the models. Once you dig deeper, it is useful to understand enough about the model to get an idea of what tuning a specific parameter for a given model actually means. Either way, always critically evaluate what your models return. Just because complex models are often black boxes and it‚Äôs hard to understand everything that goes on inside doesn‚Äôt mean you can skip your critical thinking and accept as reasonable whatever the model gives you back.\nI think by now you have also picked up on the idea that there is no recipe for choosing a specific machine learning/statistical modeling approach. It depends on the data, the question, the overall goal, what others in the field are using, and potentially further factors. In general, the most thorough approach is to try both simple and complex models, and then decide based on model performance and other considerations such as simplicity, speed, usability, etc. on a specific model. There is always a level of subjectivity involved, i.e.¬†different analysts might favor different models. As long as the thought process behind choosing a specific model is well explained and justified, you should be allowed to choose the model that you think is overall best for a given situation. Since it is very easy to fit multiple different models and compare results, it is not a bad idea to do that. You can report the results form the main model you chose as the main findings, with results from other models as supplementary material.\nAnd with those general points out of the way, we‚Äôll look at several different statistical/ML models in the following units.\n\n\nFurther resources\nThe books we have been using throughout this course all cover many aspects of ML. Especially ISL, IDS and HMLR are very good starting points for learning more about different machine learning methods. I will point to specific chapters when we discuss specific models.\nThere are also tons of online resources on machine learning models, the quality varies widely, but it might be worth looking around a bit. TheInsaneApp.com‚Äôs 10 Most Used Machine Learning Algorithms in Python with Code is a nice overview. Note that at the time I‚Äôm writing this, when I did a brief read-through I noticed some inaccuracies. E.g., they claim that a logistic regression model predicts 0/1, which is not quite right, it predicts probability which then is usually converted to 0/1 by defining a threshold. In general, when you look at resources like that, they are rarely completely wrong but might occasionally not be fully accurate (I‚Äôm sure my course website is no different üôÑ). So it‚Äôs often good to cross-check with resources that are fairly certain to be right (e.g., textbooks or Wikipedia).\nIf you want to practice some more ML modeling using the tidymodels framework, check out this free online course by Julia Silge, one of the main tidymodels maintainers. It consists of 4 case studies that teach you both general ML ideas and how to do them with tidymodels. Another online course focusing on tidymodels is Allison Hill‚Äôs course.",
    "crumbs": [
      "Content",
      "11. Machine Learning Models I",
      "ML Models Overview"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/decision-trees.html",
    "href": "content/module-model-ml-1/decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Overview\nIn this unit, we will cover a class of models generally referred to as decision trees.\n\n\n\nAn interesting classification tree. Source: xkcd.\n\n\n\n\nLearning Objectives\n\nUnderstand what tree-based methods are and how to use them\nKnow the difference between regression and classification trees\nKnow when it might be good to use a tree-based method\nUnderstand the advantages and disadvantages of single tree model\n\n\n\nIntroduction\nSimple GLMs have the main limitation of capturing only overall smooth patterns relating inputs and outcomes. Higher order terms or interactions must be manually specified. The polynomial and spline models we discussed allow for more flexible patterns, at the cost of more potential overfitting and also reduced interpretability. Another class of models that allow non-linear relation between inputs and outcomes are based on decision trees. In general, a decision tree is a tree-like structure that helps you make decisions. In machine learning, there are specific methods to build those trees. Decision trees can be used for continuous outcomes, in which case they are called regression trees or for categorical outcomes, in which case they are called classification trees. The umbrella term Classification and regression trees - or CART for short is also often used to mean the same as decision trees.\nBasic tree models have the advantage of being fairly easy to understand and use, even by non-statisticians. Trees can deal with nonlinear relations between predictors and outcome, they easily accomodate more than two categories for classification, and they can handle predictors that have missing values.\n\n\nTree Examples\nThe following shows a tree from Afonso et al.¬†2012, which is a study led by one of my colleagues Mark Ebell to determine if there are simple decision rules that can determine if a person has an influenza infection or not.\n\n\n\nSource: Alfonso et al.¬†2012\n\n\nAs you can see, this is a very simple model that is very intuitive and could be easily understood by physicians and other laypeople. The tree is used here for classification, with a binary outcome (presence/absence of flu).\nThe next example is a tree used for a regression problem to predict/model a continuous outcome.\n\n\n\nSource: Hess et al.¬†1999\n\n\nIn this study, the authors tried to determine the survival time of patients who had cancers with certain characteristics. The tree was used to group individuals, and for each group the tree/model predicts median survival.\n\n\nBuilding trees\nThe way one computes a tree is a little bit similar in concept to forward selection in a linear model. We start by looking at each predictor. We take the tree, which, when the outcome is split at some value of that predictor, leads to the best (cross-validated) performance increase in the model (e.g., lowest SSR/RMSE or highest Accuracy/F1 score). Let‚Äôs say we want to predict BMI, and our performance measure is RMSE of predicted value from the true outcome. The null model using no predictors gives us some value for the RMSE. We then try each predictor (let‚Äôs say we have exercise, calories, age, sex) and find that if we use exercise and assign everyone who exercises more than 23min per day one BMI value, and everyone who exercises less than 23min another BMI value, we get the best reduction in RMSE we could get (across all predictors and possible cut-off values). This is our first split using the exercise predictor at 23min. We now have a tree with one root and 2 nodes (in this case, they are leaves, which is the name for the terminal nodes).\nWe now take each leaf and run through all predictors again including the one we just used and see which predictor split at some value will further improve model performance. Let‚Äôs say we find that for those exercising more than 23min, we can split by sex and get 2 new BMI values, one for males exercising &gt;23min and one for females exercising &gt;23min, which gives the maximum reduction in RMSE. Similarly, we find that for the &lt;23min exercise people, splitting at 1500 calories a day is best. We now have a tree with a root, 2 internal nodes, and 4 terminal nodes.\nWe keep building the tree in this way until some criterion is met (e.g., no more than n observations in a given leaf, no further increase in performance). Note that in this procedure, some predictors may never enter the model. In that way, a tree performs an automatic subset selection, i.e., the algorithm might decide to not include predictor variables it doesn‚Äôt find useful. Also, any predictor could be used more than once.\nAlgorithms that implement the tree building routine differ in their details, which is why you will see many different options in both mlr and caret. In general, if you have a good signal in your data, any algorithm should pick up on it.\nSince trees have a tendency to overfit, it is common to regularize them by including a penalty for tree size. For instance, if we were to minimize SSR/RMSE, we would add a penalty to the cost function, C, to get\n\\[C = SSR + \\lambda T, \\]\nwhere T is the number of leaves of the tree. More leaves means a larger tree, which is being penalized. The tuning parameter \\(\\lambda\\) needs to be determined using parameter tuning or model training. Reducing a tree in this way is also called pruning.\n\n\nFast and frugal trees\nWhile trees are fairly simple to understand, sometimes, especially if there are many branching points, a complex decision tree might still not be suitable in practice. Further, even with regularization, trees might overfit. There is a type of tree, called fast and frugal tree (FFT), which can potentially help with both aspects. The difference between a regular tree and a FFT is that for the FFT, at each split/decision point, at least one of the splits needs to end in a terminal node. Check e.g.¬†the example diagrams shown in the FFT Wikipedia article to see what this means. This constraint makes the trees often simpler and thus easier to implement in practice (e.g.¬†by a doctor) and they might also be more robust, i.e.¬†their performance on future data might be better than a larger tree. Disadvantages of FFT are that sometimes they might be too simple and thus not perform as well as a full tree, and they only work for binary outcomes.\nA very nice R package called FFTrees implements FFT (and automatically compares their performance to regular trees and some of the tree based methods discussed below). You can find more about this R package in the FFTrees documentation.\n\n\nAdvantages and disadvantages of tree models\nA great feature of trees is that they are relatively quick and easy to build and especially easy to understand. They can easily be communicated to and used by non-experts (e.g., doctors, other decision-makers). As such, trees (sometimes called decision diagrams or other names) are common in many fields. As mentioned, trees are also able to handle missing data in predictors, and they are often reasonably robust in the presence of collinear or near-zero-variance predictors since trees tend to use one of the variables and ignore the others. Tree models also tend to excel at inferring the presence of nonlinear interactions between variables. Often trees don‚Äôt need predictors to be standardized either.\nThe main disadvantage of trees is that they usually have reduced performance compared to other models. Thus, if a simple, interpretable model is the primary goal, trees are ideal. If instead, a high-performance predictive model is the goal, trees are rarely the best choice.\n\n\nFurther information\nR2D3‚Äôs excellent interactive tutorial gives a very nice, visual introduction to machine learning in general and trees in particular. It covers and nicely illustrates some of the topics discussed here, as well as topics discussed in previous units. In part 2 of the tutorial the authors discuss overfitting. I strongly recommend you check it out, even if you just skim through it. It is fun and informative!\nFor more on tree models, see the first part Tree-based Methods chapter of ISLR, the Decision Trees chapter of HMLR.\n\n\n\nAn interesting classification tree. Source: xkcd.\nSource: Alfonso et al.¬†2012\nSource: Hess et al.¬†1999",
    "crumbs": [
      "Content",
      "11. Machine Learning Models I",
      "Decision Trees"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-tips-guidance.html",
    "href": "content/module-intro-tools/tools-tips-guidance.html",
    "title": "Guidance and Tips for R/RStudio/GitHub",
    "section": "",
    "text": "This document is a collection of guides and tips related to getting started and using R, R Studio and GitHub, based on stumbling blocks that I have noticed students encounter somewhat regularly.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Guidance and Tips for R/RStudio/GitHub"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-tips-guidance.html#avoid-large-files",
    "href": "content/module-intro-tools/tools-tips-guidance.html#avoid-large-files",
    "title": "Guidance and Tips for R/RStudio/GitHub",
    "section": "Avoid large files",
    "text": "Avoid large files\nGitHub is not suited for tracking large files. If you try to push/pull files larger than say 20MB, things might not work. Therefore, don‚Äôt try to track large files with GitHub! Large files are the #1 reason students have problems with their GitHub repository!\nIf you do have large files for a project, there are a few options:\n\nShrink the large file outside of GitHub. For instance if you have raw data in CSV format, you can remove parts you don‚Äôt need for your project and save the rest as an Rds or other compressed format. That file might be small enough to be tracked by GitHub.\nYou can use Git LFS to track them. But that‚Äôs a bit more advanced.\nYou can place the large file into a special folder in your GitHub repository (e.g.¬†one called largefiles) and then add an entry to the ‚Äô.gitignore` file to tell GitHub to ignore this folder. The problem with that is that if someone else wants to work on your project, they won‚Äôt automatically have those large files. If the files are generated by your code (e.g., they are the result of running a simulation), they can just re-run your code and get themselves a local copy of these files. If that‚Äôs not possible, either because the files are input (such as data) or it takes too long to re-run the code, you will have to manually share these files/folder with them.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Guidance and Tips for R/RStudio/GitHub"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-tips-guidance.html#resolve-merge-conflicts",
    "href": "content/module-intro-tools/tools-tips-guidance.html#resolve-merge-conflicts",
    "title": "Guidance and Tips for R/RStudio/GitHub",
    "section": "Resolve merge conflicts",
    "text": "Resolve merge conflicts\nAt times, you will get merge conflicts. For instance you might have made changes to your local repository that you didn‚Äôt mean to. Or someone else (or you on a different computer) added changes to the Github.com repository and things don‚Äôt merge with your local repo. If you have some local updates that you want to keep, here is a method that often works.\nAt that stage, it gets a bit tricky. GitKraken has a good tool to help you resolve conflicts, and it works well for text files (code, Rmd, md, etc.). It doesn‚Äôt work well for other files (word, Excel, etc.). Start by using the GitKraken tools, compare files and decide which version of any conflicting ones to keep (or to combine things).\nIf the GitKraken tools don‚Äôt help, you can do some manual intervention as follows:\n\nMove the files you changed and that create the problems to some ‚Äúsafe‚Äù location (outside of your repository).\n\n2a. If you are using GitKraken and you haven‚Äôt committed your local changes, there is a red trash button symbol in the top left. If you click on it, you can discard your changes. once you have done that, you can sync with the remote. Your local and remote should be in sync again, and you should be able to move to step 3.\n2b. If you already committed your changes, you can right-click in GitKraken on the latest version on the remote commit (those symbols with comments in the main window). Then pick reset main/master to this commit and then choose a hard. This overwrites anything you‚Äôve done locally with the remote version on Github.com, so make sure any local changes you want to keep have been copied to a safe place (step 1). Your local and remote should be in sync again, and you should be able to move to step 3.\n\nOnce all is ok again and your local repository and the remote one are in sync, you can copy the files (or the content from those files) you placed in a safe location in step 1 back into your repository. Then continue with the usual work flow.\n\nIn general, to minimize conflicts, it is good to regularly create issues and push/pull. You should definitely do that any time you stop working on a project. But sometimes doing updates in-between is also good. It is better to change a few files and work on just one topic, then commit and push. After that, start the next topic.\nThis is also true if you work with someone else and send them your updates as pull requests. By breaking them up into smaller units, it is more likely that conflicts are avoided or localized.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Guidance and Tips for R/RStudio/GitHub"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-tips-guidance.html#start-over",
    "href": "content/module-intro-tools/tools-tips-guidance.html#start-over",
    "title": "Guidance and Tips for R/RStudio/GitHub",
    "section": "Start over",
    "text": "Start over\nSometimes the previous fixes doesn‚Äôt work. For instance if you tried to track large files, even though you were just told not to do so, you might end up in a situation where there‚Äôs no simple fix, other than to start over. Here is how you can do that:\n\nRename the repo on GitHub.com to something like myrepo_old. You can do that in the settings of your repository. Also give your local repo/folder the same name.\nCreate a new repo on GitHub.com with the name of the prior one. Clone this new repo to your local computer.\nCopy all the good stuff from your old repo into the new one. Make sure to not copy over the bits that cause the problem.\nPush the updated local repo to GitHub.com.\nOnce certain that everything works, delete the old repo both online and on your local computer. You might decide to keep it around for a while, since by creating a new repo you lost the history, so you can‚Äôt go back to prior versions in the new repo.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Guidance and Tips for R/RStudio/GitHub"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-randrstudio.html",
    "href": "content/module-intro-tools/tools-randrstudio.html",
    "title": "R and RStudio",
    "section": "",
    "text": "Overview\nThis is a very quick introduction to R and RStudio, to get you set up and running. We‚Äôll go deeper into R and coding later.\n\n\nLearning Objectives\n\nKnow some of the strengths and weaknesses of R.\nHave R and Rstudio up and running on your computer.\nKnow how to install and load R packages.\n\n\n\nAbout R\nLike every programming language, R has its advantages and disadvantages. Feel free to do a web search on that topic, and you will encounter tons of people with tons of opinions. Some of the features that are useful to us are:\n\nR is Open Source, FREE, and cross-platform.\nR is a ‚Äúhigh-level‚Äù programming language, relatively easy to learn (compared to Fortran, C, etc.).\nR comes with many integrated functions.\nR is great for statistics, data analysis, and graphics.\nThe R Community is very dynamic, helpful and welcoming.\nThrough R packages, it is easy to get lots of state-of-the-art algorithms.\nDocumentation and help files for R are generally good.\n\nWhile we use R in this course, it is not the only option to analyze data. Maybe the most similar to R, and widely used, is Python, which is also free. There is also commercial software that can be used to analyze data (e.g., Matlab, Mathematica, Tableau, SAS, SPSS). Other more general programming languages are suitable for certain types of analyses as well (e.g., C, Fortran, Perl, Java, Julia). Depending on your future needs or jobs, you might have to learn one or several of those additional languages. The good news is that even though those languages are all different, they all share general ways of thinking and structuring code. So once you understand a specific concept (e.g., variables, loops, branching statements or functions), it applies to all those languages. Thus, learning a new programming language is much easier once you already know one. And R is a good one to get started with.\n\n\nAbout RStudio\nRStudio is an integrated development environment (IDE) made by the folks from Posit (formerly the company was also named RStudio, but that changed recently). RStudio is separate from the R programming language. It basically wraps some useful tools around R to making writing and running R code much easier. RStudio is by far the most commonly used environment for folks who write R code, but it is not the only option. There are other IDEs available for R, most notably maybe VS Code. In this course, I assume you are using RStudio and instructions are specific for it. But if for some reason you don‚Äôt want to, you can use a different way to write and run R code. You just have to figure out some things on your own.\n\n\nInstalling R and RStudio\n\nIf you haven‚Äôt already, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version, at least version 4.1 or newer. I recommend you upgrade to the most recent version (as of this writing 4.2.2, but if there is a newer one available, use that one).\nOnce you have R installed, install the free version of RStudio Desktop. Get the most recent version. If you have an older one already installed, update it.\n\nInstalling R and RStudio should be fairly straightforward. If you want detailed instructions, go through this chapter of IDS. If things don‚Äôt work, ask for help on the discussion boards.\nI personally only have experience with Windows (and a little bit of Mac), but everything should work on all the standard operating systems (Windows, Mac, and even Linux).\n\n\nInstalling and loading R packages\nMost of the functionality and features in R come in the form of add-on packages. There are tens of thousands of packages available, some big, some small, some well documented, some not. We‚Äôll be using many different packages in this course. Of course, you are free to install and use any package you come across for any of the assignments.\nThe ‚Äúofficial‚Äù place for packages is the CRAN website. If you are interested in packages on a specific topic, the CRAN task views provide curated descriptions of packages sorted by topic.\nTo install a package from CRAN, go to the R prompt at the bottom left of your RStudio session and type install.packages(\"PACKAGENAME\"). The figure shows an example where I installed a package called learnr. Often, a package needs other packages to work (called dependencies), and they are installed automatically. It usually doesn‚Äôt matter if you use a single or double quotation mark around the name of the package. Note that R cares about capitalization, so you need to get the upper and lower case exactly right. Otherwise, it won‚Äôt work.\n\n\n\nInstalling an R package. Click on the image to enlarge it.\n\n\nTry installing a package yourself. Open RStudio. Then go to the R prompt (the &gt; symbol) in the lower-left corner and type\n\ninstall.packages('DSAIDE')\n\nThis installs a package that gives you access to various infectious disease simulation models. We won‚Äôt do anything with that package in this course, we just install it for practice. If you want to learn more about DSAIDE, take a look at the package website.\nIf this is the first time you are installing packages, you‚Äôll see that a lot of other packages are installed, too. You might get a message about Installing from source packages that need compilation. You should generally say No to this. If you are on a Windows computer, compilation requires you to have Rtools installed. It‚Äôs not a bad idea to install Rtools (if you do, make sure you pick the version that matches your R version.) But even then, or if you use a Mac or Linux (which have the equivalent of Rtools already pre-installed) sometimes the compilation doesn‚Äôt work. So if you have a choice, say No. (On some Mac/Linux setups, things happen automatically, then just let it run.)\nTo see which packages are needed by a specific package, e.g.¬†DSAIDE, and thus are being installed if not present, type tools::package_dependencies(\"DSAIDE\") into the R console. Of course it can be that those packages depend on other packages, so you end up installing even more. At some point, you‚Äôll have the most common packages all installed and installing new packages will lead to less overall installing. The package install process generally works well.\nIn RStudio, you can also install (and update/remove) packages by clicking on the Packages tab in the bottom right window.\nIt is very common these days for packages to be developed on GitHub. It is possible to install packages from Github directly. Those usually contain the latest version of the package, with features that might not be available yet on the CRAN website. Sometimes, in early development stages, a package is only on Github until the developer(s) feel it‚Äôs good enough for CRAN submission. So installing from Github gives you the latest. The downside is that packages under development can often be buggy and not working right. To install packages from Github, you need to install the remotes package and then use the install_github function. We won‚Äôt do that now, but it‚Äôs quite likely that at one point later in this course we will.\nYou only need to install a package once, unless you upgrade/re-install R. Once installed, you still need to load the package before you can use it. That has to happen every time you start a new R session. You do that using the library() command (an alternative is require() but I recommend library()). For instance to load the DSAIDE package, type\n\nlibrary('DSAIDE')\n\nYou should see a short message on the screen. Some packages show messages when you load them, and others don‚Äôt. In this case, the package tells you that you can start using it by typing dsaidemenu() into the R console. DSAIDE is a package I wrote that allows you to explore infectious disease models. We won‚Äôt use it in this class. I‚Äôm just using it as an example here since you can use the package without having to write code. Try it briefly, by typing the code below into the R console\n\ndsaidemenu()\n\nA menu should open in your browser, from which you can explore different models/apps. Once you are done with DSAIDE, close it.\nThis was a quick overview of R packages. We‚Äôll use a lot of them, so you‚Äôll get used to them rather quickly.\n\n\nNote about R packages\nThe quality of R packages varies. In general, if they are on CRAN or bioconductor, they passed some quality checks. That does however not mean that the functions do the right thing, just that they run. Other packages might be more experimental, and while they might work well, there might also be bugs. In general, packages that are used by many people, packages that involve people who work at R-centric companies (e.g., Posit), and packages that have many developers/contributors and are actively maintained are good signs that it‚Äôs a stable and reliable package. That said, there are many packages that are developed by a single person and are only available from GitHub, and they are still very good packages. Ideally, for a new package, test it and see if it does things stably and correctly. If yes, you can start using it. Just always carefully inspect the results you get to make sure things are reliable. If at some point, you work with R in a non-academic setting and you might use R packages for jobs that need to run reliably for many years to come, choosing packages might be a bit more tricky and require more thought. For an academic/research setting, it‚Äôs usually ok to use almost any package, as long as it seems reliable and works.\n\n\nLearning R Studio\nIf you are new to R and RStudio and want to learn a bit more, I suggest you skim through this chapter of IDS.\nWhile one can use R and do pretty much every task, including all the ones we cover in this class, without using RStudio. However, RStudio is very useful; it has a lot of features that make your R coding life easier. It has become pretty much the default integrated development environment (IDE) for R. Since RStudio has lots of features, it takes time to learn them. A good resource to learn more about RStudio are the RStudio Essentials collection of videos.\n\n\n\nInstalling an R package. Click on the image to enlarge it.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "R and Rstudio"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-github-r-workflow.html",
    "href": "content/module-intro-tools/tools-github-r-workflow.html",
    "title": "The GitHub and R Workflow",
    "section": "",
    "text": "Overview\nThis document gives a brief discussion of working with R projects through GitHub. There is more than one way to do this, they should be equivalent in most instances.\n\n\nLearning Objectives\n\nKnow how to get your R projects synced through GitHub.\n\n\n\nPrerequisites\nI assume you read through the Git/GitHub introduction and have created a GitHub account.\nI also assume that you are using the GitKraken client. If you don‚Äôt, you need to adjust things accordingly.\n\n\nApproach 1 - start on GitHub\n\nGitHub\nFor this approach, you start by creating your repository on GitHub. The most common place to create a repository is under your own user name/account. Note that it is also possible to create repositories inside an organization (if you are part of an organization). As an example, my Github user name is andreashandel and the name of my research group/organization is ahgroup. Depending on if I want a repository under my name or under the research group/organization, I‚Äôll go to https://github.com/andreashandel or https://github.com/ahgroup/).\nOnce you are in the location/account where you want the new repository, click on Repositories to see your list of repositories (which might initially be empty). Now create a new repository by clicking the green New button. A page should show up where you can specify details for your new repository, something like this:\n\nSpecify the repository owner. Make sure it‚Äôs the one you want, e.g., yourself or an organization you are part of. Pick a good name for your repository. Set the repository as private or public. In general, you should consider doing it public, unless there is an important reason to have it private (e.g.¬†confidential data). Note that if you set it to private, you might not be able to access it with the free version of GitKraken. Check the box Initialize this repository with a README. Set the .gitignore option to R. You can keep the license box unchecked, or choose a license for your page. Click ‚Äúcreate repository‚Äù.\n\n\nGitKraken\nOpen Gitkraken, under File choose Clone Repo go to Github.com, find the name of the repository you just created and clone it to some place on your local computer (i.e.¬†copy it from Github to your local computer.) E.g. if your repository was called mynewrepo, and you place it on your desktop under windows, you might choose C:\\Users\\yourname\\Desktop as the target. I recommend having a main folder for a specific organization or course or project, and into which you place all your repositories related to it (e.g.¬†place all your repositories for this course into a folder called MADA, such as C:\\Data\\GitHub\\MADA\\.)\nRemember to not place your repository in a location that is synced by other software such as DropBox or Onedrive. Also do not place GitHub repositories inside other GitHub repositories.\n\n\nRStudio\nOpen RStudio on your computer. Choose File, New Project, Existing Directory and find the directory/folder you just created on Github and copied to your local computer (e.g.¬†C:\\Users\\yourname\\Desktop\\mynewrepo). If things work, you should see in the bottom right window of RStudio (at least) 2 files, one with .Rproj ending and a README.md.\nClick to open the README.md file. It will only contain the name of your repository with a # sign in front. At some point, it is useful to add meaningful information. For now, just to practice, you can add whatever you want.\nIn the future, if you want to edit your project, click or double-click on the .Rproj file. You can try it. Close RStudio, click the .Rproj file and the same setup as before should show up. Using R projects has several advantages. One is that it makes how you write file paths easier, so while having an .Rproj file is not strictly required I recommend you do it that way.\nNow create your content, place any files you create inside your project directory or its sub-folders.\n\n\nSyncing\nOnce you are done editing your content (for now), go to Gitkraken (or your preferred Git client) and push your changes to Github.\nIf you use GitKraken, in the top right you should see a notification about changed files. Click view changes, then Stage all Changes. Write a commit message, the Commit the changes. In the main area of GitKraken, you should see that the repository ‚Äì usually main or master, unless you work on a branch ‚Äì shows the computer icon above the one with some random logo (or your picture). That means your local repository is ahead of the one on GitHub.com. To get them in sync, you click the Push button. If things work, the two symbols should now be in the same line.\nIf your code cannot sync you will likely receive an option from GitKraken to perform a force push. A force push will overwrite the remote repo with the local repo forcibly. This means that the remote will be updated with any changes that exist on your local computer however, if any additional changes have been made to the code since you began editing (i.e.¬†someone else has made a commit to the repo while you were working on it) they will be overwritten by the force push since they are not on your local repo. Thus, use with care.\nNow that everything is established, the workflow is simple, you just need to remember to go through all the steps.\n\nBefore working on a repo/project, open GitKraken to make sure the local and remote are in sync. If the remote is ahead (e.g.¬†because someone else worked on it, or you did on a different computer), do a Pull. If the local one is ahead, you don‚Äôt need to push (but you should have right after you finished your last local session).\nOpen your project in RStudio (assuming it is an RStudio project). Make any changes you want to make. Once done for the day, go back to GitKraken and stage, then commit your changes, then push.\n\n\n\n\nApproach 2 - start with an R Project\nAbove we created the repository on GitHub first, then cloned it to the local machine, then initialized it as an RStudio project. It is also possible to do this in reverse. If you want to start a Git repo this way, you need to install Git standalone, and it must be configured to work with third-party applications. You can read chapter 6 of happygitwithr for more detailed instructions.\n\nRStudio\nOpen RStudio. Choose File, New Project, New Directory and (usually) you‚Äôll choose New Project. Pick a project name and location. Check the Create a git repository box. This button might not appear if you only installed Gitkraken and not Git.\n\nCreate your content in the project folder.\n\n\nGitKraken\nOnce you finished working on your project for now, open GitKraken. Go to File -&gt; Init Repo and choose the folder where your project is in. You‚Äôll likely get a message about an initial commit that needs to be created. Agree to it. Your repository should show up in GitKraken. In the top right, look at and stage your changes, write a commit message, then commit. Next, press the Push button.\nAt this point, if you didn‚Äôt properly connect GitKraken and GitHub previously, you‚Äôll likely get an error message. Follow the error message and the connect Gitkraken to your Github account information to get it to work.\nYou‚Äôll see a message about no remote existing and if you want to add one. Say yes. A menu on the left should show up. Make sure the repository name is the same as your local repository/folder name. Then click the green button. If things worked, your local repository has been created on GitHub.com.\n\n\nSyncing\nOnce your repository exists both locally and on Github.com, the syncing process is exactly as described under Approach 1.\n\n\n\nMerge conflicts\nSooner or later, you will encounter a merge conflict. That happens if the same file(s) changed both locally and on GitHub.com. That can happen if multiple people work on the same files, or if you work on multiple computers and forget to always push/pull before and after you work on a project. GitKraken can help you resolve conflicts. Sometimes you have to do it manually. Moving the offending files out of your local repository, then doing the merge and after that adding the updates again can help. If you know that one source is the definite one, you can force a push or pull. A more advanced approach is to use branches.\nMore information on merge conflicts and how to deal with them using GitKraken can be found in this merge conflict blog post and merge conflict tutorial.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "GitHub and R Workflow"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-ai.html",
    "href": "content/module-intro-tools/tools-ai.html",
    "title": "AI Tools for Data Analysis",
    "section": "",
    "text": "Overview\nI started to write a unit on AI tools. Then it got too big üòÅ and I decided to make it its own module, which you can find towards the end of the content list sidebar.\nPlease go through the content of the AI module. You don‚Äôt need to read through all of it this week, but do so soon. I strongly encourage you to use AI throughout this course, and you will be explicitly asked to do so in some of the exercises.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "AI Tools"
    ]
  },
  {
    "objectID": "content/module-intro-tools/assessment-course-tools-introduction.html",
    "href": "content/module-intro-tools/assessment-course-tools-introduction.html",
    "title": "Assessment - Course and Tools Introduction",
    "section": "",
    "text": "Reminder\nEach module comes with assessments. Those are things you need to do that will be graded. The table on the Schedule page lists the assessments for each module.\nIf you haven‚Äôt done so, please read through the Assessments Overview page so you know how each type of assessment should be done and how grading works.\nEach module has an Assessments page, like this one, which describes what the assessments are and what you should do. Often, there is a good bit of information for the exercises.\n\n\nQuiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\nFor this first module, you have 2 submission attempts. This allows you to get used to the submission system.\n\n\nDiscussion\nThe discussion for this module is all about getting to know each other. For your original post, complete the Exercise below, then post the URL to the website you createed by doing the exercise below into the introductions Discord channel. Do so by Wednesday 5pm.\nFor your replies, visit some of your classmates websites and leave feedback as a reply to their initial post. Do so by the Friday deadline.\nI hope this will allow you to start getting to know each other, even if only virtually.\n\n\nExercise\nFor this exercise, you are asked to start using Github, R, RStudio, and Quarto to create a website which will become your portfolio for this class. You‚Äôll write and post a short personal introduction of yourself.\n\nWorkload Note\nFor those of you who are new to Github/R/Quarto, this first project will make you do a lot of things that you are not familiar with. I know that this will be time-consuming and also might feel a bit intimidating. It‚Äôs partly unavoidable and partly on purpose. You need to learn how to quickly get up to speed with all kinds of new tools that come your way. So practicing it is a good idea. You are welcome to draw on any sources for help that you want (online, classmates, instructor, etc.). I‚Äôm confident with a bit of trial and error you‚Äôll get it to work.\nIf you get stuck (quite possible), check the materials listed in the Resources section of the course, or use Google. Also use the discussion boards to ask questions. You may use any method to get help (Google, R4DS Slack channel, AI, colleagues/friends/relatives with R/Quarto/Markdown experience, etc.). Still, try to do as much as possible yourself. We‚Äôll use all the bits you are learning here repeatedly during this course.\n\n\nPre-requisites\nYou should gone through all content in this module and should be set up with all the tools we need, namely R, RStudio, GitHub and GitKraken (or your preferred Git client).\n\n\nGithub setup\nI made a template for you to get started. Go to the Online Portfolio Template. Make sure you are logged into GitHub, then click on Use this template. Create a new repository, call it yourname-MADA-portfolio (yourname should include both your first and last name, e.g.¬†andreashandel-MADA-portfolio.) Choose public repository, then create it.\nNext, clone the repository (which will be something like https://github.com/USERNAME/yourname-MADA-portfolio/) from Github.com to your local computer (using Gitkraken or whatever software/method you decided to use). I recommend that you make a folder somewhere on your computer (e.g., called MADA-course) and place all class-related repositories (of which this is the first one) into this folder. Remember that these GitHub repositories/folders should not be synced with some other software (e.g., Dropbox/OneDrive).\n\n\nQuick local website check\nOnce your repository is on your local computer, go into the folder and click on online_portfolio.Rproj, which should open RStudio. In the bottom right, you‚Äôll see a number of files and folders that are part of your repository/project. We‚Äôll edit a few of them as described below. The rest you can ignore for now, we‚Äôll come back to them in later exercises.\nIn the top right panel in RStudio, you should see a Build tab. Click on it. It should show a Render website button, click it. You will probably get complaints about missing packages. If that‚Äôs the case, install them. Once everything works and renders without problems, a local preview of your soon-to-be new website should open in your browser.\n\nIf for some reason, you can‚Äôt see the Build tab in the top-right corner of RStudio, you can alternatively click on the Terminal tab (next to the Console tab). Type quarto render into the terminal and this should build your website (the RStudio button is basically just calling that command.)\n\nIf you can‚Äôt get this far and can‚Äôt figure out what‚Äôs going wrong, ask for help on Discord. Once you got that far, let‚Äôs create some content.\n\n\nQuarto content creation\nFor this part of the task, the comprehensive Quarto guide and the Quarto section of the 2nd edition of R for Data Science will likely be useful.\nOpen index.qmd. This is the main landing page for your website. Change the text however you want to. What you are writing here is Markdown, so now would be a good time to check out some of the Markdown resources and play with them. You will also see at the top of your R Studio window two buttons Source and Visual. This allows you to switch between editing Markdown in plain format, and using a visual editor. It‚Äôs a good idea to switch back and forth to start to learn how certain graphical elements look like in plain text formatting. You‚Äôll likely be using a mix of visual and source view a lot.\nNext, open aboutme.qmd and write some text introducing yourself, as follows:\n\nTalk a bit about yourself, your background, training, research interests. Let me/us know what kind of statistics, programming, data analysis experience you already have. I‚Äôd also be curious to know what you most hope to learn in this course. And then tell us some quirky/interesting/curious thing about you.\nGive your text a nice structure with headings, subheadings, etc.\nInclude a picture of yourself. To do so, place the picture in the repository and link it from the Quarto file.\nFinally, include a link to a picture/video/website/etc. The linked material should be somehow related to data analysis. Find something that you think is really cool/interesting/weird/etc. Provide a few explanatory sentences.\nIf you want, feel free to get creative and include other things. You can play with Quarto if you wish to, e.g., you can try to include a table or a video or any other elments you want to explore.\n\nIf you make changes and want to see how they look, save your changes and rebuild the website. Load/refresh in your browser and it should show any changes.\n\nThis portfolio website and everything you put on there is public. So don‚Äôt put anything on there that you don‚Äôt want to be publicly known.\n\n\n\nWebsite control\nFor Quarto, a lot of the look/feel and structure of a website is controlled in what‚Äôs called a YAML file (it has the ending .yml). The main one is called _quarto.yml. Open it. You‚Äôll see that it contains a bunch of information about the website, including the title, the menu structure, and some styling information.\nMake the following changes:\n\nUpdate the title of your website.\nReplace the URL-TO-THIS-REPOSITORY-HERE placeholder text with the Github URL of the repository you created (e.g.¬†for me, that would be something like https://github.com/andreashandel/andreashandel-MADA-portfolio).\nIt is possible to change the look of your website. Full styling requires knowledge of CSS and HTML. We won‚Äôt get into this. But we‚Äôll explore a quick way to change the look. Replace theme with a different one. Many themes are available. Try some, see how things change. You can also change the highlight scheme. For further styling, you can put custom CSS code into the styles.css file. We won‚Äôt do this here, but feel free to explore on your own.\n\n\nYAML (which is the format of _quarto.yml) is VERY finicky when it comes to formatting. You need to have exactly the right number of empty spaces. If you ever change a YAML file and you get a weird error message when trying to rebuild your website, it‚Äôs likely because some minor formatting is wrong, e.g.¬†an empty space is missing or too much. Just keep that in mind. (It happens to me all the time.)\n\n\n\nSending updates to Github\nOnce you are happy with your updates, or if you want to take a break, make sure to push your updates to GitHub. First close RStudio. Then go to the program you use for Github (I will assume that it‚Äôs GitKraken. If you use something else the details might vary). In the top right of Gitkraken, there should be a notification about changed files. Click view changes, then Stage all changes. Write a commit message, commit the changes. You should see the main with the computer symbol moved above the one with some random logo. That means your local repository is ahead of the one on github.com. To get them in sync, you click the push button. If things work, the two symbols should now be in the same line.\n\nSometimes, there might be issues that prevent pushing/pulling, e.g., if someone else changed the same files you did. If you are sure your local version is the correct one, you can perform a force push. A force push will overwrite the remote repo with the local repo forcibly. This means that the remote will be updated with any changes that exist on your local computer. If any changes have been made to the code since you began editing (i.e.¬†someone else has made a commit to the repo while you were working on it) they will be destroyed by the force push since they are not on your local repo. For this assignment, you are the only person working on your introduction so it is unlikely you will encounter any issues with force push, but it is good to be aware of the function of the action. Use with care. It‚Äôs generally better to resolve merge conflicts.\n\n\n\nMaking the website public\nWe want to set things such that your repository can at the same time become a public website. Github allows you to do that with its Github Pages feature. To turn on the Github Pages feature, go to your repository on github.com (e.g.¬†something like https://github.com/andreashandel/andreashandel-MADA-portfolio). On the top of the website for your repository, you should see a Settings button. Click on it and then find the Pages section. In the Branch section, switch none to main and folder as /docs. Click Save.\nThis should now have turned on your website. You might have gotten a notice about the location of your new website. If yes, follow the link. If not, you can get there by going to a URL that has the form https://username.github.io/yourname-MADA-portfolio/ (e.g.¬†for me, that is https://andreashandel.github.io/andreashandel-MADA-portfolio/). It might take a few seconds before the page is available, if you get a ‚Äònot found‚Äô error, wait a bit and try again. If everything works, your website is now public!\n\n\nGithub pages\nWe used the Github pages feature to create the website. You might have noticed that the files for your new website live on the domain github.com, while your nice-looking website lives at the domain github.io. This is how Github Pages work. It take files from your repository and turns them into a website with a distinct URL. Those files can either be in the main repository folder or in a /docs subfolder (our setup). Your website URL and your repository URL are distinct. All github actions, like pushing, pulling, cloning, etc. are done with the repository on the github.com domain. The github.io domain is only there to display websites.\nAs an example, this course website is displayed at https://andreashandel.github.io/MADAcourse/ and all the files that make up the webpage are at (https://github.com/andreashandel/MADAcourse). Note the text at the bottom of the repository view. That text comes from the Readme.md file. Speaking of which‚Ä¶\n\n\nUpdate your repository Readme file\nRe-open your project with RStudio. Click to open the README.md file. It only contains a single sentence I placed there. This file will not be part of your website, but it is what people see if they land at your github repository (i.e.¬†the behind-the-scenes location). Add text to it that points users to the URL for your new website. (You‚Äôve done the reverse above, allowing users to quickly go from your website to your Github repository).\nMake any other edits you like.\nOnce done with edits, remember to re-build/re-render your website with Quarto (by clicking ‚ÄòRender Website‚Äô in the Build tab) and then committing/pushing your local changes to GitHub (using Gitkraken or another Git client).\n\nOnce your website is up and running, don‚Äôt forget to post the link into the introductions channel!\n\n\n\nGeneral Github workflow\nIn the future, if you work on repositories like this one (or any others), your workflow should be as follows:\n\nOpen GitKraken or your preferred Git client and make sure the local copy of your repository is up-to-date (this is important if you collaborate with someone or you are working on multiple computers). If the remote is ahead of the local, do a pull.\nOnce your local repo is up-to-date, open your project in RStudio by double-clicking the name of the project (ends in .Rproj). Make all the edits you like. Make sure to render your quarto files (or knit your Rmd files). Once you are done with edits, close Rstudio, go to Gitkraken, stage/commit/push your changes back to the remote server.\n\nYou will be using this workflow a lot.\n\n\nFurther information\nI wrote a tutorial a while back describing the setup with Github pages. It has some additional information, which you can check out on my website. Note that this tutorial still used R Markdown instead of Quarto. Some of the information still applies, other parts are different with Quarto.\nUsing Github pages makes it fairly easy to create websites. I currently host all my course websites in this way. What we created was a fairly basic page. It is easy with Quarto to create other, more full-featured websites. For instance my personal website uses Quarto. If you are interested in such a more full-featured approach, check out this part of the Quarto documentation. You can also find a few more links to other resources.",
    "crumbs": [
      "Assessments",
      "1. Course and Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html",
    "href": "content/module-data/data-types-basic/data-types-basic.html",
    "title": "Basic types of data",
    "section": "",
    "text": "For this unit, we will discuss basic data types that you will come across frequently.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Basic data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html#quantitative-variables",
    "href": "content/module-data/data-types-basic/data-types-basic.html#quantitative-variables",
    "title": "Basic types of data",
    "section": "Quantitative variables",
    "text": "Quantitative variables\nThis data type, sometimes called interval scale data, generally allows one to do certain mathematical operations, e.g., subtraction or addition. Some quantiative variables are also ratio scale data, which means they have a true zero, or lowest value ‚Äî for these data, multiplying, dividing, and taking ratios makes sense.\nDifferent subcategories exist:\n\nContinuous: Can, in principle, be any number. Examples are height, weight, age, etc.\nDiscrete: Can only take discrete (integer) values, e.g., the number of siblings a person has.\nFraction/Proportion: Continuous, but between 0-1.\nSometimes other special forms (e.g., only positive, only in some range).",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Basic data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html#qualitative-variables",
    "href": "content/module-data/data-types-basic/data-types-basic.html#qualitative-variables",
    "title": "Basic types of data",
    "section": "Qualitative variables",
    "text": "Qualitative variables\nBroadly speaking, qualitative variables are those that do not allow one to perform any mathematical operations such as subtraction or addition. Qualitative data which has no intrinsic order is also called nominal (scale) data. Types of such data are:\n\nDescriptive: e.g., free text data from participant interviews. This data typically has very different values for each individual and requires extra steps to aggregate into something that can be analyzed.\nCategorical: e.g., hair color, ethnicity. No ordering is possible. These are also called polytomous variables.\nDichotomous: A special and common case of categorical data is data with exactly 2 categories, e.g., yes/no, dead/alive, diseased/healthy, exposed/unexposed. These are also commonly called binary variables.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Basic data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html#ordinal-variables",
    "href": "content/module-data/data-types-basic/data-types-basic.html#ordinal-variables",
    "title": "Basic types of data",
    "section": "Ordinal variables",
    "text": "Ordinal variables\nThis data type is usually considered a type of categorical variable, but it is worth thinking about it as something on its own. Ordinal data fall in between being strictly quantitative or strictly qualitative. For instance, if a question asks a person to rank their level of a pain on a scale from 1-10, a 7 is clearly higher than a 6, and a 6 higher than a 5. But it‚Äôs unclear if the difference between 5 and 6 is the same as 6 and 7. Thus it is not clear if one can do operations like subtraction (to get a difference of 1 in each case). Another example is level of education, which a survey might collect in categories of ‚Äòno high school‚Äô, ‚Äòhigh school‚Äô, ‚Äòsome college‚Äô, ‚Äòcollege degree‚Äô, ‚Äògraduate degree‚Äô. We could code that with numbers 1-5, and in some sense these items are ordered, but it‚Äôs unclear if one is justified in considering the difference between ‚Äòhigh school (2)‚Äô and ‚Äòsome college (3)‚Äô the same as ‚Äòsome college (3)‚Äô and ‚Äòcollege degree (4)‚Äô. Typically ordinal data is more complicated to deal with than purely qualitative or quantitative data.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Basic data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html#footnotes",
    "href": "content/module-data/data-types-basic/data-types-basic.html#footnotes",
    "title": "Basic types of data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLogistic regression, which you might be familiar, is used for classification. However, the underlying model predicts a quantitative outcome (a value between 0 and 1 usually interpreted as a probability), which is then binned to make categorical predictions. This is true of many, but not all, classification approaches.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Basic data types"
    ]
  },
  {
    "objectID": "content/module-data/data-overview.html",
    "href": "content/module-data/data-overview.html",
    "title": "All about data",
    "section": "",
    "text": "Overview\nYou need data to do a data analysis ‚Äî surprise!\nThis module discusses data. What it is, where to get it, how to load and store it, and some other basics of that sort.\n\n\nLearning Objectives\n\nBe familiar with basic data types.\nBe aware of data structure.\nKnow how to work with various data formats in R.\n\n\n\nContent\nThe module consists of the following units:\n\nBasic data types\nComplex data types\nData storage formats\nData formats in R\nThe data generation process",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data Overview"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html",
    "href": "content/module-data/data-formats-r/data-formats-r.html",
    "title": "Data formats in R",
    "section": "",
    "text": "For this unit, we will briefly discuss different types of variables and data formats in R.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#charactersstrings",
    "href": "content/module-data/data-formats-r/data-formats-r.html#charactersstrings",
    "title": "Data formats in R",
    "section": "Characters/strings",
    "text": "Characters/strings\nA string is a collection of characters. You will often hear the labels ‚Äúcharacter‚Äù and ‚Äústring‚Äù used interchangeably, though strictly speaking, a string is a collection of characters. Everything can be encoded as a character string. Unfortunately, you cannot do a lot of analysis with strings. For instance, if you have the numbers 2 and 3, you can add and subtract. But if you code those numbers as characters ‚Äú2‚Äù and ‚Äú3‚Äù, you can‚Äôt do much with them. Thus, transforming characters into other, more useful categories (if applicable) is a common task. It is common that you read in some data and there is a variable which should be numeric, but some entries are not (e.g., the original spreadsheet shows something like ‚Äú&lt;10‚Äù). In this case, R reads all of these variables as characters. You then have to go in, clean the ‚Äú&lt;10‚Äù value, and convert the rest to numeric. Sometimes you do want to work with strings directly. There are many tools and packages in R that are helpful, including base R command. The stringr package is particularly useful.\nIf you do any kind of analysis of text, you‚Äôll be dealing with strings/characters. But even if you are not planning to analyze text, it is quite likely that you will need to work with strings at some point during a data analysis, even if it is only to find specific values, clean up variable names, etc. Thus, learning more about this topic is a good idea. A very powerful, and also very confusing way to deal with strings is to use what are called regular expressions (or regex for short). This concept applies to any programming language, not just R. Being at least somewhat familiar with the concept of regular expressions is useful. The rebus package is particularly useful for building regular expressions without knowing the highly specific syntax. However, learning regex applies to many different languages and is an incredibly powerful tool in your data science repertoire.\nIf you have no experience manipulating strings, I suggest you work through the Strings chapter (14) of R4DS, and do the exercises. The string processing chapter (25) of IDS contains further good material that is worth working through. Another good source is the Character Vectors chapter in the STAT 545 book by Jenny Bryan. Take a look at those various sources, decide which one is right for your level and go through some of them. And/or consult them as needed.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#factors",
    "href": "content/module-data/data-formats-r/data-formats-r.html#factors",
    "title": "Data formats in R",
    "section": "Factors",
    "text": "Factors\nFactors is the name that R gives to categorical variables. They can be ordered/ordinal or not. You need to make sure variables that should be coded as a factor are, and that those that shouldn‚Äôt be aren‚Äôt. For instance, you might have a variable with entries of 0, 1, and 2. Those could be numeric values, e.g., the number of siblings a person has. Or it could be a factor coding for 3 types of ethnicity (unordered), or 3 levels of socioeconomic status (ordered). You need to make sure it is coded as factor or numeric, based on what you know about the variable. An excellent package to work with factors is the forcats package.\nTo learn some more about factors, you might want to go through the Factors chapter of R4DS, and do the exercises. Factors are the most complicated basic data format, so it is worth spending extra time making sure you understand a bit how factors work.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#logicals",
    "href": "content/module-data/data-formats-r/data-formats-r.html#logicals",
    "title": "Data formats in R",
    "section": "Logicals",
    "text": "Logicals\nYou can think of a logical variable as a type of categorical variable with 2 categories, TRUE and FALSE. Alternatively, in R, 0 is interpreted as FALSE and 1 as TRUE (and vice versa). R will sometimes treat logicals as numbers, and sometimes as categorical variables, so it is often more straightforward to code these variables as either numeric or categorical to be safe. You will use those logical values often when checking your data, e.g., if you want to see if your variable x is greater than 5, then the R command x &gt; 5 will return either TRUE or FALSE, based on the value of x.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#numeric-value",
    "href": "content/module-data/data-formats-r/data-formats-r.html#numeric-value",
    "title": "Data formats in R",
    "section": "Numeric value",
    "text": "Numeric value\nNumeric values can be integers or general numeric values (double). You generally do not need to care too much how exactly your numeric values are coded. Often, you can treat integers as general numeric value. (In R, a general numeric variable is called double ‚Äì there is no single.) You might rarely come across a case where some analytic method or other bits of code requires integers to be specified as such. In R, you can use the as.integer() function to convert general numeric values to integers. You don‚Äôt really need any other special packages in R to deal with numeric values. Note that when you type an integer value, e.g.¬†x &lt;- 2, into R, this is considered numeric by default. If you want to make sure it is treated as integer, add an L, e.g.¬†x &lt;- 2L.\nOne issue to be careful about is if you do comparisons. For instance if x and y each contain some number and you expect it to be the same, but there was some rounding going on, it could be that if you do x == y you get a FALSE because you had x  = 2 and y = 1.99999997 - the latter not being exactly 2 because some rounding was happening. If this is the case, the safer approach is to check for abs(x - y) &lt; 1-e10 or some other small number; or alternatively you can use the shortcut all.equal(x, y). Basically you are looking if x and y are the same within some rounding error.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#dates-and-times",
    "href": "content/module-data/data-formats-r/data-formats-r.html#dates-and-times",
    "title": "Data formats in R",
    "section": "Dates and times",
    "text": "Dates and times\nWhile dates and times are a type of continuous numeric variable, you should assign the date class explicitly in R, which allows you to do more with them. Dates can be trick to work with in base R (which usually calls them POSIX variables). The lubridate package is a good package to work with dates, and is more user friendly. Others exist.\nTo learn some more about dates and times in R, check out the Dates and times chapter of R4DS as well as the Parsing Dates and Times chapter of IDS.\nThe nice thing about having explicit date formats is that you can do things like subtract one date from another to get the days between the two dates and R will automatically account for things like leap-years.\n\nExcel is notoriously bad at storing dates. It has some internal format that‚Äôs different than what you see in the spreadsheet. That means that at times, if you read an Excel file into R, the date is messed up. In my experience, the most robust option is to explicitly format the date (and really every column) in Excel as text. Then you can read this into R and re-code as date/factor/etc.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#special-data-types",
    "href": "content/module-data/data-formats-r/data-formats-r.html#special-data-types",
    "title": "Data formats in R",
    "section": "Special data types",
    "text": "Special data types\nR has several symbols that represent a special type of object. Here‚Äôs a quick summary.\nTRUE and FALSE are words that are interpreted as logical values. The abbreviations T and F are also treated as logical values (so you should avoid overwriting those names). R also allows you to use 0 and 1 as logical (1=TRUE, 0=FALSE) but that can be confusing.\nNA (not available) is R‚Äôs default way of encoding a missing value. If something is missing, you can‚Äôt do useful things with it. For instance if you have this vector x1 &lt;- c(1,3,NA) and try to do mean(x1), you get NA again. For some functions, there is an ignore setting, e.g.¬†mean(x1, na.rm=TRUE) removes the NA, then takes the mean. You can check if you have NA with is.na(x1). You‚Äôll likely encounter NA frequently in your data analysis, and you need to be careful when you try to perform operations in the presence of NA.\nInf and -Inf can also show up when you do numerical operations, for instance divide by zero. If you perform further mathematical operations with infinity, you almost always end up with infinity again. Generally, if you end up with one of those, it‚Äôs an indication that you did some mathematical operation that you probably shouldn‚Äôt be doing (commonly divide by zero, or log of a negative number).\nNaN (Not a Number) happens when you do some mathematical operation that can‚Äôt be resolved to any number or infinity. For instance if you do 0/0 you‚Äôll get NaN. Like Inf, NaN will generally mess up any further computations, so if you encounter it you should figure out where it came from and figure out a fix.\nNULL indicates something that exists but is empty. For instance if you want to set up a new list object to hold some future results, but want to start it empty, you can do x &lt;- list(NULL). And then you can later add elements to that list. You‚Äôll likely not need to care too much about NULL initially, but if you start writing more code it is often a useful object.\nFor most of these special objects, there are functions is() and is.*(), where * is a specific data type, that let you check. For instance is.nan(x) will tell you if any element in x contains NaN.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#vectors",
    "href": "content/module-data/data-formats-r/data-formats-r.html#vectors",
    "title": "Data formats in R",
    "section": "Vectors",
    "text": "Vectors\nVectors are a simple collection of elements in a single row or column. In R, the easiest way to create vectors is with the concatenate command, c(). An example is x1 &lt;- c(3,12,5). A single vector can contain only one element type (e.g., all characters or all numeric). If you try to mix and match, everything ends up as a character. Type x1 &lt;- c(3,12,5) and x2 &lt;- c(6, 5, 'h') into R and apply the class() command to both x1 and x2 and note the difference. It is worth mentioning that vectors can be of length 1. (Unlike some other languages, there is no separate concept of a scalar in R, a scalar is just a length one vector.) For instance if you define x3 &lt;- 1 and then check it with is(x3) you‚Äôll see that is is classified as a numeric vector ‚Äî of length 1, which you can find out with length(x3).",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#matrices",
    "href": "content/module-data/data-formats-r/data-formats-r.html#matrices",
    "title": "Data formats in R",
    "section": "Matrices",
    "text": "Matrices\nA matrix is a collection of elements in rows and columns. A matrix can contain only one element type. You can think of a matrix as a collection of horizontal vectors stacked on top of each other or vertical vectors next to each other.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#data-frames",
    "href": "content/module-data/data-formats-r/data-formats-r.html#data-frames",
    "title": "Data formats in R",
    "section": "Data frames",
    "text": "Data frames\nA data frame has the same shape as a matrix, i.e., it is a collection of elements in rows and columns. The critical difference is that each column of a data frame can contain elements of different types. This makes it ideal for storing data, with each row and observation and each variable in a column, and different columns potentially with different data types. E.g., column 1 could be age and numeric, and column 2 could be gender and be categorical, etc. Data frames also have a name for each column. As the name might imply, a data frame is a very common way to store data. It is worth mentioning a version of a data frame called a tibble that is often used in the tidyverse. It is similar but not quite the same as the basic data frame. Most of the time, you can use one format or the other. You can read more about tibbles on its package website.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#lists",
    "href": "content/module-data/data-formats-r/data-formats-r.html#lists",
    "title": "Data formats in R",
    "section": "Lists",
    "text": "Lists\nLists are the most flexible data types in R. You can combine different elements as in data frames. Further, each element can be of varying length. For instance, you could have the first list element contain a person‚Äôs name, the second list element their age, the third their address. You can even have other elements inside lists, for instance, you could have a data frame as a list element containing the names and ages of the person‚Äôs parents. Lists are very flexible, and if you get deeper into data analysis, you‚Äôll be working with them. The downside is that because they are more flexible, they can also be a bit more confusing to work with. With enough practice, you‚Äôll figure it out. Also note that almost every function in R that returns something a bit more complicated to you (e.g., the result from a linear fit), returns it as a list.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#other-data-structures",
    "href": "content/module-data/data-formats-r/data-formats-r.html#other-data-structures",
    "title": "Data formats in R",
    "section": "Other data structures",
    "text": "Other data structures\nThe ones described above are the most important ones, but there are many others. Often, specific R packages come with their own structures. You are usually told how to use specific data structures, and there are generally versions of the structures just described.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data formats in R"
    ]
  },
  {
    "objectID": "content/module-data-processing/outliers/outliers.html",
    "href": "content/module-data-processing/outliers/outliers.html",
    "title": "Dealing with outliers",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss how to deal with outliers and generally ‚Äòstrange‚Äô values in your data.\n\n\nLearning Objectives\n\nBe familiar with the concept of outliers.\nKnow what to do with outliers and other strange values.\n\n\n\nIntroduction\nIt is not uncommon to have values in your data that are strange. It could be that someone at data entry made a mistake. Or your codebook doesn‚Äôt properly explain the entries. Other times, it could be real data that is just an outlier. It can be hard to decide if a value is strange but real or a mistake.\nDealing with such entries is a judgment call. The best approach is to have a pre-written analysis plan that explains exactly what to do in any such cases. Outside of clinical trials seeking FDA approval, that pretty much never happens. We usually have to decide what to do with strange entries when we run into them. Some good rules are to be consistent, fully document and explain what you do, and if you are able, do it both ways (e.g.¬†do the analysis with the values as they are, and then again with them removed). Of course both ways can quickly turn into a million different ways and at some point, you probably have to stop. However, trying it more than one way can be reassuring if you get pretty much the same answer each time. If you do not get similar results, then you have to be more careful and should describe in your report/paper in detail why and how different approaches to your data cleaning lead to different results.\n\n\nFinding outliers\nIn introductory statistics classes, students are often taught arbitrary rules for finding outliers to remove. Typically they learn to remove all points outside of the limits of 1.5 times the interquartile range (IQR). However, there is no clear definition of what an outlier is.\nGenerally speaking, an outlier is a date value that is far away from the majority of the other values of a variable or dataset. If one assumed a distribution for the data, one could define an outlier statistically, e.g., every value that is some standard deviations away from the mean could be considered an outlier. In practice, we generally don‚Äôt know the distribution from which the data is a sample, and defining any kind of cut-off for outliers is also fairly arbitrary. Therefore, a more case-by-case approach is usually best.\nA great tool to find outliers is with figures. If you plot the raw data, and some values are far away from others, they deserve further inspection and might be considered outliers.\nOnce you have identified outliers, you should try to determine if you think that strange value is obviously (or at least very likely) a wrong entry, or if it could be real, just unusual. Based on this, you might deal with the value differently. Some approaches for dealing with outliers are described next.\n\n\nRemoving outliers\nOne option is to remove any observation that contains a variable with an outlier. This is usually straightfoward. It is in some sense similar to removing observations with missing values. The problem is that this leads to less data you can analyze, and if the outliers were real - just strange - data, you might get biased estimates.\n\n\nReplacing outliers\nInstead of removing data with outliers, you could try to replace the outliers with something that makes more sense. This is similar to imputation for missing data, and you can use the same approaches.\n\n\nModeling with outliers\nYou can also decide to leave the outliers in the data. You can run your standard models. However, some common models will give the outliers a lot of importance, and therefore a few outliers can skew your results. In this case, you can use statistical methods that are not influenced by outliers as much. Those are sometimes called robust methods (although this term confusingly has many different meanings; not all things called robust methods are about dealing with outliers). We‚Äôll revisit some of those when we get to the statistical analysis and modeling modules.\n\n\nSummary\nIt is likely that your data has some strange values (outliers). Take a close look at them, then come up with a plan. If possible, try things multiple ways (also called doing a sensitivity analysis) and report results from the different versions you tried. Hopefully, you‚Äôll find that how you deal with outliers doesn‚Äôt matter much.\n\n\nFurther Resources\n\nFinney 2012 provides a nice further discussion regarding outliers. It‚Äôs very non-technical (no equations) and easy to read.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Dealing with outliers"
    ]
  },
  {
    "objectID": "content/module-data-processing/merging-data/merging-data.html",
    "href": "content/module-data-processing/merging-data/merging-data.html",
    "title": "Merging data",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss merging of different data sets.\n\n\nLearning Objectives\n\nBe familiar with the idea of merging data.\nKnow how to do merging in R.\n\n\n\nIntroduction\nIt is not uncommon that your raw data comes in more than one file. It could be multiple spreadsheets of a single study, or it could be different datasets from different sources (e.g., a dataset on asthma cases combined with a different data source that records air pollution levels). Often, the most interesting questions need to be answered by combining data from different sources. If you have multiple data sets, you will likely need to combine the data.\n\n\nMerging in R\nThe dplyr package has a great set of _join() functions that let you do different types of joining of data.\nOften, before you can merge, you might need to reshape the data so the structure of the data you are about to merge is consistent. The tidyr package and especially the pivot_longer() and pivot_wider() functions are very useful for that.\n\n\nSummary\nIf you have only a single dataset, you generally don‚Äôt need to merge data.\n\n\nFurther Resources\n\nFor an alternative package that can help with merging, especially large data, see the data.table package.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Merging data"
    ]
  },
  {
    "objectID": "content/module-data-processing/data-processing-overview.html",
    "href": "content/module-data-processing/data-processing-overview.html",
    "title": "Processing data",
    "section": "",
    "text": "Overview\nWhile statistical analysis is important and cool, you will spend the majority of time with your data in the pre-analysis stages.\nEstimates are that for any real-world analysis, up to 80% of your time is spent getting the data ready to do the actual statistical model fitting. This module focuses on that 80%.\n\n\nLearning Objectives\n\nBe familiar with the concept of data wrangling.\nKnow what to do with missing data.\nKnow how to handle outliers.\nBe aware of feature engineering.\n\n\n\nContent\nThe module consists of the following units:\n\nData processing introduction\nData loading\nDealing with missing data\nDealing with outliers \nCreating new data/variables\nMerging data",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Data Processing Overview"
    ]
  },
  {
    "objectID": "content/module-data-processing/data-loading/data-loading.html",
    "href": "content/module-data-processing/data-loading/data-loading.html",
    "title": "Loading data into R",
    "section": "",
    "text": "Overview\nFor this unit, we will briefly discuss how to load your data into R.\n\n\nLearning Objectives\n\nBe familiar with ways to get data into R.\nKnow best practices for loading data.\n\n\n\nIntroduction\nNo matter the source, you need to your data into your favorite data analysis system (in our case, that will be R). Sometimes, you get data in a format that can be read in easily, e.g., a comma-separated CSV file without any strange formatting that messes up the import. At other times, you might get data in a collection of terribly formatted Excel spreadsheets, or you get it by scraping data from some online source. The following sections discuss a few common ways of getting data into R.\n\n\nLoading spreadsheet data\nQuite often, you‚Äôll have data in Excel or another spreadsheet format.\nBase R has different functions to read CSV and similarly formatted data, e.g.¬†read.csv() or read.delim(). Those are ok to use, but in general I recommend using similar functions from the readr package, such as read_csv(). Those functions are more flexible and usually more robust.\nFor Excel data, the readxl package is a good option. For most spreadsheet/tabular formats, you‚Äôll likely find an R package that can read the data.\n\n\nProprietary formats\nA lot of software packages, such as SAS, SPSS, or Graphpad Prism, have their own proprietary format. If you get data that has that format, you need to find a way to get it into R.\nFor SAS, SPSS and STATA files, there is the haven package which lets you read them into R easily. For other file formats, you need to see if someone wrote an R package that can load them.\nSometimes there‚Äôs no tool/package to read that proprietary format. Then you need to ask the person who gave you the software to save it in a different format, or if you have access to the proprietary software, load the data in that software and save it in a format you can read.\n\n\nLoading R data\nSometimes, the data already comes in R format. There are essentially just two formats, Rds and Rda. For differences between those two formats, see e.g.¬†this article. Those are easily loaded with the functions readRDS() (or readr::read_rds() which is basically the same) or load().\n\n\nProcessing data outside of R\nAs much as possible, you should never directly edit your raw data in some software that isn‚Äôt scripted/reproducible (e.g., Excel). If you can, load the raw data into R and do all cleaning inside R with code, so everything is automatically reproducible and documented. Sometimes, you might need to edit the files in the format you got before you are even able to load them into R. For instance, sometimes an Excel spreadsheet is so messy that you can‚Äôt even properly get it into R without first doing some cleanup in Excel.\nIn those cases, you might have to make modifications in a software other than R. If you can‚Äôt directly read the data into R and need to make some changes before, make copies of your raw data files and only edit those copies. Also, write down and document all the edits you made.\n\n\nChecking data\nIt is important that once you‚Äôve loaded your data, you do a quick check to make sure it looks as expected. For instance if you expect \\(N\\) observations and \\(M\\) variables, make sure that is what you get. Those checks are simple but important. You don‚Äôt want to start processing the data unless you know it looks as you expect it to look once loaded.\nAlso, if you load a data frame, sometimes the variables don‚Äôt have the format you expect them. For instance, it is common that a variable that is a factor is loaded as text. Recoding this is then part of the data processing task.\n\n\nSaving data\nAt some point in your pipeline ‚Äî possibly more than once ‚Äî you might want to save your data or results. That is useful if you have different scripts that perform different tasks. For instance your processing script might save the cleaned data. If you plan on sharing that data, a common format such as CSV might be best. If you plan on only loading it again with another R script, then an R format like Rds or Rda might be suitable, as it preserves some meta-information (e.g., a variable coded as a factor will keep that information, while if you save it as CSV it will become text and once you load it you have to re-code it again as factor).\n\n\nSummary\nIn general, loading data into R is not too hard. At times, you might get especially messy data and you need to do a few extra steps to get it into R.\n\n\nFurther Resources\n\nThe Loading and Saving Data in R chapter of Hands-On Programming with R has some additional discussions.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Data loading"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html",
    "href": "content/module-data-processing/assessment-data-processing.html",
    "title": "Assessment - Data Processing",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#setup",
    "href": "content/module-data-processing/assessment-data-processing.html#setup",
    "title": "Assessment - Data Processing",
    "section": "Setup",
    "text": "Setup\nWe‚Äôll also use the familiar group setup. Check the group assignment to make sure you know who else is in your group for this week. Assign each member in your group an (arbitrary) number (I‚Äôm calling them M1, M2, ‚Ä¶). The order should be different than before so you get to interact with a different group member. Everyone will first work on their own repository. Then M1 will contribute to M2‚Äôs repository, M2 will work on M3, etc. The last person, will work on M1‚Äôs repository. This way, everyone will work on their own and one group member‚Äôs repository.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#part-1",
    "href": "content/module-data-processing/assessment-data-processing.html#part-1",
    "title": "Assessment - Data Processing",
    "section": "Part 1",
    "text": "Part 1\nPart 1 is due by Wednesday.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#finding-data",
    "href": "content/module-data-processing/assessment-data-processing.html#finding-data",
    "title": "Assessment - Data Processing",
    "section": "Finding Data",
    "text": "Finding Data\nPreviously, you did a quick exploration of a dataset that came with an R package (gapminder data inside dslabs package). A lot of datasets can be found inside R packages. For instance, this page lists what is likely only a small fraction. The good and the bad about datasets that come with R packages is that they are often fairly clean/tidy. That‚Äôs unfortunately not how most ‚Äúreal world‚Äù datasets look like. Getting dirty and messy datasets and wrangling them into a form that is suitable for statistical analysis is part of most workflows and often takes a lot of time. We‚Äôll start practicing this here by getting data that might or might not be very clean.\nGo to the CDC‚Äôs data website at https://data.cdc.gov/. Browse through the site and identify a dataset of interest.\nWhich dataset you choose is up to you. I suggest you pick a dataset that has at least 100 observations with 5 different variables, and a mix of continuous and categorical ones. Often, 5 variables means 5 columns. That would be the case in properly formatted data. However, some of the data provided by the CDC is rather poorly formatted. For instance CDC‚Äôs dataset on traumatic brain injury has the same variable (age) in separate columns, and it is also discretized. As we‚Äôll discuss, these are two really bad things you can do to your data, so I recommend staying away from such datasets. There are plenty on that website, so I‚Äôm sure you‚Äôll find one that is suitable and interesting to you.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#getting-the-data",
    "href": "content/module-data-processing/assessment-data-processing.html#getting-the-data",
    "title": "Assessment - Data Processing",
    "section": "Getting the data",
    "text": "Getting the data\nTo get the dataset you selected, it is easiest if you download the file to your computer and place it inside your portfolio repository. Note that in general, you should make each data analysis (or other) project its own GitHub repository, and always use a structure like the one provided in the Data Analysis Template (or something similar). However, for this small exercise and for logistic reasons, you‚Äôll use your portfolio/website repository, and just a single folder. Make a new folder called cdcdata-exercise inside your portfolio repository. Place the data into that folder.\n\nRemember that GitHub doesn‚Äôt like large files. So if you pick a large data file (&gt;50MB), first place it somewhere outside your repository, then reduce it by e.g., writing some R code that selects only a portion of the data. Once it‚Äôs small enough, you can place it into the GitHub repository.\n\n\nWhile you should be able to find data for direct download from the CDC website, sometimes you need to write a bit of code to pull data from a source. This is usually done through an API. R has packages that make this relatively easy. If you ever encounter that situation, search online for instructions. Google/Stackoverflow are your friends to figure out what commands you need to write).",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#exploring-the-data",
    "href": "content/module-data-processing/assessment-data-processing.html#exploring-the-data",
    "title": "Assessment - Data Processing",
    "section": "Exploring the data",
    "text": "Exploring the data\nNow, write code that explores the data. Add a new Quarto document called cdcdata-exercise.qmd to the folder you just created.\nStart by providing a brief description of the data, where you got it, what it contains. Also add a link to the source.\nThen write code that reads/loads the data. As needed, process the data (e.g., if there are weird symbols in the data, or missing values coded as 999 or anything of that sort, write code to fix it.) If your dataset has a lot of variables, pick a few of them (at least 5).\nOnce you have the data processed and cleaned, perform some exploratory/descriptive analysis on this cleaned dataset. Make some summary tables, produce some figures. Try to summarize each variable in a way that it can be described by a distribution. For instance if you have a categorical variable, show what % are in each category. If you have a continuous variable, make a plot to see if it‚Äôs approximately normal, then try to summarize it to determine its mean and standard deviation.\nThe idea is that your descriptive analysis will provide enough information for your classmate to make synthetic data that looks similar, along the lines discussed in the synthetic data module.\nRemember to add both text to your Quarto file and comments into your code to explain what you are doing.\nIn a final step, update the _quarto.yml file and include a menu item for ‚ÄúData Analysis Exercise‚Äù pointing to the new file. Follow the format of the existing entries. Remember to be very careful about the right amount of empty space. Re-create your website and make sure it all works and the new project shows up on the website.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#a-new-github-workflow",
    "href": "content/module-data-processing/assessment-data-processing.html#a-new-github-workflow",
    "title": "Assessment - Data Processing",
    "section": "A new GitHub workflow",
    "text": "A new GitHub workflow\nIf everything works as expected, commit and push your changes to GitHub. Instead of using the fork + pull-request workflow we‚Äôve tried a few times, we‚Äôll explore a different collaborative approach. In this approach, you and your collaborator work on the same repository. To that end, you need to add your classmate as collaborator. Go to Github.com, find your portfolio repository, go to Settings, then Collaborators. Choose Add Collaborator and add your classmate. Your classmate should receive an invitation, which they need to accept. With this, they are now able to directly push and pull to your repository, without them needing to create a fork. (You can remove them after this exercise if you don‚Äôt want them to be able to continue having write access to your repository).\nTo avoid any potential merge conflicts, once your classmate takes over, you shouldn‚Äôt make further changes to the repository.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#part-2",
    "href": "content/module-data-processing/assessment-data-processing.html#part-2",
    "title": "Assessment - Data Processing",
    "section": "Part 2",
    "text": "Part 2\nPart 2 is due by Friday.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#joining-and-cloning-the-github-repository",
    "href": "content/module-data-processing/assessment-data-processing.html#joining-and-cloning-the-github-repository",
    "title": "Assessment - Data Processing",
    "section": "Joining and cloning the GitHub repository",
    "text": "Joining and cloning the GitHub repository\nYou should have received an invitation to be a collaborator on your classmate‚Äôs repository. Accept it, then directly clone (not fork) the repository to your local computer.\nFind and open the cdcdata-exercise.qmd file. At the bottom, write a comment that says something like This section contributed by YOURNAME. This needs to be there for me to be able to grade your contribution.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#making-synthetic-data",
    "href": "content/module-data-processing/assessment-data-processing.html#making-synthetic-data",
    "title": "Assessment - Data Processing",
    "section": "Making synthetic data",
    "text": "Making synthetic data\nTake a look at the descriptive analysis your classmate did.\nNext, produce a new synthetic data set with the same structure as their cleaned/processed data. You are encouraged to use some LLM AI tools to help write the code. If you do, specify in the Quarto document or as comments the AI prompts you are using.\nWrite code that produces synthetic data, then summarizes/explores the data with a few tables and figures similar to those made by your classmate for the original data.\nAdd comments in the code and text into the Quarto document to explain what you did and how close your synthetic data is compared to the original data.\nMake sure everything works and the website renders ok.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#finishing-the-github-workflow",
    "href": "content/module-data-processing/assessment-data-processing.html#finishing-the-github-workflow",
    "title": "Assessment - Data Processing",
    "section": "Finishing the GitHub workflow",
    "text": "Finishing the GitHub workflow\nOnce all is done and works, commit, push to GitHub. Note that you are now directly pushing to the original repo which is owned by your classmate. This is easier, you don‚Äôt need to do fork and pull request (PR). It‚Äôs also more dangerous, since you could potentially mess up your classmate‚Äôs repo. So make sure things work before committing and pushing.\n\nSome more comments on GitHub workflows:\nIn general, if you work closely with someone on a project, it might make sense to add them as collaborator, and as needed coordinate with them to avoid merge conflicts. Otherwise, telling someone to contribute by forking and sending a pull request is the safer approach, and you have control if you want to accept their changes or not.\nThere is yet another common way to use GitHub, namely collaborators working in the same repository, but with different branches. Think of a branch like a fork, but it happens inside the repository. Work can occur independently on branches, and at some point one can merge branches. This allows people to work in a single repository, but minimizes possible merge conflicts. This approach is standard for larger projects with many collaborators. For this class, we won‚Äôt use branches, but note that they are useful and commonly used.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#part-3",
    "href": "content/module-data-processing/assessment-data-processing.html#part-3",
    "title": "Assessment - Data Processing",
    "section": "Part 3",
    "text": "Part 3\nThe original repository owner doesn‚Äôt really need to do anything. What their classmate added hopefully works and shows up on the website. You still might want to check to make sure that everything is ok.\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it. I will assess both the original contribution and the addition made by the second person.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-presentation/visualization-in-r.html",
    "href": "content/module-data-presentation/visualization-in-r.html",
    "title": "Visualization in R",
    "section": "",
    "text": "Visualization and graphics in R\nVisualization and graphics are strengths of R. There are currently three main ‚Äì and unfortunately incompatible ‚Äì ways of making graphs in R.\nThe first approach is to use the base R commands, with the plot function as the main workhorse, and related functions for more specialized plots (e.g.¬†boxplot). You can make a lot of very good graphs using base R, but it often takes many lines of code and a good bit of fiddling.\nThe second approach is using the lattice package, and extensions that build on this package. lattice seems to have largely ‚Äúlost‚Äù against option number 3 and I don‚Äôt see it used much anymore these days.\nThe third, and currently most popular approach is to make graphs using the ggplot2 package and various extensions to it. ggplot2 is one of the many R packages written by Hadley Wickham. ggplot2 and its extensions have by now become more or less the standard for good graphics in R. Note that while the package is called ggplot2, people interchangeably refer to it as ggplot or ggplot2. (No one uses ggplot ‚Äúone‚Äù anymore.) I‚Äôll do the same.\nMy recommendation for learning to make plots in R is to focus on ggplot2 and related packages.\nFor most graphs and figures you want for a publication, ggplot2 is probably your best choice. It, and its many add-on packages, product very high quality graphics with not too much code writing required. While you could use ggplot2 exclusively for any figures you make, it is also useful to know a little bit about how to make plots using base R, since they are often very good for quick and dirty plots, especially during the data exploration stage.\n\n\nPracticing ggplot2\nIf you are new to ggplot2, a good and gentle introduction is the set of tutorials in the Visualize Data Posit Recipes section. Similar material, though a little bit more advanced, is covered in a non-interactive form in the Data Visualization, Exploratory Data Analysis and Graphics for Communication chapters of R4DS. A nice step-by-step tutorial can be found in C√©dric Scherer‚Äôs blog post. Another good source are the chapters in the Data Visualization section of IDS. Especially the Data visualization principles chapter is worth a read.\nIf you want to learn a bit more about ggplot2 in general, the ideas behind it, and also how to use it, you can check out Hadley & co-author‚Äôs free online book ggplot2: Elegant Graphics for Data Analysis.\nFor coding/R in general, and making plots in particular, it is generally a learning by doing approach. The most common approach - and one I still use a lot - is to search online for ‚Äúhow do I do X in ggplot‚Äù, and usually you find an example and code that is close enough to what you want so you can adjust it. The tricky bit is knowing what to search for. If you don‚Äôt know the type of plot you want, it is useful to learn a bit about different plots and what they are good for (something that was discussed in the previous unit). A good reference source for how do I do X with ggplot2 is the R graphics cookbook, which is meant to give you quick recipes for common questions and plots.\nFor inspiration and ideas, there is also the R graphics gallery where you can see all kinds of plots made in R, and you can also see the code. They are nicely organized into categories. It‚Äôs definitely worth browsing around.\n\n\nBeyond static graphs\nWhile you will likely always need regular figures for papers, reports, presentation slides, etc., it is becoming increasingly common to have interactive and dynamic visualizations. R has pretty good facilities for that, some in rapid development. The ggvis package is meant to be similar to ggplot2 but allow for interactive and dynamic visualizations. The plotly package also works well with R and allows interactive graphs.\nAnother great, and related set of tools is the htmlwidgets package which allows you to make a lot of very nice, interactive web-based graphs using R and various R packages.\nFor even richer, interactive graphical operations and visualizations, including writing full graphical user interfaces, you can use the shiny package and extensions to it. The shiny gallery has examples of apps. Some of my R packages, e.g.¬†DSAIDE and DSAIRM use Shiny for the interactive graphical interface (and either ggplot2 or plotly to show graphs for different models).\nOften, getting professional-looking interactive figures, dashboards, and other widgets in R is not that difficult. However, since those are all specialized products, they usually require commands and concepts that are somewhat different from basic or tidyverse R, which generally takes some time to learn.\nInteractive graphs and apps are beyond what we‚Äôll cover in this class, but it is something that might be worth learning, and you are certainly welcome to make use of some of these packages and tools for your class project üòÉ.\n\n\nSummary\n\n\nFurther Resources\n\nThe Visualize Data Posit Recipes section has some good worked examples and code.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Visualization in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html",
    "href": "content/module-data-presentation/presenting-results-overview.html",
    "title": "Presentation Overview",
    "section": "",
    "text": "In this unit, we will discuss the main ways to show your data and results and discuss some ways to do that efficiently.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Presenting Results Overview"
    ]
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html#figures",
    "href": "content/module-data-presentation/presenting-results-overview.html#figures",
    "title": "Presentation Overview",
    "section": "Figures",
    "text": "Figures\nA well-done figure is able to convey a lot of complex information in an intuitive and understandable format. On the flip side, a poorly thought-out figure can be utterly confusing even if it is presenting something simple. Worse, figures can easily be (ab)used to mislead the reader. Your goal is to make sure that your figures are great-looking, easy to understand and not misleading. We‚Äôll go over some more details relating to figures in subsequent units.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Presenting Results Overview"
    ]
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html#tables",
    "href": "content/module-data-presentation/presenting-results-overview.html#tables",
    "title": "Presentation Overview",
    "section": "Tables",
    "text": "Tables\nTables allow you to easily present summaries of your data. Many epidemiological and other papers have a Table 1 which summarizes the data by its characteristics. For instance if you had a human cohort, this summary table will likely list the number and percentages or ranges of individuals based on gender, age, BMI, smoking status, etc.\nTables are also often used to present results of your data analysis. In my opinion, figures are often better, but if the results are simple or there are a lot of numbers that need to be shown, tables can work better at times. We‚Äôll go over some more details on how to generate tables in R in a subsequent unit.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Presenting Results Overview"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-structure.html",
    "href": "content/module-data-analysis/data-analysis-structure.html",
    "title": "Structure of a Data Analysis",
    "section": "",
    "text": "Setting paths\n\n\nFurther Resources"
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html",
    "href": "content/module-data-analysis/data-analysis-motivation.html",
    "title": "Motivating Data Analysis Examples",
    "section": "",
    "text": "We‚Äôll get into the details of what data analysis is and how to do it very soon. But to start out, I want to provide a few hopefully inspiring and motivating examples of data analysis and use. You‚Äôll be doing similar projects yourself soon! In fact, you will be reproducing several analyses below.",
    "crumbs": [
      "Content",
      "2A. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-1",
    "href": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-1",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 1",
    "text": "TED talk 1\nArguably one of the best communicators of public health data was the late Hans Rosling. He has several great entertaining and enlightening TED talks. Here is one of them: A statistics talk that has been viewed millions of times!\n\n\n\n\nYou can watch more TED talks by Hans Rosling. The software he uses to illustrate data is called Gapminder and available online on the Gapminder website.",
    "crumbs": [
      "Content",
      "2A. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-2",
    "href": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-2",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 2",
    "text": "TED talk 2\nIn this entertaining and empowering talk, Talithia Williams argues for the utility of careful assessment of data in your personal life.",
    "crumbs": [
      "Content",
      "2A. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-3",
    "href": "content/module-data-analysis/data-analysis-motivation.html#ted-talk-3",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 3",
    "text": "TED talk 3\nIn this talk, Sebastian Wernicke gives some cautionary suggestions about trusting data too much and argues for critical assessment as an important part of any data analysis and decision process.",
    "crumbs": [
      "Content",
      "2A. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#more-ted-talks",
    "href": "content/module-data-analysis/data-analysis-motivation.html#more-ted-talks",
    "title": "Motivating Data Analysis Examples",
    "section": "More TED talks",
    "text": "More TED talks\nThe TED website has a large number of data related talks. Feel free to explore as much as you like. If you find especially good ones, post them on the class discussion board.",
    "crumbs": [
      "Content",
      "2A. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#tidytuesday",
    "href": "content/module-data-analysis/data-analysis-motivation.html#tidytuesday",
    "title": "Motivating Data Analysis Examples",
    "section": "TidyTuesday",
    "text": "TidyTuesday\nAn interesting initiative started by Thomas Mock is TidyTuesday, where every Tuesday a new dataset is released and individuals are encouraged to analyze it. Later in this course, you will be participating in some TidyTuesdays yourself. Tidy Tuesday has by now been running for many years.\nIndividuals who participate in TidyTuesday often post their projects online and link to it on social media. We will look at an example by David Robinson (aka drob). David does screencast recordings showing him analyze the data.\nYou don‚Äôt need to watch all of the almost 2 hour long recording üòÑ, but do watch the first few minutes to see him coding/analyzing in real-time. You might just get hooked and keep watching. You might not be familiar with some or most of the R coding, but you will still get the overall idea of how he does the analysis. And his thinking out loud is informative as well. See for yourself:\n\n\n\n\nIf you like the ability to look over his shoulder while he does data analysis, he has a bunch more of those screencast recordings.\nThere are others who live-stream data analysis, including competitions, on platforms such as Twitch. Check out Jesse Mostipak, aka Kiersi or Nick Wan if interested.\nDavid does his work reproducibly and shares the code as RMarkdown files. For specific files, see the links provided in the video description. For convenience, download the code for his analysis. Just save the file after it opens in your browser. It should be named 2021_05_18_salary_survey.Rmd. Click on it, and it should open in RStudio.\nOnce the file is open in RStudio, you might see at the top of the file a message suggesting that you need to install several packages to run the file. Do so. If you don‚Äôt get that auto-suggestion, you need to install the packages by hand. Every package that is called with the library() command needs to be installed first.\nThis is a good example of an almost works reproducible example. It‚Äôs quite rare that things work completely without adjustments on different machines. How to do that best is a huge topic in data analysis (e.g., for drug companies that need full reproducibility for licensing). In this example, I had to make the following adjustments:\n\nChange output: html_output to output: html_document in the YAML section.\nInstall the ranger package, which is not listed under the library statements but is needed.\n\nWith those changes, I was able to run the whole script by hitting the Knit button in RStudio. If you are still missing a package, you will get an error message. If things work out, the whole script will run a while (depending on the speed of your computer), and you should get a document/report which reproduces the complete analysis he did in the video! If things don‚Äôt work out, post your error message to Slack so someone can help troubleshoot. One potential problem could arise from the fact that David uses parallel computing to make things run faster. If you get an error message, you can try commenting out or deleting the line of code that says doParallel::registerDoParallel(cores = 4) and see if it works without it. It worked ok with that line of code on my Windows machine, but might not always work. (If on Windows, you might be asked about firewall network permissions. That‚Äôs a quirk of how parallel computing works on a single machine. Just say ‚Äúyes‚Äù to both public and private networks.)\nAgain, most or all of the R code might not make sense to you just yet. That‚Äôs ok. The main point is to show you a nice example of a data analysis done in a way that others (you) can reproduce with just a few clicks. As part of this class, you will be producing those kinds of reproducible R Markdown files (well, we‚Äôll be doing Quarto files, which is the improved successor of R Markdown).",
    "crumbs": [
      "Content",
      "2A. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation.html#research-project",
    "href": "content/module-data-analysis/data-analysis-motivation.html#research-project",
    "title": "Motivating Data Analysis Examples",
    "section": "Research Project",
    "text": "Research Project\nFully open and reproducible analyses are also becoming common in academic research, with authors publishing everything needed to reproduce their paper and results. In my research group, we try to adhere to this approach as much as possible. As an example, I‚Äôm sharing a project/paper by Dr.¬†Brian McKay, a former grad student in my group. You can read the published paper. All materials to reproduce the full project and all results are available as supplementary material. Take a look at the Usage Notes section on the website.\nNext, click on Download Dataset, and unzip the file. You should end up with a folder called ProcSocB Supplemental Material.\nInside that folder you can see subfolders containing all the data and code to fully reproduce the analysis and manuscript. Find and click on Virulence_Trade-off.Rproj. This should open in RStudio. To reproduce the manuscript, go into the Manuscript folder, open Manuscript.Rmd. You‚Äôll see some code at the top which loads various datasets containing results. Further down, you‚Äôll find the text of the manuscript, combined with simple R commands that dynamically place numbers and figures into the text. Re-create the manuscript by clicking on the knit button. There might be error messages related to missing packages. If that happens, install those packages (e.g.¬†‚Äòbookdown‚Äô). If you get stuck, post your problem to the discussion boards.\nFeel free to explore further by opening various R scripts and running them. The Usage Notes section on the website where you downloaded the zip file, or the Supplementary-Material.docx document explain what the different pieces do and how to use them.\nAs you can tell, this is more complex and complete compared to the single R Markdown file example above. The details on how you structure your project and folders/files are up to you, but they should provide a way to organize things so it‚Äôs easy for you and others to understand what is going on where.\nThis example hopefully gives you an idea how nice and easy it is to write a whole paper with just a few clicks of a button. It is a combination of R scripts and R Markdown files (in this case a version called bookdown), which allow a fully automated generation of a complete data analysis and manuscript, including all figures, tables, references, etc. This is an incredibly efficient workflow and one you‚Äôll learn and use in this class. For instance if there was a mistake somewhere in the data, instead of going through the manuscript and manually changing numbers, one could just re-run the whole code, and it will produce new results based on the updated data in a fully automated manner. It‚Äôs a bit more work upfront until you get the hang of it, but once you have it set up, it can save an enormous amount of time and avoid potential copy & paste mistakes.",
    "crumbs": [
      "Content",
      "2A. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-coding-basics/rcoding-tidyverse.html",
    "href": "content/module-coding-basics/rcoding-tidyverse.html",
    "title": "Tidyverse and Friends",
    "section": "",
    "text": "One of the strengths of R (and also a source of confusion) is that it is very flexible and almost always lets you do things in more than one way. R itself comes with some functionality. This is often referred to as base R. Even with just this basic functionality, there are often many ways to accomplish a task. But the real power of R comes from its many packages. Packages (also called libraries in some other programming languages) contain additional functions and functionality that lets you fairly easily do things that would require a ton of coding effort if you tried to do it yourself. Someone basically wrote the functionality for you, and you can use it.\nWhile there are tons of packages available, some are very commonly used. For data analysis tasks, the folks from R Studio have developed many packages that are very useful. One such set of packages, the most widely used set, is called the tidyverse. By using those packages, a lot of coding applied to data analysis becomes easier, more readable, and more powerful. We will use the tidyverse packages and their functionality a lot. That said, knowing some base R is very useful. In general, you can fairly easily mix and match.",
    "crumbs": [
      "Content",
      "3. R Coding Basics",
      "Tidyverse and Friends"
    ]
  },
  {
    "objectID": "content/module-coding-basics/rcoding-tidyverse.html#tidyverse-concepts",
    "href": "content/module-coding-basics/rcoding-tidyverse.html#tidyverse-concepts",
    "title": "Tidyverse and Friends",
    "section": "Tidyverse concepts",
    "text": "Tidyverse concepts\nAs you‚Äôll find out shortly, one feature of R code written in the tidyverse style is the heavy use of the pipe operator. The original pipe operator, the %&gt;% symbol, was introduced in the magrittr package. Since then, base R got its own pipe operator, which is the symbol |&gt;.\nFor instance, this is the kind of code that you might see when doing data wrangling (note that this code doesn‚Äôt work since it only shows the chain of functions, for it to work there would need to be arguments provided to each function, i.e.¬†there needs to be something inside the ()):\n\ndata %&gt;% filter() %&gt;% \n         select() %&gt;%\n         fct_lump() %&gt;%\n         mutate() %&gt;% ...\n\nThe idea is that you pipe the results from one operation into the next, and thus potentially build a long chain of commands. That style of coding makes it often quite easy to understand what the code is doing. For instance in the example code above, you first filter the data based on some row values, then select some columns, then combine some factor variable, then mutate a variable into a new one, and so on.\nThe problem, especially when you start out, is that things can (and will) go wrong at some of those steps, and it‚Äôs difficulty to figure out where the problem is. At least when starting out, it is in my opinion often better to save the result of some cleaning operation as a new intermediate variable. That lets you more easily check for bugs, and to see how the data changed from step to step and if it does what you think it should. So instead of using a long chain of pipes, you can write the code like so:\n\ndat_small1 &lt;- data %&gt;% filter()\ndat_small2 &lt;- dat_small1 %&gt;% select()\ndat_small3 &lt;- dat_small2 %&gt;% fct_lump()\n...\n\nThis code is not quite as easy to read, and it creates all these additional variables that you might not want or need. But I think at least as you are learning the different tidyverse functions, it often helps to be able to inspect what happens at each step, and thus more easily spot when things go wrong. Once you get more comfortable with cleaning steps and coding in general, and make few mistakes, you can start chaining things together, and make your chains longer. But if you start out writing code that way, it can be harder to follow along and find bugs.\nA nice introduction to pipes is this section of the R4DS book.\nThere is also a nice project called Tidy Data Tutor that allows you to visualize the different steps in a data analysis chain of commands. Once you get used to chaining commands together in a long pipeline, or try to inspect someone else‚Äôs code, using that tool to look at each step can be quite useful. Give it a try.",
    "crumbs": [
      "Content",
      "3. R Coding Basics",
      "Tidyverse and Friends"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html",
    "href": "content/module-coding-basics/assessment-coding.html",
    "title": "Assessment - R Coding Basics",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#group-setup",
    "href": "content/module-coding-basics/assessment-coding.html#group-setup",
    "title": "Assessment - R Coding Basics",
    "section": "Group setup",
    "text": "Group setup\nAs in the previous exercise, assign each member in your group an (arbitrary) number (I‚Äôm calling them M1, M2, ‚Ä¶). Make sure you are teamed up with a different person this time. For this exercise, everyone will first work on their own and finish this part by Wednesday. Then M1 will contribute to M2‚Äôs repository, M2 will work on M3, etc. The last person (M3/M4/M5, based on the number of people in your group), will work on M1‚Äôs repository. This way, everyone will work on their own and one group member‚Äôs repository. Details are given below.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#part-1",
    "href": "content/module-coding-basics/assessment-coding.html#part-1",
    "title": "Assessment - R Coding Basics",
    "section": "Part 1",
    "text": "Part 1\nPart 1 is due by Tuesday.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#getting-started",
    "href": "content/module-coding-basics/assessment-coding.html#getting-started",
    "title": "Assessment - R Coding Basics",
    "section": "Getting started",
    "text": "Getting started\nOpen your portfolio website project in RStudio. Then open the file for the coding exercise, coding-exercise.qmd.\n\nDocumenting well is very important! Add lots of comments to your code/file. I suggest that your code should be more than half comments. For every block of code, you have a few lines of comments at the beginning explaining what the code block does, and then each line of code gets its separate line of comment with more details. Comment on both the how and why of your code. This much commentary might seem overkill initially. But as your code gets more complex, it will be very useful. Both your collaborators, and your future self looking at the code you wrote several weeks ago will be incredibly thankful for your comments!\nIf you write R code, your comments will be lines that start with #. For Quarto or R Markdown files, you can either add comments as Markdown text above/below your code, and/or add comments inside your R code chunks. Both is ideal. R Studio allows you to quickly turn sections of a document into comments or un-comment them (In the Code section of the R Studio menu). That can be useful for turning on/off code during testing, or hiding some parts of text that‚Äôs just meant for you but not for the reader.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#loading-and-checking-data",
    "href": "content/module-coding-basics/assessment-coding.html#loading-and-checking-data",
    "title": "Assessment - R Coding Basics",
    "section": "Loading and checking data",
    "text": "Loading and checking data\nWe‚Äôll look at and play with some data from the dslabs package. Write a code chunk using the library() function that loads the package (install the dslabs package first if you don‚Äôt have it yet).\n\nIt is good practice to load all packages at the beginning of your code. So if you are using some R package, instead of loading it with library just before you use it, place all your library commands at the beginning of your R script or Quarto file. Also, add a short comment explaining why you are loading a certain package. For a complex project, it might even make sense to list all packages you use in a readme file. Even better is to use something like the renv package which keeps track of all your packages and makes sure someone running your project at a later time gets exactly the same packages you use. While I‚Äôm not requiring renv for the course, I encourage you to check it out and if you are up for it, use it for your portfolio or the project.\n\nAll the code you write for this (and any other) project should be written into an R or Quarto file, not in the R console. The reason for that is that you want a permanent record of what you did, and the ability to modify and re-run your analysis easily. For this exercise, you can either write the code directly into the Quarto document, or if you prefer the setup of a separate R script and pulling code into the Quarto document (see the data analysis template for an example), you can do it that way too.\nWe‚Äôll look at the gapminder dataset from dslabs. Once you have installed and loaded the dslabs package, the dataset is available. I.e., different than datasets you get from external sources, those that come with R packages are available right after you load the package. Write a code chunk using the help() function that pulls up the help page for the data to see what it contains. Then use the str() and summary() functions to take a look at the data. Use the class() function to check what type of object gapminder is.\nTo illustrate how that should look, you should have something like these lines of code and R output so far for this exercise.\n\n#load dslabs package\nlibrary(\"dslabs\")\n#look at help file for gapminder data\nhelp(gapminder)\n\nstarting httpd help server ... done\n\n#get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n#get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n#determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#processing-data",
    "href": "content/module-coding-basics/assessment-coding.html#processing-data",
    "title": "Assessment - R Coding Basics",
    "section": "Processing data",
    "text": "Processing data\n\nYou can accomplish the next steps (and pretty much anything) with just basic R commands and not using extra functionality from packages. However, things are often easier with packages. For data processing tasks, the packages from the tidyverse are very useful. You can do the following tasks with any commands/packages you like.\n\nWrite code that assigns only the African countries to a new object/variable called africadata. Run str and summary on the new object you created. You should now have 2907 observations, down from 10545. Depending on how you do this, you might also notice that all the different categories are still kept in the continent (and other) variables, but show 0. R does not automatically remove categories of what in R is called a factor variable (a categorical variable) even if they are empty. We don‚Äôt have to worry about that just now, but something to keep in mind, it can sometimes lead to strange behavior.\nTake the africadata object and create two new objects (name them whatever you want), one that contains only infant_mortality and life_expectancy and one that contains only population and life_expectancy. You should have two new objects/variables with 2907 rows and two columns. Use the str, and summary commands to take a look at both. Make sure you add comments into your code to explain what each line of code is doing, and as needed, also add additional explanatory Markdown text to your Quarto file.\n\nI find it the least confusing to call things which store values in R objects (e.g., x is an object here: x &lt;- 2 + 2) and reserve the word variable for a data variable, i.e., usually a column. However, it is common in programming to also refer to an object as a variable. Because of that, I sometimes use that terminology (inadvertently) too. So if I talk about a variable, you need to determine from the context if I mean a certain variable in the data (e.g.¬†height or weight), or a variable in R (e.g.¬†x or result) that stores some content.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#plotting",
    "href": "content/module-coding-basics/assessment-coding.html#plotting",
    "title": "Assessment - R Coding Basics",
    "section": "Plotting",
    "text": "Plotting\nUsing the new variables you created, plot life expectancy as a function of infant mortality and as a function of population size. Make two separate plots. Plot the data as points. For the plot with population size on the x-axis, set the x-axis to a log scale.\nYou should see a negative correlation between infant mortality and life expectancy, which makes sense. You should also see a positive correlation between population size and life expectancy. In both plots, especially the second one, you will see ‚Äòstreaks‚Äô of data that seem to go together. Can you figure out what is going on here? Take another look at the africadata data we generated, which should give you a hint of what‚Äôs happening. Add descriptive text into your Quarto file to explain what you see and why.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#more-data-processing",
    "href": "content/module-coding-basics/assessment-coding.html#more-data-processing",
    "title": "Assessment - R Coding Basics",
    "section": "More data processing",
    "text": "More data processing\nI‚Äôm sure you realized that the pattern we see in the data is due to the fact that we have different years for individual countries, and that over time these countries increase in population size and also life expectancy. Let‚Äôs pick only one year and see what patterns we find. We want a year for which we have the most data. You might have noticed that in africadata, there are 226 NA (i.e., missing values) for infant mortality. Write code that figures out which years have missing data for infant mortality. You should find that there is missing up to 1981 and then again for 2016. So we‚Äôll avoid those years and go with 2000 instead. Create a new object by extracting only the data for the year 2000 from the africadata object. You should end up with 51 observations and 9 variables. Check it with str and summary.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#more-plotting",
    "href": "content/module-coding-basics/assessment-coding.html#more-plotting",
    "title": "Assessment - R Coding Basics",
    "section": "More plotting",
    "text": "More plotting\nLet‚Äôs make the same plots as above again, this time only for the year 2000. Based on those plots, there seems to still be a negative correlation between infant mortality and life expectancy, and no noticeable correlation between population size and life expectancy. Let‚Äôs apply some statistical model to this data.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#simple-model-fits",
    "href": "content/module-coding-basics/assessment-coding.html#simple-model-fits",
    "title": "Assessment - R Coding Basics",
    "section": "Simple model fits",
    "text": "Simple model fits\nUse the lm function and fit life expectancy as the outcome, and infant mortality as the predictor. Then repeat, now with the population size as the predictor variable. (Use the data from 2000 only.) Save the result from the two fits into two objects (e.g.¬†fit1 and fit2) and apply the summary command to both, which will print various fit results to the screen. Based on the p-values for each fit, what do you conclude? Add comments into your quarto file to explain what you did and found.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#sending-updates-to-github",
    "href": "content/module-coding-basics/assessment-coding.html#sending-updates-to-github",
    "title": "Assessment - R Coding Basics",
    "section": "Sending updates to Github",
    "text": "Sending updates to Github\nOnce you are done with your exercise, re-build your portfolio website. Make sure no error messages show up. A preview should show up, check that the page for this exercise looks the way you want it to. Once you are happy with how everything looks, close RStudio. Go to GitKraken, commit your changes, and push to the remote repository. Check your portfolio website online to make sure you can now see the newly created R exercise document (in addition to your previously created About page). Of course, at any point, feel free to make further enhancements to your portfolio website.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#part-2",
    "href": "content/module-coding-basics/assessment-coding.html#part-2",
    "title": "Assessment - R Coding Basics",
    "section": "Part 2",
    "text": "Part 2\nPart 2 is due by Thursday.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#adding-to-each-others-work",
    "href": "content/module-coding-basics/assessment-coding.html#adding-to-each-others-work",
    "title": "Assessment - R Coding Basics",
    "section": "Adding to each other‚Äôs work",
    "text": "Adding to each other‚Äôs work\nOnce you‚Äôve done your first part, you‚Äôll contribute to another group member‚Äôs project. As previously, find your classmates‚Äô portfolio repository and fork it, then clone to your local computer.\nOpen the repository. Remember to always do that by clicking on the .Rproj file in the main folder, which should open the project/repository in RStudio. Once you have the repository open, find the file for this exercise coding-exercise.qmd. Open it, make sure it runs/renders. Then add your part at the bottom of the file.\nStart off by adding a comment that says something like This section contributed by YOURNAME. This needs to be there for me to be able to grade your contribution.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#more-data-exploration",
    "href": "content/module-coding-basics/assessment-coding.html#more-data-exploration",
    "title": "Assessment - R Coding Basics",
    "section": "More data exploration",
    "text": "More data exploration\nPick another dataset from dslabs, whichever one you want. Unfortunately, the dslabs package doesn‚Äôt have a nice website. But you can go to its offical CRAN entry and click on Reference Manual. The pdf lists the various other datasets and for each provides a brief explanation.\nOnce you chose one of the datasets, write R code to go through similar steps as above. Specifically, do the following:\n\nExplore the dataset.\nDo any processing/cleaning you want to do.\nMake a few exploratory figures. Optionally, also some tables.\nRun some simple statistical model(s). Your choice.\nReport the results from the model(s).\nFor each step, add plenty comments to the code and explanatory Markdown text to the Quarto file.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#sending-a-pull-request-pr",
    "href": "content/module-coding-basics/assessment-coding.html#sending-a-pull-request-pr",
    "title": "Assessment - R Coding Basics",
    "section": "Sending a pull request (PR)",
    "text": "Sending a pull request (PR)\nOnce you are done with your additions, make sure the whole website renders. Then push your updates to your fork on GitHub. Then follow the instructions from last week and issue a pull request to the original owner of the repository. They should get a notification, but you can also let them know that your updates are ready.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#part-3",
    "href": "content/module-coding-basics/assessment-coding.html#part-3",
    "title": "Assessment - R Coding Basics",
    "section": "Part 3",
    "text": "Part 3\nPart 3 is due by Friday.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-coding-basics/assessment-coding.html#merging-the-pr",
    "href": "content/module-coding-basics/assessment-coding.html#merging-the-pr",
    "title": "Assessment - R Coding Basics",
    "section": "Merging the PR",
    "text": "Merging the PR\nOnce you received the pull request, check it. Hopefully all is ok and merge it. If something isn‚Äôt right, tell the person who sent the PR to fix what needs fixing and send another one.\nOnce you merged the PR, pull the updates to your local computer. Make sure everything looks fine and everything runs. Make any further needed changes/updates. Once everything is good, push your updated portfolio repository to GitHub. Check the website URL to make sure everything looks good. I‚Äôll be looking at the content shown on the website for assessment/grading purposes.\n\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it.",
    "crumbs": [
      "Assessments",
      "3. R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-ai/ai-tools/ai-tools.html",
    "href": "content/module-ai/ai-tools/ai-tools.html",
    "title": "Currently available AI tools",
    "section": "",
    "text": "Overview\nIn this tutorial, we discuss some currently available LLM AI tools and when/how you might want to use them. This is of course a rapidly moving field, so a few months from now things might have changed. I can hopefully keep up somewhat, but check the last modified date of this page üòÅ.\n\n\nLearning Objectives\n\nBe familiar with current AI tools and when/how to use them.\n\n\n\nIntroduction\nIt seems that every month a new LLM AI tool is announced. They are expected to keep getting better fairly rapidly for the foreseeable future. As you use these tools, it‚Äôs definitely useful to play around and explore. Here, I‚Äôm describing those that seem - as of this writing - the most useful ones for coding and data analysis tasks. Other AI tools might be more suitable for other tasks.\n\n\nIn brief\nCurrently, ChatGPT 4 is the most powerful LLM AI tool. You can access it through a paid subscription via OpenAi, or for free through Microsoft‚Äôs Bing. ChatGPT 3.5 is the less powerful, free version you can access by creating a free OpenAI account. Despite being less powerful, I‚Äôve found it to work pretty well to write R code. Other LLM are worth trying too, e.g.¬†Claude for data exploration/analysis.\n\n\nOpenAI ChatGPT\nAs of this writing, for the AI tools we are interested in (the LLMs), ChatGPT is probably the most useful tool. You can access it by creating a free account.\nChatGPT is also part of the Microsoft Bing search engine (currently located under the ‚ÄúChat‚Äù tab) and GitHub‚Äôs CoPilot.\nIf you haven‚Äôt already, I suggest you create a ChatGPT/OpenAI account. As of this writing, you can get a free account. Once you log in, you see a fairly plain interface with an area into which you can type your prompts. You can provide fairly elaborate prompts. We‚Äôll look at examples in other units.\n\n\nMicrosoft Copilot/Bing (still OpenAI GPT)\nA free OpenAI account gives you access to ChatPGT 3.5. While I find it often quite useful, especially for writing code, it‚Äôs not the best AI out there. ChatGPT 4.0 is generally better. You have to pay OpenAI if you want to use it. Or, you can use Microsoft Copilot - aka Bing Chat, and not to be confused with the GitHub Copilot, which is also owned by Microsoft ü§∑Ô∏è. It is recommended to use it in ‚ÄúCreative‚Äù or ‚ÄúPrecise‚Äù mode. Of course feel free to explore and play around. I will use the terms Microsoft Copilot or Bing AI/Chat interchangably. I‚Äôm sure all these names will change soon again anyway.\n\n\nGitHub Copilot\nGitHub Copilot is currently free for students with the GitHub student developer pack I asked you to sign up for. Set it up by following the Quickstart guide.\nOnce you have Copilot activated, you can use it in R Studio. (It is also possible to use it with other popular editors, such as VS Code, but for this course I will focus on R Studio. If you prefer to code in VS Code, you can install the Copilot plugin).\nTo get Copilot working in R Studio, follow these instructions. You might want to leave Copilot indexing unchecked for now. The instruction guide explains what this is. Basically, if you check this, copilot can access all files that are part of your current project. This can lead to confidentiality issues. Therefore, only check it if you are sure that all files in your project are ok being shared with the AI (and possibly make their way to Github/Microsoft/OpenAI servers).\nOnce you have Copilot up and running, you should see it show up in the bottom of your R Studio input window (the top right window where you write code). It will start suggesting things as you type, once you stop for a few seconds. You can set that time in the options.\nAn introduction to using Copilot can be found in this video by Thomas Mock from Posit/RStudio. In general, if you want bigger chunks of code, it seems currently better to go straight to ChatGPT. Copilot seems most useful to help you write bits of code as you go along. It can also be useful to help you remember the names of functions and arguments. This blog post provides some additional useful information on how to best use Copilot.\nIn general, I recommend you turn off Copilot in RStudio when you are not actively using it. And before you turn it on, make sure the document you have open, and any other files it might have access to, do not contain information you want to keep private.\n\n\nClaude/Anthropic\nAnthropic is another company developing LLM AI tools. Their main AI is called Claude. As for ChatGPT, you can sign up to get a free account. It is apparently not as powerful as ChatGPT. However, it has one nice feature that the free versions of ChatGPT or Bing do not have, namely it lets you upload files. This is useful for data analysis tasks. I‚Äôll show an example of that in the Explore data with AI unit.\n\n\nOther LLM AI\nOther companies, such as Google, are also busy creating LLM AIs. I haven‚Äôt tried them yet. Once I do, I‚Äôll add my impressions. If you are using any other LLM for coding or data analysis tasks and find it useful, please let me know.\n\n\nOther tools\nIt is quite likely that in the future, there will be several LLM AI tools that are used by the majority of users, and then there will be lots of customizations running on top of those. There are already specialized LLM AI tools for lawyers and doctors and other professions that use ChatGPT under the hood. GitHub CoPilot is such a customization for coding. I expect further tools to keep coming out at a fairly rapid pace.\nFor instance, as you learned from Thomas Mock‚Äôs video above, the chattr R package seems like another great way to use ChatGPT from within R. Unfortunately, as of this writing, I haven‚Äôt been able to get it to work. It is not entirely clear to me based on the information provided on the chattr website, but I think this is because talking to ChatGPT via the API (which is what chattr does) requires a paid account.\nI‚Äôm also aware of the chatAI4R R package, but haven‚Äôt tried it yet. My guess is since it likely also talks to ChatGPT through their API, it also requires a paid account. If that‚Äôs not the case and you try it and find it useful, please report back!\nAs you are probably aware, AI tools are changing rapidly. I‚Äôm writing this in December 2023, you will read this a few months later. It could be that by this point, some new versions of AI tools have come out that are even better. Do explore and if you find some especially promising platform or package, let me know!\n\n\nSummary\nLLM AI tools are changing rapidly. Currently, ChatGPT seems to do a good job at helping with coding and data analysis related tasks. Claude seems a useful option too, since it allows upload of files (such as data). New tools keep showing up and are generally worth exploring.\n\n\nFurther Resources\nFor a list and description of some other AI tools (e.g., those that can produce images), see this guide on using AI to do stuff. Also see the AI resource unit.",
    "crumbs": [
      "Content",
      "AI",
      "Current AI Tools"
    ]
  },
  {
    "objectID": "content/module-ai/ai-overview.html",
    "href": "content/module-ai/ai-overview.html",
    "title": "Overview of AI content",
    "section": "",
    "text": "Overview\nThis module provides a brief introduction to AI and why and how to use AI tools.\n\n\nLearning Objectives\n\nKnow some basic concepts of AI.\nKnow why to use AI tools.\nUnderstand risks and limitations of AI tools.\nKnow how to use some common AI tools.\nBe familiar with ways to use AI for data analysis tasks.\nBe aware of ways to use AI with R.\n\n\n\nContent\nThe module consists of the following units:\n\nIntroduction to AI\nIntroduction to LLM AI tools\nA conceptual framework for LLM AI tools\nRecommended AI tools\nUsing AI to write code\nUsing AI to fix code\nUsing AI to explore data\nUsing AI to analyze data\nLinks to AI resources",
    "crumbs": [
      "Content",
      "AI",
      "AI Overview"
    ]
  },
  {
    "objectID": "content/module-ai/ai-introduction/ai-introduction.html",
    "href": "content/module-ai/ai-introduction/ai-introduction.html",
    "title": "Introduction to Artificial Intelligence",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly talk about Artificial Intelligence from a general perspective.\n\n\nLearning Objectives\n\nKnow what Deep Learning and Artificial Intelligence are.\nKnow how one can (maybe) use R for such tasks.\n\n\n\nIntroduction\nArtificial intelligence (AI) is everywhere these days. AI approaches have led to some impressive recent technological advances, such as self-driving cars, very good language translation, biometrics and image recognition, and computers that beat humans at games like Chess/Go/Poker, etc. Most recently, we have seen LLM AI tools (which we‚Äôll discuss separately) that can be very useful for doing data analysis and coding tasks.\nI have no doubt that progress in AI will continue to be rapid and lead to many important advances. With everything ‚Äúnew and shiny‚Äù there comes a good bit of hype. The goal for this short unit is to give you a very brief introduction to what you should think of when you hear AI, how AI is related to data analysis/machine learning, and how you could ‚Äì if you wanted ‚Äì do AI yourself (using R).\n\n\nNeural networks (NN)\nCurrently, the main workhorse of artificial intelligence (AI) is a type of method/algorithm called a neural net/network (NN).\nThese are specific kinds of models that originated as an attempt to capture the functioning of biological neurons, and, by extension, brains. While neural nets have been around for a while and have had some successes (e.g.¬†in digit recognition), they have really taken off in recent years with the advent of more data, faster computers, and better algorithms.\nWithout going into details, you can think of how neural nets work in a few ways. One is that they are collections of individual, in silico, neurons, which are combined into layers. On one end, the input is fed into the model, at the other end, the output is produced. See e.g.¬†Wikipedia page on artificial neural networks for some schematic drawing. The input are your predictor variables, e.g.¬†lots of characteristics measured for a patient; data from an -omics array; pixels of an image; words of some text; sounds of an audio file, etc. The output is whatever label you want to predict, e.g.¬†for some images it could be the 4 categories cat/dog/neither/both. You feed the data to the model and train it. The training of a NN is conceptually similar to training other machine learning (ML) algorithms. Each neuron has parameters associated with it, and as you fit the model, these parameters are tweaked to optimize performance. The bigger your neural network (more neurons and more layers), the more flexible the model. This means it is potentially more powerful, but also more data-hungry.\nWhile the analogy to a biological brain is apt, this comparison does sometimes make NN sound more complicated and fancy than they are. To demystify NN somewhat, you can switch out the comparison to a biological brain, and instead think of them as a coupled set of logistic regressions. Each neuron is more or less described by some type of logistic regression function. It gets some input from multiple variables (either the original input, or a previous layer of neurons), and then based on that input and its parameter setting returns either 0/No or Yes/1. The output is then fed to the next layer of networks. The NN is thus a combination of individual logistic regression models, coupled together in some way.\nThis idea of connecting individual models to make a bigger, better model should be familiar by now. You saw it when we discussed making models like random forests that combine individual trees, and it was also mentioned as a general idea of ensemble models. You can think of NN as an ensemble of simpler, logistic-type models.\nOf course, neural networks are complicated. At this point, there are many different versions of NN in use, and understanding them in detail is time-consuming. However, as general user, you don‚Äôt need to understand all the details. Instead, like for other complex ML models, as long as you know how to use them and evaluate what they return, it is ok to not fully understand their inner workings. Properly training/fitting NN can be tricky, because they generally have many different parameters that need tuning to achieve good performance. Thus, a lot of development has gone into methods that allow for efficient tuning/fitting. Fortunately for us as users, these methods have become fairly good such that it is possible to use NN algorithms built by others and generally trust they work well ‚Äì similar to us using other models (GLM, trees, SVM, etc.) ‚Äì without having to worry about all the details.\nIf you want to learn more about neural nets, the Wikipedia entry is a good place to start. Also check the AI Resources page.\n\n\nDeep Learning (DL)\nYou might also hear the term Deep Learning (DL) used in the context of AI. Some folks distinguish DL and AI, considering the former a subset of the latter. Deep Learning (DL) generally refers to using a specific class of neural networks, namely those that have multiple layers of (in silico) neurons (it has nothing to do with deep as in especially insightful). These days, DL is sometimes used a bit more loosely and can refer to any NN-based complex algorithm (or sometimes even a non-NN complex model) applied to a problem. However, most often if someone says/writes that they use deep learning to address some problem, they mean using a type of neural net to fit data.\n\n\nArtificial Intelligence (AI)\nArtificial Intelligence (AI) is definitely a trendy topic these days. It is widely used and also widely mis-used in many contexts these days. Roughly speaking, AI is the use of complex models, usually NN, to solve difficult problems. If one wanted to differentiate DL and AI, one might say that AI is the use of DL approaches applied to ‚Äúcomplex‚Äù problems. However, it seems these days that DL and AI are terms that are used largely interchangeably. Overall, if you hear AI or DL, you can think of it roughly as fitting a neural net model to data. Unfortunately, since DL and AI have become such hot topics, terms are often misused these days, especially outside academia.\nIf you want to learn a bit more on the distinction between DL and AI (and just in general), the AI and DL sections of Wikipedia are good starting points.\nAs with all ‚Äúnew and shiny‚Äù things, there seems to be a bit of a current trend to use DL/AI approaches even when not needed. As an example, DeVries et al.¬†2018 used deep learning to analyze data; this follow-up correspondence article from Mignan and Broccardo 2019 showed that a single logistic regression produced more or less the same results. The reply by one of the original authors, Meade 2019, is also worth reading.\nOverall, DL and AI are certainly very promising approaches and will undoubtedly lead to significant improvements in our ability to harness data. As with most technologies going through a ‚Äúbubble‚Äù there is currently work that is both substantial and important, and work that is fluffy and full of hype.\n\n\nDL and AI in R\n\nThis section is about using R to train/fit NN models, not how to use LLM AI tools like ChatGPT with R. That is covered in a separate unit.\n\nWhile R is a great tool for data analysis, it‚Äôs currently probably not the best choice for doing AI/fitting NN models. If you really want to go deep into AI work, using something like Python, Julia, or some more specialized programming language is likely better.\nThat said, there are some ways to use R to fit some NN models. The best and easiest way is likely to use R packages that allow you to interact with powerful tools such as TensorFlow and Keras through R.\nThe RStudio TensorFlow website has a lot of good information and documentation on how to use Keras through R to do DL/AI. Starting there with the Tutorials section is probably the best way to get going. After that you can branch out to some of the other resources. While I‚Äôm sure a DL/AI expert uses more than just Keras/Tensorflow as tools, you can get very far with those. And for playing around with DL/AI, Keras through R is a great place to start. See the exercise for suggested starting points.\n\nSince DL/AI usually involves fitting large amounts of data to complex models, time constraints are often crucial and, at times, powerful computers are needed. An important development is the use of GPU (graphical processing unit) computing. While modern computers usually have more than 1 CPU, they are still generally limited to a small number. Even a very powerful single desktop generally will have less than 100 CPUs. In contrast, modern graphics cards often have &gt;1000 GPUs that can all be used in parallel to perform model fitting. Products such as Tensorflow allow one to use (mainly NVIDIA) GPUs to fit complex models to a lot of data in an often reasonable amount of time without requiring a supercomputer cluster. Unfortunately, last time I tried, R still didn‚Äôt have great GPU support. I have not recently tried to use Keras with GPUs through R.\n\n\n\nFurther Resources\nSee the AI Resources page for some additional information on various AI topics.",
    "crumbs": [
      "Content",
      "AI",
      "Introduction to AI"
    ]
  },
  {
    "objectID": "content/module-ai/ai-eda-r/ai-eda-r.html",
    "href": "content/module-ai/ai-eda-r/ai-eda-r.html",
    "title": "AI tools for exploratory data analysis",
    "section": "",
    "text": "Overview\nIn this unit, we discuss using AI tools to help with exploratory data analysis.\n\n\nLearning Objectives\n\nKnow why and how to use AI to help with exploratory analysis.\nBe aware of possible confidentiality issues.\n\n\n\nIntroduction\nIn an exploratory analysis, you generally want to look at many aspects of your data to get a good idea of what you have in front of you. While R has many powerful functions that let you explore your data quickly, combining R with AI generated code can speed up things even more.\n\n\nConfidentiality and Privacy\nIt is important to re-iterate from a previous unit: If you allow the AI to ‚Äúsee‚Äù your data, this data might end up on the servers of the company running the AI (ChatGPT, Microsoft, Google) and might be used by them for future training of their models. Therefore, be careful with what you let the AI see. If you are re-analyzing publicly available data, you shouldn‚Äôt have to worry. But if the data is in any way confidential, it might not be a good idea to allow the AI to see it.\nThe best solution in that case is to generate synthetic data that looks like your real data, then ask the AI to write code to analyze this synthetic data. Once the AI gives you working code, you can take it off-line and apply it to your real data.\nSince synthetic (artificial/fake/simulated) data is very useful for many parts of the data analyis workflow, it is covered in a separate module.\nIn the following I‚Äôm assuming that you have data that can be shared with the AI.\n\n\nExploratory data analyis (EDA) with no data\nOk, this sounds dumb, but it‚Äôs not that stupid. Instead of trying to feed the AI your data, you can ask it to generate code that does EDA on hypothetical data. For instance, you could provide a prompt like this:\n\nWrite R code to perform an exploratory data analysis of a data frame called dat. The data frame contains the continuous variables age and BMI, and the categorical variables sex and favorite color. Write code that produces a summary table, univariate plots for each variable, and a bivariate plot of age versus BMI.\n\nWhen I typed the above into Bing AI in precise mode, I got this code. It‚Äôs a good start. You can take this code and modify so it can be applied to your data.\nOf course, you can take this further and ask the AI to both generate the data you have in mind, then write EDA code. Basically ask it to write code along the lines of the examples from the synthetic data module and EDA code.\n\n\nExploratory data analyis (EDA) with copy and paste\nBing and the free ChatGPT version currently do not provide a way to upload data.\nIf you have fairly simple dataset, you can paste it directly into the prompt.\nHere is an example. This dataset comes from the first example in the\nGenerating synthetic data with R unit. I opened the CSV file, and copied the whole thing into the prompt:\n\nPerform an exploratory data analysis using R code of the following data set:\nPASTE DATA HERE\n\n\nTo get a new line in your prompts, use Shift + Ctrl/Return (or whatever the equivalent is on a Mac üòÅ.)\n\nI received this code from the AI. It runs and does a few nice exploratory analyses, such as writing summary tables and making a few plots.\nIt would of course be better if I provided much more detailed instructions regarding the types of analyses, and possible even the R packages and functions I want to have. But I think you get the idea. If the data is not too big, you can write your prompt, then copy the data underneath and get code that does EDA on the data.\nNote that there are character limits. For Bing, the maximum is 4000 as of this writing. If your data is too big, it gets cut off. In the example above, the last few lines of data got cut. I think this is often not a big problem, since the data you supply isn‚Äôt the real data anyway. All you want is enough data for the AI to generate EDA code, then you take that code and apply it to your real data later, with AI turned off.\n\n\nExploratory data analyis (EDA) with file upload\nThe free version of ChatGPT or Bing do not allow upload of files. However, Claude does. I repeated the process from above, but instead of pasting the data, I uploaded the CSV file.\n\nWrite R code to perform an exploratory data analysis of the attached CSV file.\n\nI received this code from the AI. It runs and produces some figures. Of course, as you learned previously, it would be better if I provided a much more specific prompt, saying exactly what kind of tables and figures I want to see as output. But you get the idea.\nClaude allows you (as of this writing) to upload a maximum of 5 files with 10MB each. While that prevents you to perform EDA on very large datasets, it should be enough for many purposes, and is definitely more than the limit for the copy and paste method described above.\n\n\nCopilot\nI tried using Copilot to help with EDA, but it wasn‚Äôt very successful. This screenshot shows an example of me trying to get it to produce a scatterplot. It didn‚Äôt work, Copilot did not return any suggestion. It did suggest the other lines of code. I‚Äôm not sure if I‚Äôm just not using Copilot correctly, maybe you‚Äôve got better luck. It doesn‚Äôt hurt turning it on and trying to see if it can help you. If you find it useful, let me know so I can update this.\n\n\n\nCopilot coding attempt\n\n\n\n\nchattr package\nBased on the package description, it should be able to write code based on not only the content in the current file, but also by looking into variables in the current environment. So if you have a data frame called dat in your environment, you should be able to ask the AI to write code to analyze it. Unfortunately, so far I haven‚Äôt been able to get chattr to talk to OpenAI. I think this is because talking to ChatGPT via the API (which is what chattr does) requires a paid account.\n\n\nOther options\nThe subscription-level access to OpenAI‚Äôs GPT engines gives you access to their Advanced Data Analysis tool. This allows you to upload and explore data more easily. I don‚Äôt have a subscription so I haven‚Äôt tried it yet. But if you are a heavy user of AI for data analysis, this might be worth the cost. This article describes the Advanced Data Analysis tool a bit more.\nI have also found a few other tools that seem to focus on data analysis, for instance Julius, but that one only works for Python code.\n\n\nSummary\nI haven‚Äôt tried any of the premium/paid options yet. I assume they will be superior, but I wanted to focus on free options. Among those, I currently think for EDA these 2 are the best:\n\n(Make the AI) produce code that generates synthetic data that looks like you want it to, then feed that to the AI and ask it to write code to perform EDA on the synthetic data. Once you have working code, you can apply it to your real data.\nUse Claude to upload your (synthetic or otherwise ok to share) data and ask it to write code to perform EDA on it.\n\n\n\nFurther Resources\nSee the AI resource unit.\n\n\n\nCopilot coding attempt",
    "crumbs": [
      "Content",
      "AI",
      "Explore data with AI"
    ]
  },
  {
    "objectID": "content/module-ai/ai-analysis-r/ai-analysis-r.html",
    "href": "content/module-ai/ai-analysis-r/ai-analysis-r.html",
    "title": "AI tools to analyze data",
    "section": "",
    "text": "Overview\nIn this unit, we discuss ways you can use AI to analyze your data, aka fit models to it.\n\n\nLearning Objectives\n\nBe familiar with ways to fit models to data using AI.\n\n\n\nIntroduction\nThis is really not that much different from the EDA approach. Basically, you have your data and tell the AI to fit some models to it and evaluate them. Here are a few simple examples to give you an idea.\n\n\nUploading data to Claude\nI‚Äôm using the longitudinal drug concentration dataset we created in the Generating synthetic data with R unit. I uploaded it to Claude and gave it this prompt:\n\nWrite R code to analyze the data in the attached CSV file. Fit a linear model with drug concentration as predictor and cholesterol level as outcome. Also fit a logistic model with drug concentration as predictor and adverse events as outcome. For each model, provide model performance diagnostics. Use the tidyverse packages for data manipulation tasks, and the tidymodels framework for model fitting. Add thorough documentation to your code so it‚Äôs clear what each line of code does.\n\nI got this code back, which looked pretty good, but didn‚Äôt run. So I asked the AI to fix the code with this prompt:\n\nThis code does not work. It produces this error:\nError in augment.model_fit(lm_model) :\nargument ‚Äúnew_data‚Äù is missing, with no default\nPlease fix this and any other errors in the code.\n\nThe updated code looked like this. It fixed the first problem with augment(), but the resid() function returns NULL and something about the the specification for the logistic model is also not correct. Of course if I really wanted this to work, I‚Äôd ask the AI to keep fixing, and/or intervene manually. But for this example I don‚Äôt care that much if it works fully. The right bits seem to be there.\nInstead, I was curious if I can get it to produce some halfway reasonable code for a more complex model. Since this is longitudinal, time-series data, with multiple measurements per patient, a hierarchical approach would be better. Here is an attempt at having Claude write code for a hierarchical Bayesian model. Note that this model is still not a good one since it doesn‚Äôt really take into account the longitudinal structure, just the grouping within patients. But let‚Äôs try, with this prompt:\n\nWrite R code to analyze the data in the attached CSV file.\nUse the brms package to fit a hierarchical Bayesian linear model with drug concentration as predictor and cholesterol level as outcome, with patient as the grouping.\nFor each function call, use explicit notation to specify from which R package the function comes from.\nAdd thorough documentation to your code so it‚Äôs clear what each line of code does.\n\nHere is the code I got back. It looks promising, but again doesn‚Äôt quite work. Of course I could try to have Claude fix it or fix myself. But I don‚Äôt feel like it right now, I‚Äôve got some more content to write üòÅ. Feel free to send me a working version of the code (or any above) if you decide to fix it!\n\n\nGenerating data and fitting it\nI‚Äôm not sure if some of the problems with the code above would not show up if we used the - supposedly better - ChatGPT engine. We can try. Again, it‚Äôs difficult to feed it data. We could try the copy and paste approach. But instead, I‚Äôm trying another approach. I‚Äôll give Bing the code that generates the data, then ask it to fit models to that generated data. Here is the prompt:\n\nThe code below produces a data frame called syn_dat2. Take the data in that data frame and fit a linear model with drug concentration as predictor and cholesterol level as outcome. Also fit a logistic model with drug concentration as predictor and adverse events as outcome. For each model, provide model performance diagnostics. Use the tidyverse packages for data manipulation tasks, and the tidymodels framework for model fitting. For each function call, use explicit notation to specify from which R package the function comes from. Add thorough documentation to your code so it‚Äôs clear what each line of code does.\nCOPY AND PASTE THIS CODE HERE\n\nI fed this to Microsoft Copilot in Precise mode. I also omitted the parts of the code after the data frame has been generated.\nThis is what I got back. Again looks promising but doesn‚Äôt work. I then gave it this prompt:\n\nThe code does not work. Please fix it. Also, update it such that the actual R package from which a function comes from is called, not the tidymodels collection of packages.\n\nThe result was code that used the standard lm() and glm() functions. While ok, that‚Äôs not what I wanted, I still wanted tidymodels syntax, just not the tidymodels::function notation. So my next prompt was:\n\nChange the code above such that instead of using the lm() and glm() functions, it uses the tidymodels set of functions\n\nI ended up with this. It makes the same mistake as Claude, not properly turning the adverse event variable into a factor before trying to fit a logistic model. Some other bits also didn‚Äôt fully work, but I didn‚Äôt feel like fixing further.\nI think you get the idea. Basically, the AI can produce quite useful bits of code that can speed up things, but it‚Äôs rarely fully correct and you will always need to check it, and often at the end still manually intervene. I‚Äôm sure as time goes by, what you get on the first try will get increasingly better. Still, you need to know what exactly you want and understand if the output makes sense or not.\n\n\n\n\n\n\nSummary\nI hope you can see how using AI tools to help with data analysis can potentially save a lot of time. Also, I think these examples make it quite clear: To be able to use those tools successfully, you need to know enough to understand what the AI should do, and if what it returns makes sense or not. If things don‚Äôt run, there is an obvious error. But it is quite possible that the code runs but doesn‚Äôt actually do the right thing. You only know this if you are familiar with what you want to accomplish. So while these AI tools can speed up coding a lot, they still require you to be an expert on whatever you are working on. Sorry! (Or maybe good, this means instead of not having a job in the near future, you‚Äôll likely just have a job that involves you being the master of AI tools, among other skills such as technical and subject matter expertise.)\n\n\nFurther Resources\nSee the AI resource unit.",
    "crumbs": [
      "Content",
      "AI",
      "Analyze data with AI"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html",
    "title": "Subset Selection",
    "section": "",
    "text": "In this unit, we will cover the idea of subset selection.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#forward-selection",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#forward-selection",
    "title": "Subset Selection",
    "section": "Forward selection",
    "text": "Forward selection\nIn forward selection, you start with the null-model (no predictors), then evaluate all single-predictor models. The best-performing 1-predictor model is chosen, and you build all 2-predictor models on top of that chosen 1-predictor model. You choose the best 2-predictor model, then go on to add a third, and do that until adding any further predictors does not lead to a model that performs better (again, measured by CV on the test set) than the smaller model. E.g., you might find that adding any of the remaining 3rd predictors does not lead to a better-performing model than the 2-predictor model you already have. Then you stop here and pick the 2-predictor model as your best.\nLet‚Äôs consider an example. Say you want to predict BMI based on age, sex, calorie intake, and exercise level. Since BMI is continuous, we can consider a linear regression model, and we might try to minimize RMSE between model-predicted BMI and actual measurements. We start by computing RMSE for the null model. Then we compute RMSE for all 1-predictor models. Assume that all 1-predictor models have RMSE lower than the null model, and the lowest RMSE (all computed using cross-validation) of the 1-predictor models is the one that includes calorie intake. Next, you try all 2-predictor models that include calorie intake. Among those, a model with calorie intake + exercise level has the lowest RMSE, and it‚Äôs also lower than the model with calorie intake only. Next, you try all 3-predictor models that include the 2 chosen predictors. In this example, only 2 of those 3-predictor models are possible, namely calorie intake + exercise level + age and calorie intake + exercise level + sex. Let‚Äôs assume both of those models have RMSE that is larger than the 2 predictor model. Note, it is important to compute RMSE through cross-validation since the RMSE evaluated on the data used for fitting will always be lower for the bigger model. We thus found our best model, namely one that includes the 2 predictors calorie intake + exercise level.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#backward-selection",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#backward-selection",
    "title": "Subset Selection",
    "section": "Backward selection",
    "text": "Backward selection\nBackward selection is essentially the reverse to forward selection: You start with the model with all predictors, then evaluate all models with one predictor dropped. The smaller model with the best performance is your choice. You then drop each predictor at a time from that model. You continue until dropping predictors does not improve performance anymore. Then you keep the model with the best performance. Note that this method does not work on datasets that have more predictors than observations (e.g., most -omics type data).\nLet‚Äôs consider the above example again. We now start with a model that includes all 4 predictors and compute cross-validated RMSE. Next, we drop each predictor and try all 3-predictor models. Let‚Äôs say the one that dropped exercise level is the 3-predictor model with the lowest RMSE, lower than all other 3-predictor models, and lower than the full model. Next, we drop each variable from the current best candidate 3-predictor model, which is age + sex + calorie intake. All of the 2-predictor models have RMSE larger than the 3-predictor model. We thus found our final model, with the 3 predictors age + sex + calorie intake.\nNote that it is also possible to do ‚Äúforward-backward‚Äù selection: you can choose to start with the full or reduced model, but the method is allowed to add/remove predictors at every step depending on what improves performance most.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#exhaustive-subset-selection",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#exhaustive-subset-selection",
    "title": "Subset Selection",
    "section": "Exhaustive subset selection",
    "text": "Exhaustive subset selection\nYou might have noticed that in my example, forward selection and backward selection do not necessarily produce the same model. The reason for that is that neither forward- or backward-selection try all possible models (predictor combinations). Both approaches use what is called a greedy strategy. Once they included or excluded a predictor, it can never be dropped or included. Once you picked your 1-predictor model, all subsequent models will consist of that 1 predictor model - even if there might be a better performing 2-predictor model that doesn‚Äôt include the 1st predictor you picked. The same holds for dropping predictors in backward selection.\nAlso, as in my example, often (but not always), the final model determined through backward selection tends to include more predictors than a model determined through forward selection. In such a case, you have to decide which model to choose. Alternatively, you can try a different way of choosing the model. One approach is to try models with all combinations of predictors. This is called exhaustive subset selection, all subsets selection, or best subset selection (although note that ‚Äúbest subset selection‚Äù can also refer to a different method which still hasn‚Äôt gained much traction). If you have only a few predictors, you can try every combination of predictors. However, once you have more than 5-10 predictors (depending on the number of observations), trying every combination of sub-models would be too time-consuming. Therefore, most of the time, such an exhaustive feature selection is not feasible.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#other-subset-selection-methods",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#other-subset-selection-methods",
    "title": "Subset Selection",
    "section": "Other subset selection methods",
    "text": "Other subset selection methods\nThere have recently been a number of approaches that try to be somewhat smarter in finding the best model, without having to try every combination. The caret package has several methods, such as Genetic Algorithms and Simulated Annealing. Those methods often do a good job finding a very good sub-model in a reasonable amount of time. While the only guarantee that you found the single best model is to try them all using exhaustive subset selection, these modern approaches usually find a model that is either the best or at least close to the best while doing so rather efficiently. See sections 18-22 of the caret manual for more information if you are interested. The mlr/mlr3 packages also have similar such selection methods. tidymodels currently does not have any subset selection procedure implemented (see more below).",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#manual-subset-selection",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#manual-subset-selection",
    "title": "Subset Selection",
    "section": "Manual subset selection",
    "text": "Manual subset selection\nAll the approaches above use cross-validated performance of some metric to determine which predictors should be included in the model. Another, still commonly used strategy is to do subset selection manually. The analyst usually starts with a simple model containing just one predictor (the most important one), and then decides based on model results (e.g., systematic deviations in the residuals) and/or based on scientific knowledge, if adding additional predictors is justified. For a skilled analyst with a lot of subject matter expertise, this can lead to models that are all-around very good. They have good performance, make scientific sense, and are still simple. Unfortunately, this requires a lot of knowledge, time and careful analysis. Also, the analyst needs to honestly report any model they tried, not just the one they end up with. Otherwise they are using and not reporting researcher‚Äôs degrees of freedom, which leads to overfitting. Most analyses are not done that way.\n\nI mentioned this before in various places and it‚Äôs a theme throughout the course, that unless data is collected and analyzed using pre-specified protocols (e.g., a clinical trial for FDA approval) it is almost always the case that scientists have so much flexibility during the data collection and analysis stages that p-values and similar statistical measures are meaningless. The Researcher‚Äôs degrees of freedom issue is everywhere and essentially impossible to get rid of. For more on this topic, see the Pitfalls section on the General Resources page.\nAll of this applies to manual subset selection (which is why I‚Äôm thinking about it here and adding this note), but it is more general and applies to every step in a scientific project.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-overview.html",
    "href": "content/model-improvement/model-improvement-overview.html",
    "title": "Model improvement",
    "section": "",
    "text": "Overview\nAlmost always, your first model is not that great. You generally want to improve your models to find a good one.\nThis module discusses ways to improve your models.\n\n\nLearning Objectives\n\nBe familiar with methods to improve models.\nBe able to perform different model improvement tasks.\n\n\n\nContent\nThe module consists of the following units:\n\nModel improvement introduction\nSubset selection\nRegularization\nTuning",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Model Improvement Overview"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html",
    "href": "content/model-improvement/assessment-model-improvement.html",
    "title": "Assessment - Improving Models",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#setup",
    "href": "content/model-improvement/assessment-model-improvement.html#setup",
    "title": "Assessment - Improving Models",
    "section": "Setup",
    "text": "Setup\nYou‚Äôll continue working on the exercise you started previously. We‚Äôll also do some group work again, using the ‚Äì by now familiar ‚Äì M1, M2, ‚Ä¶ setup. Assign each other a number. As much as possible, do it such that you end up working with group members you have not (or not in a while) worked with. Use the ‚Äòcircular‚Äô setup such that everyone will work on their own repository and on one other person‚Äôs repository.\n\nThis exercise is heavily inspired by the Getting Started tidymodels tutorial. If you haven‚Äôt yet, now would be a great time to at least skim through the tutorial and make sure you understand the overall steps.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#part-1",
    "href": "content/model-improvement/assessment-model-improvement.html#part-1",
    "title": "Assessment - Improving Models",
    "section": "Part 1",
    "text": "Part 1\nWe‚Äôll continue this exercise using the previous Quarto fitting-exercise.qmd file.\nAt this stage, I assume all the data wrangling and EDA code, as well as the model fitting code you worked on previously is present and fully functional. If there are still some issues that need to be resolved, go ahead and do so.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#data-prep",
    "href": "content/model-improvement/assessment-model-improvement.html#data-prep",
    "title": "Assessment - Improving Models",
    "section": "Data prep",
    "text": "Data prep\nWe are doing one more change to the data. Since the RACE variable has a few values that are coded weirdly (7 or 88, which probably means some sort of missing), we‚Äôll just for completely remove that variable.\nSo for this exercise, we‚Äôll have a data frame with 120 observations and 6 variables, namely Y, DOSE, AGE, SEX, WT, HT.\nNow, write code that takes the data and splits it randomly into a 75% train and 25% test set, following for instance the example in the Data Splitting section of the Get Started tidymodels tutorial.\n\nWe only have 120 observations here. This is not much, so for a real project, it might not make a lot of sense to do a train/test split. Instead, one would probably use all data during the model fitting process, and then use cross-validation to try and estimate model performance on unseen data. But since this is an exercise, we‚Äôll do the train/test split.\n\n\nAlso note that we are doing a random train/test split. But for a case like we have here, one might want to split differently. For instance we could split by using everyone who got the lower doses as training data, and then testing if we can predict the higher-dose outcomes. This is a common question of interest in drug development.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-fitting",
    "href": "content/model-improvement/assessment-model-improvement.html#model-fitting",
    "title": "Assessment - Improving Models",
    "section": "Model fitting",
    "text": "Model fitting\nUse the tidymodels framework to fit two linear models to our continuous outcome of interest, (here, Y). The first model should only use DOSE as predictor, the second model should use all predictors. For both models, the metric to optimize should be RMSE.\nThis is the same as you did previously, but for this exercise, you should only use the training data set for fitting.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-performance-assessment-1",
    "href": "content/model-improvement/assessment-model-improvement.html#model-performance-assessment-1",
    "title": "Assessment - Improving Models",
    "section": "Model performance assessment 1",
    "text": "Model performance assessment 1\nCompute predictions for the two models. Then use observed and predicted values to compute RMSE of the best-fitting model.\nAlso compute the RMSE of a null-model, one that would just predict the mean outcome for each observation, without using any predictor information.\nCompare the 3 RMSE values and discuss which model is performing the best (according to those metrics).",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-performance-assessment-2",
    "href": "content/model-improvement/assessment-model-improvement.html#model-performance-assessment-2",
    "title": "Assessment - Improving Models",
    "section": "Model performance assessment 2",
    "text": "Model performance assessment 2\nSo far, we have assessed performance on the training set. But as you know by now, that is often not really of interest, we want to know how the model performs in general. We could use our test data to check, but we want to reserve that until the very end, to get a final ‚Äúfair‚Äù assessment of our best model.\nWe could use an information criterion, such as AIC, to try to estimate how well the models might perform on unseen data. Even better, we can use use cross-validation to compute performance on unseen data.\nFollow the Evaluate your model with resampling section of the tutorial and do a 10-fold cross-validation. Doing so, you fit the 2 models to the data 10 times. Each time, you use 90% to fit the model, and 10% to evaluate (compute the RMSE) of the model. If you take the average for the RMSE over the 10 folds, you get an estimate for the model performance on data not used for fitting. It‚Äôs the same idea as doing the initial train/test split, but now we use it as part of the model building and choosing part, while we keep the test data as a final evaluation of the model and won‚Äôt use it before then.\nCompute the RMSE for both models again. Of course nothing changes for the null model. Compare the new RMSE estimates obtained through CV with those obtained earlier. What did and didn‚Äôt change?\nRun the code again that creates the CV folds and does the fitting. If you did set a random seed, change the value. If you didn‚Äôt set one, it will automatically draw different values. The RMSE values for the CV fits will change. That‚Äôs just due to the randomness in the data splitting. If we had more data, we would expect to get less variability. The overall pattern between changes in the RMSE values for the fits to the training data without CV, and what we see with CV, should still be the same.\n\nIf you want to get more robust RMSE estimates with CV, you can try to set repeats to some value. That creates more samples by repeating the whole CV procedure several times. In theory this might give more robust results. You might encounter some warning messages. This is likely related that occasionally, by chance, data is split in a way that some information (e.g., a certain sex or race in our data) is missing from one one of the folds. That can cause issues.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#wrap-up-part-1",
    "href": "content/model-improvement/assessment-model-improvement.html#wrap-up-part-1",
    "title": "Assessment - Improving Models",
    "section": "Wrap up part 1",
    "text": "Wrap up part 1\nMake sure everything runs and works as expected. Also make sure everything is well commented/documented/explained! Then commit, push and tell your classmate that they can take over. Finish this by Wednesday.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#part-2",
    "href": "content/model-improvement/assessment-model-improvement.html#part-2",
    "title": "Assessment - Improving Models",
    "section": "Part 2",
    "text": "Part 2\nFork and clone (or if you are added as collaborator, clone directly) your classmate‚Äôs repository. Open their fitting-exercise.qmd file.\nAdd a heading that says # This section added by YOURFULLNAME. I need this so I can grade accordingly.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-predictions",
    "href": "content/model-improvement/assessment-model-improvement.html#model-predictions",
    "title": "Assessment - Improving Models",
    "section": "Model predictions",
    "text": "Model predictions\nNothing beats a visual inspection of results. A very useful figure is one that plots predicted versus observed values. This can quickly show you if there are important deviations that suggest the model isn‚Äôt very good.\nPut the observed values and the predicted values from your 3 models into a data frame. Also add a label to indicate the model. Then use ggplot to create a figure that plots (as symbols) observed values on the x-axis and predictions (from each of the 3 models, including the null model) on the y-axis. Let both x and y axes go from 0 to 5000 and add a 45 degree line. For a great model, the points will fall along that line, namely observed and predicted values agree - with some scatter.\nFor your figure, you should see that the the null model, it‚Äôs a straight horizontal line. We expect that, since we predict exactly the same value (the mean) for each observation. For the model that only includes dose, you should the data fall along 3 horizontal lines. Can you understand why? If not clear, look at the DOSE variable and the values it takes. That should tell you why you only get 3 different predicted values for the outcome.\nThe model with all predictors looks the best, though there‚Äôs still a lot of scatter around the diagonal line, suggesting that there‚Äôs still a lot of the outcome pattern that our model can‚Äôt explain. That could either be because we are missing important information, i.e.¬†we need more variables. Or it could be that the model is too simple, for instance it could be that the outcome depends on some variable in a nonlinear way.\nWe can‚Äôt do anything about the former problem (other than collecting more data), but we can try to explore if more complex models can capture the data better. We‚Äôll do that in an upcoming exercise.\nFor this exercise, we‚Äôll do one more exploration of our existing models.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-predictions-and-uncertainty",
    "href": "content/model-improvement/assessment-model-improvement.html#model-predictions-and-uncertainty",
    "title": "Assessment - Improving Models",
    "section": "Model predictions and uncertainty",
    "text": "Model predictions and uncertainty\nAs you learned, model predictions without a measure of uncertainty are usually not that useful. We generally always want to determine uncertainty in our estimates/predictions.\nWhile for a linear model it is possible to get uncertainty directly from the uncertainty of the parameter estimates, for more complex models, this is not possible. However, it is always possible to use the bootstrap method to sample the data, fit models to the data, and get uncertainty that way.\nWe‚Äôll give that a try. Since our plot above suggested that model 1 is not good, we‚Äôll focus here on model 2.\nUse the bootstraps function from the rsample package to create 100 bootstraps.\nThen write a loop (or use a map or apply function) to fit the model to each of the bootstrap samples and make predictions from this model for the original training data. Record all predictions (e.g., in an array or a list).\nIf you are wondering how to get the individual bootstrap samples out of the object that bootstraps gave you back: Assuming you called the object that you got back dat_bs, you can get a single bootstrap sample with this code:\ndat_sample = rsample::analysis(dat_bs$splits[[i]])\nOnce you have all your predictions stored, compute the mean and confidence intervals. Assuming your predictions are stored in an array called pred_bs that has as many rows as samples and columns as data points, this bit of code computes mean and 89% confidence intervals.\npreds &lt;- pred_bs |&gt; apply(2, quantile,  c(0.055, 0.5, 0.945)) |&gt;  t()\nFinally, make a figure that plots observed values on the x-axis, and mean and the upper and lower bounds of the bootstrapped confidence intervals for your predictions on the y-axis. You can for instance use black symbols for mean and some color to indicate lower and upper confidence limits. As above, make sure x- and y-axis are on the same and add a 45 degree line.\nInterpret what you see in this plot.\n\nThe bootstrap uses sampling, and CV uses sampling. The difference is that for bootstrap, we sample from all the (training) data with replacement. For CV, we randomly partitioned some of the data into the part used to fit, the other part was used to evaluate/assess. So the two approaches are similar, but not the same.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#final-evaluation-using-test-data",
    "href": "content/model-improvement/assessment-model-improvement.html#final-evaluation-using-test-data",
    "title": "Assessment - Improving Models",
    "section": "Final evaluation using test data",
    "text": "Final evaluation using test data\nLet‚Äôs do a final model evaluation, this time using the test data.\nUse the fit for model 2 you computed previously, and now use this model to make predictions for the test data.\nThen make a plot that shows predicted versus observed for both the training data (which you did above) and in the same plot, also show predicted versus observed for the test data (using, e.g.¬†different symbols or colors).\nYou should see that the observed/predicted values for the test data are mixed in with the train data. That‚Äôs a good sign. If the test data points were systematically ‚Äúoff‚Äù, it would indicate a problem such as overfitting to the training data.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#overall-model-assessment",
    "href": "content/model-improvement/assessment-model-improvement.html#overall-model-assessment",
    "title": "Assessment - Improving Models",
    "section": "Overall model assessment",
    "text": "Overall model assessment\nLet‚Äôs critique all our models. Here are some points for discussion:\n\nWe want to make sure that any model we have performs better than the null model. Is that the case?\nDoes model 1 with only dose in the model improve results over the null model? Do the results make sense? Would you consider this model ‚Äúusable‚Äù for any real purpose?\nDoes model 2 with all predictors further improve results? Do the results make sense? Would you consider this model ‚Äúusable‚Äù for any real purpose?\n\nUse all the assessment bits you computed above to reason about the models. And I‚Äôm not really looking for ‚Äúthe right‚Äù answer here (if that even exists), I just want to see that you are thinking about what your fit results and thus underlying models mean, and that you can assess their strengths and weaknesses and interpret what you are doing and seeing.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#wrap-up-part-2",
    "href": "content/model-improvement/assessment-model-improvement.html#wrap-up-part-2",
    "title": "Assessment - Improving Models",
    "section": "Wrap up part 2",
    "text": "Wrap up part 2\nMake sure everything runs and works as expected. Then commit, push and if you forked the repo, initiate a pull request. Tell our classmate that its done.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#test-and-website-update",
    "href": "content/model-improvement/assessment-model-improvement.html#test-and-website-update",
    "title": "Assessment - Improving Models",
    "section": "Test and website update",
    "text": "Test and website update\nThe original repository owner should make sure everything works. Make any further updates that you think need to be made. Then rebuild your portfolio website and make sure everything works and shows up as expected. Then commit and push.\nSince this is part of your portfolio site, you don‚Äôt need to post anything, I know where to find it.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-overview.html",
    "href": "content/model-fitting/model-fitting-overview.html",
    "title": "Model fitting",
    "section": "",
    "text": "Overview\nAt the heart of most data analyses is the step at which the data is brought into close contact with models. This is at times called the statistical analysis part, though often that term is also applied to the whole data analysis process. A more descriptive term is model fitting. The model is being fit to the data - what exactly that means and entails is the topic of this module.\n\n\nLearning Objectives\n\nBe familiar with steps involved in model fitting.\n\n\n\nContent\nThe module consists of the following units:\n\nModel fitting introduction\nFitting basic models\nModel fit evaluation\nModel fitting in R",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model Fitting Overview"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html",
    "title": "Model performance metrics",
    "section": "",
    "text": "This unit discusses the idea of assessing a model based on its performance through some metric.\n\nI use the term model performance here in a narrow sense, quantified through some metric.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#least-squares",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#least-squares",
    "title": "Model performance metrics",
    "section": "Least squares",
    "text": "Least squares\nYou are likely familiar with one of the most widely used functions for f, the least-squares method. If you have ever fit a linear model (e.g.¬†using lm() in R or an equivalent function in a different statistical software), chances are you used least squares as your objective function (maybe without knowing that you did). For least squares, we compute the squared difference between model prediction and data for each observation and sum it all up. In equation form:\n\\[C = f(Y_m,Y_d) = \\sum_i (Y_m^i - Y_d^i)^2\\] Again, \\(Y^i_m\\) are all the predictions made by the model for the outcome, and the \\(Y^i_d\\) are the actual outcomes from the data. The quantity \\(C\\) for this equation has many names. A common one is least squares error, or sum of square residuals (SSR), or residual sum of squares (RSS), or sum of squares (SS), or residual square error (RSE), and a bunch of similar names. You will usually be able to tell from the context what is being used as the performance metric.\nYou will often see a variation where one divides by the sample size, i.e.¬†\\(C\\) will look like\n\\[C = \\frac{1}{N} \\sum_i (Y_m^i - Y_d^i)^2\\]\nThis is called mean squared error (MSE). Of course, other names exist.\nDividing by the sample size has the advantage of allowing you to compare values across samples of different size from the same dataset (but it doesn‚Äôt really work for comparing across different datasets). For instance if you compare model performance on training and test data (to be discussed shortly), and each has different sample size, you need to make sure you standardize by it.\nIf you want to compare different models on the same dataset which might include some missing values, and one of your models can deal with missing data while the other cannot, you need to be careful. One option is to fit both models only to the data without missing values. If you decide to allow one model to use the observations that have some missing values, while the other model does not, you definitely need to standardize by the sample size. Even then, care is needed, since the samples with some missing data might be systematically different from those without and thus the datasets might not be equivalent anymore.\nAnother variant is a version where at the end you take the square-root, i.e.\n\\[C = \\sqrt{\\frac{1}{N} \\sum_i (Y_m^i - Y_d^i)^2}\\]\nwhich is called the root mean squared error (RMSE). The advantage of taking the square-root at the end is that now the units of \\(C\\) are the same as those of your outcomes. THis often makes interpretation of the results easier. In general, it is best to use MSE or RMSE. In tidymodels, the yardstick package has the rmse() metric built-in.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#coefficient-of-determination",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#coefficient-of-determination",
    "title": "Model performance metrics",
    "section": "Coefficient of determination",
    "text": "Coefficient of determination\nAn equivalent alternative to SSR is to use a quantity called Coefficient of determination, or more commonly \\(R^2\\) (R-squared). This quantity is defined as \\[R^2 = 1-RSS/TSS\\] where RSS is the residual sum of square introduced above and TSS is the total sum of squares.3\nThe latter is defined as\n\\[TSS =  \\sum_i (Y_{av} - Y_d^i)^2\\]\nwhere \\(Y_{av}\\) is the mean of the data. Therefore, the equation for \\(R^2\\) is\n\\[R^2 = 1-  \\frac{\\sum_i (Y_m^i - Y_d^i)^2}{\\sum_i (Y_{av} - Y_d^i)^2}\\]\nSince TSS is fixed for a given dataset, minimizing SSR is equivalent to maximizing \\(R^2\\). You might see \\(R^2\\) reported in papers, and it is highly likely that the fitting was performed by minimizing SSR (or MSE or RMSE). The SST is a useful quantity since it tells us the performance of a dumb/null model which uses no information from any predictor variables, but instead just predicts the mean of the outcomes. Any model you build that includes predictors needs to do better than this dumb null model.\nLeast squares fitting is simple and frequently used. A lot of standard routines in major statistical packages use least squares. It often makes good sense to penalize predictions that deviate from the actual value with the squared distance (and under certain conditions, this is equivalent to maximizing the likelihood). However, sometimes a different way to define the function f might be useful. We‚Äôll discuss a few of them briefly.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#beyond-least-squares",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#beyond-least-squares",
    "title": "Model performance metrics",
    "section": "Beyond least squares",
    "text": "Beyond least squares\nAn alternative to least squares is to penalize not with the distance squared, but linearly with the (absolute) distance. This metric is called (mean) absolute error (MAE) or (least) absolute deviation, and the model is\n\\[C = f(Y_m,Y_d) = \\sum_i |Y_m^i - Y_d^i|\\]\nThis approach can be useful if the data contains outliers (that are real, and one can‚Äôt justify removing them during cleaning). With a squared distance penalty, outliers have a strong impact on the model fit. With a linear penalty, such outliers carry less weight. Because of this, the linear difference approach is sometimes called a robust estimation.\nRobust estimation methods such as MAE are not as common as least squares, but most software has support for it. For instance, there are multiple packages in R that support MAE and other robust fitting methods.\nYou might wonder why we don‚Äôt just use those robust methods all the time, just to be safe. There are fundamental statistical reasons why optimizing the MSE is often more aligned with the question we want to address, so you should switch to a robust method only after you thought about it and decided it‚Äôs the right approach for your specific scenario. You can also always do it both ways and compare results to get an idea of the potential influence on your results from the outliers. Doing things more than one way ‚Äì and of course reporting each approach you tried ‚Äì is generally a great idea. This is often called performing a sensitivity analysis.\nAnother way to define f is with step functions. The idea is that as long as model and data are within some distance, the penalty is zero. Once model and data differ by some threshold, a penalty is given, e.g., a fixed value or a linear or quadratic penalty. Such types of schemes to define f are common in the class of models called Support Vector Machines, which we will look at later in the course.\nNo matter what scheme you choose, it might at times be useful to weigh data points. In the examples given above, each model-data pair was given the same importance. Sometimes it might be that you have some data points that should carry more weight. A common case is if you have measurements not on the individual level but some aggregate level. As an example, assume you have a fraction of heart attacks among all patients for some time period in different hospitals, and you want to fit that fraction. You don‚Äôt know the number of people who had heart attacks, only the fraction. But you do know something about the total number of beds each hospital has. You could then argue that hospitals with more beds have more patients and thus likely more heart attacks, and therefore the data from larger hospitals should get more weight, and you could e.g., multiply each term in the sum by bed size. Note that this is a scientific decision based on your expert knowledge of the data. Almost all fitting routines allow you to provide weights for your data and you can then perform weighted least squares (or a weighted version of whatever other approaches you choose).",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#accuracy",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#accuracy",
    "title": "Model performance metrics",
    "section": "Accuracy",
    "text": "Accuracy\nThe simplest way to determine performance of a model for categorical data is to count the fraction of times the model did (not) correctly predict the outcome. If we instead count the fraction of correct predictions made by the model, it is called accuracy. If we focus on the number of times the model got it wrong, it is called the (mis)classification error. The yardstick package has it as metric accuracy().\nWhile accuracy is often not a bad choice of metric, sometimes just counting the number of times a model didn‚Äôt get it right might not be the best idea. A common situation where accuracy (just counting how often the model prediction is right/wrong) is not very useful is for rare (but important) outcomes. Data of this type is often called unbalanced data. As an example, say we had some algorithm that tried to use brain images to predict if people have brain cancer. Fortunately, brain cancer is rare. Let‚Äôs say (I‚Äôm making this number up) that 1 in a million people who undergo this screening procedure actually have this cancer. Therefore, a model that predicts that nobody has cancer would be a very accurate model, it would only make one mistake in a million. However, missing that one person would be a very important mistake. We likely would prefer a model that flags 10 people as (potentially) having a cancer, including the person who really does and 9 false positives. This model has worse accuracy since it gets 9 out of 1 million wrong. But it‚Äôs likely more important to catch the one true case, even if it means temporarily scaring several other individuals, until further checks show that they are healthy. Of course this trade-off is very common. You likely know it in the context of balancing sensitivity and specificity of say a clinical test. Other areas where this happens is, e.g., predicting the (hopefully rare) errors when building a plane engine, flagging the (rare) credit card fraud, and many others of that type.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#beyond-accuracy",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#beyond-accuracy",
    "title": "Model performance metrics",
    "section": "Beyond accuracy",
    "text": "Beyond accuracy\nIn instances where accuracy/misclassification is not a good performance metric, other metrics are more helpful. A large variety of such metrics exist. Some only apply to the special (but very common) case of a binary outcome, and others apply more generally. Fairly common ones are: (Cohen‚Äôs) Kappa, Area under a Receiver operating curve (AUC ROC), Matthews correlation coefficient and F-score.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#the-confusion-matrix",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#the-confusion-matrix",
    "title": "Model performance metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\nFor the common case of a binary outcome, one can construct what is called the confusion matrix (also known as 2x2 table in epidemiology). The confusion matrix tracks 4 quantities: true positives (TP, model predicts positive, and data is positive), true negative (TN, both model and data are negative), false positive (FP, model wrongly predicts positive) and false negative (FN, model wrongly predicts negative).\nThe confusion matrix comes with very confusing terminology since many of the quantities are labeled differently in different fields. For instance, epidemiologists tend to call the quantity TP/(TP+FN) sensitivity, while in other areas, such as machine learning, it is called recall. For a good overview of those different quantities and terminology, see this Wikipedia article.\nOne can of course also make a table for a case with more than 2 outcomes and track the true values versus the predictions. Some of the metrics generalize to such a scenario of more than 2 outcomes, but not all do.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#footnotes",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#footnotes",
    "title": "Model performance metrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis business of varying the model in a smart way until you find the best one is an example of what‚Äôs called optimization. Developing routines that can quickly, efficiently, and reliably find the optimum is a huge area, and depending on the model and data you are trying to fit, having smart algorithms to do the optimization can be very challenging. You can make a whole career just working on optimization topics. Fortunately, for most purposes, we can generally use existing software and rely on their built-in optimizers.‚Ü©Ô∏é\nTechnically, for some simple models, it is possible to find the best parameter values other than by ‚Äòtrial and error‚Äô, but for many other models, this is more or less what is happening.‚Ü©Ô∏é\nAlso called SST, sum of squares total.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html",
    "href": "content/model-fitting/assessment-model-fitting.html",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#setup",
    "href": "content/model-fitting/assessment-model-fitting.html#setup",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Setup",
    "text": "Setup\nThis will be part of your portfolio site. Make a new folder called fitting-exercise. Also make a new Quarto document inside the folder and call it fitting-exercise.qmd.\n\nIn general, I suggest using a folder structure like the one from data-analysis-template and having separate pieces of code and Quarto files. For the purpose of this exercise, we‚Äôll keep it simple and do everything inside the fitting-exercise folder and fitting-exercise.qmd file.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#data",
    "href": "content/model-fitting/assessment-model-fitting.html#data",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Data",
    "text": "Data\nWe are going to use data on a drug candidate called Mavoglurant. This is The original paper describing and analyzing the data. You don‚Äôt need to look at it, unless you want to üòÅ. In case you do, since the paper might be behind a paywall, here‚Äôs a local copy of the pdf.\nThe data is part of an R package called nlmixr2data. You could get it by installing that pacakge. But the easiest way to get the data is to download it from this Github repository, which is part of another paper you don‚Äôt need to read - but might want to peek at, see below.\n\nGo to the GitHub repository, download the Mavoglurant_A2121_nmpk.csv file and place it into the folder you just created.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#data-processing-and-exploration",
    "href": "content/model-fitting/assessment-model-fitting.html#data-processing-and-exploration",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Data processing and exploration",
    "text": "Data processing and exploration\nBefore we can do any analysis, we need to explore and process the data. The data we are looking at here has the ‚Äì very common ‚Äì feature that on first glance, it is confusing üôÑ. Variables have weird names and there seems to be a strange structure. In almost any real-world setting, this is the kind of data you‚Äôll be given. So let‚Äôs do some exploration and processing.\nUnfortunately, the data does not come with a codebook. The documentation of this dataset in the nlmixr2data package provides a bit of information. However, this documentation is very sparse and not enough. Another way to learn a bit more is to take a quick look at the papers that were published covering the data.\nThe second paper mentioned above is likely giving you a quicker overview compared to the first one. Take a look at the Data section and Figure 3, which shows the data as symbols. Based on that, you‚Äôll figure out that this is time-series data of drug concentrations.\nYou can get a quick visual idea of the data by plotting the outcome variable (which according to the documentation is called DV here) as a function of time, stratified by DOSE and using ID as a grouping factor.\n\nWrite code to load the data into R. Then write code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose (e.g., use a different color for each dose, or facets).\n\nAs you look closer, the formatting of the dataset still looks a bit strange. So let‚Äôs dig deeper. One thing you will notice is that there are some individuals that seem to have received the drug more than once, indicated by having both entries with OCC=1 and OCC=2. Since we are not sure what the difference is, and to keep things simple, we only keep one dataset for each individual. Therefore, remove all entries with OCC=2.\n\nWrite code that keeps only observations with OCC = 1.\n\nYou will also see that each individual has an entry at time 0 that has DV=0 and some non-zero value for AMT. This is the dosing entry for everyone. All the other entries are the time-series values for the drug concentration. We won‚Äôt do a time-series analysis here, so instead we‚Äôll compute the total amount of drug for each individual by adding all the DV values. Note that this is a pretty bad idea, since some individuals might have more or less data points. The proper way to do this would be to do some form of integration to get the area under the curve, e.g.¬†with a simple trapezoid rule, or to model the whole time-series with a function and then compute the AUC from that function. But to keep things simple, we‚Äôll go ahead - keeping in mind that in general, outside of a practice example, this is not a good idea.\n\nWrite code to exclude the observations with TIME = 0, then compute the sum of the DV variable for each individual using dplyr::summarize(). Call this variable Y. The result from this step should be a data frame/tibble of size 120 x 2, one column for the ID one for the variable Y. Next, create a data frame that contains only the observations where TIME == 0. This should be a tibble of size 120 x 17. Finally, use the appropriate join function to combine those two data frames, to get a data frame of size 120 x 18.\n\nFinally, we‚Äôll do a bit more cleaning. At this point, we don‚Äôt need most of these indicator variables anymore (e.g., OCC or EVID). We also want to convert RACE and SEX to factor variables.\n\nWrite code that converts RACE and SEX to factor variables and keeps only these variables: Y,DOSE,AGE,SEX,RACE,WT,HT\n\nCheck the data and make sure everything looks ok. As needed, perform further exploration or cleaning steps.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#eda-revisited",
    "href": "content/model-fitting/assessment-model-fitting.html#eda-revisited",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "EDA revisited",
    "text": "EDA revisited\nYou probably already produced some EDA (figures and tables) as part of the exploratory process above. Do a few more here, once the data is clean.\n\nMake some useful summary tables.\nShow some scatterplots or boxplots between the main outcome of interest (total drug, Y) and other predictors.\nPlot the distributions of your variables to make sure they all make sense.\nLook at some pair/correlation plots.\n\nThis is a fairly open exploration, I want you to play with the data enough to get a good idea of what you are looking at and what‚Äôs ‚Äúinside‚Äù the data.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#general-wrangling-and-eda-comments",
    "href": "content/model-fitting/assessment-model-fitting.html#general-wrangling-and-eda-comments",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "General wrangling and EDA comments",
    "text": "General wrangling and EDA comments\nAlways make sure you document well. Prior to each code piece that produces some output, you should add text describing what you are about to do and why. After you produced the result, add some text that comments on what you see and what it means. E.g. you could write something like Histogram for height shows two persons at 20in, everyone else else is above 50in. Checked to see if those are wrong entries or not. Decided to remove those observations.\nOnce you‚Äôve come this far in any kind of data analysis, you should have some code that explores and cleans the data, produces some figures and tables, and (in general, not required here) saves the cleaned data set to a file.\nYou should also have documentation, as comments in the code and as text in a Quarto document, as well as some notes in README file(s), that explain what the different parts of data and code are/do, what variables the data contain, what each variable means, which are the main outcome(s) and main predictor(s), etc. Providing all this meta-information is important so that someone else (or your future self) can easily understand what is going on.\nAny good data collection and analysis requires that data is documented with meta-data to describe what it contains, what units things are in, what variables are allowed, etc. Without good meta-data, analyses often go wrong. There are famous examples in the literature where the coding 0/1 was assumed to mean the opposite of what it was, so all conclusions were wrong. To guard against this, careful documentation is crucial. Also, giving variables easy to understand names and values is best. The data we have here is a good example of how NOT to do things. There‚Äôs no codebook, and we don‚Äôt really know what SEX 0/1 stands for (which ones are male/female). We also don‚Äôt know what the 4 entries 1/2/7/88 for RACE stand for. If we had a codebook, or some better labeling ‚Äì or ideally both ‚Äì, would be very helpful. Unfortunately, a lot of real-world data is rather poorly documented.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#model-fitting",
    "href": "content/model-fitting/assessment-model-fitting.html#model-fitting",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Model fitting",
    "text": "Model fitting\nFinally, we‚Äôll do some model fitting. We‚Äôll keep it simple here. We‚Äôll use the tidymodels framework. I suggest you go to the build a model tutorial in the Get Started section on the tidymodels website. There, you‚Äôll see an example of fitting some data with a linear model. From that tutorial, and other sources in the tidymodels website, as well as the TMWR book and other online sources figure out how to do the steps below. Of course AI tools are also an option (that‚Äôs how I did part of it üòÅ).\nWrite code that does the following:\n\nFit a linear model to the continuous outcome (Y) using the main predictor of interest, which we‚Äôll assume here to be DOSE.\nFit a linear model to the continuous outcome (Y) using all predictors.\nFor both models, compute RMSE and R-squared and print them.\n\nRepeat the steps above, but now we‚Äôll consider SEX as the outcome of interest (that doesn‚Äôt make too much scientific sense, but we want to practice fitting both continuous and categorical outcomes).\n\nFit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, which we‚Äôll again assume here to be DOSE.\nFit a logistic model to SEX using all predictors.\nFor both models, compute accuracy and ROC-AUC and print them.\n\nInterpret the results and add additional comments and thoughts to the document, so a reader can understand what you did, why you did it, and what it all means. For instance do you see any pattern between Y and DOSE or SEX and DOSE and does what you see make sense? How about the other predictors?\nIf you are extra ambitious or just enjoy playing around with tidymodels, add a k-nearest neighbors model fit to the continuous and/or categorical outcome.\n\nFor this exercise, it isn‚Äôt so important that the outcomes and predictors are of actual scientific interest. We won‚Äôt do exciting science here, just explore the analysis workflow. However, I want to remind you that having good/interesting question to answer (and data you can use to answer it) is the most important part of any research project. Rather a great question and basic stats than really fancy stats answering a meaningless question that nobody cares about!",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-assessment/reality-based-assessment/reality-based-assessment.html",
    "href": "content/model-assessment/reality-based-assessment/reality-based-assessment.html",
    "title": "Reality-based assessment",
    "section": "",
    "text": "This unit discusses the idea of assessing a model based on expert knowledge.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Reality-based assessment"
    ]
  },
  {
    "objectID": "content/model-assessment/reality-based-assessment/reality-based-assessment.html#footnotes",
    "href": "content/model-assessment/reality-based-assessment/reality-based-assessment.html#footnotes",
    "title": "Reality-based assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm not the first one to come up with the map analogy. I‚Äôm not sure where I heard it first, but I know I didn‚Äôt invent it. Only the specific example shown here is from our previous paper.‚Ü©Ô∏é\nThis uses immunology as an example, but of course this could be any topic to make the same analogy.‚Ü©Ô∏é\nThis is not unique to mathematical/statistical models. If you do an experiment in a BALB/c mouse, you are using a model system. It doesn‚Äôt mean the results would be the same in another type of mouse, let alone in humans.‚Ü©Ô∏é\nThis paper shows a nice example in which the authors compared several influenza models. By comparing the models to each other and to data, they were able to discuss how models differed and which ones agreed with the data (spoiler alert: there was no one single best model, most of them were good at reproducing some data and not good at reproducing other data).‚Ü©Ô∏é\nYou might think that nobody would do something silly like that. You would be wrong. You can find a lot of extrapolations gone wrong in the literature. Often, it is subtle, but sometimes it is blatant. This is a good example of the latter.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Reality-based assessment"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-overview.html",
    "href": "content/model-assessment/model-assessment-overview.html",
    "title": "Model assessment",
    "section": "",
    "text": "Overview\nCritical thinking is important. That also applies to your (or someone else‚Äôs) models.\nThis module discusses ways to assess your models.\n\n\nLearning Objectives\n\nBe familiar with methods to assess models.\nBe able to perform model assessment on your own and others‚Äô models.\n\n\n\nContent\nThe module consists of the following units:\n\nModel assessment introduction\nReality-based assessment\nModel performance revisited\nModel evaluation with cross-validation\nModel evaluation with Information Criteria\nModel diagnostics\nModel uncertainty",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Overview"
    ]
  },
  {
    "objectID": "content/model-assessment/information-criteria-assessment/information-criteria-assessment.html",
    "href": "content/model-assessment/information-criteria-assessment/information-criteria-assessment.html",
    "title": "Information criteria based model assessment",
    "section": "",
    "text": "Overview\nThis unit discusses the idea of assessing model performance based on information criteria.\n\n\nLearning Objectives\n\nBe familiar with the idea of information criteria.\nKnow when to use information criteria.\n\n\n\nIntroduction\nThe previous units introduced the idea that you want to assess the performance of your models on external/new data. You then learned about the train/test and cross-validation approaches to try and get a more honest assessment of the quality of your models.\nHere, we‚Äôll cover alternative approaches that you can use if you are not able to use CV (e.g., because you don‚Äôt have enough data or your data/model take too long to run a CV analysis.)\n\n\nInformation criteria\nInformation criteria, such as AIC, BIC, DIC, WAIC and similar, compute a measure that is a trade-off between good fit to the data (low-cost function/high performance) and model complexity (number of parameters). Approaches based on such selection criteria essentially try to guess how the model would perform if it were to be fit to new data, without actually trying to do it (in contrast to CV).\nThe disadvantage is that these guesses as to how the model might perform on new data are not as reliable as actually evaluating model performance on such data through CV. The advantage is that no sampling is needed, which means these approaches are much less computationally intensive. For all those information criteria, things are set up that a model with a smaller value is considered better. These measures - thankfully! - do not have the arbitrary p&lt;0.05 value cut-off common in frequentist statistics. For AIC, a rule of thumb is that a difference of 10 between 2 models is meaningful. (Unfortunately, people seem to not be able to make their own decisions and need crutches, so arbitrary cut-offs for AIC/BIC/etc. have started to show up in the literature.)\nThere is a lot of math behind information criteria. It doesn‚Äôt hurt learning a bit more (see the further resources section). However, you can also just use these criteria without having to understand too much about them. A lot of statistical software can compute these criteria, and if you are not able to use CV approaches, using a information criterion can be a good idea. But beware of ‚Äúresearcher degrees of freedom‚Äù. You shouldn‚Äôt compute a lot of different criteria and then pick the one that gives you the answer you are looking for. If you compute more than one, you need to report that. And if they disagree (e.g., AIC prefers model 1, and BIC prefers model 2), you‚Äôll have to figure out what that means.\n\n\nSummary\nThis summarizes the whole idea of model performance evaluation. To repeat (again): We generally want to know how well a model performs in general and on new data - not the sample we fit it to. Testing/reporting model performance for the data the model was fit to very often leads to overfitting and optimistic/wrong conclusions about new/future data. To minimize overfitting, here is my recommended strategy (I‚Äôm sure it‚Äôs not the only one, so what matters most is that you clearly think about what each step in your analysis means/implies):\n\nIf you have enough data and care about predictive performance, set some data aside for a final validation/test. If you don‚Äôt have a lot of data, you might need to skip this split.\nIf you choose to use all your data for model fitting purposes, and don‚Äôt evaluate your model on data not used during model building/training, you need to interpret your findings as exploratory and hypothesis generating, and you need to be careful about trying to draw generalizable conclusions.\nIf you have enough data (&gt;100s observations) and CPU power, use cross-validation (CV) approaches to determine the best model. If for some reason (mainly computational time or small data) CV is not feasible, use AIC & Co.\nThink carefully about your cost function/metric! A model that is great at predicting the wrong outcome is useless!\nNo matter what approach you use, choosing a model based on performance alone is not enough.\n\n\n\nFurther Resources\n\nModel Selection and Multimodel Inference is a book that covers information criteria in a lot of detail.\nThe Statistical Rethinking book has - in my opinion - one of the best explanations of the concepts and ideas behind information criteria.\n\n\n\nTest yourself\nWhich of the following is NOT a common information criterion?\n\n VIC BIC DIC AIC\n\nInformation criteria provide estimates of model performance on new data that are as good as those obtained from a train/test approach. TRUEFALSE You should try many different information criteria and choose the one which gives the best results. TRUEFALSE\n\n\nPractice\n\nCheck publications in your area of interest to find an example of a data analysis that used an information criterion approach to choose between models. Assess if you think the authors implemented and reported their findings appropriately. Consider if the authors could/should have used a train/test and/or CV approach instead.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model evaluation with Information Criteria"
    ]
  },
  {
    "objectID": "content/model-assessment/diagnostics-assessment/diagnostics-assessment.html",
    "href": "content/model-assessment/diagnostics-assessment/diagnostics-assessment.html",
    "title": "Model diagnostics",
    "section": "",
    "text": "This unit discusses different diagnostics that are helpful to assess models.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "content/model-assessment/diagnostics-assessment/diagnostics-assessment.html#footnotes",
    "href": "content/model-assessment/diagnostics-assessment/diagnostics-assessment.html#footnotes",
    "title": "Model diagnostics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, it can actually be quite tricky to interpret coefficients for anything but a linear model. For instance for a logistic model (and many other GLM), the impact of a change in a predictor on the outcome depends on the value of the predictor. Sometimes a 1-unit increase in some predictor (say drug dose) can lead to a strong change in the outcome (say cholesterol level), while for other values of that predictor, increasing it further by 1 unit might have almost no impact on the outcome. So one needs to be careful even when interpreting fairly simple GLM. However, the whole model is known so you can always figure out how one part relates to the other. That‚Äôs not the case any more for complex ML models.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "content/model-assessment-r/model-assessment-r-overview.html",
    "href": "content/model-assessment-r/model-assessment-r-overview.html",
    "title": "Model assessment with R",
    "section": "",
    "text": "Overview\nThis module discusses ways to assess your models with R.\n\n\nLearning Objectives\n\nBe able to perform various model assessment tasks with R.\n\n\n\nContent\nThe module consists of the following units:"
  },
  {
    "objectID": "content/model-assessment-r/diagnostics-assessment-r/diagnostics-assessment-r.html",
    "href": "content/model-assessment-r/diagnostics-assessment-r/diagnostics-assessment-r.html",
    "title": "Model diagnostics with R",
    "section": "",
    "text": "Overview\nThis unit shows how to use R to run different diagnostics that are helpful to assess models.\n\n\nLearning Objectives\n\nKnow how to implement different diagnostic approaches with R.\n\n\n\nIntroduction\nWe previously discussed how to use different diagnostics to assess models. Here, we‚Äôll focus on their implementation with R. We‚Äôll mostly work with the tidymodels set of packages.\n\n\nAlgorithm assessment\n\n\nNull model comparisons\nIn tidymodels, you can use the null_model() function to compute the performance for a null/base model.\n\n\nSingle predictor comparisons\n\n\nSummary\nWhen assessing your models, it is important to go beyond the performance metric and look at individual model predictions and how closely they agree with the data. If you spot residual patterns, that might indicate that you could (but don‚Äôt have to) make your model more complex/flexible to try and capture additional details of the data.\n\n\nFurther Resources\nNone ATM.\n\n\nTest yourself\nWhat‚Äôs the term for the differences between data and model that you want to see distributed like a symmetric cloud?\n\n Overfitting Variance Residuals Bias\n\nOn a predicted versus observed plots, the points should cluster along a horizontal line to indicate a good model fit. TRUEFALSE If your overall model metric (e.g.¬†Accuracy) is very good, you don‚Äôt need to look at individual predictions. TRUEFALSE\n\n\nPractice\n\nRevisit any of the papers you found in one of the previous exercises. See if the authors used any of the approaches discussed here to assess their model(s). Often, this kind of information would be in the supplement. At the minimum, you‚Äôd want the authors to mention that they did these checks. Unfortunately, you‚Äôll often see it missing. It seems that at times, authors/analysts don‚Äôt want to look too closely, otherwise they would need to acknowledge that their cherished model is actually not that good üòÅ."
  },
  {
    "objectID": "content/content-overview.html",
    "href": "content/content-overview.html",
    "title": "Content Overview",
    "section": "",
    "text": "This section contains almost all of the content, organized into modules. Each module has several units.\nAssessments for each module are in the separate Assessments section.\nThe Schedule page lists what content is covered and what assessments you need to do each week.",
    "crumbs": [
      "Content",
      "Content Overview"
    ]
  },
  {
    "objectID": "content/model-assessment-r/cross-validation-r/cross-validation-r.html",
    "href": "content/model-assessment-r/cross-validation-r/cross-validation-r.html",
    "title": "External data based model assessment with R",
    "section": "",
    "text": "Overview\nThis unit discusses how to do cross-validation and similar approaches using R.\n\n\nLearning Objectives\n\nUnderstand the concept of overfitting.\nKnow that a model should generally be assessed by its performance with independent data.\n\n\n\nIntroduction\nWe previously discussed cross-validation and train/test splitting from a conceptual level. Here‚Äô well be looking at some examples on how to implement these in R.\nWe‚Äôll focus on the tidymodels set of packages.\nTo do the train/test split in R, you can use the rsample package which is part of tidymodels.\nCross-validation works for most situations and is quite robust. It is also fairly easy to implement. In R, the tidymodels framework allows for easy CV, and we‚Äôll make use of that.\n\n\nSummary\n\n\nFurther Resources"
  },
  {
    "objectID": "content/model-assessment-r/model-assessment-r-introduction/model-assessment-r-introduction.html",
    "href": "content/model-assessment-r/model-assessment-r-introduction/model-assessment-r-introduction.html",
    "title": "Model assessment introduction",
    "section": "",
    "text": "This unit provides an overview of the different ways one can and should assess models - both your own and those of others."
  },
  {
    "objectID": "content/model-assessment-r/model-assessment-r-introduction/model-assessment-r-introduction.html#agreement-with-reality",
    "href": "content/model-assessment-r/model-assessment-r-introduction/model-assessment-r-introduction.html#agreement-with-reality",
    "title": "Model assessment introduction",
    "section": "Agreement with reality",
    "text": "Agreement with reality"
  },
  {
    "objectID": "content/model-assessment-r/model-assessment-r-introduction/model-assessment-r-introduction.html#model-perfomance",
    "href": "content/model-assessment-r/model-assessment-r-introduction/model-assessment-r-introduction.html#model-perfomance",
    "title": "Model assessment introduction",
    "section": "Model perfomance",
    "text": "Model perfomance\nThis is generally the most clear-cut metric. After you define and quantify the metric you want to optimize, How well do the predictions from the model compare to the actual data?\n\nModel complexity: How easy or hard is it to understand the model? How fast does it run? How robust is it to violations of the assumptions you made?\nModel uncertainty: How much uncertainty is associated with the predictions made by the model?\nModel misspecification: How much systematic deviation is there between model predictions and actual data?\n\nWe will discuss these different ways one can assess model quality in this module."
  },
  {
    "objectID": "content/model-assessment-r/model-assessment-r-introduction/model-assessment-r-introduction.html#usefulness",
    "href": "content/model-assessment-r/model-assessment-r-introduction/model-assessment-r-introduction.html#usefulness",
    "title": "Model assessment introduction",
    "section": "Usefulness",
    "text": "Usefulness\nPerformance is not everything. Even if you have a model that performs best on the independent/cross-validated data, you might want to opt for a ‚Äúworse‚Äù model that is easier to interpret and potentially use. Having smaller and simpler models can sometimes be of little importance and other times of great importance.\nFor instance, if you want to build a model that allows doctors to predict the chance that a patient has a certain disease, you might want to have a model that only uses the fewest (or easiest/cheapest to measure) variables to obtain good performance. So if you collect a lot of data, some based on checking patient symptoms and some on lab results, you might not want to use all those variables in your model. Let‚Äôs say that you had data on 10 predictors, 5 for easy to measure symptom variables (e.g., body temperature and similar), and 5 variables that come from different lab tests. You‚Äôll evalute models with different predictors (performing e.g., subset selection or LASSO, which we‚Äôll discuss soon) and find that the best performing model retains 3 symptom variables and 2 lab tests. Let‚Äôs say its performance is 95% (I‚Äôm purposefully fuzzy about what that performance exactly is since it doesn‚Äôt matter. It could be accuracy, or F1 score, or AUC, or‚Ä¶). But you also find that another model that contains 4 symptom variables and no lab tests has 85% performance. Which do you choose? That comes back to our discussion about assessing model quality: Performance is an important measure, but it‚Äôs not the only one. In this case, since you could get data on the 4 symptoms very quickly and cheaply, you might want to recommend that model for most doctors offices, and only use the better, but more time-consuming and expensive model with the 2 lab tests in settings such as high-risk populations in the hospital.\nIn contrast, if you are a bank that tries to predict fraud by having complicated models that constantly analyze various data streams, you might not care how complicated and big your model is, only that the performance in flagging fraudulent transactions is as good as possible."
  },
  {
    "objectID": "content/model-assessment/assessment-model-assessment.html",
    "href": "content/model-assessment/assessment-model-assessment.html",
    "title": "Assessment - Evaluating Models",
    "section": "",
    "text": "Quiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\n\n\nExercise\nWe definitely need to practice some of the topics we covered in this (and previous modules). But since at the end of the week there is another project deadline, I want you to focus on that. Thus no exercise this week, we‚Äôll practice the topics we covered in upcoming exercises.\n\n\nProject\nFinish (and make sure to push) part 3 of the project. There is no need to submit a link. Unless you renamed your repository, I, and your classmates, know where to find it based on your previous submissions. Make sure the link you posted previously to the project-related Discord channel is accurate. If you are using a private repository, add the students who will review your project as collaborators (see announcement channel).\n\n\nDiscussion\nWrite a post in this week‚Äôs discussion channel that answers this question:\nWhich of the concept(s) we covered in this module is/are the most interesting/surprising to you, and why? And which concept(s) or topic(s) from this module‚Äôs materials do you find the most confusing, and why/how so?\nPost by Wednesday, then reply to each other by Friday. I hope together we can reduce any remaining confusion and reinforce the ideas from this module.",
    "crumbs": [
      "Assessments",
      "9. Model Assessment"
    ]
  },
  {
    "objectID": "content/model-assessment/external-data-assessment/external-data-assessment.html",
    "href": "content/model-assessment/external-data-assessment/external-data-assessment.html",
    "title": "Model assessment using cross-validation",
    "section": "",
    "text": "Overview\nThis unit discusses the idea of using existing data to assess how well a model might perform on external data.\n\n\nLearning Objectives\n\nUnderstand the concept of cross-validation.\nKnow that advantages and disadvantages of cross-validation.\n\n\n\nIntroduction\nIn the previous unit, we discussed the idea that using the data that was used to build and fit the model gives an overly optimistic view of model performance. Overfitting is a big risk and often leads to a model that does not perform well on new data.\nHere, we‚Äôll discuss a useful approach that can help minimize overfitting.\n\n\nHonest performance evaluation\nMore data: The ultimate test is, of course, to check your model performance on newly collected data, or an existing, independently collected dataset. However, collecting or getting new data is not easy, and often you might not have a completely independent dataset that measures the same variables as the one you are trying to analyze/fit. Therefore, for the remainder of this discussion, we assume that getting more data is not an option. (Though if you have that option, certainly go for it!)\nReserving some data: If you have enough data, you can perform an evaluation of model performance that tries to imitate the idea of evaluating the model on newly collected data. What you do is that right at the start of your analysis, you set some of your data aside and do not look at it until the very end! A common fraction of data that is set aside is 20%-40%, but there is no clear rule. This data you set aside is called your test/validation data. You need to completely ignore that data and are not allowed to use it while doing any statistical model fitting. You then take the rest of your data, referred to as training data, and do all your model building and fitting and model choosing, etc.\nAt the end and only once at the very end do you run your model on the test data and compute the performance. The way you do this is to give your model the input variables (predictors) from the new data, ask the model to make predictions for the outcome, and use your metric/cost function to evaluate the performance of your model on that part of the data that your model has never seen before. This gives you an estimate of your model performance that is as honest as you can get without collecting new data. Be aware that even with this approach, you are likely still getting slightly better results than if you had evaluated your model on independently collected data (with somewhat different study design and data collection approaches). But in the absence of truly independent data, this is the best/most honest model performance evaluation you can do.\nIf you look at data analysis competitions such as Kaggle, this is how they operate. People need to submit their model predictions, and the model will be evaluated on a final dataset, only once, at the end of the competition. Whoever scores highest on that final dataset wins, no matter how well they did on the leaderboard.\nWhile such a train/test split is an excellent thing to do, there are some constraints. First, you need to have enough data. If your dataset is small (&lt;100-1000 observations, depending on the number of predictors and model complexity), then this approach will make you give away data that you need to build and fit your model reliably, and won‚Äôt provide robust insights. Second, you still need a way to decide which model to chose as your final model, and as mentioned above, using the one with the smallest cost function as evaluated on the data that was used to build the model is not good. An approach called cross-validation can help.\n\n\nCross-validation\nEven with the train/test approach just described, you still need to figure out how to evaluate which model is best during the model building process. To that end, an approach called cross-validation has become very widely used and is currently - in my opinion - the best method to evaluate and choose your model.\nCross-validation (CV) is a reasonably straightforward approach. It mimics the train/test idea just described, but is now applied during the model fitting process. The idea is that you take the data you set aside for the model building process (i.e., your training data), and during the model building procedure, you repeatedly split your data into a portion to which you fit our model, and a portion which you use to evaluate model performance. You repeat this many times to get several samples and always use some portion of the data to fit the model, and the remaining part to evaluate it.\n\nYou might not be surprised to learn that this is another place where terminology is confusing and inconsistent. The words training data and test/validation data are used both for the initial split described above, and the split done in the CV procedure. One terminology is to call the splits done by CV the analysis and assessment portions of the data. This is shown nicely in this schematic. I will try to use that terminology, but I might not be consistent. And other materials you see/read certainly do not follow that terminology, as you will see already below.\n\nThere are different ways to do the data splitting during cross-validation. A common setup is to randomly divide the data set into k blocks (called folds). You then use the data in all but one of the blocks to fit the model, then compute the cost function/performance metric (e.g., the RMSE) using the remaining block. You repeat that for all k blocks. The average value of the model performance over the k blocks is the target that you want to optimize. One usually chooses k = 5 or 10 blocks/folds, which has shown in simulation studies to give the best variance-bias trade-off. The figure shows an example of a 4-fold cross-validation for a categorical outcome (color of the balls). Note that this figure uses the train/test terminology for labeling the blocks, not the analysis/assessment terminology.\n\n\n\nSource: Cross Validation Wikipedia Page\n\n\nIn addition to the standard way of splitting the data, there are different sampling variants. For instance, if you have longitudinal data, or otherwise data with structure, you might want to sample in a different way (e.g., sample the earlier times and predict later times). One variant of the cross-validation approach is the leave-one-out (LOO) cross-validation, which removes each data point one at a time as the test data and therefore performs the train/test part N times. You will also often see repeated cross-validation, which just adds another round of sampling on top to get better distributions (e.g., a 10-fold CV 10 times repeated gives 100 samples, which will be used to compute model performance).\nCross-validation works for most situations and is quite robust. It is also fairly easy to implement. Thus CV is a good default strategy. However, sometimes it is not possible or desirable to use CV. Occasionally, datasets are so small that you need all the data just to fit your model. Alternatively, your dataset might be large and/or your model complex, which might mean running a CV method takes too long. (Often, one can speed up things by down-sampling the data, using faster algorithms, or running code in parallel on a computer cluster.)\n\n\nCross-validation and model complexity\nIf you are mostly fitting GLM type models or other fairly simple models, you might wonder what I mean when I say during the model building/fitting process, since for a given simple model, there is just one set of model parameters that fits best, and that‚Äôs it. However, many more complex models have what is often referred to as tuning parameters. These are not part of the model structure (e.g., they are not the coefficients in front of your predictor variables), but they influence model performance. A great example are AI models, many of which easily have millions of parameters that need to be tuned to get good model performance. For those models, it is critical to evaluate model performance on data that is somewhat indepedent from the sample that is used to fit/tune/optimize the model.\nTo provide an analogy for a GLM, the number of variables you put in your model is somewhat similar. You learned that larger models fit better, so sticking more predictors into your model will give better performance (e.g., lower RMSE). However, this might lead to worse performance on independent data. You can then use something like CV to determine how many predictors might be best for your model.\n\n\nSummary\nTrain/test data splitting and cross-validation are great approaches to more honestly assess the performance of a model. If your goal is to simply determine if there are patterns in the data you have, you might not need CV. But if you in any way have prediction as a goal, and want to understand if your model likely works in general - on data like yours, but not the one you used to fit the model - then you should use CV if you can.\n\n\nFurther Resources\nFrank Harrell‚Äôs blog post discusses some of the potential caveats for using methods such as cross-validation.\nChapters 11 and 12 of the Tidy Modeling with R book provide additional discussion of the sampling/CV approaches. Another discussion of CV is chapter 30 of IDS.\n\n\nTest yourself\nWhat‚Äôs the name commonly given to the data you do your final model evaluation?\n\n Split Data Train Data Test Data Analysis Data\n\nOne needs to use at least 10 folds/blocks in CV to get a reliable answer. TRUEFALSE Using indepedently collected data to test model performance is better than using train/test or CV. TRUEFALSE\n\n\nPractice\n\nCheck publications in your area of interest, or look online on websites like Kaggle, to find an example of a data analysis that used the train/test and/or the CV approach. Read how the authors did things and what they found and report.\n\n\n\n\nSource: Cross Validation Wikipedia Page",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model evaluation with cross-validation"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html",
    "title": "Model assessment introduction",
    "section": "",
    "text": "This unit provides an overview of the different ways one can and should assess models - both your own and those of others.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#agreement-with-reality",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#agreement-with-reality",
    "title": "Model assessment introduction",
    "section": "Agreement with reality",
    "text": "Agreement with reality\nFirst and foremost, you should ask yourself if a given model makes sense. If there‚Äôs an obvious conflict between the model structure or the model results and reality, something can‚Äôt be right about the model. This is where your subject matter expertise comes in. To be a good data analyst, you not only need statistical and data analysis skills, you should also know a fair bit about the system you are trying to study. If there‚Äôs disagreement between model and reality, reality wins. Let‚Äôs say, if based on you used a linear model to look at correlations between children‚Äôs age and height, and then use the model to predict that a 30 year old person will be 20 feet tall, you know something isn‚Äôt right. In this case, you extrapolated outside the range of where the model might be useful - a common occurrence. For this example, the solution is to either only use the model in the range where it was calibrated, or if you want to predict the height of a 30 year old person, you need to change your model.\nSo always think critically about the model and its results and ensure they pass the ‚Äòreality test‚Äô. Only if they do does it make sense to move to more narrow ways of assessing the model.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#model-perfomance",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#model-perfomance",
    "title": "Model assessment introduction",
    "section": "Model perfomance",
    "text": "Model perfomance\nThis is generally the most clear-cut metric. After you define and quantify the metric you want to optimize, you can compare model performance among different models. We discussed some of those metrics before, such as RMSE or accuracy. Comparing those values among models can indicate which model is better. How exactly one should do that performance evaluation is something we‚Äôll discuss in later unit.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#predictions",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#predictions",
    "title": "Model assessment introduction",
    "section": "Predictions",
    "text": "Predictions\nThe model performance metric gives you a single quantity describing how well the model matches the data, given the metric you defined. That‚Äôs useful, but you generally want to dig deeper and look at how predictions from the model for individual observations compare to the actual data. You can do that best through various graphical methods, which we‚Äôll discuss in a later unit.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#uncertainty",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#uncertainty",
    "title": "Model assessment introduction",
    "section": "Uncertainty",
    "text": "Uncertainty\nAny estimate you obtain from a model comes with uncertainty. You need to look at that uncertainty and decide if it is acceptable. You might have trade-offs. For instance you might have a model that provides a better performance as measured by your metric (say RMSE), but when you look at the uncertainty around the estimate, it might be larger than what you get from a smaller model. Which model to choose in such a case is a scientific question. It depends on your goals.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#complexity",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#complexity",
    "title": "Model assessment introduction",
    "section": "Complexity",
    "text": "Complexity\nThere is often a trade-off between simple models that are easy to interpret but don‚Äôt perform quite as well, and complex models that give better performance but that can be hard to interpret. More complex models also tend to take longer to execute, and might not generalize that well since they might be overly tuned to the data you used to build the model. Again, it is up to you to decide which model is more suitable for your purpose.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#usefulness",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#usefulness",
    "title": "Model assessment introduction",
    "section": "Usefulness",
    "text": "Usefulness\nUsefulness is in some sense the combination of the categories above. Based on what you want a model to do, you choose one (or multiple) that are most useful for your purpose.\nFor instance, if you want to build a model that allows doctors to predict the chance that a patient has a certain disease, you might want to have a model that only uses the fewest (or easiest/cheapest to measure) variables to obtain good performance. So if you collect a lot of data, some based on checking patient symptoms and some on lab results, you might not want to use all those variables in your model. Let‚Äôs say that you had data on 10 predictors, 5 for easy to measure symptom variables (e.g., body temperature and similar), and 5 variables that come from different lab tests. You‚Äôll evaluate models with different predictors (performing e.g., subset selection or LASSO, which we‚Äôll discuss soon) and find that the best performing model retains 3 symptom variables and 2 lab tests. Let‚Äôs say its performance is 95% (I‚Äôm purposefully fuzzy about what that performance exactly is since it doesn‚Äôt matter). But you also find that another model that contains 4 symptom variables and no lab tests has 85% performance. Which do you choose? In this case, since you could get data on the 4 symptoms very quickly and cheaply, you might want to recommend that model for most doctors offices, and only use the better, but more time-consuming and expensive model with the 2 lab tests in settings such as high-risk populations in the hospital.\nIn contrast, if you are a bank that tries to predict fraud by having complicated models that constantly analyze various data streams, you might not care how complicated and big your model is, only that the performance in flagging fraudulent transactions is as good as possible.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#footnotes",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#footnotes",
    "title": "Model assessment introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe term evaluation is often used interchangeably with assessment. I‚Äôm trying to reserve evaluation when one has a clear performance metric that one can measure, for instance the mean squared error or the value of a likelihood function. I use the term assessment in a broader sense. But I might not be consistent, so if you see either word, just figure out what is meant based on context üòÅ.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-performance-revisited/model-performance-revisited.html",
    "href": "content/model-assessment/model-performance-revisited/model-performance-revisited.html",
    "title": "Model performance revisited",
    "section": "",
    "text": "Overview\nThis unit discusses the idea of assessing a model based on how well it performs on external data.\n\n\nLearning Objectives\n\nUnderstand the concept of overfitting.\nKnow that a model should generally be assessed by its performance with independent data.\n\n\n\nIntroduction\nWe discussed how to specify a metric and use that metric to try to find the model with the best performance. We covered the idea of defining a single numerical value (cost function/metric) and optimizing (usually minimizing) it to find the best model. There is however a very big caveat to this. The main point is: It usually doesn‚Äôt matter how well your model performs on the data that you used to build and fit your model!\nThis is a very important point, and one that unfortunately a majority of scientific papers still get completely wrong! It is one area where modern machine learning is much more careful compared to the traditional way statistics is taught/used. In the machine learning field, it is very much recognized that it doesn‚Äôt matter a whole lot how well your model performs on the data that you used to build your model! What matters is performance on similar new data like the data that was used to fit the model.\nI‚Äôm going to repeat this and similar sentences a bunch of times throughout the rest of the course üòÅ. If you only take away 2 main points from this course, this would be one if them. The other is that doing data analysis in a reproducible (automated) manner is critical.\nSo let‚Äôs go into some more details regarding this important point.\n\n\nShould we really minimize the cost function?\nWe discussed that once we have chosen a cost function for our problem, we are trying to find a model that minimizes this cost function (e.g., minimizes the RMSE or the misclassification error), and models with smaller cost functions are better. The problem with this concept is that in general, a bigger model will be more flexible and thus able to fit the data better. However, when we do data analysis, we generally are not (only) interested in having a model that works well for the specific data sample we used to fit your model. Our main question/hypothesis usually does not concern the actual data we have/fit. Instead, we generally want to say something about ‚Äòthe larger world‚Äô.\nIf we are asking inferential questions, we are interested in what the data analysis teaches us about this system in general. E.g., if we analyze data to see if there is a correlation between levels of atmospheric pollutants and cases of asthma among our study population, we are usually really interested in knowing if such a correlation is real in general.\nIf we are asking predictive questions, we are interested in a model that can predict future observations, not the ones we already have. E.g., if we analyze data for a specific treatment, we are not very interested how well the model predicts the effect of the drug on the people for which we collected the data (we already know that). Instead, we want to make general predictions about the effectiveness of the treatment on future patients.\nIn either case, what we want is a model that is generalizeable (also sometimes called externally valid ), and that applies equally well to new and similar data beyond the data we already collected.\nWhat truly matters is how well our model can explain/predict other/future data, not just the data we already observed!\nIf we build a very complex model in an effort to match our existing data as closely as possible, what generally happens is that our model overfits. That means it becomes very good at modeling the data we use to build the model, but it won‚Äôt generalize very well to the general, broader context of other/future data. The reason for that is that there is noise (random variability) in any dataset, and if we have a model that is too flexible, it will not only match the overall signal/pattern (if there is any) but will also capture all the noise in our sample, which leads to worse performance on future data that have different amounts and types of noise/variability.\n\n\nBias-variance trade-off\nAs model complexity increases, models tend to perform/fit better to the data that is used to build the model. However, it is also generally the case that such complex models perform worse on future/new data compared to simpler models. This is an important general concept and is known in statistics as the bias-variance trade-off.\nBias describes the fact that a model that is too simple might get the data ‚Äúsystematically wrong‚Äù. A more restricted model like a simple linear model usually has more bias. Another way of saying this is that the model underfits, i.e., there are still patterns in the data that the model does not capture. More complex models generally reduce the bias and the underfitting problem..\nVariance describes how much a model would vary if it were fit to another, similar dataset. If a model goes close to the training data, it will likely produce a different fit if we re-fit it to a new dataset. Such a model is overfitting the data. More complex models tend to be more likely to overfit.\nWhile the concept sounds somewhat technical, you can get a very good and quick intuitive understanding by looking at the following figure.\n\n\n\nBias-variance tradeoff. Source: ISLR.\n\n\nIn the example shown in this figure, the data was produced by taking the black curve and adding some noise on top. This gives the data shown as circles. Three models are fit. A linear model (yellow) is too restrictive and misses important patterns. The next model (blue line) is more flexible and is able to capture the main patterns. The most complex model (green line) gets fairly close to the data. But you can tell that it is trying to get too close to the data and thus overfits. If we had another data sample (took the black line and added some noise on top), the green model would not do so well. This is shown on the right side, where the grey line plots the MSE for each model for the given dataset. As the model gets more complex/flexible, they get closer to the data, and the MSE goes down. However, what matters is the model performance on an independent dataset. This is shown with the red curve. Here, you can see that the blue model has the lowest MSE.\nThe same concept holds for categorical outcomes, and for models with multiple predictors. No matter what the model, there is always a sweet spot for model complexity somewhere ‚Äúin the middle‚Äù. This ‚Äúmiddle‚Äù depends on the data and the question. Often, linear models are as good as one can get, and more complex models will overfit. Even for linear models, we might have to remove predictors to prevent overfitting (we‚Äôll discuss that later). At other times, somewhat complicated models (e.g., neural nets) might perform best. In general, the more data (both quantity and richness), the less likely it is that a more complex model will lead to overfitting. However, we always need to check.\n\n\nOverfitting and machine learning\nIf you only fit simple models (e.g., a linear model), and maybe decide based on scientific knowledge which predictors need to be in the model, then your risk of overfitting ‚Äì while still present ‚Äì is not that large. However, in machine learning, you often have complex models with many components that can be adjusted/tuned (we‚Äôll get into that) to improve model performance. The danger is that if you have a very flexible model that can be finely tuned to perform well on the data, you have a very large risk of overfitting, namely of ending up with a model that is well tuned and performs very well on the data you use to build the model, but does not work so well on other data. Therefore, while overfitting is always something the be careful about, once you start using larger and more flexible models, you definitely need to guard against overfitting.\n\n\nDealing with overfitting\nNow that you learned that the model that performs best (using whatever metric you chose) is not necessarily the best one, how can we evaluate model performance in a better way? There are different options, which we‚Äôll discuss in the next units.\n\n\nSummary\nTo repeat (again): We generally want to know how well a model performs in general and on new data - not the sample we fit it to. Testing/reporting model performance for the data the model was fit to very often leads to overfitting and optimistic/wrong conclusions about new/future data. There are several good ways to minimize overfitting, which we‚Äôll cover next.\n\n\nFurther Resources\nMLNAR‚Äôs blog post provides a nice further discussion of the idea of generalization and how different areas of data science (statistics, machine learning, causal modeling) think about this problem. I think the most important paragraph is actually the short last one. I want to add to this that while different areas might think about the question of generalization differently, all of them more or less agree, that in the end, what is important is the general conclusions you can draw from your statistical modeling analysis and it doesn‚Äôt matter (by itself) how well your model performs on the data that you used to build your model! What matters is what it means more generally.\nChapters 2 ISL covers the bias-variance trade-off.\n\n\nTest yourself\nWhich of the following is NOT an important topic discussed in this unit?\n\n Overfitting Variance Convergence Bias\n\nOverfitting is only a problem for large machine learning models. TRUEFALSE Most of the time, we want a model that performs well in general/on new data, not just for our sample. TRUEFALSE\n\n\nPractice\n\nRevisit the papers you found for the previous unit‚Äôs exercise. Go through them again and specifically focus on the model structure and complexity, and if results make sense. Try, as best as you can, to critically evaluate if the authors made suitable choices and explained their choices well.\n\n\n\n\nBias-variance tradeoff. Source: ISLR.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model performance revisited"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html",
    "title": "Model uncertainty assessment",
    "section": "",
    "text": "This unit discusses model uncertainty, how to compute and and how to assess it.\n\n\n\nA study with n=1 has large uncertainty. Source: SMBC.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#confidence-interval-ci",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#confidence-interval-ci",
    "title": "Model uncertainty assessment",
    "section": "Confidence interval (CI)",
    "text": "Confidence interval (CI)\nConfidence intervals (CIs) are a common way to report uncertainty. Those intervals capture uncertainty in the estimates for the model parameters and thus uncertainty in the mean outcome. CIs are very useful if we want to estimate the uncertainty in the population average outcome. For instance, going back to the example above, if we report an average drug-induced improvement in some quantity by 20%, with a CI (having some width, such as 89% or 95% or‚Ä¶) of 15% to 25%, we can conclude that if we gave the drug to another population like the one we studied, this is roughly the range of impacts we should expect to see in the new study.2",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#standard-error-se",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#standard-error-se",
    "title": "Model uncertainty assessment",
    "section": "Standard error (SE)",
    "text": "Standard error (SE)\nStandard errors (also often called standard error of the mean, SEM) are closely related to CI. For SE, you need to assume some kind of distribution for the quantity of interest. Almost always, that‚Äôs a normal distribution (which is often what you implicitly do when you optimize RMSE). In the common case that one looks at 95% CI and assumes a normal distribution, the relation is that CI = 1.96*SE. Thus, with SE, you get tighter bounds (they correspond to 68% CI). As for CI, an increase in data usually leads to more precise estimates of the mean outcome and therefore tighter bounds. It seems to me that SE are popular because the give the tightest bounds and thus the impression of preciseness üòÅ.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#prediction-interval-pi",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#prediction-interval-pi",
    "title": "Model uncertainty assessment",
    "section": "Prediction interval (PI)",
    "text": "Prediction interval (PI)\nIf we are interested in model predictions, we generally also want to know how much uncertainty is associated with those predictions. The difference between a CI and a prediction interval (for some reason, almost never abbreviated as PI - but I‚Äôm doing it here to make my typing life easier üòÅ) is that the latter also accounts for variation around the estimated mean.\nIt is important to be aware of the fact that prediction intervals are not the same as confidence intervals. The latter quantifies the uncertainty in model parameters, e.g., the bi in a regression model. Since those bi have uncertainty, the model predictions for the expected observation has uncertainty. However, each real observation has additional scatter around the expected value. This additional uncertainty needs to be factored in when trying to make predictions for individual outcomes. Thus, PI are always wider than CI - sometimes substantially so.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#standard-deviation-sd",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#standard-deviation-sd",
    "title": "Model uncertainty assessment",
    "section": "Standard deviation (SD)",
    "text": "Standard deviation (SD)\nStandard deviation is a measure that tries to assess the spread in the data itself. Thus, it is closely related to the prediction interval. SE and SD are related through SE = SD/\\(\\sqrt(N)\\), where \\(N\\) is the sample size. What that means in practice is that as you get more data, your estimates of the mean (or in general population average outcome) become more precise. However, the actual variability in the data will likely not change much, so the SD remains similar.\nThis is the same idea for CI and prediction intervals. As you get more data, your CI generally shrink. But your prediction intervals likely will not, since the spread in the data will probably not change much as you get more data.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#credible-interval",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#credible-interval",
    "title": "Model uncertainty assessment",
    "section": "Credible interval",
    "text": "Credible interval\nIf one uses Bayesian methods for data analysis, the equivalent of a confidence interval is a credible interval. Credible intervals are sometimes - unfortunately - also abbreviated as CI, sometimes CrI. A credible interval is essentially the Bayesian equivalent to a confidence interval. It informs you about the uncertainty in the population estimate.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#posterior-prediction-interval",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#posterior-prediction-interval",
    "title": "Model uncertainty assessment",
    "section": "Posterior prediction interval",
    "text": "Posterior prediction interval\nThe posterior prediction interval is the equivalent of the prediction interval discussed above, only computed in a Bayesian framework. It can be interpreted pretty much the same as a frequentist prediction interval.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#parametric-methods",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#parametric-methods",
    "title": "Model uncertainty assessment",
    "section": "Parametric methods",
    "text": "Parametric methods\nIf you make the - explicit or implicit - assumption that your outcome follows some distribution (often normal for continuous outcomes), you can use built-in routines to quickly calculate the above-described quantities. Almost all software that fits models will give you these estimates.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#bootstrapping",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#bootstrapping",
    "title": "Model uncertainty assessment",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nFor more complex models or approaches, there might not be a readily available method to compute uncertainty measures. A general approach to produce confidence or prediction intervals is with a sampling method that is very similar in spirit to cross-validation, namely by performing bootstrapping.\nThe idea for bootstrapping is fairly straightforward. Bootstrapping tries to imitate a scenario in which you repeated your study and collected new data. Instead of having actual new data, the idea is that the existing data is sampled to form a new ‚Äúdataset‚Äù, which is then fit. Sampling is performed with replacement to obtain a sample the size of the original dataset. Some observations now show up more than once, and some do not show up. For each such sample, you fit your model and estimate parameters. You will thus get a distribution of parameter estimates. From those distributions, you can compute confidence intervals, e.g., the usual 95% interval. For each fit, you can also predict outcomes and thus obtain a distribution of prediction outcomes (see next).\nLike cross-validation, the bootstrap method is very general and can be applied to pretty much any problem. If your data has a specific structure, you can adjust the sampling approach (e.g., if you have observations from different locations, you might want to sample with replacement for each location.) Limitations for the bootstrap are that you need a decent amount of data for it to work well, and since you are repeating this sampling procedure many times, the procedure can take a while to run.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#p-values",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#p-values",
    "title": "Model uncertainty assessment",
    "section": "P-values",
    "text": "P-values\nSo what about the p-value? This quantity doesn‚Äôt measure uncertainty directly, though it is in some way related to it. p-values are sometimes useful for hypothesis testing. But in my opinion, p-values are overused, generally not too meaningful, and can most often be skipped (though sometimes one needs them just to make reviewers happy). I‚Äôm thus not talking about them further.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#footnotes",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#footnotes",
    "title": "Model uncertainty assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm sure you are wondering where that 89% number comes from, and why I‚Äôm not saying 95%. I‚Äôm following Richard McElreath‚Äôs example from his Statistical Rethinking book. The main point being that any number is arbitrary. 95% (and it‚Äôs associated p-value of 0.05) has emerged as a standard - and to avoid arguing with reviewers and colleagues, I do often use 95% in publications. But it‚Äôs a totally arbitrary standard, and there is no one right or wrong number. For some scenarios, 80% might be good enough, for others you might want to have 99%. You can of course use 95%, but be aware that there is nothing special about that number, other than that everyone else is also using it.‚Ü©Ô∏é\nI‚Äôm purposefully fuzzy here. The problem with CI in a frequentist framework is that their technical definition is rather confusing. According to Wikipedia: Given a confidence level X (95% and 99% are typical values), a CI is a random interval which contains the parameter being estimated X% of the time. You‚Äôll find equally confusing definitions in other places. Most folks actually interpret CI as Bayesian credible intervals (see below) when they talk about it, even if they do their statistical analysis in a frequentist framework.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-fitting/basic-statistical-models/basic-statistical-models.html",
    "href": "content/model-fitting/basic-statistical-models/basic-statistical-models.html",
    "title": "Basic Statistical Models",
    "section": "",
    "text": "Overview\nIn this unit, we will discuss a few basic, common models.\n\n\nLearning Objectives\n\nLearn about several types of basic models.\nUnderstand the relation between basic models and statistical tests.\n\n\n\nIntroduction\nThere are lots of models out there. It is fair to say that nobody is familiar with all of them. There are however several often-used types of models that everyone doing data analysis should be familiar with. These models are easy to implement and often surprisingly useful and powerful.\nYou are likely already familiar with these models. In fact, at some point you probably have taken - or should if haven‚Äôt - a course just on those models.\n\n\nSimple models for continuous outcomes (regression)\nThe most common model type applied to continuous outcomes is a linear regression model. This model is covered in many of the course materials we have been using (and of course other places).\nAdvanced Data Science ‚Äì which used to be part of the old IDS ‚Äì has a chapter on it. ISLR also has a chapter on the topic. So does HMLR.\nThis is such a fundamental type of model and is thus covered in a lot of resources. There are plenty more available, including the good old Wikipedia. Find some you like and read/learn enough about these models that you feel comfortable understanding conceptually what they are used for and how they work.\n\n\nSimple models for categorical outcomes (classification)\nFor categorical outcomes with 2 categories (also called binary or dichotomous outcomes), logistic regression is the most common approach. You might be wondering why this method is called logistic regression even though it is used for classification. This is because logistic regression predicts the probability ‚Äì a numeric value between 0 and 1 ‚Äì of having the outcome. These predicted probabilities are then turned into yes/no predictions (i.e., classifications) using a cut-off, usually (but not always) a probability of 0.5. Probabilities below that cut-off are classified as 0/not present/no and above that as 1/present/yes.\nTo learn about logistic regression, or refresh your knowledge if you have previously been exposed to those models, read through the Logistic Regression chapter in HMLR. This reading goes into maybe more detail than you want or need at this point. So just make sure you go through it to get the main points of what logistic models are all about. We‚Äôll come back to some of the other topics discussed there later.\nLogistic models are also covered in the Classification chapter in ISLR. Read through the first part of this chapter in enough detail to get the overall concepts. You can also go through the sections following the logistic regression section to learn about some additional methods for classification. We‚Äôll revisit some of those types of models later.\nOf course, you can also find lots of additional resources online describing logistic regression models. If you find a good one, let me know.\n\n\nGeneralized linear models\nBoth linear and logistic models belong to a class of models called generalized linear models (GLM). Those types of models allow you to fit outcomes of specific types, for instance, if you have outcomes that are counts (integers), you can use Poisson regression, or if you have continuous outcomes that are all positive, you could use a Gamma regression.\nGLMs all have the same structure: namely there is a linear combination of predictor variables (e.g., \\(b_1*age + b_2*height\\)) and those are connected to the outcome through what is called a link function.\nGLM are very commonly used. Among GLM, linear and logistic regression models are by far the most commonly used ones. Other models that assume specific distributions of the outcome, e.g.¬†a Poisson distribution, can be accommodated by choosing the appropriate link function, which connects a linear combination of predictor variables with the outcome. GLM models are fast and easy to fit (using e.g.¬†the glm function in R), fairly easy to interpret, and often provide performance that is hard to improve upon with more complicated models, especially if the data is on the small side. To prevent overfitting, variable/feature selection or regularization approaches are often used. GLM assume a specific relation between predictors and outcome (e.g.¬†linear, logistic) and as such might not perform well on data that does not show the assumed pattern.\nAt the end of the Classification chapter in ISLR, there is a nice brief discussion of GLMs, I suggest you read through that in enough depth to get the overall ideas, but there is no need to go through all the math.\n\n\nKNN - another simple model worth mentioning\nAnother model that you might have already encountered (e.g., in the ISLR reading) is one called k-nearest neighbors (KNN). The idea is very simple: For any new observation, you compare the values of the predictor variables with those in your data. You then predict the outcome of the new observation to be the average of the outcomes of the K observations whose predictors most closely resemble the predictors of the new observation.\nAs an example for a continuous outcome (regression), if you want to predict height as the outcome and you have age and weight as the predictors, you would take an observation, look at the age and weight values for that observation/individual and compare to the K individuals in your dataset with the closest values (we won‚Äôt go into detail how ‚Äúclose‚Äù is defined). Then you take the average height of those K closest individuals and that‚Äôs your prediction for the new observation.\nThe same idea can be applied to categorical outcomes (classification). Say you wanted to predict sex instead of height. You again looked at the K individuals whose predictor values (here age and weight) are closest, and use the majority to predict the outcome for the new observation. (Say K=5, and 3 of the 5 closest individuals are male. Then you would predict the new observation to be male.)\nKNN often perform well, but they are not very ‚Äúportable‚Äù. For other models, once you trained/fit a model, you can ‚Äútake it with you‚Äù and apply the model to new data, leaving the old data you used for model building behind. KNN are a somewhat strange since the data is the model. All the data is used to predict new observations by comparing them to existing ones and predicting outcomes based on closeness of predictor variables. Which means you always need to have the data to predict new outcomes, often making it not too useful in practice. You also don‚Äôt learn too much in terms of inferential insight. Still, it is a useful model to know about, and it works for both continuous and categorical outcomes. It is also commonly used for imputation of missing values. In that situation, you treat the predictor you want to impute as the outcome and use the remaining predictors as the data. KNN are described in the K-Nearest Neighbors chapter in HMLR and also show up in chapters 2 and 3 of ISLR and the Cross-validation and Examples of Algorithms chapters of IDS. Take a look at any of those resources if you want to learn a bit more about KNN.\n\n\nSimple models for no outcomes\nAs we discussed previously, if you have an outcome variable, your data is called labeled data and the methods you use are called supervised methods/models. Such data with one (or several) outcome variable(s) are most common. But as previously discussed, sometimes you might have data without clear outcomes (unlabeled data) and you still want to determine if there is some pattern in your data. This calls for unsupervised methods.\nThere isn‚Äôt really one single, standard go-to method for unsupervised learning ‚Äì in contrast to the very common linear and logistic models for supervised learning. There are often used methods, such as Principal Component Analysis (PCA) (which falls into a larger group of methods called dimensionality reduction methods) and a group of methods called clustering methods (the most common among these are probably k-means clustering and hierarchical clustering). Since most of you will work on labeled data using supervised methods most of the time, we won‚Äôt go much into unsupervised methods. If you want to learn a bit more, check out for instance the Unsupervised Learning chapter of ISLR. There are also chapters on PCA and different types of clustering in HMLR.\n\n\n\nChoosing K is an important task. 3 sometimes works. Source: xkcd.com.\n\n\n\n\nThe zoo of statistical tests\nIn some of your (bio)statistics classes, you likely came across a variety of statistical tests, e.g., t-test, Wilcoxon, Kruskal-Wallis, and others from the huge collection of tests. Each test makes certain assumptions and is most adequate for certain types of data. For instance you might remember hearing about parametric tests (which are most applicable if the underlying data has a certain distribution, usually a normal distribution) and non-parametric tests, which make fewer assumptions about the distribution of the data, but don‚Äôt have quite the same statistical power. Those more traditional statistical tests certainly have their uses (though are quite often overused and misused). We just don‚Äôt have time to cover them.\nPretty much all of those tests have equivalent formulations as multivariable models (either a GLM type as described here, or others that we‚Äôll cover later). The advantage of more general models, such as GLM, is that they easily allow for as many predictor variables as you want to add. Also, once you understand the general setup of GLM (and other models), this understanding transfers to other methods. For statistical tests, they are often taught as ‚Äúif the data look like this, then test like that‚Äù without any underlying fundamental and transferable understanding. Overall, I am not a big fan of the way many intro (bio)stats courses are still taught these days.\nBecause of that, and since this course is about modern data analysis, we will not cover any classical statistical tests and instead start with multivariable, GLM-type models and then move on to more complex/modern machine learning models.\nIf you never thought about the relation between classical statistical tests and GLM-type models, you might want to check out Common statistical tests are linear models by Jonas Kristoffer Lindel√∏v which explains the relation between the two in a good bit of detail. Statistical tests as linear models by Steve Doogue is also good, it is based on the original text by Lindel√∏v and covers some topics in more detail. Both resources show examples in R, and have links to further materials. At some point in your career as a data analyst, you might need to use or interpret classical statistical tests, and having a general idea that they can map to GLM-type and other models might be helpful.\n\n\nFurther reading\nChapters 3 and 4 of ISL discuss linear and logistic models. So do chapters 4 and 5 of HMLR.\nFor another source that discusses almost all the models just mentioned, each one very briefly, see the Examples of Algorithms chapter in IDS.\n\n\n\nChoosing K is an important task. 3 sometimes works. Source: xkcd.com.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Fitting Basic Models"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-introduction/model-fitting-introduction.html",
    "href": "content/model-fitting/model-fitting-introduction/model-fitting-introduction.html",
    "title": "Model fitting introduction",
    "section": "",
    "text": "This unit provides a brief introduction and overview to the topic of fitting models to data.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model Fitting Introduction"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-introduction/model-fitting-introduction.html#footnotes",
    "href": "content/model-fitting/model-fitting-introduction/model-fitting-introduction.html#footnotes",
    "title": "Model fitting introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor some inexplicable, dumb reason, it is convention to capitalize Bayesian but not frequentist.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model Fitting Introduction"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-r/model-fitting-r.html",
    "href": "content/model-fitting/model-fitting-r/model-fitting-r.html",
    "title": "Fitting (Simple) Statistical Models in R",
    "section": "",
    "text": "In this unit, we will discuss common approaches and packages that are useful for fitting statistical models in R.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Fitting Models in R"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-r/model-fitting-r.html#metrics-with-tidymodels",
    "href": "content/model-fitting/model-fitting-r/model-fitting-r.html#metrics-with-tidymodels",
    "title": "Fitting (Simple) Statistical Models in R",
    "section": "Metrics with tidymodels",
    "text": "Metrics with tidymodels\nThe yardstick package implements a lot of different metrics in the tidymodels framework. For details, see the yardstick package website and the Metric types vignette.\nYou can also define your own metrics, as described in this article on custom metrics.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Fitting Models in R"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-introduction/model-improvement-introduction.html",
    "href": "content/model-improvement/model-improvement-introduction/model-improvement-introduction.html",
    "title": "Model Improvement Introduction",
    "section": "",
    "text": "In this module, we will discuss ways one can try to improve model performance.\n\n\n\nSource: xkcd.com",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Model Improvement Introduction"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-introduction/model-improvement-introduction.html#footnotes",
    "href": "content/model-improvement/model-improvement-introduction/model-improvement-introduction.html#footnotes",
    "title": "Model Improvement Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm again using the term performance in the narrow sense of a model that does well (however quantified) on the metric you chose (based on scientific considerations).‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Model Improvement Introduction"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "In this unit, we will cover the concept of regularization. We‚Äôll also briefly mention a few related approaches.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#ridge-regression",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\nOne way to choose the function that penalizes the predictors is to weigh each predictor by the predictor‚Äôs coefficient squared. Choosing the penalty term as the square of the coefficient is called ridge regression (AKA L2 regularization, Tikhonov regularization, weight decay, and potentially lots of other names). This leads to the cost function:\n\\[C = SSR + \\lambda \\sum_j^p b_j^2.\\]\nThe parameter \\(\\lambda\\) decides the balance between the goodness of fit (low SSR) and the penalty for having large coefficients. Instead of trying different subsets as above and picking the best based on lowest CV performance, we now try different values of \\(\\lambda\\) and pick the model with the lowest (cross-validated) value for our performance measure, C. The parameter \\(\\lambda\\) is often referred to as the tuning parameter or the penalty. Sometimes \\(\\lambda\\) is also called a hyperparameter of the model, which just means that the best value of \\(\\lambda\\) cannot be found by fitting the model one time only.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#lasso",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#lasso",
    "title": "Regularization",
    "section": "LASSO",
    "text": "LASSO\nAn alternative is to penalize the coefficients by their absolute value, namely using this cost function:\n\\[C = SSR + \\lambda \\sum_j^p |b_j| \\]\nThis method is called L1 regularization or the Least Absolute Shrinkage and Selection Operator (LASSO). One nice feature of LASSO (which ridge regression does not have) is that coefficients may go to 0. That means the predictor has been dropped from the model, similar to the subset selection approach described previously. One can think of the LASSO as an efficient approach for performing subset selection. It is not quite equivalent though, since, in the LASSO, the predictors that remain might have been shrunk in their impact due to the regularization penalty.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#elastic-net",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#elastic-net",
    "title": "Regularization",
    "section": "Elastic net",
    "text": "Elastic net\nOne can also combine ridge regression and LASSO into an approach called elastic net, which has a cost function that is the combination of the previous two, namely:\n\\[ C = SSR + \\lambda \\left( (1-\\alpha) \\sum_j^p b_j^2 + \\alpha \\sum_j^p |b_j|\\right)\\]\nNow one needs to try different values for \\(\\lambda\\) (called the penalty parameter) and \\(\\alpha\\) (called the mixture parameter) to determine the model with the best (cross-validated) performance. \\(\\lambda\\) determines the overall weight given to the penalty factor, while \\(\\alpha\\) determines how the penalty should be distributed between the 2 alternative terms. There are also a few variants of this method, such as relaxed elastic net or adaptive elastic net which you can look into if you are interested but we won‚Äôt discuss here.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#tuning-a-regularization-model",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#tuning-a-regularization-model",
    "title": "Regularization",
    "section": "Tuning a regularization model",
    "text": "Tuning a regularization model\nDepending on the kind of regularization model you fit, you have to determine 1 or 2 extra parameters (\\(\\lambda\\) and \\(\\alpha\\)). These parameters are called tuning parameters (or hyperparameters) and it is the first time we see a model that has them. Most complex machine learning models have such tuning parameters, and determining good values for those is part of the model fitting/training process. We‚Äôll talk about that in the next unit.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#bayesian-regularization",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#bayesian-regularization",
    "title": "Regularization",
    "section": "Bayesian Regularization",
    "text": "Bayesian Regularization\nIf one fits models in a Bayesian framework, one needs to supplies priors for each parameter. If one provides informative priors that constrain the possible values of the model parameters based on prior scientific information, it also leads to regularization and generally reduces overfitting.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-tuning/model-improvement-tuning.html",
    "href": "content/model-improvement/model-improvement-tuning/model-improvement-tuning.html",
    "title": "Model Tuning",
    "section": "",
    "text": "Overview\nIn this unit, we will cover the concept of parameter/model tuning and training.\n\n\nLearning Objectives\n\nLearn what parameter tuning/model training are.\nBe able to implement training/tuning in R.\n\n\n\nIntroduction\nWe discussed at length that one (but not the only) goal of model fitting is to find a model that has good performance when applied to new/different data.\nFor linear, logistic, and similar models (generalized linear models), we discussed the idea of subset/feature selection that can help determine a good model. For different sub-models, we fitted the model to some of the data and evaluated model performance on another part of the data using cross-validation. The model with the best cross-validated performance across all sub-models is then designated as the best (at least with regard to our chosen performance metric).\nWe then discussed regularization, which tries to solve a problem similar to subset selection, namely preventing a model that is too complex and thus overfits. In the regularization approach, one does not compare sub-models with different predictor variables. Instead, all predictors are present, and one (or two) parameters (the regularization or penalty parameters, We called them \\(\\lambda\\) and \\(\\alpha\\)) are varied to influence model complexity. For each value of that parameter, the model is evaluated through cross-validation, and the \\(\\lambda\\) which produces the model with the best performance is chosen (or sometimes one picks a somewhat larger \\(\\lambda\\) to further prevent potential overfitting).\nThis approach of taking a model parameter and evaluating models for different parameter values is called model/parameter tuning. These model-specific tuning parameters are often also referred to as hyperparameters. Simple models, like linear or logistic regression, do not have any parameters that can be tuned. However, more complicated models, which we will discuss soon, generally have one or more tuning parameters. Very flexible models, such as neural nets used in artificial intelligence tasks, can have thousands or even more parameters that need tuning. For any models with tuning parameters, it is essential to tune the parameters/train the model. Without it, the model will likely not perform very well.\n\n\nModel training/parameter tuning recipe\nTraining the model by tuning its parameters follows a general approach that is conceptually the same for all models. You need to go through these steps:\n\nSelect some values for your tuning parameters.\nUsing cross-validation, fit model to a part of the data (the analysis portion), evaluate model performance on the remainder of data (the assessment portion).\nSelect new values for your tuning parameters, and repeat step 2.\nKeep going until you hit some stopping criterion, e.g., you tried all parameter combinations you wanted to try, you hit the maximum number of tries, or you hit the maximum amount of time you allocated for this parameter tuning. (Or you found the absolute best tuning parameter values, but that‚Äôs only likely for simple models with few tuning parameters).\nPick your best model as the one with the parameter values that produced the overall best model performance.\n\nYour final model consists of both the type of model and the values of the parameters.\n\nYou might (or might not) have wondered if this procedure of repeatedly refitting the model for different values of the tuning parameters (hyperparameters) and re-fitting the model for different values of the model parameters (e.g., for the coefficients \\(b_i\\) for a linear or generalized linear model) isn‚Äôt more or less the same. Both conceptually and in practice, those approaches are quite similar. For GLM, there are differences in how things happen, namely the \\(b_i\\) can be determined in a single step, without the need for trial and error. But for other models, such as some of the ML models we‚Äôll explore, tuning parameters and internal parameters associated with the model might both need to be determined by iterative procedures. Thus, while one can try to distinguish between model and tuning parameters (see e.g., Jason Brownlee‚Äôs blog post, which discusses that point a bit more), this is often fuzzy. And the usual caveat applies: The terminology is not consistent, and what some people might call a model parameter might be called a tuning parameter by others. The good news is that in practice it doesn‚Äôt matter much what you call a specific parameter. Some can be tuned and you can chose to do so (or not and fix it), others cannot be tuned and are determined internally.\n\n\n\nSearching the tuning parameter space\nThe problem of trying a lot of different tuning parameter values to find the ones that lead to the best performance is very similar to the problem of trying to test a lot of different sub-models during subset selection to find the best model. Not surprisingly then, the procedures to perform the search over parameter space are similar to the ones one can use to search over subset/sub-model space. The most basic one for subset selection was to try every possible model (exhaustive search). This works in principle for tuning parameters as well, but only if the tuning parameters are discrete. For continuous tuning parameters (e.g., \\(\\lambda\\) in regularization), it is impossible to try all values. One instead chooses discrete values for the parameter between some lower and upper limits and then searches that grid of parameters. This is called grid search. The advantage of a grid search is that you know you tried every combination of parameters in your grid.\nFor instance, if you have 2 continuous tuning parameters and 1 categorical tuning parameter with 3 categories, if you chose 10 discrete values for the 2 continuous parameters each, your grid would be 10x10x3. In this case, you would need to evaluate the model for 300 different tuning parameter values. That‚Äôs not too bad. But you can see that one problem with this approach is that as the number of tuning parameters increases, or if you want to try many different discrete values (e.g., 100 instead of 10 for each continuous parameter), the number of times you need to run the model increases rapidly. That‚Äôs the same problem as the exhaustive search for subset selection.\nTo solve this problem, there are a few major methods:\n\nOne can efficiently choose parameter values to estimate ‚Äì instead of searching the entire grid, you can use a sampling method to find a smaller grid that covers the same space in the most efficient way. The most well-known algorithm for this is probably Latin hypercube sampling.\nMethods similar to those mentioned for subset selection can be applied, e.g., one can use Genetic Algorithms, Simulated Annealing, Racing Methods, or many other optimizer routines for tuning. While it is, in principle, possible to write your own code that implements the tuning procedure with whatever method you want to use, for most instances, it is easier to use pre-existing methods.\n\n\n\nTuning with R\ntidymodels, and its package tune currently have a few different algorithms for searching the tuning parameter space implemented. Grid search is the main one, but the package also implements Iterative Bayesian optimization. The finetune which is not yet on CRAN implements Simulated Annealing and Racing Methods. To learn more about those, see chapters 12-14 in Tidy Modeling With R.\nThe mlr3/mlr package I keep mentioning also has algorithms to tune parameters, including some that are not available (yet) in tidymodels. For tuning in mlr see e.g.¬†tuning tutorial and advanced tuning tutorial. For this course, we‚Äôll focus on what‚Äôs available in tidymodels, but if you ever need to do some major parameter tuning/model optimization, checking out mlr might be worth it (or implementing your own with tidymodels, which is possible.)\n\n\nFurther comments\nOnly if your model has very few tuning parameters and your data is manageable in size can you find the absolute best parameter values in a reasonable amount of time. More likely, you‚Äôll find parameter values that give you a close-to-optimal model.\nAt times, cross-validation might take too much time, and you might have to use a computationally faster method, such as AIC or similar, to try to estimate model performance on future data. That‚Äôs not ideal, you might want to consider other approaches first (fewer parameter evaluations, faster computer, running things in parallel‚Ä¶).\nAny process that adjusts the model repeatedly based on prior fits to data has the danger of overfitting, even if you try to guard against this using approaches such as CV. Thus, sometimes less tuning might actually give you a more robust/generalizable model.\nThe more tuning parameters in your model, the more data you need to be able to train the model properly. If you have a mismatch between the amount of data and model complexity, you are likely going to overfit. This is why complex models such as neural nets need vast amounts of data (millions or billions of observations).\n\n\nFurther information\nMost relevant and maybe good to visit next are chapters 12-14 in Tidy Modeling With R, which discuss the general tuning process, and then explain how to do grid search and iterative search using tidymodels.\nSection 2.5.3 of HMLR provides a very short section on tuning. ISLR mentions tuning in various places but doesn‚Äôt describe it in a dedicated section. IDS mentions it in the Machine Learning chapters but also does not have a dedicated section on the topic.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Model Tuning"
    ]
  },
  {
    "objectID": "content/module-ai/ai-conceptual/ai-conceptual.html",
    "href": "content/module-ai/ai-conceptual/ai-conceptual.html",
    "title": "Conceptual ways to think about AI",
    "section": "",
    "text": "Overview\nThis unit consists of some thoughts and musings about AI and how to best think of and use it in a bigger picture framework.\n\n\nLearning Objectives\n\nKnow about a few ways one can conceptualize current AI tools.\n\n\n\nIntroduction\nAI, and especially generative AI like LLM are very new tools. Everyone is still trying to figure out how to use them, what they mean for the future, etc. While one can obviously use these tools without much further thought, it can be helpful to think about them in a conceptual way to have a potentially useful framework of interaction. Below are a few conceptual frameworks that I have heard from others or that I‚Äôve been thinking about.\n\n\nAI as the intern/1st year graduate\nI‚Äôve heard this concept multiple times by now. The idea is that you should think of LLM AI tools as being good at tasks that an intern, or a new graduate could do without too much training. For example, asking ChatGPT to solve world hunger is not a good idea. However, asking it to give you a list of countries where malnutrition is the worst and a summary of likely reasons for that, is a task where it will probably produce a result that you can use as starting point for whatever your larger project is.\nWhat that means is that to get the most out of the AI, you should break your tasks into manageable, well-prescribed bits, and ask the AI to tackle each one. The more details and instructions you provide, the more likely you will get something useful.\n\n\nAI as a brainstorming partner\nWhile AI is very good in doing specific, well-prescribed tasks, it can also be useful as a type of sparring partner or brainstorming device. You can throw ideas at the AI that are more open-ended, and ask it to provide its thoughts. Then you can iterate and that way possibly explore a topic and various options much faster than if you just thought about it yourself. This doesn‚Äôt always lead to good results, but it‚Äôs so quick and easy, it‚Äôs often worth a try. Note that if you use AI in this way, you would interact with it differently compared to the above approach. To get specific work done, e.g.¬†getting the AI to write you a piece of code, you want to be as specific and detailed as possible. You will often provide very long prompts. In contrast, if you use AI as brainstorming partner, you can have shorter, more vague prompts and do more of a back and forth. Just be clear what you are trying to accomplish and adjust your interactions accordingly.\n\n\nAI as electricity\nI haven‚Äôt heard this idea too much, but it seems to me that long-term, AI is going to be a bit like electricity. It‚Äôs going to be everywhere, it will power a lot of the environment around us, and it will become both more ubiquitous and possibly also more invisible. We use electricity all the time, and we rarely think about it. My guess is that AI will become that way. It will be interesting to watch how we get there. In the early days of electricity, there were fights about AC versus DC and lots of things were tried that didn‚Äôt work and it took a while before we had a (kinda) functioning electric grid that mostly just works. It will take AI a while. But I think we need to be prepared to have it be part of ‚Äúeverything‚Äù in the not-too-distance future.\n\n\nThe composer/conductor and the orchestra\nThis is another one I haven‚Äôt seen online, but I‚Äôm sure I‚Äôm not the first one to think of it. In fact, I asked the LLM (Bing AI in creative mode) to give me its thoughts on this analogy with this prompt:\n\nWrite a half-page paragraph that compares an LLM AI user to a composer or conductor, and the LLM AI tool to an orchestra.\n\nThe returned paragraph was pretty weak and not what I had in mind (try yourself, maybe you get something better). Here is my thought: The AI is a very versatile tool and you can do a lot of things with it, kinda like an orchestra. As a composer or conductor, you don‚Äôt need to be able to play each instrument of the orchestra. But you do need to know enough about each instrument to compose meaningful instructions as to what everyone should play, and you should know what to expect, so when you tell the trumpets to play a certain tune, you should be able to assess if what they produce is what you have in mind, and correct as needed.\nOf course, this analogy goes beyond AI tools. We can say the same about other complex tools, for instance the R programming language or a car. You don‚Äôt need to understand all the details of how these complex systems work under the hood (unless you want to become a full-time programmer or car mechanic), but you do need to know enough to give useful instructions, use them effectively, and critically assess what the machine returns and correct as needed.\n\n\nSummary\nI‚Äôm not sure these thoughts and musings about AI are useful. It helps me to find conceptual frameworks to think about new things. It might help you too. Or not üòÅ.\n\n\nFurther Resources\nSome of the resources listed on the AI resources page in the ‚Äògeneral‚Äô discuss topics similar to what I wrote here.",
    "crumbs": [
      "Content",
      "AI",
      "AI Concepts"
    ]
  },
  {
    "objectID": "content/module-ai/ai-fix-code-r/ai-fix-code-r.html",
    "href": "content/module-ai/ai-fix-code-r/ai-fix-code-r.html",
    "title": "AI tools for fixing code",
    "section": "",
    "text": "Overview\nIn this unit, we discuss using LLM AI tools to help fix code.\n\n\nLearning Objectives\n\nKnow how to use current LLM AI tools to help fix code.\n\n\n\nIntroduction\nThis topic is very similar to the approach for writing code. In that unit, we already discussed how to iteratively update and improve the code. Fixing non-working code is really just another version of asking the AI iteratively to improve things. However, it seemed distinct enough to me that I thought it‚Äôs worth placing it into a separate unit.\nThe assumption is that you have some code (likely AI generated, but could also be code you wrote) and need to fix some typos. If the code was generated in the current session, you can just refer to it in your prompt. If the code was generated previously, you will supply the code as part of the prompt. The latter approach is more general, so I‚Äôm focusing on that one.\n\n\nFixing through iteration\nThe most basic approach is to just feed the AI non-working code and tell it to fix it. Here is an example continued from the non-working code we had at the end of the AI and code writing unit. To mimic a general setting, with code possibly produced earlier or not by the AI, I started a new session with the Bing AI in ‚Äòprecise‚Äô mode and gave it this prompt.\n\nThis code does not work. Please fix it.\nPASTE ALL CODE FROM THIS FILE HERE.\n\nHere is what I got back:\n\nI see that you‚Äôre trying to generate a scatter plot of BMI against weight, stratified by sex. The code you‚Äôve provided seems to be correct. However, the issue might be due to the specific versions of the ggplot2 and dplyr libraries you‚Äôre using, or it could be a problem with your R environment.\nIf you‚Äôre getting a specific error message, could you please provide it? That would help me understand the issue better and provide a more accurate solution. If you‚Äôre not seeing an error message, could you please describe what‚Äôs happening when you say the code doesn‚Äôt work? For example, is the plot not displaying as expected, or are the calculations not producing the right results? This additional information will help me assist you more effectively.\n\nSo this didn‚Äôt work. What‚Äôs next? One option is to see if another LLM AI can do it. For instance I could try and switch to another mode in Bing (creative or balanced) or try another LLM. I‚Äôm not doing this for now, but it might at times be worth giving it a try. What I‚Äôll do here is to help the AI (as it suggested) by being more specific about the problem. Here is my updated prompt:\n\nThe code below produces this error message:\nError in eval(ei, envir) : object ‚Äòsex‚Äô not found\nPlease fix it.\nPASTE ALL CODE FROM THIS FILE HERE.\n\nWith this prompt, I got it to work. The AI gave me this helpful explanation and fully working code\n\nThe error message you‚Äôre seeing is due to the sex variable not being recognized within the ifelse function in your data.frame creation. This is because the sex variable is not yet defined at the time ifelse is called.\nYou can fix this by first creating the sex variable and then using it in the ifelse function. Here‚Äôs the corrected code:\nCODE IS HERE\n\nThe end result is still not perfect, the difference in weight by sex is too large and we might want more scatter. But that can easily be fixed with either further iterations of asking the AI to update the code, or manual interventions. Speaking of manual interventions‚Ä¶\n\n\nManual intervention\nThe overall goal is to get working code as quickly as possible, not to have things completely AI generated. Therefore, if you reach a stage where you realize it‚Äôs faster if you just fix the code yourself, go ahead and do so.\nThis occurs at two stages. The most obvious one is close to the end. If the AI has given you code that is very close to what you want, but has a few issues (either conceptual or coding errors) that you immediately know how to correct, then just go ahead and finish things off manually. At this point, you should be able to ‚Äúturn off‚Äù the AI and have working code that you can apply to your project.\nAnother situation you might want to intervene manually is if the AI gets stuck. For instance, if you have to keep asking the AI to fix/update some code and it still doesn‚Äôt produce what you want, you could go ahead and make some manual adjustments to the code that you know should happen, and then feed the code, which still may not be fully working, back to the AI and ask it to update further.\nThe nature of the AI algorithms is that they can ‚Äúget stuck‚Äù and if you give them a slight change of scenery, they might be able to get unstuck and get you closer to your goal.\n\n\nSummary\nAs with code writing, for fixing code, being as specific as possible and iterating are key. Usually, if you tell the AI exactly what needs fixing and do a few iterations, you‚Äôll get to something that fully works, or is so close that you can quickly finish the rest manually.\n\n\nFurther Resources\nSee the AI resource unit.",
    "crumbs": [
      "Content",
      "AI",
      "Fix code with AI"
    ]
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html",
    "title": "Introduction to current LLM AI tools",
    "section": "",
    "text": "This unit provides an introduction to the use of Large Language Model (LLM) based Artificial Intelligence (AI) tools (such as ChatGPT) for coding and data analysis.",
    "crumbs": [
      "Content",
      "AI",
      "Introduction to LLM AI tools"
    ]
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#privacyconfidentiality",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#privacyconfidentiality",
    "title": "Introduction to current LLM AI tools",
    "section": "Privacy/Confidentiality",
    "text": "Privacy/Confidentiality\nWhen you use AI to help with data analysis, you might want to show your data to the AI and ask it to do something with it. The problem is that pretty much all these systems take your data and move it to the servers of whichever company you use for processing. Often, these companies keep your data to help improve their models. If you have data which is sensitive, e.g.¬†human data, or data that you don‚Äôt want to share, then you need to be careful not to let the AI have access to the data. A good solution is to generate synthetic data that has the structure of your real data but is made up. Then you can ask the AI to process this synthetic data and give you the code it generated. In a later step, you can go ‚Äúoffline‚Äù and use the code the AI helped you write on your real data. We‚Äôll discuss how to generate synthetic/artificial data shortly.\nBeyond the data, it is important to keep in mind that if you have the AI running (e.g., through the R Studio CoPilot integration), it might access not just your current file but possibly also other sources you link to, e.g.¬†other stuff on your computer. If you have things you don‚Äôt want the company to ‚Äúgrab‚Äù and copy to their servers, then be careful about what you let the AI access.\nIn general, be careful what information you let the AI access and be aware that it might end up on the server of whatever company is running the AI tool.",
    "crumbs": [
      "Content",
      "AI",
      "Introduction to LLM AI tools"
    ]
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#non-working-code",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#non-working-code",
    "title": "Introduction to current LLM AI tools",
    "section": "Non-working code",
    "text": "Non-working code\nWhile most of the AI tools have become pretty good at coding in R and other languages, at times they don‚Äôt get it right. You might get code back that doesn‚Äôt work. AI are known to hallucinate, i.e., make up stuff. For instance it‚Äôs not uncommon that the AI invents an R command that does not exist. So when you get your code back, you will often need to do some trouble-shooting. At times, you can tell the AI that the code is not correct and ask it to fix it. If this doesn‚Äôt always work, then you can make the fixes by hand or try to reformulate your request.",
    "crumbs": [
      "Content",
      "AI",
      "Introduction to LLM AI tools"
    ]
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#reproducibility",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#reproducibility",
    "title": "Introduction to current LLM AI tools",
    "section": "Reproducibility",
    "text": "Reproducibility\nThe companies making AI tools constantly update and improve the algorithms. Further, the underlying methods often have random components. This means that if you give the same instructions to an AI tool on different occasions, the results/code you get might differ. This means things might be non-reproducible.\nWhile I encourage you to keep track of any input prompts you use to have the AI generate code, note that providing those prompts does not allow someone else (or future you) to exactly reproduce things. Thus, while AI tools can be useful helpers during the data analysis process, they should not be considered part of the final workflow, which instead should contain results/code (possibly generated with AI help) which can run in such a way to allow full reproducibility.",
    "crumbs": [
      "Content",
      "AI",
      "Introduction to LLM AI tools"
    ]
  },
  {
    "objectID": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#ethics-of-using-ai",
    "href": "content/module-ai/ai-llm-introduction/ai-llm-introduction.html#ethics-of-using-ai",
    "title": "Introduction to current LLM AI tools",
    "section": "Ethics of using AI",
    "text": "Ethics of using AI\nHere I‚Äôm not talking about the fact that most AI tools were trained on data without the permission of the original data generators, or that a lot of AI algorithms produce biased (unethical?) output. Those topics are too complex to tackle here. Instead, I‚Äôm talking about the ethics of using AI to help with your data analysis, or more generally your academic/professional work.\nSince these tools are so new, nobody knows yet how to properly acknowledge AI help. I‚Äôm not sure either. If I search Google for an example to help with my code, and find something on StackOverflow that I use as basis for my code, should I cite it? I sometimes do add a link to the original post in my code. Partly to give credit, and partly to remind myself where I got it from. But I don‚Äôt think there are clear rules on this.\nSimilarly, if you copy text from Wikipedia or some other source, you need to cite it. But if you read it, then repeat in your own words, when do you need to cite it? I don‚Äôt think it‚Äôs clear. This is similar with AI. If you have a full code or large chunk of text generated by AI and you use it ‚Äúas is‚Äù, you probably need to state that. But it‚Äôs more likely that the AI will give you some parts of the code or text, and you write the rest. What is the rule for that? I don‚Äôt think there are clear rules.\nI suggest you follow the guideline of ‚Äúif in doubt, cite/acknowledge‚Äù. For instance, at the beginning of some code, you can make a statement saying ‚Äúpart of this code was generated by ChatPGT using the following prompt‚Äù and then state the prompt you use. Or if you use AI to help you interpret your data, you can state somewhere in your output (e.g., your Quarto document), that you used AI to help generate insights/text/etc. Providing this kind of information prevents you from being accused of ‚Äúcheating‚Äù (if someone thinks using AI is cheating, for this course, you are encouraged to do so), and it also helps somewhat with the reproducibility (see above).",
    "crumbs": [
      "Content",
      "AI",
      "Introduction to LLM AI tools"
    ]
  },
  {
    "objectID": "content/module-ai/ai-resources/ai-resources.html",
    "href": "content/module-ai/ai-resources/ai-resources.html",
    "title": "A collection of links to AI resources",
    "section": "",
    "text": "Overview\nThis unit consists mainly of a collection of links to AI resources that might be useful. I tried to group them into themes and provide a bit of commentary. The focus is on generative AI tools.\n\n\nLearning Objectives\nBy the end of the unit, you should:\n\nBe aware of a few places where you can learn more about AI.\n\n\n\nIntroduction\n\n\nAI in general\n\nOne Useful Thing is a Substack blog by Ethan Mollick where he discusses ideas around (generative) AI from a very applied user perspective. You might find a lot of what he writes echoed in my writing, because that‚Äôs where I get a lot of my information from üòÅ.\nUnconfuse Me is a podcast by Bill Gates that covers all kinds of topics, but has a few interesting AI-themed ones, namely (as of this writing) episodes 2 and 5.\nDeepLearning.AI is a platform/company that offers a lot of teaching material related to AI. Some of the materials are free.\nA really good (but non-free) book is Artificial Intelligence: A Guide for Thinking Humans. The author, Melanie Mitchell, also has an AI blog that is worth checking out.\nThis Coursera course gives a brief, non-technical introduction to current LLM AI tools (such as GPT).\n\n\n\nTechnical aspects of AI\n\nThe Deep Learning Chapter of HMLR provides a nice basic introduction.\nThe Statquest Youtube channel has multiple videos with nice explanations on various AI related topics, at times going fairly deep into the technical details, while still being rather easy to understand.\n\n\n\nAI and R\n\nThese materials from a recent workshop by Phil Bowsher describe using Tensorflow to do deep learning/AI in R.\n\n\n\nAI and teaching\n\nWharton Interactive is an initiative by Ethan and Lilach Mollick and colleagues at Wharton that - among other things - looks at the use of AI for teaching and learning. The website contains some interesting ideas and links to papers that look further into AI use.\nMIT has a nice collection of AI learning resources. The focus is on using those tools for teaching/learning.\n\n\n\nAI and Coding\n\nGitHub Copilot is meant specifically for coding. It is generally not free, but if you get the GitHub student developer pack you get it included. The GitHub Copilot documentation contains a lot of good information to get started and to dig deeper.",
    "crumbs": [
      "Content",
      "AI",
      "AI Resources"
    ]
  },
  {
    "objectID": "content/module-ai/ai-write-code-r/ai-write-code-r.html",
    "href": "content/module-ai/ai-write-code-r/ai-write-code-r.html",
    "title": "AI tools for code writing",
    "section": "",
    "text": "Overview\nIn this unit, we discuss using LLM AI tools to help write code.\n\n\nLearning Objectives\n\nKnow how and why to use current LLM AI tools to help with coding.\nBe familiar with several approaches of LLM AI assisted coding.\n\n\n\nIntroduction\nIf you write code, there are (at least) two major parts to the effort. First, you have to figure out what exactly you want to accomplish with your code. Second, you need to write a bunch of commands in the programming language of your choice to get what you are hoping to accomplish. The first part is generally the intellectually more challenging one, and a step which AI (currently) are not very good at. The second part is generally less hard, but it can be very tedious, especially if you are new to coding or if you need to write a lot of code. AI are getting pretty good at helping with writing code.\nI expect that soon, instead of people writing code, most code will be written by AI under the guidance and direction of people. This should eventually lead to much more efficient, and potentially also better code. We aren‚Äôt quite there yet, but the current AI tools are already quite useful for helping with code tasks, so you should use them.\n\n\nGood prompting\nTo get good results from the AI, it is important that you be as specific as you can with your prompt.\nTry this prompt with one of the LLM AI:\n\nWrite R code that generates a scatterplot and a violin plot.\n\nYour result might or might not be close to what you had in mind. If you are not providing a lot of details, the AI can decide what to do and sometimes it is close to what you had in mind, but often it is not.\nNow try this prompt:\n\nWrite R code that generates a dataset of 100 individuals with ages from 18 to 49, BMI values from 15 to 40 and smoking status as yes or no. Assume that age and BMI are uncorrelated. Assume that smokers have a somewhat lower BMI. Then use the patchwork R package to generate a panel of ggplot2 plots. The first panel should show a violin plot with BMI on the y-axis and smoking status on the x-axis. The second panel should show a scatterplot with age on the y-axis and BMI on the x-axis. Add thorough documentation to your code.\n\nWhen I gave the second prompt to ChatGPT 3.5, it gave me this fully working code. It is possible that when you try this, the code will look slightly different. Remember, these AI tools are not fully deterministic and can produce different results each time. Hopefully, what you get will run. If not, you might need to either fix the code or ask the AI to fix it (we‚Äôll look at that in another unit).\nAs you can see, good prompts are often quite long. It makes sense to write those down outside the AI first. That also helps somewhat with reproducibility. I generally try to stick AI prompts at the top of my R/Quarto files, or into a separate file.\nYou also notice that for the second prompt, I had to know more about the programming language, for instance I had to know that there is a package called ggplot2 and one called patchwork.\nYou will find that the more you know in general about a topic, the more useful those AI tools become. This means that you still have to learn some coding (or whatever the topic is) and understand it enough on a big picture level to be able to be useful. But you don‚Äôt necessarily need to be an expert.\nAn analogy I like is that of a composer. A composer needs to know enough about the various instruments of an orchestra to be able to write music for each instrument. But they don‚Äôt need to be able to play each instrument. Similarly, you need to know enough about coding or whatever the topic is you are working on to compose prompts for the AI and evaluate what it produces, but you don‚Äôt necessarily need to be an expert coder.\n\n\nIterating\nIt‚Äôs rare that you get exactly what you want from the AI with your first prompt. Quite likely, you realize that you weren‚Äôt specific enough, or that you really wanted something slightly different but didn‚Äôt properly specify it in the prompt. Often, the code might also not quite work. The AI might have just made up a package or function that doesn‚Äôt exist, or otherwise introduced mistakes.\nWhile it would be nice to get a great product on the first try, the process is so fast that it doesn‚Äôt matter much. Just try again. You can either update your prompt and feed it to the AI again. Or you can tell it what changes you want to make. Try this as a starting prompt:\n\nWrite R code that generates a dataset of 100 individuals, half of them female, with ages from 18 to 49, weight from 100 to 500 pounds and BMI from 15 to 35. Make a figure that shows weight on the x-axis, BMI on the y-axis, and stratification by sex. Add thorough documentation to the code.\n\nWhen I fed that to Bing in ‚Äúprecise‚Äù mode it produced this code. Note that it didn‚Äôt really give me an R script, I copied what it produced into an R file.\nThe code runs and produces a figure, but there are problems. Right now, the data is generated assuming each variable is independent of the other. We know that weight is strongly correlated with BMI, since it‚Äôs part of the equation defining BMI. We also know that males are generally heavier. We can easily fix this. One approach is to update your prompt and make it more explicit, such as:\n\nWrite R code that generates a dataset of 100 individuals, half of them female, with ages from 18 to 49, weight from 100 to 500 pounds and BMI from 15 to 35. Assume that males are generally somewhat heavier than females. Also assume that BMI and weight are positively correlated, based on the definition of BMI. Make a figure that shows weight on the x-axis, BMI on the y-axis, and stratification by sex. Add thorough documentation to the code.\n\nWith this updated prompt, I got this code, here are the relevant bits:\n\n# Generate a dataset of 100 individuals\ndata &lt;- data.frame(\n  # Generate sex variable with half of them female\n  sex = rep(c(\"male\", \"female\"), each = 50),\n  \n  # Generate ages from 18 to 49\n  age = sample(18:49, 100, replace = TRUE),\n  \n  # Generate weight values from 100 to 500 pounds\n  # Assume that males are generally somewhat heavier than females\n  weight = c(runif(50, min = 100, max = 400), runif(50, min = 200, max = 500)),\n  \n  # Generate BMI values from 15 to 35\n  # Assume that BMI and weight are positively correlated\n  bmi = c(runif(50, min = 15, max = 25), runif(50, min = 25, max = 35))\n)\n\nThere are several issues. It labeled the first 50 individuals as males, but assigned them the lighter instead of heavier weight. It also didn‚Äôt properly create BMI values as function of weight.\nWe could of course modify the prompt above further, trying again. Or we can directly iterate at the prompt and ask the AI to update the code. Here is an example:\n\nUpdate the code such that males and females are randomly distributed, make sure that males are on average heavier than females, and compute values for BMI as a function of weight with some random variation added.\n\nWith this prompt, I got this further updated code, here are again the relevant bits:\n\n# Generate a dataset of 100 individuals\ndata &lt;- data.frame(\n  # Generate sex variable with males and females randomly distributed\n  sex = sample(rep(c(\"male\", \"female\"), each = 50)),\n  \n  # Generate ages from 18 to 49\n  age = sample(18:49, 100, replace = TRUE),\n  \n  # Generate weight values from 100 to 500 pounds\n  # Make sure that males are on average heavier than females\n  weight = ifelse(sex == \"male\", runif(50, min = 300, max = 500), runif(50, min = 100, max = 300))\n)\n\n# Compute values for BMI as a function of weight with some random variation added\n# Assume a height of 1.75 meters for simplicity\n# BMI = weight(kg) / height(m)^2\n# Convert weight from pounds to kg (1 pound = 0.453592 kg)\ndata$bmi &lt;- (data$weight * 0.453592) / (1.75^2) + rnorm(100)\n\nThat looks better. However, when you try to run it, you‚Äôll find that it doesn‚Äôt work. The problem is that it tries to use the variable sex before that data frame is defined. So we need to fix that code. We could of course do it by hand, but we can also see if we can get the AI to fix its own code. Which brings us to the next unit on using AI to fix code.\n\n\nSummary\nLLM AI are very helpful at assisting with writing code. The more you know what you want, and the more specific you can be (which requires some level of subject matter expertise) the better your results. Rarely do you get exactly what you want on the first try, but iterating is easy and fast.\nThe list of tips provided in an earlier unit for general LLM AI use also applies for using it as a coding helper. Here is a version of that list again:\n\nBe as detailed and specific as possible.\nIterate. Either only AI based iterations or a mix of manual and AI iterations.\nTry different AI engines or settings or prompt phrasings.\nAsk the AI to add a lot of comments into the code to explain what each line of code does.\nBreak down big tasks into smaller tasks, ask the AI to solve the smaller tasks, then put it together.\nWrite down your prompts so you and others can go back to them later and see what you did.\n\n\n\nFurther Resources\nSee the AI resource unit.",
    "crumbs": [
      "Content",
      "AI",
      "Write code with AI"
    ]
  },
  {
    "objectID": "content/module-coding-basics/rcoding-basics.html",
    "href": "content/module-coding-basics/rcoding-basics.html",
    "title": "R Coding Basics",
    "section": "",
    "text": "Overview\nIn this module, we will talk a bit about writing code in R. Since this is not a learning R class, we can unfortunately only dedicate one module to it, everything else will be learning by trying/doing.\n\nFor those among you who have not used R or any other programming language before, this unit (and everything else related to R in this course) will be time-consuming. Be prepared to put in a good bit of time and effort. Budget your time accordingly and plan ahead! If you do, I‚Äôm fairly certain you will find it worth it. If you are not able or willing to allot the time needed to learn enough R to make things work, this course might not be ideal for you.\n\n\n\nLearning Objectives\n\nGain starting knowledge of R coding\nBecome familiar with the tidyverse\nBecome familiar with resources that help you learn R\n\n\n\nLearning to code\nR (or any other programming language) is best learned by ‚Äúdoing it‚Äù. You will learn more R as we go along, but the focus of the class is on data analysis, so while I will provide you with resources to figure out the R bits, outside of this module we will not focus on ‚Äòlearning R‚Äô. You will learn by doing as we go through the course. As with anything, the more you practice, the better you will get. You should approach learning to code with an attitude of fearless curiosity. You will get stuck, you will get frustrated with some weird error message in your code (still happens to me at least once a week), and you will eventually figure it out. Make use of the great resources that are out there. They are all listed below and in the Resources section.\nThis figure illustrates the journey of learning to write code:\n\n\n\nThe journey of learning to code. Source: https://www.thinkful.com/blog/. Original post is not online anymore.\n\n\nMy goal is that during this course, you will reach the beginning of the upswing of awesome, at least when it comes to being able to use R to perform data analyses. But to get there, you‚Äôll have to go over the cliff of confusion and through the desert of despair, and I‚Äôm confident that you‚Äôll get there and won‚Äôt be stuck in the hand-holding honeymoon. In fact, at times I‚Äôm providing you less detailed instructions than I could to get you quickly to the stage where you have to figure out bits yourself. I guess one could say that instead of hand-holding, I let you stumble and fall some, and then will help you to get back up üòÉ. It might feel a bit more frustrating at least initially, but it‚Äôs a much better way to learn.\nIt goes without saying that learning to code (or learning anything else) is not a linear process. Even after many years of coding and using R, I regularly encounter the cliff of confusion and the desert of despair if I‚Äôm trying to do something new that I haven‚Äôt done before and invariably get stuck.\n\n\nLearning the basics of R\nThis course is not a learn-to-code course. Therefore, we‚Äôll be writing code as-needed as we go along. Depending on your level of R coding experience, this might go very quickly or rather slowly. Be prepared to spend a good bit of time learning some coding. I think it‚Äôs a skill that‚Äôs worth the effort üòÉ.\nIf you have no prior R coding experience, you might want to try a few of the swirl tutorials or work your way through the beginning chapters of Hands-On Programming with R. Chapters 1 through 3 of IDS are also a good option.\nAlternatively, pick any other source you like, e.g.¬†those listed in the resources section of this course, or anything else you find. (If you find something not listed that is really useful, feel free to share!) Of course, AI tools can also be very helpful, but to use them well you do need to know some coding yourself.\n\n\nMore R coding\nWe only focus explicitly on R coding in this module. That is obviously not enough to become proficient at it. For the rest of the course, I expect you to learn the coding bits you need to get things done on the side. I understand this will be a challenge. Don‚Äôt hesitate to seek help. The Posit Recipes cover many of the coding related topics we will use in this course, so go through any and all resources you find useful whenever you are able (or when you need to know). I also strongly encourage AI tools and StackOverflow and other online resources, which Google will help you find fairly easily.\n\n\nGetting help\nMaybe the most important skill for learning any programming language is figuring out how to find and get help with any problem. Google, StackOverflow, and the Internet are your friend. The Posit Community Forum is also a great place to ask for help.\nIf you have a problem with your code, it‚Äôs likely someone else had the same/similar problem before you and asked a question (and probably got an answer). So search the web, and you‚Äôll find something useful most of the time.\n\n\n\nFortunately rare for R. Source: xkcd.com.\n\n\nIn those rare cases when you cannot find information online that helps you figure out your problem, feel free to ask for help.\nA great source for answers is asking questions online. Sometimes, people complain that replies to questions they ask online are unfriendly or harsh. While this is at times true, consider that all the people providing answers are volunteers. They‚Äôre doing it because they want to help others, they don‚Äôt get paid for it. It is therefore important that the person asking the question does not waste people‚Äôs time by asking poorly formulated questions or questions that have been previously answered. In general, those kinds of questions get rude replies. If you have done your homework (i.e., searched online first to see if the answer is already available) and can precisely formulate the question/problem, ideally with a reproducible example, the chance that you get an unfriendly reply is very low.\nI have found that a good way of posing question is to write something like this: ‚ÄúI need help with SPECIFIC PROBLEM, I have searched around and found LINKS/DESCRIPTION OF SIMILAR ISSUES but that doesn‚Äôt quite solve my problem yet.‚Äù If you have a coding problem, add ‚ÄúHere is some code illustrating what I want to achieve and where the problem is.‚Äù and then add a minimal reproducible example.\nThe more you show you‚Äôve done your homework and are truly stuck (instead of just being lazy and wanting others to do the work for you), and the easier you make it for others to understand what your problem is, the more likely you will get good answers.\nIf you want to learn more about how to ask a good question online, check out this video by Roger Peng.\n\n\n\n\nAsking AI for help can often faster and more efficient than trying to find the answer via Google/StackOverflow. So it‚Äôs a good first stop, and if the AI is not helpful, move on to other sources.\n\n\nSome further comments\nAs you continue on your coding journey, keep in mind: The great thing about programming is that you (usually) can‚Äôt really ‚Äúbreak‚Äù things too much. In the worst case you get an error message. So experiment and try out anything you like!\nR has a bunch of quirks. You‚Äôll likely encounter a number of them. A common one is that there are two ways of assigning something to an object. One can write x &lt;- 2 or x = 2 and often (but not always) you can use either. People argue about which way to do it. You‚Äôll see both versions used frequently. If you are completely new to programming, I recommend the first version, i.e.¬†x &lt;- 2. The problem is that most other programming languages do it the second way, so if you learned to code in another language first (like I did), it‚Äôs more natural to write x = 2. It‚Äôs your choice. Just be aware that both notations exist. When I write code, I usually do a mix (without any logic to it). That‚Äôs of course a bad idea, so don‚Äôt do what I do, instead try to pick one way or the other and stick with it.\nMore confusion: In R, you can also write this in the opposite direction, i.e.¬†2 -&gt; x. This is unususal coding style and I recommend avoiding it. Just know that it exists too.\n\n\n\nThe journey of learning to code. Source: https://www.thinkful.com/blog/. Original post is not online anymore.\nFortunately rare for R. Source: xkcd.com.",
    "crumbs": [
      "Content",
      "3. R Coding Basics",
      "R Coding Basics"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html",
    "href": "content/module-data-analysis/assessment-data-analysis.html",
    "title": "Assessment - Data Analysis",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#more-information",
    "href": "content/module-data-analysis/assessment-data-analysis.html#more-information",
    "title": "Assessment - Data Analysis",
    "section": "More information",
    "text": "More information\nSome more information on forks and pulls and what to do if things don‚Äôt go right can be found in happygitwithR. Note that a lot of the commands described for use on the command line (e.g.¬†stash) can be applied graphically through GitKraken.\n\nGithub also has branches. Those are similar to forks but meant for more internal use. For instance, if you have a project and want to implement something new, but it might not work, you can create a branch, work in that branch, and once everything is ok, you can merge the branch into the main/master. This is useful if you write software that others are using, and you don‚Äôt want to break the whole thing. It is also helpful if you work with collaborators on a project. To be able to use branches, you need to be an owner or member of a repository. In contrast, you can fork any public repository.",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#another-fork-and-pull-exercise",
    "href": "content/module-data-analysis/assessment-data-analysis.html#another-fork-and-pull-exercise",
    "title": "Assessment - Data Analysis",
    "section": "Another fork and pull exercise",
    "text": "Another fork and pull exercise\nThis is optional. You can do it at any time during this course (and more than once) üòÅ.\nHelp improve the course with your contributions! Find something wrong/unclear/worth improving with this course (e.g.¬†a typo, something confusing, a broken link, a suggestion for a new reference, or anything else). Go to the Github repository for this course. Follow the steps outlined above: Fork the course to your personal account, clone it to your local computer, implement your updates, push it back to GitHub, then initiate a pull request. I will get a notification of your pull request. If things look ok and no conflicts exist, I will merge your improvements into the course. And just like that, you have contributed to improving this course! (And of course, you will be listed in the Acknowledgments section of the main course page.)\nAnother option for helping to improve the course website is to file a GitHub Issue. Feel free to do so any time during the course to let me know of anything that needs fixing.",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-overview.html",
    "href": "content/module-data-analysis/data-analysis-overview.html",
    "title": "Data Analysis Overview",
    "section": "",
    "text": "Overview\nIn this unit, we will talk about the whole data analysis workflow from a general, big-picture perspective. We‚Äôll then revisit the different components in more detail later.\n\n\nLearning Objectives\n\nUnderstand the elements of a data analysis.\nKnow the role and importance of each component of a data analysis.\n\n\n\nData Analysis Workflow\nThe following figure provides a conceptual illustration of the whole data analysis workflow.\n\n\n\nData analysis workflow\n\n\nThe spiral is meant to represent that while the different steps in an analysis are rarely linear, things should improve over time until you have the data and code in a form that everything fits and you can produce final results.\nSome things that might send you around in circles (or hopefully spirals) are:\n\nExploring data indicates that more cleaning needs to happen.\nExploring data indicates that the current question can‚Äôt be answered.\nAn analysis fails and indicates that cleaning/pre-processing wasn‚Äôt adequate.\nA change in analysis method requires different kinds of cleaning and pre-processing.\nEvaluating results indicate that something ‚Äòisn‚Äôt quite right‚Äô and needs to be fixed.\nNew analysis based on a better understanding of the data and how to best analyze the data and present results.\n\nIn the introductory chapter of their great book R for Data Science (R4DS), Garrett Grolemund and Hadley Wickham have a similar diagram. Check out the figure and read through the introductory chapter.\nNote that the process of getting the data ready for analysis goes by different names often used slightly differently by different people. In my figure above, I separated cleaning and pre-processing, while in the R4DS chapter and figure, they distinguish between tidying and transforming and together call them wrangling (some people also say munging). Sometimes, these terms mean a specific part of the cleaning process, sometimes they are used interchangeably. You need to guess from the context what is meant, but often you can think of these terms as all having similar meanings.\nLet‚Äôs look at the different components of an analysis in a bit more detail. The following sections are meant as a conceptual overview. Apart from the Question and Data Match discussion, we will get into each of the other steps and how to do them in more detail later in the course.\n\n\nQuestion & Data Match\nMaybe the most important challenge of a good scientific project (and thus an interesting and useful data analysis) is to find a combination of questions and data that lead to a project that is both interesting and doable. One could argue that having a good question and suitable data to answer it is part of science/research but not strictly part of the analysis. However, for your data analysis to be meaningful, it is critical to get the question-data match right.\nIf you start with a boring (or dumb) question or with data that is essentially garbage/noise, or you have a bad question/data match, no part of your analysis matters!\nThere are different approaches to finding a question-data match. The classical one is to have a specific question or hypothesis, then design a study to collect data, and analyze the collected data to address that specific question.\nIf you have a question, but you do not have the resources to collect data specifically to answer your question, you can look for existing data that might help you answer your question. You rarely find data that allows you to answer your initial question exactly. But you might find data that enables you to answer a very similar question. You might need to iterate a few times by modifying your question and changing the datasets you use before you have a good match, i.e., a good question and data that allows you to answer it.\nAnother approach that is becoming more common is to start with the data. You might come across one or multiple datasets that interest you. You can then investigate the data to see what questions you might be able to answer with it. Since the data was not collected to answer your specific question, you will have to be somewhat flexible with the question and let the data guide your analysis. This approach can be very powerful since more and more data are becoming available for analysis.\nAny of these different approaches for coming up with a good data-question match are valid. You should always keep in mind that unless you start with the question and analysis plan, then collect the data, then follow your plan exactly, the usual statistics (e.g., p-values and such) do not really apply.\nBy fiddling with the data or adjusting data and question, you are using what is sometimes called investigator‚Äôs degrees of freedom or less kindly, data dredging or data torturing. Essentially, if you fiddle with your data and question long enough, you will eventually find something statistically significant. You cannot interpret this in the same way as a statistically significant result obtained by pre-specifying the question, collecting data specifically to answer the question, and analyzing the data to answer the initial question.\n538‚Äôs website provides a nice example called Hack Your Way To Scientific Glory that illustrates how you can adjust your question and data in very reasonable ways so you can get whatever statistically significant result you want. This is researchers‚Äô degrees of freedom in action, and the main reason why p-values are most often meaningless.\nThat doesn‚Äôt mean you can‚Äôt do and publish interesting work based on such a secondary data analysis approach! But it does mean that whatever you find should be considered preliminary: hypothesis generating, not hypothesis testing.\nAgain, unless you fully pre-specify your hypothesis and your exact analysis plan (which is almost never done outside of clinical trials that seek regulatory approval), statistical measures such as p-values are largely meaningless. Unfortunately, to get published, you often need to include those. It‚Äôs a sad state of affairs that a lot of published work comes with junk statistics. It will take a good bit of time (if it ever happens), before things become more reasonable (and we will get rid of p-values in most situations).\nFor some more thoughts on this topic, read chapters 1-3 of The Art of Data Science (ADS). It is a pay what you want book with a minimum price of zero, which means you can get it for free (note that the book + video bundle has a non-zero minimum price). Chapter 2 has a figure similar to the one above.\n\nWhile I am generally in favor of supporting creators of books (or other content) and thus paying for their products, in this case since I‚Äôm making you get/read the book, and also since students have very limited budgets, I think it is entirely acceptable for you to get the book for free.\n\n\n\nGetting and loading data\nIn some sense, the best, but also the most expensive and time-consuming way of getting data, is to collect it yourself. Doing so ensures that the data can answer the question you have. You can also control the quality of the data that is being collected ‚Äì at least to some extent. However, it is often not feasible to collect your own data. You might not have funding or time or interest to do so. The next best source of data is directly from someone who collected it. The advantage of such data is that you can ask someone for clarification. Also, the person who collected the data is likely a subject matter expert, who can help you generate better questions. Finally, you can get data that is generally available, e.g., in some online resource. This gives you access to a lot of different datasets. The drawback is that the data was not collected to answer your specific question, and there is usually nobody you can ask for clarification.\nNo matter the source, you need to get the data into your favorite data analysis system (in our case, that will be R). Sometimes, you get data in a format that can be read in easily, e.g., a comma-separated CSV file without any strange formatting that messes up the import. At other times, you might get data in a collection of terribly formatted Excel spreadsheets, or you get it by scraping data from some online source. We‚Äôll look at ways of getting data into R in a later module.\n\n\nExploring and cleaning data\nOnce you have the data loaded into your analysis software, such as R, you want to explore the data to get an overall idea of what is going on. At the same time, you‚Äôll likely have to engage in data cleaning.\nData exploration can and should be done both graphically and by looking at text output of various summaries describing your data, e.g., summary tables for all variables. We will get into that in more detail later. As you explore, you will likely find that your data is dirty, i.e., there are things wrong with it. The amount of data cleaning (also called tidying or wrangling) you need to do depends very much on your data.\nA general rule is that you should never directly edit your raw data. If you can, load the raw data into R and do all cleaning inside R with code, so everything is automatically reproducible and documented. Sometimes, you need to edit the files in the format you got before you are even able to load them into R. In those cases, you might have to make modifications in a software other than R. If you can‚Äôt directly read the data into R and need to make some changes before, make copies of your raw data and ONLY EDIT THOSE COPIES. Also, write down the edits you made.\nThere are certain cleaning actions you will need to do with almost any dataset. You will have to find and deal with data entry errors. If the mistake is obvious, you can fix it. If you know the value is wrong but don‚Äôt know what it should be, you might need to discard that data. Sometimes you get data that seems way outside the range of all other values, but it is not obviously wrong. For instance, if you had a group of individuals and their weights, a person with a weight of 8000 pounds is obviously not possible. Someone with an 800 pound weight seems wrong, but it‚Äôs not impossible. You will have to decide how to deal with such outliers, either during the cleaning or the analysis stage.\nIn almost any dataset, some values will be missing. First, you need to figure out how missing values are coded and what missing means. Sometimes an empty entry means it was not recorded. At other times, it could be that it is not applicable. For instance on a survey that asks if someone ever smokes, and the next question is for how long, if they say they never smoked, the ‚Äúhow long‚Äù question is moot and might be empty, so it is not really a missing value even though nothing is written down. For your analysis, you might want to code it as a duration of 0. We‚Äôll talk more about missing values later in the course.\nAs you do all the cleaning, you need to get your data into a shape that is ready for analysis. If you apply more than one analytic method, you might need different clean versions of your dataset.\n\n\nPause and ponder\nAt pretty much every stage of your analysis workflow, you should include short pauses where you sit back and briefly contemplate what it all means and what is going on. Careful thinking and reflection is the most important (and unfortunately most under-used) component of a data analysis.\nSomewhere during the exploring and cleaning process, you might want to take a longer break. You should have come to a stage where you will be fairly sure if your project is feasible or not, i.e., if you can use the data to answer your question. If not, alter or abandon. Ideally, you will want to alter. That means either changing your question to one that is similar (or sometimes quite different) to your original question, and that can be answered with the data. Or, if feasible, it is to change the data, i.e., get a different dataset or another dataset that contains information which, when combined with your first dataset, lets you answer the question.\nSometimes, you might find that changing the question or data is not possible or will lead to a project that is of no interest to you. In that case, you should abandon your analysis. This is hard, but the honest thing to do.\nDon‚Äôt keep going hoping against all hope that ‚Äúmagic might happen‚Äù and you‚Äôll get a good final product after all. If you keep going, you will get some results. Often, you might even be able to publish them. Unfortunately, they will be useless and wrong - which is a common occurrence in the academic literature - and probably even worse outside of the peer-reviewed academic literature.\n\n\nPre-process Data\nAdditional data processing could be considered part of the cleaning process, or you can think of it as a separate step. Depending on the analytic method you use, some further data processing might be suitable. For instance, some methods work better if you center and scale your predictors. Some methods, such as linear models and related approaches, are not able to handle missing values, so you need to decide how to deal with those. Sometimes it might be good to not include all variables in your model, or to use the existing variables to create new ones (e.g., instead of including height and weight in your model, you might decide to compute BMI and use that variable in your model instead.) The specific pre-processing steps that need to happen depend on the statistical modeling approach(es) you plan on using.\n\n\nAnalyzing Data\nYou finally reached the step where you can apply actual statistical analysis to your data. This topic takes up the majority of time in most (bio)statistics courses. While it is obviously very important to get the statistical analysis right, for any real data analysis you will likely spend &lt;25% of your time on this task. The majority of your time (though not computing time) goes into the other tasks.\nAt this stage in the process, you should know your data well enough to apply statistical methods and approaches that are suitable for your data and question. You can either use a single approach or explore multiple different ones. As you will learn in this course, trying more than one approach is quite easy. If you try multiple models, you need to be careful to not overfit by only picking and reporting the models and analyses that give you the results you want. If you analyze data in multiple ways, report all results.\n\n\nEvaluating and interpreting results\nOnce you have your results, and before you decide on how (or if) to report them, you should take another break and carefully contemplate what your findings mean. Do the results make sense? Occasionally, there is some new and interesting finding that goes against everything we think we know. However, most of the time if you find something that doesn‚Äôt ring true (e.g., increased exercise shortens your lifespan), it probably isn‚Äôt. You need to think carefully about what your result actually means and how to interpret results. We can certainly think of a way to get a result that suggests increased exercise shortens lifespan. For instance, we could look at a group of people who practice some kind of extreme sport that not infrequently leads to death, and compare that group to people who do not practice this sport. You might find that those with increased exercise live less long in that particular scenario. But of course, that doesn‚Äôt mean you have overall shown that normal exercise shortens lifespan. For a real epidemiology example, you can read about the birth weight paradox, where researchers found that smoking during pregnancy lead to lower infant mortality ‚Äì of course this was false and was later explained by sampling bias.\nUnfortunately, the scientific literature is full of examples where the interpretation of the results are likely not right. A good and amusing example was published in the New England Journal of Medicine, a very prestigious journal, by Messerli 2012. The showed that chocolate consumption and Nobel prizes were correlated. While there is no arguing with the data, the interpretation that ‚Äú‚Ä¶it seems most likely that in a dose-dependent way, chocolate intake provides the abundant fertile ground needed for the sprouting of Nobel laureates‚Ä¶‚Äù is of course nonsense, see also this article for a more general discussion. You can find more rebuttals online. Unfortunately, since the original study was in a prestigious journal, the nonsense was widely reported in the media. You can see Forbes, BusinessInsider, Time Magazine, and some other news outlets reporting the finding fairly uncritically. To this day, I do not know if this article was meant as a satire or for real.\n\n\nReporting and communicating your results\nUnless you do an analysis just for yourself, you will want to communicate your findings to others. In academia, this is usually done with peer-reviewed publications, as well as presentations at conferences in the form of talks (for which most people use slides) and posters. In those settings, the readers or listeners are usually your peers.\nOutside of academia, you might also need to present your work in written or oral form, but it might have a different target audience. For instance, you might want to communicate your findings to a lay audience, to your superiors, to groups of stakeholders, to colleagues and team members, etc.\nWhile you will need to tailor your products and way of communication to your situation and audience, there are components that are common. Figures, tables, and descriptive text are almost always used.\nYou want to make as much of your reporting automated as possible. For that, all figures, tables, and numeric results should be auto-generated by code. Similarly, for an efficient and automated workflow, you will want as many aspects of the products (reports, slides, etc.) as possible to be automated. Of course, you have to write the text yourself. But results should be loaded automatically, and text layout should be automated, as should formatting of references. You also want to use a system that is as flexible as possible. Quarto is ideal because you are writing in plain text and can easily send the text into different output formats, e.g., reports, slides, posters, websites, blogs, etc. This is much harder to do with other common software. E.g., going from Word to Excel to Powerpoint is much less seamless and certainly not automated.\nYou want automation for both efficiency and reproducibility purposes. Especially if you design an analysis that might be repeated often (maybe not so common in academia, but very common in other settings), you want to be able to automatically re-run and regenerate as much of the whole process as possible.\n\n\nFurther reading\nAbove, I gave you my thoughts related to the various steps of the data analysis workflow. It‚Äôs good to hear it from multiple sources. To that end, go back to The Art of Data Science (ADS) and read through chapters 4 and 9‚Äì11.\n\n\nSome practice\nThere is code shown in chapter 4 of ADS. If you want, you can try to work along and reproduce what they are showing. This is optional. If you want to try and work along, note that the link provided in the book to download the data does not work anymore (not easily reproducible!). I did find the data, which you can download as a zip folder. Download and unzip and you should be good to work along.\nIf you code along, I suggest you start a new R script and place their code into that script (instead of copying it into the console). We‚Äôll talk about that more soon. You will need to slightly adjust the code shown in the book to make it work. E.g., if you don‚Äôt place the data into a data folder, you will need to change the path in the read_csv command. Also, for that command and some others, the book has a formatting with + signs across multiple lines of code, that might not work if you just copy and paste their code, and you might need to remove them and move everything into a single line of code. You can write everything into a single line and delete all + signs.\nYou will also notice that the data is different than what they report. Their book was published on 2018-06-22, but on the website where the data comes from, a time stamp of 2019-05-07 is listed. Thus the data got updated after the book was published, again making things not quite reproducible. (Changes to data are common, which is a reason why an automated and reproducible workflow is so important!)\nFinally, there are minor errors/typos in the book, e.g., what the code does and what the text says are not the same when looking at time (page 41).\nOverall, while you should be able to reproduce most of what they show, it will be slightly different since the data has changed, and you will need to make adjustments to the code. If you have no R coding experience, trying to get it to work might be too tedious, so skip for now and consider revisiting later once you learned some R coding.\n\n\n\nData analysis workflow",
    "crumbs": [
      "Content",
      "2A. Data Analysis Introduction",
      "Data Analysis Overview"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html",
    "href": "content/module-data-presentation/assessment-presenting-results.html",
    "title": "Assessment - Presenting Results",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#find-a-graph-to-reproduce",
    "href": "content/module-data-presentation/assessment-presenting-results.html#find-a-graph-to-reproduce",
    "title": "Assessment - Presenting Results",
    "section": "Find a graph to reproduce",
    "text": "Find a graph to reproduce\nFind an interesting graph from a news website. The main requirements are that the original graph must be interesting and good looking (a basic scatter plot is not enough), AND that the graph is freely available online, not behind a pay-wall. Ideally, the data that went into making the graph should be available too. If it is not, you will have to go through the extra step of extracting the data from the graph using tools like DataThief or WebPlotDigitizer.\nOne suggestion is to look at graphs from FiveThirtyEight. They are known to produce high-quality graphs, and for some of their stories and figures, they also provide the original data source. Good starting points for a chart from FiveThirtyEight might be their annual summaries of weirdest charts, which you can find here for 2015, 2016, 2018, 2019, 2020, 2021, 2022. (No, I don‚Äôt know why there‚Äôs no 2017.) Or you can go straight to their data, and see what graphs they produced with it and find a good one.\nIf you can‚Äôt find a graph on FiveThirtyEight that you like and would like to reproduce, you can also check out other major news outlets (USA Today, NY Times, Wall Street Journal, The Guardian, etc.).",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#setup",
    "href": "content/module-data-presentation/assessment-presenting-results.html#setup",
    "title": "Assessment - Presenting Results",
    "section": "Setup",
    "text": "Setup\nThis exercise will be part of your portfolio and will live in the presentation-exercise folder. Place your data (see next) in that folder, and add your code and explanation to the Quarto starter file in that folder.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#get-the-data",
    "href": "content/module-data-presentation/assessment-presenting-results.html#get-the-data",
    "title": "Assessment - Presenting Results",
    "section": "Get the data",
    "text": "Get the data\nAs mentioned, you either need to have the data in a file, or you need to extract it from the graph and place it into a file. I recommend a CSV file. Place it into the presentation-exercise folder.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#ask-ai-to-re-create-the-original-graph",
    "href": "content/module-data-presentation/assessment-presenting-results.html#ask-ai-to-re-create-the-original-graph",
    "title": "Assessment - Presenting Results",
    "section": "Ask AI to re-create the original graph",
    "text": "Ask AI to re-create the original graph\nAsk an AI of your choice (or try several) to re-create the original graph. I‚Äôm not sure what the best prompt is. You probably want to feed it the data and instructions on what kind of plot to make. I‚Äôm not sure if giving it the original plot, either as uploaded file or as a URL, will help. You can try. You will likely have to go through multiple iterations of asking the AI to update the code/figure.\nYou will likely want to use ggplot2 & friends, but if you want to use base R, lattice or another plotting approach that‚Äôs ok too, as long as everything happens with R code.\n\nIf you already feel comfortable making high-quality graphs with ggplot2 and want to try something different, feel free to create an interactive graph using one of the many options for that (e.g., Shiny, plotly). You can also recreate a previous static graph and make it interactive (e.g., allowing people to turn on/off specific parts).\n\nDocument (some of) your AI prompts in the visualization-exercise.qmd Quarto file. Also show the code that the AI produced. Depending on how many iterations you do, you don‚Äôt need to show prompts and code for all intermediate steps. The AI will likely not get you all the way to where you want to get. Manually edit your code to try and get as close as possible to the original graph.\nYou should end up with a Quarto file that shows your AI prompts, possibly intermediate code that the AI returned, and finally a chunk of code that loads/takes the data and re-produces the original plot as closely as possible. Also show the original plot in youe Quarto file next/below your reproduction for comparison.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#create-a-publication-quality-table",
    "href": "content/module-data-presentation/assessment-presenting-results.html#create-a-publication-quality-table",
    "title": "Assessment - Presenting Results",
    "section": "Create a publication quality table",
    "text": "Create a publication quality table\nFor this part, the easiest approach is to use the same data you used for the graph (or a subset of the data) and create a nice looking table.\nAlternatively, find a good-looking table somewhere online and try to recreate it. Of course, you need all data necessary to re-create the table. Often (but not always) that‚Äôs just the numbers in the table.\nAgain, start with AI. You can let it decide which R table package to use, or you can specify that as part of your prompt. Use whichever R table package you like. This tutorial shows how to make nice tables with the gt package. Our group uses flextable a lot.\nNo matter what R package you use to make the table, it should look professional, visually pleasing, and nicely convey the information. For inspiration, you can for instance check out this R table gallery. The R graph gallery also has a section on tables.\n\nAs for the graphs, if you want to challenge yourself, you can try to make your table interactive. There are several good R packages that allow you to create interactive tables inside a Quarto document. Interactivity only works if your output format is HTML, it won‚Äôt work for pdf or word. But in your case it‚Äôs part of your website, so it‚Äôs HTML and can include interactivity.\n\nAdd the code and explanatory text (e.g., your AI prompts) into the same presentation-exercise.qmd Quarto document. You should end up with code that takes some data and generates a nice-looking table.\nOnce all done, re-build your website and make sure that your data visualization exercise shows up nicely. Then, post the link to your online page showing your visualization into the discussion channel for this module. Do that by Wednesday so your classmates can take a look and comment by Friday.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r.html",
    "href": "content/module-data-presentation/tables-in-r.html",
    "title": "Tables in R",
    "section": "",
    "text": "Unit to come.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Tables in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r.html#simple-markdown-tables",
    "href": "content/module-data-presentation/tables-in-r.html#simple-markdown-tables",
    "title": "Tables in R",
    "section": "Simple markdown tables",
    "text": "Simple markdown tables\nVisualizations are somewhat easy for Quarto to use, because they are always some type of image file (PNG, JPEG, etc.). However, tables are not so easy ‚Äì because they consist of just text and grid lines (usually), they need to be converted into a specific format for Quarto to use, and the best format to use depends on what output you need Quarto to produce. Remember that Quarto runs on Markdown, so the simplest option is to use a Markdown table.\nAlthough you can make those by hand, you shouldn‚Äôt! There are a few functions that can make simple Markdown tables for you, including knitr::kable(), and pander::pandoc.table(). Both of these are decently customizable and work well for a lot of cases. Because they generate Markdown text from code, they should work with Quarto regardless of the output format you use.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Tables in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r.html#tables-with-complex-formats",
    "href": "content/module-data-presentation/tables-in-r.html#tables-with-complex-formats",
    "title": "Tables in R",
    "section": "Tables with complex formats",
    "text": "Tables with complex formats\nHTML is the most flexible format for generating tables. Any table you can see on the internet is made with HTML (and sometimes CSS and JavaScript). R has many packages for generating HTML tables, including the easy-to-use kableExtra. The kableExtra package is basically an extension of kable to have a lot of Extra features for making nicer tables. HTML has the special power to generate interactive tables using packages like [DT] and [Reactable].\nIf you are using a PDF format (common in math, physics, theoretical statistics and other math-heavy fields), you can write LaTeX code directly in Quarto, which means you can write tables using any of the LaTeX packages you want. The kableExtra package is also compatible with LaTeX.\nWord is the most complicated format, and in Epidemiology the most common. Many table packages do not work with Word, or have limited functionality. In my experience, the best package for word output is flextable, which was designed by the creator of the officer package specifically to work with Word outputs. Any table making package that outputs raw Markdown should be compatible with Word output. Quarto specifically will attempt to convert any HTML tables into the correct output format. What this means in real life is that many HTML tables will work in Word output, but if they are too complicated they may not look how you expect, or may not work. As an alternative, I also know several researchers who like huxtable, although I personally prefer flextable.\nThe package tables deserves an honorable mention as (one of) the oldest table-making package on CRAN. If you want to generate an HTML, LaTeX, or plain text table, or use a table for further calculations in R, tables can probably do it. However, this package is very old and in my opinion is quite difficult to learn and quite clunky to use ‚Äì the interface is (unsurprinsingly) quite old-fashioned and different from tidyverse-style code. A relatively new addition to the tablemaking scene is tinytable. The tinytable package provides a modern interface for tables and notably has very few dependencies, which can be helpful for reproducibility.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Tables in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r.html#modern-table-packages",
    "href": "content/module-data-presentation/tables-in-r.html#modern-table-packages",
    "title": "Tables in R",
    "section": "‚ÄúModern‚Äù table packages",
    "text": "‚ÄúModern‚Äù table packages\nFor better or worse, the table package ecosystem in R has largely coalesced around three major families of packages. Each of these packages has pros and cons, and can generally make whatever table you need to make, and they are all very good packages.\nThe first of these sets of packages is the previously-mentioned knitr::kable() and kableExtra. These packages are relatively easy to use, but are not quite as flexible as the other two options. They will suffice for most tables if you like using them.\nThe second of these is the also-previously-mentioned flextable. The flextable package is notable for its consistency across many formats ‚Äì for several years, flextable was the only table package that could guarantee your table would look the same in HTML, PDF, and Word. While Flextable certainly has a learning curve, I think the lengthy manual with several examples written by the author is fairly approachable, and flextable is the main package I have used in my research up to now.\nThe final family is the gt family of packages. The gt family, which stands for ‚Äúgrammar of tables‚Äù is developed by Posit, and intended to be the table version of ggplot2. This family revolves around the gt package. The syntax for making tables in this package is designed to be similar to tidyverse-style syntax, and is therefore probably the easiest to learn as part of this course. There are also many, many examples and tutorials showing how to use gt to make gorgeous tables. In the old days, gt was not a viable option for us because it didn‚Äôt work with Word, but that has been remedied and now gt is the official table package supported by Posit, which means there are a lot of resources showing how to use gt. The gt package can do almost everything, but if you need it, the gtExtras package provides even more options. There is currently no ‚Äúgt extension gallery‚Äù like there is for ggplot2, but in time I think there will be.\nIn short, while all of these packages are great, I highly recommend that new users start practicing with the gt package, since this package will have the most learning resources and the most consistent development support for the foreseeable future.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Tables in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/visualization-introduction.html",
    "href": "content/module-data-presentation/visualization-introduction.html",
    "title": "Visualization Overview",
    "section": "",
    "text": "Overview\nIn this unit, we will discuss visualization (a.k.a. making figures), what they are good for, and how to do them in R.\n\n\n\nFlowcharts are sometimes, but not always good visualization tools. Source: xkcd.com.\n\n\n\n\nObjectives\n\nUnderstand when and how to use graphs.\nBecome familiar with major types of graphs and when to use them.\nLearn how to implement high-quality graphs in R.\n\n\n\nMotivating Examples\nThe topic of visualization is enormous, and impossible to cover thoroughly in a week (like pretty much any topic we talk about in this course üòÉ). To get you started, the two following TED talks provide some examples of interesting and inspiring data visualizations.\nIn the first video Mona Chalabi talks about good ways to present data and statistics and how good graphs can help in understanding.\n\n\n\n\nIn the second video Chris Jordan discusses how one can take abstract facts and make them much more tangible through visualization.\n\n\n\n\nIf you are interested, you can find more interesting talks on visualization in this TED topics site and this TED Studies collection.\n\n\nVisualization uses\nFigures and graphs are useful at any stage of your analysis. Use them early to explore your data, to get an idea for what you have, to figure out what cleaning you might need to do, and to check what analysis approaches might be useful. Use visualizations once you have done your analysis to check your results and interpret them. Use them at the end to communicate your results. While tables can be useful tools of communication as well, they tend to be much harder to understand and are ‚Äì in my opinion ‚Äì overused in science. Good figures are, most of the time, easier to understand and a more effective way to look at data.\n\n\nTypes of graphs\nI am sure you are familiar with several common types of graphs, such as scatter plots, boxplots, and bar charts. Many more chart types exist, some are much better at conveying potentially complex information than the simple standard ones. An excellent resources is the From Data to Viz website. It lists a lot of different graphs based on the kind of data you want to plot, and for each graph it gives a brief description and often a link to the R Graph Gallery which shows examples with code. Sometimes you start with data and are looking for the best way to plot it. Other times you see a great graph and want to know what it is called. These 2 websites are great resources for either situation.\nAnother great source that provides an overview of different graphs is the Directory of Visualizations chapter of the excellent Fundamentals of Data Visualization book by Claus Wilke. Further chapters in his book describe specific types of plots in more detail, and also provide general best practices for making plots. If you are interested in specific types of plots and general concepts of visualization, skim through his book.\n\n\nFurther visualization resources\nSee the General Resources page for some additional sources.\n\n\n\nFlowcharts are sometimes, but not always good visualization tools. Source: xkcd.com.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Visualization Overview"
    ]
  },
  {
    "objectID": "content/module-data-processing/censored-data/censored-data.html",
    "href": "content/module-data-processing/censored-data/censored-data.html",
    "title": "Title",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss\n\n\nLearning Objectives\n\nBe familiar with\n\n\n\nIntroduction\nSometimes data are censored, i.e., you know that the value is ‚ÄúX or greater/less‚Äù but not the exact value. This is common in longitudinal (survival or similar) data. It is also common in experimental data, where you might have a measurement procedure with lower or upper limits of detection. The way to deal with censored data depends on what kind of censored data you have, and what you want to do with it. In R, packages that are used to analyse survival data, e.g.¬†the survival package, can deal with some censored data. Otherwise, you will likely need to google around to figure out how you should deal with the censored data scenario you have and how to do it in R.\n\n\nSummary\n\n\nFurther Resources"
  },
  {
    "objectID": "content/module-data-processing/data-processing-introduction/data-processing-introduction.html",
    "href": "content/module-data-processing/data-processing-introduction/data-processing-introduction.html",
    "title": "Processing Data",
    "section": "",
    "text": "Overview\nThis unit provides a brief introduction to the topic of data processing.\n\n\nLearning Objectives\n\nBe familiar with the topic of data processing\nUnderstand the importance of proper processing for data analysis\n\n\n\nIntroduction\nI want to remind you of this diagram:\n\n\n\nData analysis workflow\n\n\nDifferent data-analysis related tasks rarely happen in a linear sequence. Instead there is a general stage before statistical analysis where you mess with your data (that‚Äôs a technical term üòÅ) to get ready for the main formal analysis. And often, as you start that formal analysis, you‚Äôll have to go back and do some more cleaning/exploration.\nWhile one can distinguish the different tasks of Cleaning, Exploring and Processing, one can also more generally think of them as all being a part of data processing. Alternative terms often used instead of processing are data wrangling or tidying. Cleaning, munging.\nAs with any part of the data analysis process, processing should be automated, reproducible, and well-documented. No ‚Äúfixing by hand‚Äù!\n\n\nData processing tasks\nAlmost all datasets require some processing before they are in a format that can be used with statistical models. For any data analysis, the majority of time is likely spent in the data processing and exploration steps before the main model fitting. Some folks suggest that this processing part is as much as 80% of the time spent on the whole project. In my experience, this is about right.\nWhile every dataset is different, there are certain tasks that are common. Those involve missing data, recoding variables, and dealing with outliers. Further ones are merging and reshaping of the data. We will discuss some of them in further units.\n\n\nData processing and exploratory data analysis (EDA)\nAs you process your data, you need to explore your data to figure out what needs processing. As such, exploratory data analysis (EDA) and data processing generally go together.\nEDA generally relies on making figures and tables, and we‚Äôll talk about that in a separate module. There is no clear definition for exploratory analysis. Beyond figures and tables, it can sometimes involve simple statistical computations, for instance you might compute correlations among predictors.\nYou can think of EDA as any process that happens before you start building and fitting your main models. Again, it often happens concurrently with the processing of the data.\n\n\nSummary\nData processing is a major and crucial part of any data analysis project. It can be tedious, but with practice and good tools, you might even reach a point where you enjoy the process somewhat üòÅ.\n\n\nFurther resources\n\nChapters 4 and 5 of The Art of Data science (ADS) discuss some processing and exploration tasks and examples. if you want to work along, see the Some practice section of the Data Analysis Overview page for some details on how to do it.\nThe whole R4DS book focuses on the early stages of data analysis, including processing and exploration. Definitely keep checking this book if you are looking for more information on a specific topic.\n\n\n\n\nData analysis workflow",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Data Processing Introduction"
    ]
  },
  {
    "objectID": "content/module-data-processing/feature-engineering/feature-engineering.html",
    "href": "content/module-data-processing/feature-engineering/feature-engineering.html",
    "title": "Feature engineering",
    "section": "",
    "text": "For this unit, we will discuss the idea of processing existing variables to potentially improve model performance. This is often called Feature Engineering.1",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "content/module-data-processing/feature-engineering/feature-engineering.html#removing-variables",
    "href": "content/module-data-processing/feature-engineering/feature-engineering.html#removing-variables",
    "title": "Feature engineering",
    "section": "Removing variables",
    "text": "Removing variables\nIt is often the case that we have more data than we need to answer a scientific question. For almost any analysis, one therefore needs to remove some variables before starting the statistical model fitting process. A simple example is an identifier (ID) variable or the names of subjects. Often this information is not needed for the modeling process and can thus be removed before fitting. Other examples are instances where the data was collected for some purpose other than the planned analysis. In this case, it is likely that there are variables in the data which are irrelevant for our analysis.\nSuch removal of variables is done on scientific grounds, based on expert opinion. Ideally, you should report in enough detail which parts of the data you included and excluded in that way to allow the reader to make an informed decision if they agree with what you did. And of course, you should ‚Äì as much as possible ‚Äì also provide the raw data and the R scripts which do and document removal of specific variables ‚Äì such that someone who doesn‚Äôt agree with your choice could re-run the analysis with different inclusion/exclusion criteria.\nSometimes, you might have variables that could, in principle, be useful, but the reported values show little diversity and thus contain little information. For instance, if you had a sample of several hundred individuals and only 3 of them were smokers, then it might not be useful to include the smoking variable for the analysis of this dataset, even if, in general, it might be worth considering. Such variables that do not contain much information are called ‚Äúnear-zero variance‚Äù variables. Some models perform better if those variables are removed. Other modeling approaches do not care since they have built-in mechanisms to remove variables that are not useful in predicting the outcome.\nAnother instance where removing variables might be useful is if predictors are strongly collinear/correlated. A trivial example is if you have height reported as both inches and centimeters in your data. Obviously, one of them should be removed. Other variables might not be so obviously containing the same information, but might be related enough (collinear) that including both makes the model performance worse. An example might be age and height among children.\nAnother example we already discussed are variables with missing values. In this case, you might want to remove the variable, or the observations, or a mix of them.\nAnother important consideration are variables that could bias your outcome. This is a very broad and complex topic. The best current approach is to take a causal modeling analysis perspective, and based on your causal diagram decide which variables to include. It is a bad idea to just add all variables into your model, even after you removed obviously useless ones. Unfortunately, while a very important topic, we can‚Äôt cover causal modeling here. If you want to learn more, the best resource I‚Äôm aware of is Statistical Rethinking. An alternative are some papers by Judea Pearl. Some of his work is very technical, but he has several readable introductions.\n\nAll of these inclusion/exclusion decisions are based on expert (that‚Äôs you!) judgment. There is no magic recipe you can follow. Some statistical methods can help (see below), but they are only so useful. If someone on the internet tries to sell a magical way of knowing how to process your data before fitting, run away!",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "content/module-data-processing/feature-engineering/feature-engineering.html#footnotes",
    "href": "content/module-data-processing/feature-engineering/feature-engineering.html#footnotes",
    "title": "Feature engineering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFeature is another word used for predictor or independent or input variable. The term feature is especially common in the machine learning literature.‚Ü©Ô∏é\nOf course, sometimes exceptions apply. But you need to be able to fully justify why for your situation it is ok to use one of these generally bad approaches. And ‚Äúeveryone else is doing it‚Äù is not a good justification üòÅ.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "content/module-data-processing/missing-data/missing-data.html",
    "href": "content/module-data-processing/missing-data/missing-data.html",
    "title": "Missing Data",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss missing data and what to do about it.\n\n\nLearning Objectives\n\nBe familiar with the problem of missing data.\nBe able to apply approaches to handle missing data.\n\n\n\nIntroduction\nIn almost any dataset, there are some missing entries. Data can be missing for different reasons. It could have not been asked/recorded, the person refused to provide it, the variable is not applicable (e.g.¬†for a non-smoker, the variable ‚Äúnumber of cigarettes per day‚Äù might be left blank.) Understanding why data are missing is important, so we can form an action plan based on that.\nThere are different ways to deal with missing data, here are some common ones.\n\n\nRemove missing data\nMany analysis approaches do not allow for missing data. In this case, you need to reduce your dataset such that nothing is missing.\nThe easiest approach is to remove all observations with missing data (this is called ‚Äúlistwise deletion‚Äù). That can get problematic if you have a lot of variables, and each variable has some missing values; you might be left with almost no observations that have complete data. Another approach is to exclude them in analyses where they have missing data in variables of interest, but include then in analyses where they have information for all variables that are being considered (this is called ‚Äúpairwise deletion‚Äù). Both of these methods can lead to bias in different ways and should be used with careful consideration.\nAnother option is to remove all variables with missing data from further analysis. Unfortunately, it is common that at least some values are missing for each variable, which means you would be left with no variables. Also, if the variable is important for the question you are trying to answer, you obviously cannot remove it.\nAnother option is to use a combination of removing variables and observations. You could start removing variables with missing values above some threshold, e.g.¬†any variable that has more than 10% or 20% (or some value you pick) missing. There is no rule for this, and you need to justify it. Then once all variables with missing values above some threshold are removed, you remove any remaining observations that still contain missing data. This mix of removing variables and observations might preserve the most amount of data.\nThere are two problems when removing data. One is the obvious fact that you lose data, and thus statistical power. The other problem is if the data are not missing completely at random. In that case, by removing those observations with missing data, you introduce bias into your dataset. Again, it is important to understand why and how data are missing so you can have an understanding of potential problems such as introducing bias.\n\n\nImpute missing data\nInstead of removing rows and columns (observations and variables) until you have no more missing entry, you can also impute the missing values. Basically, you make an educated, data-driven guess as to what the missing value might have been and stick that value into the missing slot. In principle, any regression and classification method that you can use to estimate and predict an outcome can be used by temporarily thinking of the variable you want to impute as your outcome and the other variables as predictors, and then predicting the missing values. Methods such as k-nearest neighbors or random forest, which we discuss later in the course, are useful for this. Imputation adds uncertainty since you made guesses for the missing values, and often, the estimated/guessed values are randomly drawn from a distribution. Multiple imputation creates several different imputed datasets, and you can then run your analysis on each of those imputed datasets, hopefully with similar results for each. In R, the recipes package, which is part of the fairly new set of tidy modeling tools called tidymodels, and others such as the mice package allow for imputation. We‚Äôll try some of those in a later unit.\n\n\nFeature engineering\nAs the creating new data/variables unit discusses, it is possible to create new variables/data from the existing ones. This can sometimes help with missing values.\nFor instance, you could create new variables/predictors/features with less missing. As an example, if you have data that records if a person drinks beer (yes/no), wine (yes/no), or hard liquor (yes/no) and each of those variables has some missing, maybe you can create a new variable labeled any alcohol and code it as yes/no. If a person has a yes for at least one of the 3 original variables, they would be coded as yes in the new one. If they have all no, they would be coded as no. For anyone left, you do need to decide what to do with missing values in the original variables, i.e.¬†if you interpret them as yes or no. You could either stick with one, e.g.¬†if you had some additional knowledge that suggests anyone who doesn‚Äôt have that value recorded is more likely a no. For categorical data, you could also treat missing values as their own category (this can be useful when missingness provides information about the value of the response).\n\n\nKeep missing data\nWhile many standard statistical models, such as linear and generalized linear models don‚Äôt work with missing values, there are some statistical algorithms that can handle missing values. For instance tree-based methods (e.g., random forest, boosted regression trees) can take predictors that have missing values. If you know that the method you plan on using can handle missing values, you can decide to not get rid of them but keep them in the data. Though it‚Äôs still useful to think carefully about your missing data and possibly process it. You might get better results from your models, even from those that can handle missing values.\n\n\nAvoid missing data\nAs you can tell, missing values can cause headaches. It is therefore very important if you design and collect data to try to do it in such a way as to minimize missing values. Of course, if you analyze data collected by someone else, there is not much you can do, and you have to decide how to deal with missing values. For that, having a good understanding of what the data mean and how they were collected is essential.\n\n\nMissing data in R\nIn R, missing values are coded as NA. When you read data into R and that data for instance codes missing as 99, you should recode to NA. NA in R is a bit tricky, since any operation on NA returns NA. The tidyverse functions tend to be pretty good in dealing with NA, but for base R code you often have to be more careful. The function is.na() is often useful. Some functions, e.g.¬†mean() and sum() can deal with NA if you tell them what to do. Sometimes, functions just deal with NA in some built-in way. You need to check that this is what you want. Always perform careful checks when handling missing values! The tidyverse packages tidyr and dplyr have tools for dealing with missing values during the wrangling process.\n\n\nSummary\nIn pretty much any real dataset, you‚Äôll encounter missing data. You should understand as much as possible why these entries are missing. Based on that, and your analysis goals, you should formulate a good approach of handling the missing data.\nOften, it‚Äôs good to do things more than one way and show that your findings are robust. For instance if you either remove or impute some missing data and you get different results, that means the missing data is influential and you need to be very careful about the conclusions you draw. On the other hand, if you get more or less the same result, you can be somewhat reassured that the missing data might not matter too much.\n\n\nFurther Resources\n\nThere is a chapter on missing data in the book Feature Engineering and Selection: A Practical Approach for Predictive Models.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Dealing with missing data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html",
    "href": "content/module-data/assessment-data.html",
    "title": "Assessment - All about Data",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "4. All about Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#overview",
    "href": "content/module-data/assessment-data.html#overview",
    "title": "Assessment - All about Data",
    "section": "Overview",
    "text": "Overview\nYou have two main options for the exercise this week:\n\nOption 1: Try your hand at a bit of an exploration of some complex data type.\nOption 2: Generate and explore synthetic data.\n\nFor either option, I expect well-documented code and lots of explanatory text in the Quarto document.\nWhile this is an open-ended exercise and there are no specific requirements as to what you need to produce, I expect more than just a single figure or 10 lines of code. Since you are getting better each week, it should definitely be at least as comprehensive as last week‚Äôs exercise üòÅ.",
    "crumbs": [
      "Assessments",
      "4. All about Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#setup",
    "href": "content/module-data/assessment-data.html#setup",
    "title": "Assessment - All about Data",
    "section": "Setup",
    "text": "Setup\nThis exercise doesn‚Äôt have an existing folder in your portfolio. Make a new folder, call it data-exercise. Copy one of the existing Quarto files into that new folder (e.g., the one called tidytuesday-exercise.qmd), rename it to data-exercise.qmd and open it (make sure you open your portfolio by clicking on your R project file, then open the Quarto file inside R Studio).",
    "crumbs": [
      "Assessments",
      "4. All about Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#option-1",
    "href": "content/module-data/assessment-data.html#option-1",
    "title": "Assessment - All about Data",
    "section": "Option 1",
    "text": "Option 1\nFind some data that falls into the categories described in the Complex data types unit. It could be some text or images or video or audio or any other complex data. Basically anything that‚Äôs not rectangular data with rows as observations and columns as variables.\nThen look for some suitable R packages that allow you to explore the data. You don‚Äôt need to do any statistical analysis at this stage. Focus on loading the data, processing as needed, and then do some descriptive/exploratory analysis, e.g.¬†by making plots or tables or‚Ä¶ Basically anything that you can think of.",
    "crumbs": [
      "Assessments",
      "4. All about Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#option-2",
    "href": "content/module-data/assessment-data.html#option-2",
    "title": "Assessment - All about Data",
    "section": "Option 2",
    "text": "Option 2\nWrite code that generates a synthetic dataset. This dataset should have multiple variables, and there should be some associations between variables.\nThen write code that explores the data by making plots or tables to confirm that your synthetic data is what you expect it to be.\nThen fit a few simple models to the data. For instance, use the lm or glm functions to fit a linear or logistic model. Make sure your model can recover the associations you built into the data. Explore if and how different models might be able to capture the patterns you see.\nTake some inspiration from the examples shown in the Synthetic Data module.",
    "crumbs": [
      "Assessments",
      "4. All about Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#wrap-up",
    "href": "content/module-data/assessment-data.html#wrap-up",
    "title": "Assessment - All about Data",
    "section": "Wrap-up",
    "text": "Wrap-up\nOnce you got your exercise done, you need to add it to your website. To that end, open the _quarto.yml file. In the section that says menu and lists several of your projects, add lines at a -text: and file: entry that points to your new folder/file. Be careful to have exactly the same number of empty spaces at the beginning of the line as all the other lines.\nWhile you are in that file, if you haven‚Äôt done so, update other parts, e.g.¬†such as the title and the github URL. See the instructions for the first exercise if you need a reminder.\nOnce all done, render your website to make sure everything looks ok and your new exercise shows up. Once things looks fine on your local computer, push your updates to GitHub and check that they also look ok online.\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it.",
    "crumbs": [
      "Assessments",
      "4. All about Data"
    ]
  },
  {
    "objectID": "content/module-data/data-generation-process/data-generation-process.html",
    "href": "content/module-data/data-generation-process/data-generation-process.html",
    "title": "Thinking about data generation",
    "section": "",
    "text": "Overview\nWe are generally in the business of analyzing data, not generating it (but see the synthetic data content). Nevertheless, it is very important to think about the processes that generated the data you are trying to analyze.\n\n\nLearning Objectives\n\nUnderstand why it‚Äôs important to think of the data generation process\nBe familiar with common features and structures in data\n\n\n\nIntroduction\nEvery dataset is generated by processes that can often be very complex. At times, seemingly similar data is generated by different processes. For instance, most of the text you read in this course was written by a single person. In contrast, a lot of the text you read in Wikipedia was co-authored and co-edited by many individuals. Thus, while both are pieces of text, the processes that generated them are different. Often, this difference in processes is reflected in patterns in the data, and needs to be accounted for. For any kind of analysis you do, it is useful to think about the likely processes that generated the data. This can influence your analysis approach.\n\n\nRandomness\nMany natural processes that lead to the generation of data have some inherent randomness associated with them. For instance if you want to analyze the effect of a drug on some health outcome, you expect to see variation among individuals. Some of the variation is due to individual characteristics (e.g., age, weight), but some variation is likely random. While one could discuss if true randomness exists, for the purpose of pretty much any analysis, randomness just means anything that can‚Äôt be measured in our system and that fluctuates randomly and contributes to variation. Being aware of which processes might be affected by more or less of such random variation is important. In the worst case, if you have too much random variation, it might swamp your signal and you won‚Äôt see any meaningful pattern in your data.\n\n\nMeasurements\nMeasurements that lead to data always have inherent limitations. For instance if you use a scale to measure someone‚Äôs weight, it might only report it to one decimal place. Similarly, if your scale has a minimum and maximum value it can record, anything that is lighter and heavier than that value will not produce a numeric result, just an indication that it‚Äôs too light (the scale might show zero) or too heavy (the scale might show some error message or the maximum value). It is important to consider these kind of measurement limitations and account for them if possible.\n\n\nInteractions\nOften, the components in our system of interest interact. The simplest interaction is a direct one between a predictor and an outcome. Say you give some kind of cancer drug to a patient and see if it reduces the size of their tumor. This is often the scientific question of interest. However, there might be other factors interacting with the variables of interest. For instance the drug might induce some immune response which could either help or hinder the ability of the drug to shrink the tumor. Let‚Äôs say the immune system synergistically helps the drug reduce the tumor in individuals that have blood type A-positive and it hinders the drug in everyone else. These might be important aspects that ‚Äî if you know them ‚Äî need to inform your model.\nThe whole field of causal modeling tries to - among other things - get at such situations and help you devise the best analysis approach given what you know or assume about the system. It‚Äôs worth learning causal modeling, but we can‚Äôt cover it in this class.\n\n\nData structures\nDepending on the processes that generate the data, it is common for data to have certain structures. Thinking about those structures, and as suitable accounting for them in your analyses, is important. The Complex Data unit discussed some important structure such as temporal/spatial/hierarchical. These structures will likely determine the details of your modeling approach, since you will likely want to use models that account for such structures.\n\n\nSummary\nOccasionally, you might do an analysis that tries to explicitly model (some of) the processes that lead to the observed data. More often, you will want to discover patterns and you don‚Äôt directly care about the processes that led to the data. Nevertheless, thinking about them is important since it might influence the way you go about analyzing the data, the specific questions you might try to answer, and the conclusions you can draw from your analysis. Therefore, as a good analyst, you want to understand as much as possible about the system and processes that led to the data you are analyzing.\n\n\nFurther resources\n\nA great source for causal modeling, and statistics in general, is Richard McElreath‚Äôs online course - some of the materials are free, though his book is not.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "The data generation process"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html",
    "title": "Storing and Loading data",
    "section": "",
    "text": "For this unit, we will discuss different common formats for storing data.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data storage formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#text",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#text",
    "title": "Storing and Loading data",
    "section": "Text",
    "text": "Text\nPlain text is the most portable and easy to use format. Most code, no matter what the programming language is, is written and stored as text. The Quarto files you have been working on are also text. Text can be read by any text editor and can be modified easily. The major disadvantage is that it‚Äôs not that suitable for storing potentially complex data.\nFixed-width files are an important special case of plain text data, which are becoming more rare as time goes on, but that you may still run into. These files have a set length for each variable (so column one may be 8 characters long, column 2 may be 3 characters long, etc.), so they are easy for the computer to read, but you will need to use an appropriate function and set the widths. While fixed width files are usable in most programs, we do not recommend them because they are (generally) more annoying to work with than other options.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data storage formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#csv-and-delimited-formats",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#csv-and-delimited-formats",
    "title": "Storing and Loading data",
    "section": "CSV (and delimited formats)",
    "text": "CSV (and delimited formats)\nThe CSV (comma-separated values) format is really just a text format that‚Äôs very well suited for rectangular, spreadsheet-type data. In a CSV, you have rows and columns of entries, and the entries are separated by commas. CSV files are easily readable by most software. If you are storing information that contains commas as part of the values (e.g., if you are storing text in some columns), CSV files can be problematic. TSV files, which stands for tab-separated values, are a variation that uses a tab instead of a comma to separate entries, thus sometimes being more robust. Semicolons are also sometimes used to separate values.\nIn general these files are called ‚Äúdelimited‚Äù formats, and the separator character (comma for CSV, tab for TSV) is called the ‚Äúdelimiter‚Äù. A special case is space-delimited files, which are typically saved in DAT or TXT file types instead of a specialized format.\nDelimited storage formats are likely the most common data-sharing format, and are a good, all-purpose way to share data files across platforms.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data storage formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#compressed-data",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#compressed-data",
    "title": "Storing and Loading data",
    "section": "Compressed data",
    "text": "Compressed data\nWhile CSV and text files are very easily readable formats, one major disadvantage of those files is that they can get relatively large. They don‚Äôt store information in the most efficient manner. CSV files also can‚Äôt store meta-information, for instance the fact that some variable encodes a date. In a CSV file, it‚Äôs all just text.\nThere are formats that are more space-efficient and can preserve some meta-information. Examples are the Rds and Rda formats in R. For differences between those two formats, see e.g.¬†this article. The disadvantage of this and similar formats is that they require specific software to read the data. As long as the data format and software needed is open and free, it‚Äôs not that big of an issue, though sometimes it can hinder portability.\nFormats like parquet and Arrow invented by Apache for their Arrow software are also very efficient and have become quite common in industry recently. The Arrow R package is a fairly intuitive interface for these file formats if you come across them in your data science work.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data storage formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#other-open-formats",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#other-open-formats",
    "title": "Storing and Loading data",
    "section": "Other open formats",
    "text": "Other open formats\nDepending on the data type, there are other open formats for images, audio, video, web content, etc. As much as possible it is good to use formats that are open and follow some international standard. This makes it most likely that a wide variety of tools can work with these files, both in the present and the future.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data storage formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#excel",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#excel",
    "title": "Storing and Loading data",
    "section": "Excel",
    "text": "Excel\nExcel has its own format for storing data. It is common enough that a lot of software can read Excel files. One problem with the Excel format is that the way the data is stored is not always how it looks when you open the file and look at the data, due to additional formatting that‚Äôs encoded. This means you think you might have the data in one format, but really it‚Äôs different once you load it into another software. Dates are good example where this can happen.\nIf possible, it is safer to store information in CSV files. If you get data in Excel files, you‚Äôll ideally want to read it into another software - such as R - as quickly as possible.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data storage formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#sasspssstataetc.",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#sasspssstataetc.",
    "title": "Storing and Loading data",
    "section": "SAS/SPSS/STATA/etc.",
    "text": "SAS/SPSS/STATA/etc.\nMost commercial software has its own format of storing code and data. The problem with these formats is that you need the right software to read the data, and you often won‚Äôt know what you are getting unless you can open the file with the right program.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data storage formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#databases",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#databases",
    "title": "Storing and Loading data",
    "section": "Databases",
    "text": "Databases\nAt times, the data is so complex or large that it can‚Äôt be stored in a single file. In such situations, data is often stored in databases. There are tons of types of databases in existence. SQL based databases are very common. Databases are a huge topic and definitely something worth learning about if you need to work with them. However, since there are so many types, it is best to learn ‚Äòas needed‚Äô about the specific database you plan on working with.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Data storage formats"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html",
    "href": "content/module-data/data-types-complex/data-types-complex.html",
    "title": "Complex types of data",
    "section": "",
    "text": "For this unit, we will discuss different types of data and how data type influences the analysis approach.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#text",
    "href": "content/module-data/data-types-complex/data-types-complex.html#text",
    "title": "Complex types of data",
    "section": "Text",
    "text": "Text\nWorking with and analyzing larger sections of text is becoming increasingly important. Complex and powerful AI tools (e.g., ChatGPT) have become rather good at working with text. The analysis of text often goes by the term natural language processing. Such text analysis will continue to increase in importance, given the increasing data streams of that type. If you are interested in doing full analyses of text data, the tidytext R package and the Text mining with R book are great resources. A short introduction to this topic is the Text Mining chapter (27) of IDS.\nOf course, with the rise of text-based LLM AI tools and similar such offerings, this whole area of data analysis has grown very large on its own. If you want to really dig deep into this, check out some of the course and training offerings from DeepLearning.AI.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#audio",
    "href": "content/module-data/data-types-complex/data-types-complex.html#audio",
    "title": "Complex types of data",
    "section": "Audio",
    "text": "Audio\nAudio recordings can contain text/speech, or other sound. In the former case, models need to be applied to transform audio into text. After that, any text analysis tool can be used. If the audio does not contain text, one likely wants to apply other analyses. For instance one might want to detect if a certain sound is present, such as the sound of a fire alarm going off. This generally requires partitioning and transforming the audio into pieces that can be turned into some form of numeric variables and analyzed. Audio has a time-series structure that needs to be taken into account when analyzing it.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#images",
    "href": "content/module-data/data-types-complex/data-types-complex.html#images",
    "title": "Complex types of data",
    "section": "Images",
    "text": "Images\nFor analysis purposes, one can think of an image is essentially a matrix of pixels, with each pixel having color and intensity values. This large amount of - usually very structured ‚Äî data needs to be processed.\nImages are generally converted into multiple matrices of values for different pixels of an image. For instance, one could divide an image into a 100x100 grid of pixels, and assign each pixel RGB (color) values and intensity. That means one would have 4 matrices of numeric values, each of size 100x100. One would then perform operations on those values. As you can imagine, that quickly leads to fairly large amounts of data. These days, most successful image analysis is done using some form of neural nets, which are generally considered an artificial intelligence (AI) method. (But recall that ML and AI terminology is somewhat fuzzy. The general current usage is that AI are a type of ML, specifically the neural net type.)",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#video",
    "href": "content/module-data/data-types-complex/data-types-complex.html#video",
    "title": "Complex types of data",
    "section": "Video",
    "text": "Video\nVideo is essentially a time-series of images with audio. As such, approaches that work for image and audio analysis can also be applied to video. Of course, the time-series nature of the images makes things more complicated. Neural-net/AI based methods are most often used.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#genetics-and--omics-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#genetics-and--omics-data",
    "title": "Complex types of data",
    "section": "Genetics and -omics data",
    "text": "Genetics and -omics data\nThe main genetic type of data is based on sequences. A lot of specialized tools exist to work with what is often fairly noisy data. Aligning sequences, comparing them, placing them on phylogenetic trees, and other such operations are so common and important that there is a large area of tools for those purposes.\nData that is often called -omics (e.g., metabolomics, glycomics) is often rectangular in structure, but often has distinguishing features, such as few individuals/rows and many observations/columns. Such data needs special treatment. Often, variable/feature reduction is a common step in the analysis workflow.\nThe bioconductor website is your source for (almost) all tools and resources related to genetics and omics-type data analyses in R.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#big-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#big-data",
    "title": "Complex types of data",
    "section": "Big data",
    "text": "Big data\nThis isn‚Äôt really a type of data in the same sense as the ones listed above. Still, it is its own type of data simply by the fact that it encompasses large amounts of data. Some of the complex data types above, e.g., videos, quickly become big data.\nWhile the term big data is a bit fuzzy, in general people mean any dataset that doesn‚Äôt easily fit into the memory of a regular computer (or cluster) and thus needs to be analyzed using special tools. Alternatively, big data might be defined as data that is so big that doing analysis on it takes too long using standard tools (e.g.¬†R on a regular computer) and instead requires special treatment. Of course this also depends on the type of analysis, not only the type of data. As computers keep getting faster and tools more flexible and integrated, the label big data is a moving target.\nGenerally, big data is stored somewhere in a database. SQL type databases are most common. You then want to access that database in a form that allows you to perform your analysis. There are different ways of dealing with big data. Most methods are general and apply independent of the programming language you use. This article describes a few general approaches and explains how they can be implemented in R. This webinar gives a bit more information and a nice description of the overall setup for big data. As you learn in that tutorial, R is often used together with other software to analyze big data. A tool that is often used for big data analysis is Spark. For R, there is the sparklyr package, which allows one to nicely interface with Spark.\nIn general, when you work with big data, you will have to carefully look at the data, the type of database it is stored, and the analysis goals. Based on that, you should use a stack of tools that allows analysis. The Databases task view gives a good overview of different R packages for specific types of databases. You will use R for your analysis, and R will then interface with other software. This interface is usually fairly seamless.\nBig data is commonly modeled using complex models, such as machine-learning or AI algorithms. The reason for that is that those models are very powerful but need a lot of data. Thus, if you have big data, you can use them. That said, you can analyze big data with any model you want, including simple GLM or similar such models.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#hierarchical-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#hierarchical-data",
    "title": "Complex types of data",
    "section": "Hierarchical data",
    "text": "Hierarchical data\nThis type of data is actually quite common. A hierarchy (also at times called multi-level or nesting structure) occurs when there is some type of grouping inherent. For instance, if you analyze heart attacks from different hospitals, there might be systematic differences between hospitals that are worth taking into account. You can then build models that take that into account. I think there is a strong case to be made that one should often start with a hierarchical modeling approach, and only drop it if one is convinced that there are no systematic differences in the data. Unfortunately, such hierarchical models are still a little bit harder to implement, and often a lot harder to understand and interpret. For such models, a Bayesian framework is often very useful. A good introductory resource for hierarchical modeling, and especially how to do it with R, is Statistical Rethinking (the second half of the book). There is also a CRAN Task view that lists R packages relevant to hierarchical modeling.\nAnother term you might hear is recursively nested data. While this type of data is a type of hierarchical data, when we discuss hierarchical data we often think about nesting in terms of statistical clusters. Computer scientists and web developers often use hierarchical data for storage and data transfer purposes, and often call it recursively nested data instead.\nRecursively nested data has become extremely common on the internet the past few years, and is essentially the default data format of APIs. While statisticians often store hierarchical data in one or more rectangular data sets, recursively nested data is stored in recursive lists. For example, consider the following list of Cars in Jay Leno‚Äôs Garage.\n\nMazda Miata\n\nYear: 1996\nOriginal MSRP: $18450\nColor: Red\nConvertible: True\n\nChevrolet Volt\n\nYear: 2011\nOriginal MSRP: $4100\nColor: Steel Metallic\nInterior: Black\nConvertible: False\nHybrid: True\n\nEtc.\n\nWhile we can imagine a world where this data is stored in a rectangular format (see mtcars for example), we would have to do some processing to get it in that format, and we would have to decide what to do for the fields that are not shared across all the entries. These data can also have lists nested within lists (nested within lists, to very high levels of list-recursion), which can be more challenging to ‚Äúrectangle‚Äù (that is, convert to a rectangular format).\nData stored in this format is usually stored in JSON or XML formats, which can be read into R using the rjson or xml2 packages, among others. The new edition of R4DS has a chapter on working with hierarchical and recursively nested data data (but does not cover the fitting/analysis part). The repurrrsive package, by Jenny Brian, contains some examples of this type of data.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#longitudinaltime-series-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#longitudinaltime-series-data",
    "title": "Complex types of data",
    "section": "Longitudinal/Time-series data",
    "text": "Longitudinal/Time-series data\nLongitudinal or time series are a specialized data type that are autocorrelated (i.e., each measurement is correlated with at least one measurement that came before it), and we can therefore get better predictions by using specialized models that take this data structure into account. A good resource is the free textbook Forecasting: Principles and Practice and a lot of the other work by Rob Hyndman.\nA very useful set of tools to allow times-series work in R is the set of packages called the tidyverts. The modeltime R package allows one to use the tidymodels framework to analyze time-series data.\nCRAN also has a Task View for Time Series Analysis. (A Task View on CRAN is a site that tries to combine and summarize various R packages for a specific topic). Another task view that deals with longitudinal/time-series data is the Survival Analysis Task View.\nNote that, somewhat confusingly, longitudinal data are common in epidemiology, and despite being measured over time, ‚Äútime-series methods‚Äù are not usually appropriate. Time-series and forecasting methods are mainly appropriate for data where the times are dense, which means many time points are measured somewhat close together. The term ‚Äúlongitudinal data‚Äù (also called panel data in some fields) is most commonly used to refer to sparse data collected over time, where each study unit is only measured a handful of times which may be spread apart. Most cohort studies therefore fall into this longitudinal framework, not into a time series framework.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#spatial-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#spatial-data",
    "title": "Complex types of data",
    "section": "Spatial data",
    "text": "Spatial data\nLike time series data, spatial data also feature autocorrelation, but typically in two dimensions. (i.e., latitude and longitude.) Spatiotemporal data, typically collected as repeated measurements of spatial data over time, is also somewhat common. One could also have an elevation coordinate and have 3D (or even 4D spatiotemporal data) spatial data, but most analyses focus on 2D spatial autocorrelation structures.\nWhile there is (to my knowledge) no current way to fit specific spatial models in tidymodels (i.e.¬†no modeltime analogue), one can use spatial resampling through the spatialsample package, which provides resampling methods that take spatial autocorrelation into account.",
    "crumbs": [
      "Content",
      "4A. All about Data",
      "Complex data types"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html",
    "href": "content/module-intro-tools/introduction-course.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to the class Modern Applied Data Analysis (MADA)!\nThe course is listed as EPID8060E/BIOS8060E. It doesn‚Äôt matter if you are enrolled under the EPID or BIOS label. I will generally refer to the course as MADA.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#introduction",
    "href": "content/module-intro-tools/introduction-course.html#introduction",
    "title": "Course Introduction",
    "section": "",
    "text": "Welcome to the class Modern Applied Data Analysis (MADA)!\nThe course is listed as EPID8060E/BIOS8060E. It doesn‚Äôt matter if you are enrolled under the EPID or BIOS label. I will generally refer to the course as MADA.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#module-overview",
    "href": "content/module-intro-tools/introduction-course.html#module-overview",
    "title": "Course Introduction",
    "section": "Module Overview",
    "text": "Module Overview\nThis first module provides a brief introduction to the course, the tools we will be using, and the topic.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#learning-objectives",
    "href": "content/module-intro-tools/introduction-course.html#learning-objectives",
    "title": "Course Introduction",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe specific learning objectives for this unit are:\n\nKnow what this course is all about.\nKnow how this course is set up and what you are expected to do.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-goals",
    "href": "content/module-intro-tools/introduction-course.html#course-goals",
    "title": "Course Introduction",
    "section": "Course Goals",
    "text": "Course Goals\nThe main goal for this course is for you to learn the whole process of performing a data analysis project. The focus is on applied analysis of real world, messy data.\nA second goal is to introduce you to some modern analysis approaches commonly referred to as Machine Learning.\nA related goal is to introduce you to a set of tools that allow for a modern, reproducible workflow of your analysis.\nFor more detailed learning objectives, see the Syllabus.\n\n\n\nIn this course, I randomly switch back and forth between singular and plural. Source: phdcomics.com.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-philosophy",
    "href": "content/module-intro-tools/introduction-course.html#course-philosophy",
    "title": "Course Introduction",
    "section": "Course Philosophy",
    "text": "Course Philosophy\nHere are my goals, promises, and expectations for this course.\n\nI expect you to be self-motivated and committed to learning the material by putting in the effort needed to succeed.\nI will try to maximize the rewards you get by hopefully teaching methods that are useful to you, and I will try to provide as much help as needed to maximize your learning.\nThis class strives to be challenging but non-threatening. As such, I‚Äôll make you work hard, and expect you to do the assigned tasks by the deadlines, but in the end, I usually don‚Äôt grade hard - unless you fail to keep up your end of the agreement and don‚Äôt put in the work.\nThis class is open everything. You can use the internet, AI, ask your classmates, myself and others, get help from wherever you can. I trust you will find the right balance of getting help when you need it while still putting in enough effort to experience real learning.\nI will not perform any policing to try to prevent you from taking shortcuts (i.e., not doing work yourself). The class contains graded assessments with deadlines, but those are meant to help you stay on track. If you somehow cheat - and cheating will be easy - you are mainly cheating yourself out of learning.\n\nOverall, I hope this course is going to be useful, interesting, challenging and also interactive. Online courses are always a bit tricky with interaction/participation. I hope we can create something online that feels like a classroom. Please participate, ask questions, etc. The more you engage in the course, the more you‚Äôll get out of it.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-setup",
    "href": "content/module-intro-tools/introduction-course.html#course-setup",
    "title": "Course Introduction",
    "section": "Course Setup",
    "text": "Course Setup\n\nThe course is split into modules. Each module will usually be covered in a week. The Schedule document gives an outline. The schedule is not fully finalized and will change, so check frequently.\nEach module consists of one or several units/documents containing a mix of things I wrote, and writings or videos by others. They are listed in the order you should go through them.\nAll material for the course can be accessed through this course website. Some material might not be available yet and will be unlocked as the course proceeds.\nThere is generally a lot of material for each unit. You are expected to go through the main components at a level that allows you to get the big picture and be able to answer the quiz questions. Once you get the overall idea, consider the materials to be resources you can visit on demand, e.g., when you are working on the exercises.\nYou will be placed in groups throughout the course. The hope is that this will create a support group of classmates where you can help each other. Assignments often require interaction among group members (like in a real-world team), and I hope you will interact with your group members outside the assignments as well. You are welcome and encouraged to interact with anyone else in the course.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-tools",
    "href": "content/module-intro-tools/introduction-course.html#course-tools",
    "title": "Course Introduction",
    "section": "Course Tools",
    "text": "Course Tools\nThis is a brief overview of the tools we will be using for this class. The next module, which you can and should start right after finishing this one, describes all these tools in more detail and gives instructions on how to set them all up.\n\nThis website is the main resource for all materials.\nWe will use Discord for discussions and help.\nWe will use an online system for quizzes.\nWe will use the R software for writing code.\nWe will use RStudio (a graphical frontend to R). You can use a different editor if you want, but all instructions assume you are using RStudio.\nWe will use Quarto, which nicely plays with R/Rstudio to let you create easy, automated workflows.\nWe will use Git/GitHub for exercises and the project.\nWe will use GitKraken as Git/GitHub frontend. You can use a different software, but all instructions assume you are using GitKraken.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#assessments",
    "href": "content/module-intro-tools/introduction-course.html#assessments",
    "title": "Course Introduction",
    "section": "Assessments",
    "text": "Assessments\nFor details on assessments, see the Assessments page.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#course-resources",
    "href": "content/module-intro-tools/introduction-course.html#course-resources",
    "title": "Course Introduction",
    "section": "Course Resources",
    "text": "Course Resources\nWe‚Äôll be drawing on a lot of different resources. I compiled a list with the ones we‚Äôll use and others you might find helpful in the Resources section of the class website.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course.html#getting-help",
    "href": "content/module-intro-tools/introduction-course.html#getting-help",
    "title": "Course Introduction",
    "section": "Getting help",
    "text": "Getting help\nI do not expect you to figure it all out yourselves. You will get stuck and are encouraged to seek help. You can ask for help from your classmates or your instructor. Also, use the wider community online. For specific places to get help, see the Resources pages.\n\n\n\nIn this course, I randomly switch back and forth between singular and plural. Source: phdcomics.com.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-github-introduction.html",
    "href": "content/module-intro-tools/tools-github-introduction.html",
    "title": "Git/GitHub Introduction",
    "section": "",
    "text": "Overview\nThis document gives a brief explanation of Git, GitHub and related tools, and describes why and how we will use them for this course.\n\n\nLearning Objectives\n\nKnow what Git and GitHub are.\nKnow why one might want to use them.\nCreate and set up a GitHub account.\nSet up a Git/GitHub workflow using GitKraken or an equivalent setup.\n\n\n\nWhat are Git and GitHub\nGit is software that implements what is called a version control system. The main idea is that as you (and your collaborators) work on a project, the software tracks, and records any changes made by anyone. GitHub is distinct from Git. GitHub is in some sense on online interface/website, and Git the underlying engine (a bit like RStudio and R). Since we will only be using Git through GitHub, I tend to not distinguish between the two. Throughout the course, I will usually refer to all of it as just GitHub. Note that other interfaces/platforms for Git exist, e.g., GitLab or Bitbucket.\n\n\nWhy use Git/GitHub\nYou want to use GitHub to avoid this:\n\n\n\nSource: phdcomics.com\n\n\nGitHub gives you an organized way to track your projects. It is very well suited for collaborative work. Historically, version control was used for software development. It has since become broader and is now used for many types of projects, including data science projects. If you want to learn a bit more about Git/GitHub and why it is a great tool for data analysis, check out this article by Jenny Bryan.\n\n\nWhat to (not) use Git/GitHub for\nGitHub is ideal if you have a project with (possibly many) smallish files, and most of those files are text files (such as R code, LaTeX, Quarto/Markdown, etc.) and different people work on different parts of the project.\nGitHub is less useful if you have a lot of non-text files (e.g.¬†Word or PowerPoint) and different team members might want to edit the same document at the same time. In that instance, a solution like Google Docs, Word+Dropbox, Word+Onedrive, etc. might be better.\nGitHub also has a problem with large files. Anything above around 100MB can lead to very slow syncing and sometimes outright failure. Unfortunately, once you have a failed attempt to sync a large file, it can be quite tricky to fix the repository to work again. Therefore keep large (&gt;100MB) files out of your GitHub repositories. If you have to work with such files, try to reduce them first before placing into the GitHub repository. Or as alternative, place those files in another sync service (e.g.¬†Dropbox, OneDrive, GoogleDrive) and load them from there.\nFinally, if you have data, you need to be careful since by default, GitHub repositories (the GitHub name for your projects) are public and everyone can see them. You can set them private, but you need to be careful that you don‚Äôt accidentally expose confidential data to the public. It is in general not a good idea to have confidential data on GitHub. First anonymize your data enough that it is not at risk of identifiability, then you can place it in a private repository. If you put it public, be very careful that you only make things public that are ok to be made public.\n\n\nGetting Git/GitHub\nGit is a piece of software. It can be installed from the Git website. If you use GitKraken as described below, you don‚Äôt need to install Git separately. However, it might still be worth doing so, in case at some point you need to do some Git operation outside of GitKraken (e.g, through RStudio or a terminal).\nGitHub is an online platform. You don‚Äôt need to install anything, but if you don‚Äôt have a GitHub account yet, you need to create one on GitHub.com. Note that GitHub is widely used professionally. You might use it beyond this class and for your future career. You might also want to allow other people to see your GitHub presence. You should therefore use a future-proof, professional sounding user name for your GitHub account. You can use your UGA or a different email for GitHub. Whichever email you use is where you will receive GitHub activity related notifications (including our GitHub class activities).\n\n\nHow to use Git/GitHub\nThe most powerful and flexible way of using Git/GitHub is to open a command-line terminal (e.g., the one that comes with the Git install) and type a bunch of commands. This is often very confusing and we will not be using this approach.\nThe approach I recommend for this class, and the one I assume you will be using, is to interact with GitHub through a graphical interface. Specifically, for this class I recommend a software called GitKraken.\nIf you are already an experienced GitHub user and have your own preferred setup (e.g.¬†the command line or some other 3rd party software), or for some other reason you don‚Äôt want to use GitKraken, you can instead use another graphical client. GitHub itself provides a graphical interface with basic functionality. RStudio also has Git/GitHub integration, for R projects. Other graphical interfaces exist. I assume if you decide to do it your way, you are experienced enough to translate my GitKraken-centric instructions to your setup. While my instructions throughout this course assume you use GitKraken, you can do everything without using it. But then you have to figure out on your own how to do it üòÅ.\n\n\nGitKraken\nDownload and install GitKraken. The free version can do pretty much everything we need. One important limitation is that the free version does not allow you to access private repositories. As student, you can (and should) upgrade to the Pro version for free, see the GitHub developer pack section below on how to do it. By using GitKraken for your GitHub workflow, you do not need to go through the command line (unless things get so bad that you can‚Äôt fix them inside GitKraken). You also do not need to install Git separately, you get it with GitKraken. For this course, I assume you will be using the Pro version of GitKraken.\nOnce you have your GitHub account set up and GitKraken installed, make sure you connect the two.\n\n\nGitHub developer pack\nPlease get the GitHub student developer pack. This pack gives you a lot of nice free goodies that you might be interested in. I encourage you to explore all the different products and deals. For this class, we most want the following:\nGitHub Pro account: This will give you your own free private repositories. We will mostly use public repositories, but there might be times you want to have a private repository, e.g.¬†for data you don‚Äôt want to be public. Normally, if you want private repositories, you have to pay. But with the student pack (or as an academic), you get them for free.\nGitKraken Pro: Free GitKraken Pro access. While the free version of GitKraken might work ok for this course, you can‚Äôt access private repositories with it. Often, being able to use private repositories is useful.\nGitHub Copilot: Which gives you AI support for writing code.\nThere are a lot of other goodies that come with the student pack. We won‚Äôt use them in the course, but some might be of interest to you.\n\n\nGit/GitHub terminology\nGit/GitHub has a lot of specialized terminology that takes getting used to. The GitHub folks posted a handy page with short descriptions of some of the important terms. Some of the terms you will encounter soon are Repository (also often called repo), User, Organization, Branch, Fork, Push, Pull, Commit and Stage. For some reason, the last term is not explained in the list linked to above. Staging is the step you need to do after you made changes and before you commit them. For pretty much all of my work, I find this a not too helpful extra step, but you need to know what it means and you need to do/use it as part of the GitHub workflow.\nEach repository can have multiple branches (think of it as alternative versions of all files, allowing different people to work on different aspects of a project with minimal interference). In this course, we won‚Äôt really use branches. We will mainly just work with a single branch. The name of the main branch can be anything, but these days the default is main. Until not too long ago, the main branch was referred to as master. This has mostly been discontinued, but you might still see occasionally some Github repositories where the main branch is called master. It might happen that in these course documents, I still occasionally refer to the main branch by master (let me know if you spot the old terminology so I can update). Again, the name can be in principle anything. Just be aware that master and main usually refer to the main branch.\n\n\nGit/GitHub and other cloud based sync options\nIn some way, you can think of GitHub as a more controlled (and more complex) alternative to systems such as Dropbox, OneDrive, GoogleDrive, etc. With the GitHub workflow, you can work locally on different machines, and everything is backed up in the cloud (on github.com). The (sometimes annoying) difference to Dropbox & Co is that the syncing between your local computer and github.com is not automatic. So don‚Äôt forget to pull before you start work on a repository and push once you are done!\nI am a heavy user of both GitHub and Dropbox. I use Dropbox for a lot of regular files, e.g.¬†MS Office documents, pdfs, images, etc. Some of my research projects, especially those where I collaborate with people that don‚Äôt use GitHub, are run through Dropbox. Dropbox or a similar service are convenient since all the syncing happens automatically. For anything where I want a more structured and organized approach, e.g.¬†coding projects, some research projects or course materials like this site, I use GitHub.\nYou can do any setup you want. However, don‚Äôt mix and match. Because GitHub works similar to Dropbox, it is a bad idea to locally store your GitHub repositories in a folder that is synced by Dropbox. If you do, the Dropbox sync process and the GitHub sync process can conflict, potentially leading to syncing mess. For any GitHub repository, store it locally in a folder that is not synced across your computers by another software (such as Dropbox, OneDrive, Box, etc.)\nSimilarly, don‚Äôt store a GitHub repository inside another repository. While this is technically allowed, it can easily lead to a mess. Each repository should be separate and not connected to any other syncing service.\n\n\nGitHub for this course\nWe will make heavy use of GitHub for this course. Instead, everyone will just be working through their own GitHub accounts. The advantage of this setup is that once the course ends, everything you did for the course automatically stays with you.\nPrivacy note: As mentioned on the Course Syllabus page, this course is set up such that you work ‚Äúin the open‚Äù through public GitHub repositories. Please make sure to read the Privacy/FERPA section of the syllabus and let me know if you have any concerns. If I don‚Äôt hear from you, I consider this as you consenting with producing public materials as part of this course, and thus make your participation in this course public.\nFrustration note: Git/GitHub can be finicky, even for seasoned users. Sooner or later, you‚Äôll run into some issues and strange error messages. If that happens, don‚Äôt panic. You‚Äôll figure out how to fix things. The internet and Jenny Bryan‚Äôs book happygitwithr are great resources. And/or ask on Slack.\n\n\nFurther Resources and help\nGitHub can be confusing. Start slow. Using the graphical interface (GitKraken) makes getting started fairly easy. I‚Äôm also trying to give specific instructions whenever we do something new with GitHub. Both the GitHub documentation and GitKraken support page are good resources for information.\nIf the quick install instructions above are not detailed enough, follow these in Jenny‚Äôs paper or her great online book happygitwithr.\nAnother source worth looking at for GitHub information is this chapter of IDS. There is also a lot of good beginner material online.\n\n\n\nSource: phdcomics.com",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "GitHub Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-quarto.html",
    "href": "content/module-intro-tools/tools-quarto.html",
    "title": "Quarto & Co",
    "section": "",
    "text": "Overview\nThis unit provides a brief introduction to the Quarto system.\n\n\nLearning Objectives\n\nKnow what Quarto is good for\nBe able to use Quarto for data analysis\n\n\n\nIntroduction\nQuarto is (according to its website) An open-source scientific and technical publishing system. Quarto is developed by the same company (posit) that develops RStudio. It therefore has very nice integration with R code - but also supports other code, such as Python and Julia. It uses Markdown as the format to write text in.\n\n\nQuarto overview\nThe idea behind Quarto is that you have a system that allows you to combine code with text and simple formatting to easily create various useful outputs, such as html files (websites and presentation slides), word and pdf documents (manuscripts and reports), and more.\nYou can apply layout and styling to those documents, which is done separately from the content. This means you can quickly switch between outputs. Quarto calls various other pieces of software to - almost magically - turn text and code input into a variety of different output formats. The good thing is, you generally don‚Äôt need to care what goes on under the hood, it all happens (almost always) smoothly behind the scenes.\n\n\nMarkdown\nThe text inside a Quarto document is markdown. Markdown is a simple way of writing text with some formatting. Markdown is a simple way of formatting text to get some reasonably nice layouts with minimal effort. It‚Äôs a bit like HTML code (or TeX/LaTeX if you are familiar with that), inasmuch as it combines plain text with instructions for formatting it. Markdown is simpler than HTML or LaTeX.\nIf you have no experience with Markdown, I suggest you go through this nice, short interactive tutorial. A good reference to look up formatting for Markdown until you have it memorized is this online cheat sheet.\nThe fact that markdown is mainly plain text means you can edit it with any text editor, you can easily track changes, and also turn the text into different output formats.\n\n\nLiterate programming\nYou can write pure markdown text documents in Quarto that do not contain any code (like this document you are reading). But things become very powerful once you combine text with code in a fairly automated manner, which is the main idea behind Quarto.\nThis concept behind combining text with code is often referred to as literate programming. It has been around for a while, Quarto is the currently best and most feature-rich implementation of this concept.\n\n\nGetting started with Quarto\nThe whole Quarto system is very flexible and powerful (and continues to grow). Because it is very feature rich and you can do a ton, trying to read all about it does not make much sense. The best is a learn it as you need it approach. The Quarto website has great documentation, you‚Äôll likely be going there often.\nTo get started, follow the get started guide on the Quarto website. Install Quarto and - if you haven‚Äôt already - R Studio. (Note that current versions of R Studio have Quarto integrated, but it‚Äôs not always the latest version, so doing a separate install of the latest Quarto version is not a bad idea.)\nThen go through the very nice Getting Started with Quarto tutorial. The main point is to get an idea of what you can do and play around a bit. You‚Äôll repeatedly be coming back to the Quarto documentation as you try to figure out how to do specific things. So if some of the material in the tutorials is not quite clear on your first pass-through, that‚Äôs ok. Just get an overall idea and play around so you feel comfortable.\n\n\nQuarto versus R Markdown\nQuarto is the more powerful successor of R Markdown. R Markdown is still around and functional. However, most effort is going into implementing new features for Quarto, and it is already much more full-featured compared to R Markdown.\nOccasionally, you might still need to use R Markdown, and you will find a lot of online information for R Markdown. Quarto and R Markdown are very similar, so often what works in one will also work in the other.\n\n\nSummary\nQuarto is a powerful, flexible framework that allows you to perform data analysis and other scientific and technical projects in a very automated, reproducible setup.\n\n\nFurther Resources\n\nThe R4DS book has a Quarto chapter.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Quarto & Co"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-reference-management.html",
    "href": "content/module-intro-tools/tools-reference-management.html",
    "title": "Reference Management",
    "section": "",
    "text": "Overview\nIn this short unit, you will learn how to manage your references within the R/Quarto/Markdown stack of tools.\n\n\nLearning Objectives\n\nKnow how to do reference management that plays nicely with Quarto/Markdown\n\n\n\nReference management with Quarto/R Markdown\nFor almost any data analysis, especially if it is meant for publication in the academic literature, you will have to cite other people‚Äôs work and include the references in your work. In this class, possibly for some of our exercises and definitely for the class project, you need to include references and cite other people‚Äôs work like in a regular research paper.\nI assume you are familiar with how citing references works, and hopefully, you are already using a reference manager. You might be using EndNote or Mendeley or PaperPile or‚Ä¶\nTo have something that plays well with Quarto/R Markdown, you need references in a format called bibtex. This is essentially a large file of structured plain text. You don‚Äôt ever want to edit the bibtex file yourself. Instead, you use a reference manager that is bibtex friendly. I mostly use Zotero, which is free. If you use Zotero, you should use the Better BibTeX extension, which allows you to keep your bib file in sync as you update references in Zotero.\nYou could probably use others. I don‚Äôt have much experience with any others. Whichever reference manager you use, the important part is that you need to be able to save or export your references as a bibtex file.\nTo use references in your Quarto or RMarkdown files, specify the name and location of your bibtex file in the YAML header. You can also specify a CSL style file there. This is a file which controls how the references are formatted. Styles for almost any journal are available for download from Zotero‚Äôs styles page. When you create your bibtex file, each reference gets a key, which is a shorthand that is generated by the reference manager or you can create yourself. For instance, I use a format of lower-case first author last name followed by 2 digit year for each reference. If I cite a paper by the same first author that was published in the same year, then a lower case letter is added to the end. For instance, for a paper that I wrote as 1st author in 2010, my bibtex key might be handel10 or handel10a. You can decide what scheme to use, just pick one and use it forever.\nIn your Quarto document, you can then cite the reference by adding the key, such as ...in the paper by Handel et al. [@handel10].... When you process the file, it looks for a reference with the specified tag in your bibtex file and places all references at the end of your document using the formatting you specified.\nFor references with Quarto, see this Quarto documentation.\nSome more details on citations and references in R Markdown/bookdown can be found in the RMarkdown Cookbook bibliography section and citations section.\nYou will see a few examples of bibtex use together with Quarto/R Markdown in the course, so when it‚Äôs time for you to use it (e.g.¬†for the class project), you should know how to go about doing it.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Reference Management"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html",
    "title": "Assessment - Machine Learning Models 1",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#setup",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#setup",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Setup",
    "text": "Setup\nThis is a solo-exercise, and it‚Äôs part of your portfolio.\nThe goal is to have both a complete and well-organized data analysis example at the end of the exercise. So as you go through this exercise, in addition to doing the indicated tasks and writing the code, keep organizing things as needed. You want to make sure you have readme files or other documentation that briefly explain what files are where and in which order to run them. Make sure your R/Quarto files are easy to understand and that they all run.\nAs you re-organize, you can decide the mix of R and Quarto files and what you want to combine and what to split. However you do it, make sure it‚Äôs documented and easy to understand.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#pre-processing",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#pre-processing",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Pre-processing",
    "text": "Pre-processing\nWe previously covered pre-processing, but haven‚Äôt specifically looked at that yet in an exercise. So let‚Äôs add that part. This can be added in various places to the code/files you already created previously. You can for instance make it part of the cleaning/wrangling code, or include it in the recipe part of your tidymodels workflow (or a mix).\n\nFeature/Variable removal\nYou probably realized that some of the variables contain the same information in slightly different ways. Specifically, Weakness, Cough and Myalgia exist on both a severity score and as Yes/No.¬†Further, there are 2 variables for cough yes/no. These variables are strongly correlated and thus at a minimum, don‚Äôt help when you model, and are actually more likely to confuse your model and lead to errors/warnings/poor model performance (e.g.¬†‚Äúpredictions from a rank-deficient fit may be misleading). So let‚Äôs fix that. For those symptoms where you have both multiple levels and yes/no, remove all the yes/no versions. That should remove 4 variables.\n\n\nCategorical/Ordinal predictors\nSome of your predictors are categorical (e.g., Yes/No) and the 3 symptom severity factors are ordinal, with None &lt; Mild &lt; Moderate &lt; Severe.\nWe can code the categorical variables as unordered factors and the others as ordered factors. I want you to do that as practice. The functions step_dummy() and step_ordinalscore() will help. See e.g.¬†the help file example for step_ordinalscore(). Not that to deal with ordered factors in a statistical analysis, one needs special approaches (and we won‚Äôt actually do those for this exercise.)\n\n\nLow (‚Äúnear-zero‚Äù) variance predictors\nIf you look at your data, you‚Äôll see that some predictors are fairly unbalanced, with most patients reporting No and only a few Yes. If almost everyone is in one category and almost nobody in others, that often (but not always) means those predictor variables are not very helpful in fitting/predicting the outcome. Furthermore, if you do cross-validation and one of your samples happen to not include one predictor level at all, things might go wrong when applied to the holdout-sample. (There are methods to deal with this specific problem, but they are more complicated than what we will do here.) Thus, it is worth considering if we want to remove them. The recipes package in tidymodels has the function step_nzv() which can do that for you automatically. If you have lots of predictors, that might be good to use. But it‚Äôs often better to decide manually for each variable based on your scientific expertise if you want to remove it or not. We‚Äôll take that approach here. After looking at the data, we decide to remove those binary predictors that have &lt;50 entries in one category (there are 2). Write code to remove them.\nYou should end up with a data frame that has 730 observations and 26 variables. This is the dataset we‚Äôll use for modeling.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#analysis-code",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#analysis-code",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Analysis code",
    "text": "Analysis code\nStart a new Quarto file for this analysis, call it machinelearing.qmd, place it into the code folder. Note that for this exercise, some of your code might take long to run. This is one of the situations where it is often good to have a setup where some of the heavy computations are done by separate R scripts/functions, and the results saved, pulled into and displayed in the Quarto file. How you want to do it here is up to you, as long as the Quarto file shows all the main results and your code/file(s) are well documented.\nTo keep things a bit simpler, for this exercise we focus on a single outcome, the continuous, numerical value of Body temperature. Thus, we are fitting regression models here. Once you finished the whole workflow for that outcome, you are welcome to (optionally) do it again and run a classification for the categorical outcome of Nausea. Since we are doing a regression, and we don‚Äôt have any specific expert knowledge that tells us how we should build our performance metric, we‚Äôll go with one of the usual ones, namely RMSE. But remember to always think about your performance metric and don‚Äôt just use the default without at least considering other options.\n\nData Setup\n\nStart by setting the random seed to 123. This should make everything reproducible and everyone should get the same results.\nSplit the dataset into 70% training, 30% testing. Also use the outcome BodyTemp as stratification. This allows for more balanced outcome values in the train and test sets. See e.g., section 3 of the Get Started tutorial.\nWe want to do 5-fold cross-validation, 5 times repeated. (There‚Äôs no specific reason to do this 5x5 pattern, other than to show you that there are different ways to pick the sample, and that I want you to not use the default.) For the CV folds, we also want to stratify on BodyTemp, as we did for the main train/test split. Use the vfold_cv() function to create a resample object for the training data with these specifications.\nCreate a recipe for the data and fitting. You won‚Äôt need to do much, just make sure you code the categorical variables as dummy variables, otherwise things might not work smoothly. For that, you want to use the step_dummy() function and pick all nominal predictor variables (which are actually all predictor variables here, since the only continuous variable is our outcome).\n\n\n\nNull model performance\nWrite some code to compute the performance of a null model, i.e.¬†a ‚Äúmodel‚Äù that doesn‚Äôt use any predictor information. For a continuous outcome and RMSE as our metric, a null model is one that always predicts the mean of the outcome. Compute the RMSE for both training and test data for such a ‚Äúmodel‚Äù. We‚Äôll use that later to compare it to the performance of our real models. Of course, we expect/hope our real models that use predictor information to be better. If they aren‚Äôt that means they are no good.\n\n\nModel tuning and fitting\nWe‚Äôll fit a tree, a LASSO model, and a random forest. I chose those because they are used in the tutorial on the tidymodels website. You can of course add further models. For the tree, see the Tune model parameters section of the Get Started tutorial. For LASSO and the random forest, check out the Case Study section of the Get Started tutorial. Note that you will need to adjust the code for our scenario since we have a continuous outcome.\nIf you follow the tutorial, you‚Äôll likely use the packages rpart, glmnet and ranger to fit those 3 models. Make sure they are installed and loaded.\nI suggest you write code for each model separately. A lot of the code will look similar, so once you got the first one set up, the other two should be easier. They mainly differ in the commands specifying the tuning parameters and the tuning grid.\nEach of these models requires some tuning. For the choices regarding the tuning parameters, you can follow the examples. Most of the models have more things that can be tuned, but for now you can stick to what they show in the tutorial. Follow the examples by setting up a workflow, set a tuning grid, and then use the tune_grid() function to tune the model using cross-validation.\nNote that the tuning part, i.e., calling tune_grid() might take a good bit of time to run (possibly minutes).\nThe steps (block of code) you should have here are 1) model specification, 2) workflow definition, 3) tuning grid specification and 4) tuning using cross-validation and the tune_grid() function.\n\n\nModel evaluation\nOnce you have done the tuning, you can take a look at some diagnostics by sending your object returned from the tune_grid() function to autoplot(). For instance if you tuned the tree and saved the result as tree_tune_res, you can run tree_tune_res %&gt;% autoplot(). Depending on the model, the plot will be different, but in general it shows you what happened during the tuning process.\nNext, you want to get the model that the tuning process has determined is the best. You can get the best-fit model with select_best() and finalize_workflow() and then do one more fit to the training data with this final workflow using the fit() function. Follow the examples in the tutorial.\nTo evaluate the final fit for each model, do the following.\n\nMake two plots, one that shows model predictions from the tuned model versus actual outcomes, and one that plots residuals. The actual outcomes you get straight from the data, the predicted outcomes you can get by applying the predict() function to the final fit.\nLook at/print the model performance and compare it with the null model (still only on training data). Here, we want the performance of the tuned, best-fitting model on the CV dataset (we are not yet touching the test data). You can get that for instance with the show_best() function, which gives you the mean cross-validated performance for the best models. It also shows the standard deviation for the performance. Compare that model performance with the null model.\n\nThe mean and standard deviation of the performance give you a measure of overall performance and variability in that measure. The plots show you if there are any systematic deviations between model and data. Taken together, these can be compared for the different models and based on those (and as wanted, other considerations) a final model can be chosen.\nImplement the model tuning/fitting and evaluating steps for all 3 models.\n\n\nComments for specific models\nHere are some more suggestions and hints. Most of this is optional but worth trying.\nFor the tree model, if you want to plot the tree, you can use the rpart.plot package and run this command rpart.plot(extract_fit_parsnip(best_tree_fit)$fit) (assuming your result from the final workflow fit is called best_tree_fit). You might get a warning message, but the tree will show. You will likely find when you look at the actual/predicted plot or the residual plot that the tree model does not perform very well, and the model only predicts a few discrete outcome values. That‚Äôs also noticeable when you compare RMSE for the tree model and the null model, they are very similar.\nFor the lasso model, you will likely find that it performs a bit better than the tree, but not a lot. If you want to see a plot for how the number of predictors included in the LASSO model changes with the tuning parameter, you can extract the model from your final fit (say it‚Äôs called best_lasso_fit) with x &lt;- best_lasso_fit$fit$fit$fit and then plot(x, \"lambda\"). I know, this is awful code having to dig that deep into the best_lasso_fit object. You might also be able to use the extract_fit_engine() function to get the underlying fit object produced by LASSO, e.g., x &lt;- extract_fit_engine(best_lasso_fit) might work (this is new in tidymodels and I haven‚Äôt fully tried it yet).\nFor the random forest model, you will likely again find that it performs a bit better than the tree, but not a lot. The tuning setup might require some fiddling around, I had a few initial tries where the whole tuning failed. For a model like random forest, all variables stay in the model. There are ways to look at the variables that are most important. If you want to do that, you again need to pull out the fit object. Say it‚Äôs called best_rf_fit, you can do that with x &lt;- best_rf_fit$fit$fit$fit and then use the vip() function from the vip package to plot the importance of the variables. Alternatively extract_fit_engine() should also work.\n\n\nModel selection\nOnce you have implemented above steps for the 3 models, you should have a ‚Äúbest‚Äù fit for each one based on the tuning process. For each best model you should have performance, uncertainty around the performance measure, and some diagnostic plots. While for any real research project, you likely want to look deeper (e.g.¬†at uncertainty in predictions instead of just overall performance), for now this is enough. Pick one of the three models. Explain why you pick it. There is no single answer that‚Äôs correct, I just want you to practice reasoning for why you are doing something: in this case justify why you are picking the model you do.\n\n\nFinal evaluation\nOnce you picked your final model, you are allowed to once ‚Äì and only once ‚Äì fit it to the test data and check how well it performs on that data. This gives you a somewhat honest estimate of how the model might perform for new, unseen data. You can do that using the last_fit() function applied to the model you end up choosing. For the final model applied to the test set, report performance and the diagnostic plots as above.\nAnd that concludes what is likely a fairly long exercise. The code itself is not that long, but it will take you time to cobble it together from the tidymodel tutorial and possibly other sources.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#test-and-website-update",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#test-and-website-update",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Test and website update",
    "text": "Test and website update\nMake sure your analysis and all results are nicely documented and everything runs/renders correctly. Then, add the newly created Quarto document as an entry into your _quarto.yml file, as a sub-menu of Flu fitting. Call it Machine Learning. Recompile your portfolio website and make sure everything works and shows up as expected. Then commit and push.\nSince this is part of your portfolio site, you don‚Äôt need to post anything, I know where to find it. Therefore there is no exercise Slack channel for this module.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/many-tree-models.html",
    "href": "content/module-model-ml-1/many-tree-models.html",
    "title": "Many-Tree Based Models",
    "section": "",
    "text": "Overview\nIn this unit, we will cover an extension of simple decision tree models, namely models that combine multiple trees for improved performance (but reduced interpretability).\n\n\nLearning Objectives\n\nBe familiar with several tree-based methods\nUnderstand the advantages and disadvantages of single trees versus tree ensembles\n\n\n\nMany trees\nAs discussed in the last unit, trees have many good properties, such as being interpretable, able to work for both regression and classification, deal with missing values, and auto-select predictors.\nThe main disadvantage of single-tree models is that their performance is often not that great‚Äìand complicated trees which perform better are vulnerable to overfitting. However, building models which are combinations of many single trees is possible. Usually these so-called ensemble models (the general name for an ML model that gets an aggregate result by combining many other models) sacrifice some interpretability for performance. The idea behind more sophisticated tree-based methods is to take multiple individual trees and add/average them together in a smart way to end up with an ensemble of trees that often performs much better than a single tree. Some of the most common approaches are Bagging (Bootstrap aggregation), Random Forests, and Boosting. We‚Äôll cover each one very briefly.\n\n\nBagging\nThe main goal of bagging is to reduce variance, which is the main factor that generally negatively affects the performance of a single tree (even if one uses pruning or cross-validation). Remember that if we have N independent observations, each with the same variance/standard deviation, SD, the total variance is SD/N. If we had M datasets for the same process, each with N observations, and we built a model for each dataset, we could average over the models and thereby reduce the variance by 1/M. We don‚Äôt have M different datasets, but we can re-sample our existing data (using bootstrapping), and build a model for each sample. We can then, in the end, average over the different models to reduce variance. Here, each model is a tree, but we could also apply this approach to other models. The final model is the average of all the trees/individual models. Since the bagging procedure reduces the variance, individual trees are not pruned. Bagging leads to models with less variance and thus generally better model performance/predictive power. What is lost now is the ease of interpretation, since our final model is now the sum of a (possibly large) number of individual trees.\n\n\nRandom forest\nThe random forest model/algorithm is similar to bagging. The difference is that as we split each tree, instead of considering all possible predictors, we pick a random subset of predictors and only split on the best among those. Since we are artificially limiting ourselves, we obtain many trees that don‚Äôt perform too well on their own. However, this random choosing of subsets of predictors leads to more ‚Äúdiversity‚Äù in the tree structure (i.e., it avoids the greedy nature of the standard tree building algorithm). This helps in de-correlating the trees when we sum across them in the final model. Often this further improves model performance. The cost is the same as for bagging, namely the final model is a sum of trees which is hard to understand and interpret.\n\n\nBoosting\nBoosting takes a somewhat different approach than the other two. In this case, instead of averaging over full trees, we build many small trees. The procedure starts by building a tree with a specified, often small number of splits (this number is a tuning parameter). This small tree is added to the model, the change in performance is computed (e.g., reduction in RMSE or misclassification error) and a new tree is built that tries to reduce the leftover (residual) errors. In this form, many small trees are added, each one trying to take care of the under-performance produced by previous trees. In the end, one ends up with a sum of many small trees as the final model. This tree ensemble is expected to perform much better than each of the individual trees. Again, the final model is somewhat hard to interpret.\n\n\nAdvantages and disadvantages of many-tree models\nAll the advantages mentioned for a single tree model apply to many-tree models. The main additional advantage is that these models often have (much) improved performance compared to a single tree. It is often possible to use an algorithm that implements one of these many-tree models and with little adjustment and tuning, obtain a model with excellent performance. It is, however, still important to tune/train the model.\nI already mentioned the main disadvantage: These models are hard to interpret. If the goal is to have a user-friendly model that could be used by humans, trees are best. If the user is ok with typing values into a computer (or smartphone) and letting some algorithms run in the background and produce answers, then more complex models might be ok.\nAnother possible disadvantage of many-tree models is that they generally take longer to train, so if speed is important, one might not be able to use all of these model variants to their full extent.\n\n\nAdditional comments\nAs mentioned, bagging and boosting are approaches that can be applied to methods other than trees. For instance, one can bag/boost linear models, GAMs, etc.\nThe many-tree methods described above are examples of what is often called ensemble methods. Boosting is an example of using an ensemble of weak learners, i.e., a combination of models that individually don‚Äôt perform that well, but when combined, they often have outstanding performance. The combination of different individual models/learners (weak or not) is often called model averaging or model stacking, and such methods lead to some of the best-performing models in machine learning. For more on this, see e.g., the Stacked Models chapter of HMLR. We will not further cover these models, but if you have a situation where you need the best model performance available, such models might be worth looking at.\n\n\nImplementing tree models in R\nThe tidymodels framework allows access to many different types of packages/algorithms that implement tree-based models. You can see the full list on the parsnip model search page.\nMany of these methods and packages are similar and differ in the details of the model implementation, the speed at which they run, if one can parallelize them, etc. Some common ones that people use are rpart for individual trees, ranger for random forests, and xgboost for boosting. Many others are available too, e.g., the bonsai package has several other methods. Often it doesn‚Äôt matter much which package you use. Sometimes you might need specific features that only a certain package can give you, so you need to go with that one.\nSince tidymodels is still under development, there are many other packages for tree-based and related model fitting that can‚Äôt yet be accessed through it. If you absolutely need one that‚Äôs not yet supported, you can either try to implement it through tidymodels or just write code that uses the model directly.\n\n\nFurther information\nFor more details on the many-tree methods described here, see chapter 8 of ISLR and chapters 10-12 of HMLR.",
    "crumbs": [
      "Content",
      "11. Machine Learning Models I",
      "Many-Tree Models"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/polynomial-spline-models.html",
    "href": "content/module-model-ml-1/polynomial-spline-models.html",
    "title": "Polynomial and Spline Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover some statistical models that provide more flexible extensions to generalized linear models.\n\n\nLearning Objectives\n\nBe familiar with polynomial and spline models.\nUnderstand advantages and disadvantages of these models.\nKnow when to use them and how to minimize overfitting.\n\n\n\nGLM review\nSo far, we‚Äôve focused on generalized linear models (GLM), and mainly on the two workhorses of statistical analysis: linear models for regression modeling of continuous outcomes, and logistic models for classification modeling of categorical (binary) outcomes.\nAs mentioned, what GLMs have in common is that the predictor variables show up as a linear combination, which is then mapped to the outcome with some function. In general, one can write a GLM like this:\n\\[f(Y)=b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n,\\] where \\(Y\\) is the outcome, the \\(X_i\\) are the predictors, and the choice of the function \\(f(Y)\\) (called the link function) determines the specific model.\nFor a standard linear model, it‚Äôs simply \\(f(Y) = Y\\) and the model is \\[\\textrm{linear model:} \\qquad Y=b_0 + b_1X_1 + b_2X_2 + \\ldots + b_n X_n.\\] For a logistic model, it is \\(f(Y)=\\log \\left( \\frac{Y}{1-Y} \\right)\\) (this function is called the logit function) and the whole model becomes \\[\\textrm{logistic model:} \\qquad \\log \\left( \\frac{Y}{1-Y} \\right) = b_0 + b_1X_1 + b_2 X_2 + \\ldots + b_nX_n.\\] You can re-write this such that there is just \\(Y\\) on the left and exponentiated terms of the \\(X\\) on the right, you might have seen either notation for the model before in one of your statistics classes.\nIf, instead, your outcome was integer counts of something, you could use a Poisson model, with \\(f(Y) = log(Y)\\), and the model would be\n\\[\\textrm{Poisson model:} \\qquad \\log \\left( Y \\right) = b_0 + b_1X_1 + b_2 X_2 + \\ldots + b_n X_n.\\]\nSimilar GLMs for outcomes that are distributed in other ways are possible, see e.g.¬†the list on Wikipedia. In R, if you use the glm() function, you can set the link function by specifying the family argument. If you use tidymodels, there are similar ways to set the link function, depending on the underlying fitting function you use.\nThis model structure is easy to understand, one can test the impact of individual predictor variables by taking them in and out of the model (subset selection) or alternative approaches (e.g.¬†regularization), and one can see in the final model which predictor variables survive, and what their impact is, as given by the coefficients in front of the variables.\n\nIf you have a model that has a mix of categorical and continuous input variables, and you want to compare their impact by looking at the values of their coefficients, you need to standardize your continuous variables so they are on a similar scale as your categorical variables.\n\nBasic linear and generalized linear models (LMs and GLMs) such as logistic or poisson regression impose a specific structural relation between your inputs/predictors and your outcomes. This means that if you have more complex patterns relating your inputs with your output(s), basic GLMs might not capture it fully. For instance if you had a scenario where you measured the activity/movement of some animal and how that might be impacted by temperature, it is quite feasible to expect a curved relationship. At low temperatures, the animals try to stay warm and don‚Äôt move much. At high temperature, the animals also don‚Äôt move much and instead try to stay cool. At some intermediate temperature, their movement is largest. So if you had data for animal movement as outcome, and temperature as your predictor variable, a linear model would not be good. You wanted a model that allows the pattern to curve. If you start with a linear model and look at the plot of your residuals, or an outcome-prediction plot, you would see systematic deviations, telling you that your model is not missing important patterns in the data and you are therefore underfitting.\nIf that‚Äôs the case, you might want to move on to more flexible models.\n\nEven if you suspect (or know) that the true relation between input and output is not linear - and in most cases, that is true, - it is still often good to start with a linear model. Often, things are approximately linear, and if you don‚Äôt have a lot of data, have a good bit of noise, and are trying to capture the relation between multiple input variables and the outcome, it is quite possible that the data doesn‚Äôt allow you to go beyond checking for linear relations. So it‚Äôs generally good to start with a linear model, then explore the model results (e.g., residual plots) and if you see indications for underfitting, start extending your model.\n\n\n\nPolynomial models\nA natural extension of GLM is to include higher-order terms for the predictors, either with itself or with other predictor variables (the latter case is called an interaction term). For instance for a simple linear model, this model includes second-order terms for \\(X_1\\) and \\(X_2\\) and also an interaction term between the two variables:\n\\[Y=b_0 + b_1X_1 + b_2X_2 + b_3 X_1^2 + b_4 X_2^2 + b_5 X_1X_2\\]\nSuch models with higher order terms (\\(X^2\\), \\(X^3\\), ‚Ä¶) are often called polynomial models. Note that statisticians still call this a linear model, since the coefficients show up in linear combinations, even though the variables do not. (Statisticians or statistics books will often say that a linear model is ‚Äúlinear in the parameters.‚Äù This means the same thing as having a linear combination of coefficients.) It took me years to figure out that terminology, since a physicist/engineer would call this a non-linear model!\nThe advantage of polynomial models is that they can more flexibly capture patterns in the data. A major shortcoming of polynomial models is that as the predictor values get very large or small, the predicted outcomes go ‚Äúthrough the roof‚Äù (a technical statistical term üòÅ). Let‚Äôs say you had data for the animal movement-temperature example that had temperature up to 30 degrees Celsius and showed a marked decline in movement going from 20C to 30C. A 2nd order polynomial (also called a degree 2 polynomial‚Äìthe number is the highest power that the dependent variable is raised to in the equation) that curved down as temperature went up did a good job at modeling the data. It is quite likely that if you try to predict higher temperatures, say 35C or 40C, movement might turn negative! That of course can‚Äôt happen, an animal can‚Äôt move less than not at all. These are general problems with polynomial functions, they impose an overall structure on the data and generally therefore behave poorly for values outside the range of the data. Thus, while easy to use, those models are generally not the best choice, the ones we discuss next are generally better.\n\n\nSpline models\nSpline models are similar to polynomial models, inasmuch as they allow higher order terms of the predictor variables to show up in the model, and thus can capture patterns that go beyond linear relations. They try to deal with the potential problem of imposing a global structure done by polynomial models by applying polynomial combinations of predictors only to parts of the data, with connection points (knots) between those parts. The end result is a smooth function that allows for capturing of potentially non-linear patterns without the need to impose a global structure. This often improves the quality of the fits. Though even for these kinds of functions, extrapolation outside the range of observed/measured predictor values can be tricky and often lead to wrong results (in fact, a general problem for models). These models are often more difficult (or at least more complicated) to interpret as well.\n\n\nLocal regression\nWhen using ggplot2, you likely came across a local regression curve. The default smoothing for small data if you use geom_smooth() is a LOESS (LOcally Estimated Scatterplot Smoothing) model, which is a type of local regression. You can think of it as being conceptually very similar to spline models, though the details of how it‚Äôs implemented differ. It is perfectly fine to use smoothing functions without making any adjustments during data exploration. But if you want to use any of these for actual statistical fitting, you have to tweak/tune them as discussed below.\n\n\nStep functions\nA version of the models just discussed are those that allow non-smooth jumps in the outcome as a predictor varies. As a simple example, say you want to model current time as function of geographical location. Since humans divided the world arbitrarily into time zones, as you move from one zone to another, there is an abrupt change in the outcome (time) with a small change in the predictor (longitude). A bad way of modeling that is to discretize your predictors, since most often you don‚Äôt know where the cut-offs are. Maybe the example I just gave is a bad one, since this is a rare situation where we do know the cut-offs, because they are human made üòÑ. But say we assume some outcome changes somewhat abruptly with age. It is unlikely that we know exactly what age(s) we should set the cut-off(s) at. By using a model, we can let the computer decide where to set the cut-off (see below). Other model types that are good at handling rapid changes in outcome for a change in a predictor are tree-based models, which you‚Äôll be learning about shortly.\n\n\nGeneralized additive models (GAM)\nGeneralized additive models (GAMs) are in some sense (though not completely, see below) a general form of the different models just discussed. The preserve the feature that each predictor enters the model additively. But now, each predictor can be related to the outcome with a ‚Äì potentially complicated, potentially unknown ‚Äì function. In equation form, a GAM can be written as:\n\\[g(Y) = x_0 + f_1(x1) + f_2(x2) + \\ldots + f_n(x_n).\\] Here, the \\(f_n\\) are functions of the precictor \\(x_n\\), and \\(g()\\) can again be some overall link function.\nFor instance one could have a GAM where some predictor is sinusoidally related to the outcome, e.g., \\(f_1(x_1) = sin(x_1)\\). GAMs allow for more flexible relations between predictors and outcome than GLMs, and are still fairly easy to fit and interpret (but not quite as easy). Also, to provide good functions to map predictors to outcome, one needs to know something about the underlying system. If that‚Äôs not the case, one often uses spline function, and let the data determine the exact shape of the splines. One restriction of GAM is the same as all additive models: having each predictor enter the model separately can lead to interactions being missed.\n\n\nFitting polynomial/spline models\nThe models just discussed are more flexible and allow capturing more intricate patterns in the data. Of course, the downside of this is that they might not only capture patterns that are due to real signals in the data, but might also try to capture spurious patterns that are entirely due to noise/randomness in the data. In other words, these more flexible models are easily prone to overfitting.\nBy now, you have learned what you can do to try and minimize overfitting. Most importantly, you can build your models and decide how complex to make them by cross-validation. You can think of model parameters that determine if you should have a 2nd/3rd/4th/etc. order spline functions as a tuning parameter, and you let the cross-validation procedure decide if a 2nd order or 3rd order model is better ‚Äì based on model performance during the cross-validation process. As you will see, using the tidymodels framework allows you to easily do this model training/tuning.\n\n\nModel fitting in R\nA lot of separate packages exist to fit the models discussed here. Not all, but several of the most important ones, can be accessed through the tidymodels framework. We will explore some of them in the exercises so you get a general idea of how this works. You can then explore more models as interested or as needed for your specific research questions.\n\n\nFurther information\nThis short summary closely followed the information in the Moving Beyond Linearity chapter of ISL. Check out that chapter for further information on the models covered here, as well as a few additional variations. HMLR also discusses several models of this type. See for instance the Multivariate Adaptive Regression Splines chapter of HMLR. The Smoothing chapter of IDS also covers the topic discussed here. For specific ways of using some of these models through tidymodels, check out some of the tutorials in the tidymodels ‚Äúlearn‚Äù section.\nThe references above for each class of models provide further reading. Those 3 sources, namely ISL, IDS and HMLR are very good starting points for learning more about different machine learning methods. The Resources section of the course provides some further pointers to additional material, and of course there is a lot of other, often free, information available online. You should be able to find more details on any of these methods with just a few online searches.",
    "crumbs": [
      "Content",
      "11. Machine Learning Models I",
      "Polynomial and Spline Models"
    ]
  },
  {
    "objectID": "content/module-model-ml-2/da-models.html",
    "href": "content/module-model-ml-2/da-models.html",
    "title": "Discriminant Analysis Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover discriminant analysis, which is a set of machine learning/statistical models that is very useful for classification.\n\n\nLearning Objectives\n\nBe familiar with discriminant analysis models.\nUnderstand advantages and disadvantages of these kinds of models.\n\n\n\nIntroduction\nWhile logistic regression is the most widely used approach for classification, it has several limitations. For one, it doesn‚Äôt easily generalize to outcomes that have more than 2 categories. Also, for certain types of data, logistic regression might not perform very well. Other methods like tree-based models, KNN or SVM can be used for multi-category outcome classification and often provide good performance. Another option are Discriminant Analysis (DA) methods. Those approaches are fairly quick and easy to implement and fit. At times, they provide better performance than logistic regression and might do as well as more complicated models.\n\n\nGeneral Idea behind DA\nDA make assumptions about the distributions for each predictor, and then uses those distributions to model and predict the outcome. This is called a generative approach.\nThere are different forms of DA. Linear Discriminant Analysis (LDA) is the simplest approach, it assumes that predictors can be modeled with a multivariate normal distribution. Nonlinear discriminant analysis approaches, e.g., quadratic DA, allow for a more flexible distribution of the predictors. This makes the model more flexible, but also bigger (so possibly more prone to overfitting). One can show that LDA and logistic regression are mathematically rather similar (see e.g.¬†the ISLR book). Other nonlinear variants of DA exist, e.g.¬†regularized, mixture, flexible, ‚Ä¶\nAs for any model, there is always a bias-variance/underfitting-overfitting trade-off between less flexible approaches like LDA and more flexible approaches like some of the nonlinear DAs.\n\n\nStrengths and Weaknesses of DA\nA nice feature of DA is that it is one of the algorithms that allows for classification with more then 2 outcomes. For two outcomes, there are situations where logistic regression struggles (if the outcomes are well-separated), and DA might perform better.\nDA models are not as common as logistic regression, thus it might require a bit more explanation to your audience if you decide to use them. Also, DA models make assumptions about the distributions of the predictors. If those assumptions do not hold, DA might not perform well. In situations where logistic regression works well, it usually performs better. Like logistic regression, DA is used for classification, not regression.\n\n\nDA in tidymodels\ntidymodels has the discrim pacakge, which implements several forms of discriminant analysis.\n\n\nFurther Resources\nChapter 4 of ISLR discusses discriminant analysis. So does the Generative Models section in the Examples of Algorithms chapter in IDS.",
    "crumbs": [
      "Content",
      "12. Machine Learning Models II",
      "Discriminant Analysis Models"
    ]
  },
  {
    "objectID": "content/module-model-ml-2/svm-models.html",
    "href": "content/module-model-ml-2/svm-models.html",
    "title": "Support Vector Machine Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover support vector machines (SVM), another useful type of machine learning/statistical model.\n\n\nLearning Objectives\n\nBe familiar with SVM.\nUnderstand advantages and disadvantages of SVM.\n\n\n\nIntroduction\nSVM were developed in the computer science and engineering communities and were among the best-performing types of models for a while. They are not that exceptional anymore (now the big trend is neural networks), but still often perform rather well, especially sometimes for data where other models don‚Äôt perform quite as well. So SVM are often a good idea to try.\n\n\nGeneral Idea behind SVM\nSVM have cost/objective functions that do not penalize discrepancy between true and predicted outcome if the two are within a certain range, and do penalize outside that range. This is different than penalties based on basic least squares, which implement a penalty for any difference between model and data. SVM divide the predictor space into areas using multi-dimensional hyperplanes until observations within the same ‚Äúvolume‚Äù are sufficiently accurately predicted (a bit like trees).\nSVM can come in different flavors, for instance depending on how discrepancies between data and model are penalized (absolute distance, quadratic, etc.) or how the boundaries of the hyperplanes are defined.\n\n\nStrengths and Weaknesses of SVM\nSVM can be used for both classification and regression. They are very flexible and often have very good predictive performance. They are often well suited for data that contain outliers.\nThe math behind SVM is not that easy to follow, and since SVM models are somewhat complex, they tend to be a bit of a black box and the model itself is hard to interpret.\n\n\nSVM in tidymodels\ntidymodels/parsnip has several SVM models, such as svm_linear() and svm_poly(). They can be used as part of the tidymodels workflow like any other ML model you have already seen. They also need tuning.\n\n\nFurther Resources\nTo learn a bit more about SVM, you can check out chapter 9 of ISL which discusses SVM. So does chapter 14 of HMLR.",
    "crumbs": [
      "Content",
      "12. Machine Learning Models II",
      "Support Vector Machine Models"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-introduction/ready-introduction.html",
    "href": "content/module-ready-workflow/ready-introduction/ready-introduction.html",
    "title": "READy workflow introduction",
    "section": "",
    "text": "This unit provides an overview of the ideas for a data analysis that follows Reproducible, Efficient, Automated, and Documented (READy) workflow principles.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "READy introduction"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-introduction/ready-introduction.html#footnotes",
    "href": "content/module-ready-workflow/ready-introduction/ready-introduction.html#footnotes",
    "title": "READy workflow introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nREADy is an acronym I invented. If you look around online, you‚Äôll find lots of discussions on the topic of reproducible or automated data analysis, and why documentation is important, but you‚Äôll likely not find anything calling it READy.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "READy introduction"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html",
    "title": "Project organization",
    "section": "",
    "text": "For this unit, we will discuss general principles for organizing your project in a READy way.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#project-structure",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#project-structure",
    "title": "Project organization",
    "section": "Project structure",
    "text": "Project structure\nYou want to set up your analysis in such a way that it makes sense to you and others and allows for a good and efficient workflow. The main components of your analysis will be data, code, results (tables, figures, etc.) and products (reports, interactive apps, slides, etc.).\nYour project could be a Github repository ‚Äî a good idea ‚Äî or not. If you use a software that supports the concept of projects, you should use it. For instance in R, R projects are defined by having a folder structure with an .Rproj file at the top level. Most of the time making use of such project support is recommended.\nBy having any project you work on as an R project, relative paths will always be relative to the main directory in which your .Rproj file is located. As long as someone loads the project by clicking on the .Rproj file, and you only use relative paths, things should work well on any computer.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#folder-structure",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#folder-structure",
    "title": "Project organization",
    "section": "Folder structure",
    "text": "Folder structure\nIn general, the different components of your project should be mapped to folders inside your main project folder.\nInside your folders, you can have subfolders, e.g., separate folders for figures and tables. Or you could have subfolders for different types of analyses. There is no one correct way to set up things, but you should think of a logical and consistent structure before you start your project.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#file-structure",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#file-structure",
    "title": "Project organization",
    "section": "File structure",
    "text": "File structure\nYou will want different files that do specific tasks. You‚Äôll likely have files that contain data in raw and processed form, scripts with code (such as .R files), scripts with results (such as Quarto .qmd files), files that contain images, files that contain the content for tables, etc.\nThe number and type of scripts and other files depends on your project and your preference. In general, keeping things modular is useful. If you had one file that did the analysis and created a report, that would be ok for a small project. But then if you wanted to make a set of slides based on your results, you‚Äôll have to find a way to include the code in those slides. It would be easier to have code produce and save results such as figures, which can then be included in both a report/manuscript and slides.\nAnother consideration is computational time. For simple projects, your code will likely run fast. Once your analyses or data become large, parts of your code might run very long. You will then want to structure your scripts such that the computationally-intensive part is only run when absolutely needed. You definitely want to avoid a scenario where you have to wait minutes or hours as you play around with a figure to make it look the way you want. A good way to split this is to have files/code that does the various pieces (cleaning, processing, exploratory analysis, etc.) that happen before your main analysis (and usually run fairly quickly), then have one or multiple scripts that perform the - potentially computationally expensive - main analysis and save the outcomes from that analysis, and then further scripts that take these results and turn them into good-looking figures and tables. Finally, you‚Äôll have one or a few files that produce manuscripts or reports in an automated fashion by pulling in the different results you generated.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#setting-file-and-folder-paths",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#setting-file-and-folder-paths",
    "title": "Project organization",
    "section": "Setting file and folder paths",
    "text": "Setting file and folder paths\nIt is important to ensure that everything can be reproduced not just on your computer, but on the systems of others.\nA common problem that prevents others to reproduce your work is the use of paths that are specific to only your computer. Do not set paths or load files from paths that only exist on your machine! Instead, you should only use relative paths. A relative path is a file path that is relative to some directory.\nAs an example, loading a file from location C:/myusername/mydesktop/myproject/data/rawdata.csv is a bad idea, because someone else will not have that setup. They will have a different user name, they might be on a Mac or Linux machine, they might have different names for their hard drives, etc.\nInstead, you should load data relative to your main project directory. So for instance reference data/rawdata.csv and use a tool that automatically looks for that folder inside your project folder (in this example, myproject). With this, as long as someone else has a copy of your project and its subfolders, everything should work, no matter where they store your project.\nFor nice video tutorial, see this video on relative versus absolute paths. The setting for this is development of websites, instead of data analysis. So he talks about html files, but that doesn‚Äôt really matter, everything that is explained about the paths applies in general.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#here-r-package",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#here-r-package",
    "title": "Project organization",
    "section": "here R package",
    "text": "here R package\nThe here R package and its associated here() command in R is very useful for working with paths and loading/saving files. It works nicely with R projects. If you use the here command to load a file, it always looks for the file relative to the main project directory. So you can load a file with a command like dat &lt;- read.csv(here::here('data','rawdata,csv')). Malcolm Barrett‚Äôs blog post is a great, short explanation of why one should use the here package and how to do it.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-project-management/ready-project-management.html",
    "href": "content/module-ready-workflow/ready-project-management/ready-project-management.html",
    "title": "Project Management",
    "section": "",
    "text": "For this unit, we will discuss ways to make sure working collaboratively on a project is efficient and that project progress is tracked.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-project-management/ready-project-management.html#file-sizes",
    "href": "content/module-ready-workflow/ready-project-management/ready-project-management.html#file-sizes",
    "title": "Project Management",
    "section": "File sizes",
    "text": "File sizes\nGitHub has a problem with large files. Anything above 100MB will lead to a failure which is annoying and tricky to resolve. In general, files above 50mb will work, be will be slow and give you a warning. Unfortunately, once you have a failed attempt to sync a large file, it can be quite tricky to fix the repository to work again. Therefore keep large (&gt;100MB) files out of your GitHub repositories. If you have to work with such files, try to reduce them first before placing into the GitHub repository. As alternative, place those files in another sync service (e.g.¬†Dropbox, OneDrive, GoogleDrive) and load them from there. Fortunately, there are some nice R packages that can usually make retrieving those files reproducible, even if they are in a file syncing service outside of your repo. Finally, your overall repo must be less than 5GB in size, so consider storing your huge data files and audio/video files somewhere else.\nThere is an option to have larger files with GitHub using Git Large File Storage. This can be a bit tricky, if you think it‚Äôs worth trying, see the documentation here. GitHub Pro users (including students using the Student Developer Kit) have free access to limited LFS features.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-project-management/ready-project-management.html#confidentiality",
    "href": "content/module-ready-workflow/ready-project-management/ready-project-management.html#confidentiality",
    "title": "Project Management",
    "section": "Confidentiality",
    "text": "Confidentiality\nBy default, GitHub repositories are public. Since you might not want to share your analysis publicly, sometimes using a private repository is useful. I think you generally don‚Äôt need to worry about getting scooped. Everyone is so busy with their own stuff, nobody really cares what you are working on üòÅ. However, you might often have data or other information as part of your project that you don‚Äôt want to share publicly.\nTherefore, for most data analysis projects, it is probably good practice to start with a private repository, so you can have potentially confidential information in the repo. You can add collaborators to the private repository. This way only those individuals who you want to give access will be able to see, and potentially contribute to the project.\nEventually, to ensure your project is publicly reproducible, you want to share some version of the data and the code. Be aware that if you make a repository public, others can also see the full history. This can be important if you want to make your final results public - after you made sure everything is anonymous enough for sharing. If at any time you worked in the repository with the full dataset and tracked it, others can go back in your GitHub/Git history and see it. The best option is likely to start a new public repository and copy only the elements you want to share into that repository, while keeping your original repository private.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Project management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-tools/ready-tools.html",
    "href": "content/module-ready-workflow/ready-tools/ready-tools.html",
    "title": "Tools for a READy workflow",
    "section": "",
    "text": "Overview\nThis unit covers some tools that can help with a READy workflow.\n\n\nLearning Objectives\n\nBe familiar with some tools that can help with a READy workflow.\n\n\n\nIntroduction\nThere are many different ways to make a data analysis READy. The following is a brief discussion of some common tools.\n\n\nVersion control software\nVersion control software, such as Git/GitHub, is used for tracking project changes and is discussed in the Project Management unit.\n\n\nQuarto\nQuarto is tool that allows for easy scientific writing in a reproducible manner, by seamlessly integrating code with text. Quarto is further discussed in the Quarto unit.\n\n\nJupyter\nJupyter is another platform that supports combining code with text. I‚Äôm not very familiar with it. As far as I understand it, it doesn‚Äôt support the generation of different outputs that Quarto does. Since Quarto supports the same languages as Jupyter, I am not sure what the advantages are of using Jupyter. There might be some üòÅ.\n\n\nPluto\nFor the Julia programming language, there is Pluto.jl, which has similarities to Jupyter and Quarto. Again, I don‚Äôt have experience with it, so can‚Äôt say what ‚Äì if any ‚Äì advantages it has over Quarto. But if you do some coding in Julia, it might be worth checking out.\n\n\nSweave and such\nThere are some older tools, such as Sweave that can also help in reproducible work. You might still occasionally run into old projects that use it. But in my opinion it is essentially obsolete and you should use Quarto or another of the tools listed above.\n\n\nPipeline tools\nQuarto and similar programs generally chain together a series of commands to get from the initial result to the final product. For instance for a Quarto file, when you render it, it first runs any code, then processes the markdown file, then turns it into the final product.\nMore generally, tools that allow you to automatically run such chains of steps are often referred to as pipeline tools. In R, there is the targets package. Outside of R, Unix/Linux type systems support Make, which lets you combine multiple steps into often complicated workflows. If you find yourself needing a workflow that executes multiple different steps that go beyond what Quarto can execute, the targets package might be worth checking out.\n\n\nOther tools\nIn general, any software tool you use that allows you to document each step - usually in the form of code and comments - should allow others to reproduce your results. Many common software packages and languages such as Mathematica, Matlab, Julia, Python, etc. allow you to add documentation to your code and therefore support reproducibility.\n\n\nHall of shame\nThere is a long list of software that is commonly used for research and data analysis, but which one should stay away from. This encompasses any software that does not contain a full documentation of all steps taken to allow reproducibility.\nExcel and similar spreadsheet software is a major culprit. Using such software for data entry and storage is perfectly fine, as long as one follows guidelines that make later analysis easy, as for instance described in this paper.\nHowever, spreadsheet software is a poor choice for analysis tasks (though nevertheless widely used). The problem with standard spreadsheet software like Excel is that if you make a change to the data and save it, there is usually no track record of what happened. Therefore, any data processing that happens is not reproducible.\nStatistical and data analysis software that relies on graphical user interfaces (GUI), such as e.g.¬†Graphpad Prism or SPSS are equally problematic. Some of these software tools can record the commands you perform through a GUI and you can save those at the end. This does allow for reproducibility. But if that step is missing or not implemented, the analysis is again not reproducible.\nSimilarly, any software to produce graphics by hand (e.g., Photoshop) should not be used. It is ok to use it make schematics, but not to edit any results.\nSimilarly, results tables should be generated automatically and not produced by copying and pasting numbers from one location to another. Again, that means no Excel.\n\n\nSummary\nThere are many useful tools that help you implement a READy workflow. The important part is that things are scripted, automated and repeatable, as well as documented. Any tools that support such a workflow are useful. Stay away from tools that require manual steps.\n\n\nFurther Resources\nNone at the moment. Let me know if you come across any good ones that describe further tools.",
    "crumbs": [
      "Content",
      "2B. READy workflows",
      "Tools for a READy workflow"
    ]
  },
  {
    "objectID": "content/module-stats-overview/descriptive-analysis.html",
    "href": "content/module-stats-overview/descriptive-analysis.html",
    "title": "Descriptive and preliminary analysis",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss descriptive and simple, preliminary statistical analyses.\n\n\nLearning Objectives\n\nKnow what descriptive analysis is\nUnderstand the value of doing preliminary statistical analyses\n\n\n\nIntroduction\nYou usually want to analyze your data with one or several models that are tailored to the question you are trying to address. These models might often include multiple variables and are potentially complex. Instead of starting with those complex models, it is often a good idea to summarize the data and fit simple models to it to gain a better understanding of what to expect from your full analysis.\n\n\nDescriptive analysis\nBefore any statistical analysis, you should summarize and describe your data. Only some of that will end up in a finished product (e.g., a paper or a report), but it‚Äôs important that you do it thoroughly and exhaustively to ensure you fully understand your data and thus know what statistical approaches are and are not appropriate.\nDescriptive analyses generally consist of making tables and figures that explore and summarize the data. Generally, you start with one variable at a time, and likely focus on the most important ones first. That means first you look at your outcomes of interest, then your main predictors/exposures of interest, then as much as feasible at all other variables. For continuous variables, you can use summary statistics (e.g., the summary command in R) or histograms (e.g.¬†the hist command). For categorical variables, you can look at tables (e.g.¬†the table command) or barplots (e.g.¬†the barplot command). The idea is to see if anything unusual or interesting is going on. You might notice problems with the data (e.g., a person with a negative weight) that need cleaning. You might find that you have a categorical variable with a lot of categories that contain only a few entries and only a few categories with a lot of entries (e.g., religion could be such a variable). You might want to flag that for further processing (we‚Äôll talk about that in the next unit).\nOnce you‚Äôve explored each variable by itself, you can start looking at combinations. For instance plots with each outcome of interest on the y-axis and each main predictor on the x-axis could be of interest. Depending on the type of variable for the outcome or predictor (continuous, categorical, etc.), you might want different types of plots, as discussed in the visualization module. Tables are possible too.\nFurther figures of interest might be correlation plots which show correlations between all predictors in your data and might indicate potential issues you need to address before fitting statistical models. For instance if you have height, weight, and BMI in your data, you know that BMI does not have independent information (it‚Äôs a combination of height and weight), and thus these variables are perfectly correlated. Many ‚Äì though not all ‚Äì statistical models don‚Äôt like such strongly correlated variables, so you might need to decide to remove some strongly correlated data beforehand. We‚Äôll talk about that more.\n\n\nPreliminary/exploratory statistical analysis\nSometimes, it is useful to start running some simple models on your data. Sometimes, such models will become part of the main analysis, but often they are just used to help explore the data. For instance if you have two continuous variables and make a scatterplot, adding a linear regression line through the data might be helpful to see if there is a pattern. That is easily done if you make your figure with ggplot2, as you can just add a stats geom to it.\nYou could also explore your data by running simple bivariate statistical models, e.g.¬†by fitting each predictor individually to the outcome of interest. We won‚Äôt talk much about standard regression approaches in this class, but I‚Äôm sure you are familiar with standard statistical tests that can be applied to two variables, depending on the types of variables (linear regression, t-test, etc.).\nA note of caution: It is ok (in my opinion) to use standard statistical tests and readouts (e.g., p-values) to guide main model choices and as suitable, to report them in a final product. It is however only ok to interpret p-values in the hypothesis testing sense if you predefined the hypothesis and the full data generation and analysis protocol. Outside a clinical trial, that‚Äôs almost never done, thus pretty much any reported p-value you see in the literature is improperly used/interpreted. Everyone does it, and pretty much everyone does it wrong. The state of affairs when it comes to statistical literacy, even among PhD holders, is still sad‚Ä¶\n\n\nSummary\nWhile your eventual goal is likely to use tailored, potentially complex models for your analysis, you should always explore the data through a descriptive analysis and fitting of simple (e.g.¬†bi-variate) models. The insights gained doing so will help you refine your main models.\n\n\nFurther Resources\n\nBoth the IDS and R4DS books provide good coverage of descriptive and simple analyses.",
    "crumbs": [
      "Content",
      "7. Statistical Analysis Overview",
      "Descriptive and preliminary analysis"
    ]
  },
  {
    "objectID": "content/module-stats-overview/statistical-terminology.html",
    "href": "content/module-stats-overview/statistical-terminology.html",
    "title": "Statistical Modeling Terminology",
    "section": "",
    "text": "Overview\nThe field of data analysis has, in recent years, seen a lot of progress and changes. While Data Science or Data analysis was long synonymous with Statistics, in past years, other fields such as Computer Science, Engineering, and Business have all started to contribute toward the overall progress in data analytics. With a lot of old and new players engaged in this topic, a lot of new terminologies have emerged. Some of this terminology can be quite confusing, especially if different terms are used for the same concept. Throughout this course, I try to mention alternative names for any concept whenever I introduce it. The following is a brief discussion of the main names given to the task of analyzing data.\nOne of the hot ‚Äì and also confusing ‚Äì terms in recent years is Machine Learning (and the related terms of Data Mining, Deep Learning, and Artificial Intelligence), and how they relate to Statistics. There is a lot of debate about what exactly those different areas are. To get a bit of an idea of what people are talking about, skim through Frank Harrell‚Äôs blog post. As you can see from the post and the many other sources he cites, there is no real agreement on what exactly these terms mean.\nIn my view, it is not worth spending too much time trying to come up with a clear definition. But it‚Äôs good to have some frame of reference so when you see all these terms, you know what they mean. So here are my ‚Äì arguably fuzzy, but hopefully still somewhat useful ‚Äì thoughts on how to distinguish those topics. While making those distinctions can be at times useful, the reality is that the terminology is not clearly defined and all over the place.\nFor additional attempts at defining terms related to data analysis, see the Glossary page ‚Äì and certainly feel free to contribute!\n\n\nStatistics, Machine Learning and Artificial Intelligence\nStatistics is the classical machinery driving data analysis. Depending on the type of data, many different approaches have been developed (parametric vs.¬†non-parametric methods, longitudinal analysis, time-series analysis, and many more). Models are, in general, simple and interpretable, and the goal is to understand how inputs (predictors) relate to outcomes. Statistics was developed when data was sparse, computers didn‚Äôt exist, and mainly scientists interested in a deep understanding of their data used it. Because of this, statistical models tend to be simple and work well on small datasets. Most of classical statistics focuses on associative/exploratory/inferential analysis types.\nIn Machine Learning (ML), the models tend to be more complex, and the goal is mainly on getting a ‚Äúpowerful‚Äù model, i.e., a model that is good at prediction. Understanding how different inputs lead to different outcomes is of secondary importance. Data is often abundant, so more complex models can be used.\nArtificial intelligence (AI) can be considered a type of machine learning. The types of complex models that dominate AI these days are generally based on neural nets. Neural nets sound fancy, and they are certainly quite complex, but technically speaking, you can think of them as a large collection of logistic regression type models combined together.\nAs mentioned before, the terminology is fuzzy. Thus, a fairly simple model like a linear or logistic regression model could be considered a ‚Äúclassical‚Äù statistical model, while a more complex support vector machine or random forest (we‚Äôll visit those later) are generally considered machine learning models. However, the terminology is poorly-defined, and you will see pretty much any approach can be given any label. Often it makes sense to think of the two terms as describing more or less the same thing, and that is applying some kind of mathematical or computational model to gain insight from the data. For some similar ideas, and a bit more details, read Joshua Ebner‚Äôs blog post.\n\n\nRelated Terms\nThe term Data Mining is often used interchangeably with Machine Learning. It might sometimes indicate a ‚Äúfishing‚Äù approach of combing through data to look for patterns, without pre-defined hypotheses to be tested. As such, results from Data Mining explorations need to be confirmed with independent data. Exploratory Data Analysis or Secondary Data Analysis mean pretty much the same thing, though the latter are often done using smaller (Statistics) models as opposed to larger (Machine Learning) models. Again, terminology and use of the words is fuzzy.\nThe term Deep Learning is generally used when one class of machine learning models, namely neural nets, are applied to a data analysis problem. The ‚Äúdeep‚Äù part comes from the fact that the neural net models often have many stacked layers (it has nothing to do with deep as in especially insightful).\nThe use of deep learning and related approaches applied to ‚Äúcomplex‚Äù problems is often labeled Artificial Intelligence.\nNeural nets (and similar complex methods) usually need a lot of data to perform well. Thus, the term Big Data often shows up together with these other terms.\nIn general, there is currently a lot of hype around these topics, and people ‚Äì especially in the business, but also the research community ‚Äì use the words Deep Learning and Artificial Intelligence quite liberally, even if all they do is fit a linear model to data.\nWe‚Äôll discuss these topics a bit more at the end of the course, but since most scientific inquiry focuses on understanding patterns (i.e., questions tend to be inferential/causal/mechanistic) and amounts of data ‚Äì while growing ‚Äì still tend to be on the small side. So we won‚Äôt spend much time on it.\n\n\nTerminology abuse\nIn the last several years, there has been a tremendous interest among industry in anything related to Data Science. Most notable, the huge successes of AI in some areas and for some companies (Google, Amazon, Uber, ‚Ä¶) has meant that suddenly everyone wants AI and everyone sells AI, with a lot of players not having a clue what it is and can/can‚Äôt do. This has led to a lot of marketing hype and another host of acronyms. (For instance, I was recently at an online industry workshop with the title (quoting from memory) ‚ÄúFrom AI over BI to CI‚Äù - which apparently stand for Artificial Intelligence, Business Intelligence and Continuous Intelligence.) Most often, these terms can mean whatever you want them to mean. So some companies who have been doing data analysis by running simple linear or logistic regression models are now calling what they are doing AI. In the end, the only way to know what anyone is actually doing is to look at the detailed description of their methods. And if those are not provided, be very skeptical. Most of all, don‚Äôt be unduly impressed by all these big words. There is good data analysis, and there is bad data analysis, and just because someone uses a deep neural net AI model doesn‚Äôt mean what they are doing is any good. Critical and careful thinking about your data and the question you want to answer always trumps any new fancy modeling approach.\n\n\nFurther reading\nThis course has an optional module on Deep Learning, AI and Big Data. This is a very brief introduction you can check out if you are interested. For a broader introduction, the (non-free) book Artificial Intelligence: A Guide for Thinking Humans is a very good non-technical overview book. And of course, topics like ML and AI are so ‚Äúhot‚Äù these days, tons of free resources are available online‚Äìthough be sure to look at the writer‚Äôs credentials and think critically when reading online blogs.",
    "crumbs": [
      "Content",
      "7. Statistical Analysis Overview",
      "Some Statistical Model Fitting Terminology"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html",
    "title": "Scrambling existing data - R",
    "section": "",
    "text": "In this tutorial, we discuss how to scramble existing data to make it ‚Äúnew‚Äù.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Scrambling existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#ai-help",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#ai-help",
    "title": "Scrambling existing data - R",
    "section": "AI help",
    "text": "AI help\nSince you are working with the real data, you probably don‚Äôt want to use AI for this, unless your AI tool operates in a secure environment (e.g., fully on your companies‚Äô servers).",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Scrambling existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#example",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#example",
    "title": "Scrambling existing data - R",
    "section": "Example",
    "text": "Example\nTime for a simple example. You can find the code shown below in this file.\n\nSetup\nFirst, we do the usual setup steps of package loading and other housekeeping steps.\n\nlibrary('dplyr')\nlibrary('ggplot2')\nlibrary('here')\n\n\n# setting a random number seed for reproducibility\nset.seed(123)\n\n\n\nData loading and exploring\nWe‚Äôll look at some real data from this paper. As is good habit (and should be the standard), the authors (which includes some of us) supplied the data as part of the supplementary materials, which can be found here.\nIf you want to work along, go ahead and download the supplement, which is a zip file. Inside the zip file, find the Clean Data folder and the SympAct_Any_Pos.Rda file. Copy that file to the location where you‚Äôll be placing your R script.\nFirst, we load the data. Note that the authors (that would be us üòè) used the wrong file ending, they called it an .Rda file, even though it is an .Rds file (for a discussion of the differences, see e.g.¬†here).\n\n\nThe data\n\n#assuming your R script is in the same folder\n#rawdat &lt;- readRDS('SympAct_Any_Pos.Rda')\n# this is for my setup\nrawdat &lt;- readRDS(here::here('data','SympAct_Any_Pos.Rda'))\n\nNext, we take a peek.\n\ndim(rawdat)\n\n[1] 735  63\n\ndplyr::glimpse(rawdat)  \n\nRows: 735\nColumns: 63\n$ DxName1           &lt;fct&gt; \"Influenza like illness - Clinical Dx\", \"Acute tonsi‚Ä¶\n$ DxName2           &lt;fct&gt; NA, \"Influenza like illness - Clinical Dx\", \"Acute p‚Ä¶\n$ DxName3           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Fever, unspecified\"‚Ä¶\n$ DxName4           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Other fatigue\", NA,‚Ä¶\n$ DxName5           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Headache\", NA, NA, ‚Ä¶\n$ Unique.Visit      &lt;chr&gt; \"340_17632125\", \"340_17794836\", \"342_17737773\", \"342‚Ä¶\n$ ActivityLevel     &lt;int&gt; 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, ‚Ä¶\n$ ActivityLevelF    &lt;fct&gt; 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, ‚Ä¶\n$ SwollenLymphNodes &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y‚Ä¶\n$ ChestCongestion   &lt;fct&gt; No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ ChillsSweats      &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, ‚Ä¶\n$ NasalCongestion   &lt;fct&gt; No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ CoughYN           &lt;fct&gt; Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, ‚Ä¶\n$ Sneeze            &lt;fct&gt; No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, ‚Ä¶\n$ Fatigue           &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ SubjectiveFever   &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes‚Ä¶\n$ Headache          &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes‚Ä¶\n$ Weakness          &lt;fct&gt; Mild, Severe, Severe, Severe, Moderate, Moderate, Mi‚Ä¶\n$ WeaknessYN        &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ CoughIntensity    &lt;fct&gt; Severe, Severe, Mild, Moderate, None, Moderate, Seve‚Ä¶\n$ CoughYN2          &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes‚Ä¶\n$ Myalgia           &lt;fct&gt; Mild, Severe, Severe, Severe, Mild, Moderate, Mild, ‚Ä¶\n$ MyalgiaYN         &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ RunnyNose         &lt;fct&gt; No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No‚Ä¶\n$ AbPain            &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N‚Ä¶\n$ ChestPain         &lt;fct&gt; No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, ‚Ä¶\n$ Diarrhea          &lt;fct&gt; No, No, No, No, No, Yes, No, No, No, No, No, No, No,‚Ä¶\n$ EyePn             &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye‚Ä¶\n$ Insomnia          &lt;fct&gt; No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ ItchyEye          &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, Yes,‚Ä¶\n$ Nausea            &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y‚Ä¶\n$ EarPn             &lt;fct&gt; No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y‚Ä¶\n$ Hearing           &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, No, No, No, No,‚Ä¶\n$ Pharyngitis       &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, ‚Ä¶\n$ Breathless        &lt;fct&gt; No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, ‚Ä¶\n$ ToothPn           &lt;fct&gt; No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N‚Ä¶\n$ Vision            &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, ‚Ä¶\n$ Vomit             &lt;fct&gt; No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N‚Ä¶\n$ Wheeze            &lt;fct&gt; No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N‚Ä¶\n$ BodyTemp          &lt;dbl&gt; 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, ‚Ä¶\n$ RapidFluA         &lt;fct&gt; Presumptive Negative For Influenza A, NA, Presumptiv‚Ä¶\n$ RapidFluB         &lt;fct&gt; Presumptive Negative For Influenza B, NA, Presumptiv‚Ä¶\n$ PCRFluA           &lt;fct&gt; NA, NA, NA, NA, NA, NA,  Influenza A Not Detected, N‚Ä¶\n$ PCRFluB           &lt;fct&gt; NA, NA, NA, NA, NA, NA,  Influenza B Not Detected, N‚Ä¶\n$ TransScore1       &lt;dbl&gt; 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5‚Ä¶\n$ TransScore1F      &lt;fct&gt; 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5‚Ä¶\n$ TransScore2       &lt;dbl&gt; 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4‚Ä¶\n$ TransScore2F      &lt;fct&gt; 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4‚Ä¶\n$ TransScore3       &lt;dbl&gt; 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3‚Ä¶\n$ TransScore3F      &lt;fct&gt; 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3‚Ä¶\n$ TransScore4       &lt;dbl&gt; 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4‚Ä¶\n$ TransScore4F      &lt;fct&gt; 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4‚Ä¶\n$ ImpactScore       &lt;int&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ ImpactScore2      &lt;int&gt; 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,‚Ä¶\n$ ImpactScore3      &lt;int&gt; 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, ‚Ä¶\n$ ImpactScoreF      &lt;fct&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ ImpactScore2F     &lt;fct&gt; 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,‚Ä¶\n$ ImpactScore3F     &lt;fct&gt; 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, ‚Ä¶\n$ ImpactScoreFD     &lt;fct&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ TotalSymp1        &lt;dbl&gt; 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1‚Ä¶\n$ TotalSymp1F       &lt;fct&gt; 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1‚Ä¶\n$ TotalSymp2        &lt;dbl&gt; 8, 10, 17, 16, 11, 14, 10, 11, 13, 10, 14, 19, 13, 1‚Ä¶\n$ TotalSymp3        &lt;dbl&gt; 8, 9, 16, 15, 11, 14, 10, 10, 12, 9, 13, 18, 12, 16,‚Ä¶\n\n\nSo it looks like these are 735 individuals (rows) and 63 variables (columns). A lot of them have names of symptoms and are coded as Yes/No.¬†Some variables are harder to understand, for instance without some meta-data/explanation, it is impossible to guess what TransScore3F stands for. Hopefully, your data came with some codebook/data dictionary/information sheet that explains what exactly everything means. For this specific data set, you can look through the supplementary materials to learn more. We won‚Äôt delve into it now, and just pick out a few variables to illustrate the data scrambling process.\n\n\nData processing\nFor simplicity, let‚Äôs assume we are interested in just a few of these variables, namely ActivityLevel, Sneeze, Nausea, and Vomit. We‚Äôll select those and look at the first 10 entries.\n\ndat &lt;- rawdat |&gt; dplyr::select(\"ActivityLevel\",\"Sneeze\",\"Nausea\",\"Vomit\")\nhead(dat,10)\n\n   ActivityLevel Sneeze Nausea Vomit\n1             10     No     No    No\n2              6     No     No    No\n3              2    Yes    Yes    No\n4              2    Yes    Yes    No\n5              5     No    Yes    No\n6              3    Yes    Yes    No\n7              4     No     No   Yes\n8              0    Yes     No    No\n9              0     No    Yes    No\n10             5     No    Yes    No\n\n\n\n\nData Scrambling\nNow we‚Äôll scramble the data. I‚Äôm doing this here with a simple loop. I‚Äôm looping through each variable, and I sample from the old values without replacement, which basically just rearranges them. There are computationally faster and more concise ways of doing this, but the loop makes it hopefully very clear what‚Äôs going on.\n\n# define a new data frame that will contain scrambled values\ndat_sc &lt;- dat\nNobs = nrow(dat) #number of observations\n# loop over each variable, reshuffle entries\nfor (n in 1:ncol(dat))\n{\n  dat_sc[,n] &lt;- sample(dat[,n], size = Nobs, replace = FALSE)\n}\n\nhead(dat_sc,10)\n\n   ActivityLevel Sneeze Nausea Vomit\n1              5     No     No    No\n2              0     No     No    No\n3              3    Yes     No    No\n4              5     No    Yes    No\n5              5     No    Yes    No\n6              1    Yes     No    No\n7              3    Yes     No    No\n8              8    Yes     No    No\n9              6    Yes     No    No\n10             4    Yes    Yes    No\n\n\nThe first 10 entries look different, so that‚Äôs promising.\n\n\nComparing old and new data\nNow let‚Äôs see if things worked. First, we summarize both the old and the new data. We should see that they are the same, since we just re-arranged the values across individuals. This is indeed the case.\n\nsummary(dat)\n\n ActivityLevel    Sneeze    Nausea    Vomit    \n Min.   : 0.000   No :340   No :477   No :656  \n 1st Qu.: 3.000   Yes:395   Yes:258   Yes: 79  \n Median : 4.000                                \n Mean   : 4.463                                \n 3rd Qu.: 6.000                                \n Max.   :10.000                                \n\nsummary(dat_sc)\n\n ActivityLevel    Sneeze    Nausea    Vomit    \n Min.   : 0.000   No :340   No :477   No :656  \n 1st Qu.: 3.000   Yes:395   Yes:258   Yes: 79  \n Median : 4.000                                \n Mean   : 4.463                                \n 3rd Qu.: 6.000                                \n Max.   :10.000                                \n\n\nWe can also look at correlations between variables. Here is where we run into the above-mentioned problems. Correlations that might exist in the original data can be wiped out. We see that here. In the original data, more individuals (approximately 63% + 9%) reported either absence or presence of both nausea and vomiting. In the scrambled data, this dropped to around 58% + 4%. We would expect that these 2 symptoms are somewhat related, and the scrambling removed it. Similarily, the original data showed lower activity levels for those with vomit as symptom. This pattern is gone in the scrambled data.\n\n# cross-tabulation of 2 symptoms\ntb1=table(dat$Nausea,dat$Vomit)\nprop.table(tb1)*100 #as percentage\n\n     \n             No       Yes\n  No  62.993197  1.904762\n  Yes 26.258503  8.843537\n\ntb2=table(dat_sc$Nausea,dat_sc$Vomit)\nprop.table(tb2)*100\n\n     \n             No       Yes\n  No  58.095238  6.802721\n  Yes 31.156463  3.945578\n\n# looking at possible correlation between activity level and Vomit\np1 &lt;- dat |&gt; ggplot(aes(x=Vomit,y=ActivityLevel)) + geom_boxplot()\nplot(p1)\n\n\n\n\n\n\n\np2 &lt;- dat_sc |&gt; ggplot(aes(x=Vomit,y=ActivityLevel)) + geom_boxplot()\nplot(p2)\n\n\n\n\n\n\n\n\nThat means any statistical conclusions based on the scrambled data are not valid. This kind of data is just useful at testing the overall workflow and making sure everything can run, but one can‚Äôt conclude anything from it.\nIt is of course possible to try to scramble while preserving potential correlations, but that gets tricky and at this stage one might maybe just re-create the data based on some of the concepts discussed in the previous unit.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Scrambling existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-model-testing-r/synthetic-data-model-testing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-model-testing-r/synthetic-data-model-testing-r.html",
    "title": "Use synthetic data to test models",
    "section": "",
    "text": "Overview\nIn this unit, we explore the use of synthetic data to test your models.\n\n\nLearning Objectives\n\nUnderstand the use of synthetic data for testing models.\nBe able to implement model testing on synthetic data in R.\n\n\n\nIntroduction\nAt the beginning of this module, I mentioned that one great use-case of synthetic data is the ability to see if whatever patterns you stuck into the data when you generated it can be recovered by your analysis. If your analysis fails to recover what you built, it means your model is likely not suitable and you need to modify it.\nOf course, keep in mind that just because your model can recover the patterns/correlations you stuck into the data doesn‚Äôt mean it can do the same for the real data, or that any patterns in the real data are the same. However, if you can NOT recover the patterns for your simulated data, it‚Äôs a strong sign to stop and reconsider your analysis approach. It will very likely also not produce useful results for the real data (but you won‚Äôt know, since you don‚Äôt know what processes generated the real data).\n\n\nAn example\nWe will revisit the first example from the Generating synthetic data with R unit. If you don‚Äôt remember, take a look at the code shown in that unit to see that we created a variable called BloodPressure which was randomly distributed, and a variable called Cholesterol which varied by treatment group.\nIf we fit a model, we expect to recover these patterns. We can for instance fit a linear model with Cholesterol as the outcome and BloodPressure and TreatmentGroup as predictors.\nHere‚Äôs code to do that (you can again find the full R script here).\nSetting things up\n\n# Load the readr package for reading csv data\nlibrary(readr)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(parsnip)\nlibrary(here)\n\n# Read in the csv data \ndata &lt;- read_csv(here(\"data\",\"syn_dat.csv\"))\n\nWe‚Äôll do a little bit of processing before fitting\n\n# select variables of interest\n# not strictly needed, but can sometimes make for more robust code to only keep what's necessary\ndata &lt;- data %&gt;%  \n  select(Cholesterol, BloodPressure, TreatmentGroup)\n\n# Standardize BloodPressure and Cholesterol\n# Helps with interpretation of coefficients\ndata &lt;- data %&gt;% \n  mutate(\n    BloodPressure = scale(BloodPressure),\n    Cholesterol = scale(Cholesterol)\n  )\n# turn TreatmentGroup into a factor\ndata$TreatmentGroup &lt;- as.factor(data$TreatmentGroup)\n\n# check to make sure data looks ok before fitting\nsummary(data)\n\n    Cholesterol.V1      BloodPressure.V1   TreatmentGroup\n Min.   :-2.3886369   Min.   :-1.7201173   A      :43    \n 1st Qu.:-0.7631619   1st Qu.:-0.8109520   B      :30    \n Median : 0.0587370   Median : 0.1357530   Placebo:27    \n Mean   : 0.0000000   Mean   : 0.0000000                 \n 3rd Qu.: 0.7052625   3rd Qu.: 0.9111831                 \n Max.   : 2.5375210   Max.   : 1.4801446                 \n\n\nFitting the first model\n\n# Fit linear model \nmodel1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol ~ BloodPressure + TreatmentGroup, data = data)\ntidy(model1)\n\n# A tibble: 4 √ó 5\n  term                  estimate std.error statistic  p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)            -0.898     0.0822    -10.9  1.58e-18\n2 BloodPressure          -0.0822    0.0548     -1.50 1.37e- 1\n3 TreatmentGroupB         1.19      0.129       9.24 6.48e-15\n4 TreatmentGroupPlacebo   2.00      0.133      15.1  4.05e-27\n\nglance(model1)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.718         0.709 0.539      81.6 2.61e-26     3  -78.0  166.  179.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nWe find what we hope to find. The blood pressure variable shows no noticeable correlation with cholesterol, while treatment group does.\nWe can explore other models. Here is one with an interaction term between BloodPressure and TreatmentGroup. We know there is none, since we know how the data was generated. So to test our model, we fit it to confirm that this is what we get:\n\n# fit a model with interaction\nmodel2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol ~ BloodPressure + TreatmentGroup + BloodPressure*TreatmentGroup , data = data)\ntidy(model2)\n\n# A tibble: 6 √ó 5\n  term                                estimate std.error statistic  p.value\n  &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                          -0.901     0.0825   -10.9   2.15e-18\n2 BloodPressure                        -0.139     0.0766    -1.82  7.18e- 2\n3 TreatmentGroupB                       1.19      0.131      9.10  1.52e-14\n4 TreatmentGroupPlacebo                 2.02      0.134     15.1   8.90e-27\n5 BloodPressure:TreatmentGroupB         0.0856    0.129      0.665 5.08e- 1\n6 BloodPressure:TreatmentGroupPlacebo   0.163     0.144      1.13  2.60e- 1\n\nglance(model2)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.722         0.708 0.541      48.9 1.09e-24     5  -77.3  169.  187.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nAnd we do see no evidence for interaction being present. So for this simple example, the model works, namely it recovers the patterns we stuck into the data. This means it might be ok to use this model on our real data.\nLet‚Äôs now look at an example where the model does not properly recover the pattern in the data.\n\n\nA failing example\nWe‚Äôll start with the same data as before, but now mess around with it. Specifically, I‚Äôm creating a new variable called Drug and a new cholesterol variable that depends on drug concentration in a nonlinear manner.\n\n# Load the readr package for reading csv data\nlibrary(readr)\nlibrary(dplyr)\nlibrary(here)\nlibrary(tidymodels)\nlibrary(vip)\n\n# Read in the csv data \nset.seed(123)\ndata &lt;- read_csv(here(\"data\",\"syn_dat.csv\"))\n\n\n# Make Cholesterol depend on blood pressure in a nonlinear manner\ndata$Drug &lt;- runif(nrow(data),min = 100, max = 200)\n# making up Cholesterol value as function of Drug\n# this equation makes it such that cholesterol depends on drug in a nonlinear manner\ndata$Cholesterol2 &lt;- sqrt(100^2 + 10* (mean(data$Drug) -  data$Drug)^2)\n# plotting correlation between Drug and new cholesterol variables\nggplot(data,aes(x=Drug, y=Cholesterol2)) + geom_point()\n\n\n\n\n\n\n\n\n\n# select variables of interest\n# not strictly needed, but can sometimes make for more robust code to only keep what's necessary\ndata &lt;- data %&gt;%  \n  select(Cholesterol2, Drug, BloodPressure, TreatmentGroup)\n\n# Standardize continuous variables \n# Helps with interpretation of coefficients\ndata &lt;- data %&gt;% \n  mutate(\n    BloodPressure = scale(BloodPressure),\n    Drug = scale(Drug),\n    Cholesterol2 = scale(Cholesterol2)\n  )\n# turn TreatmentGroup into a factor\ndata$TreatmentGroup &lt;- as.factor(data$TreatmentGroup)\n\n# check to make sure data looks ok before fitting\nsummary(data)\n\n   Cholesterol2.V1          Drug.V1          BloodPressure.V1   TreatmentGroup\n Min.   :-1.2222251   Min.   :-1.7471666   Min.   :-1.7201173   A      :43    \n 1st Qu.:-0.9203239   1st Qu.:-0.8880440   1st Qu.:-0.8109520   B      :30    \n Median :-0.1257083   Median :-0.1129431   Median : 0.1357530   Placebo:27    \n Mean   : 0.0000000   Mean   : 0.0000000   Mean   : 0.0000000                 \n 3rd Qu.: 0.8741998   3rd Qu.: 0.9014615   3rd Qu.: 0.9111831                 \n Max.   : 2.1080599   Max.   : 1.7393649   Max.   : 1.4801446                 \n\n\nNow we‚Äôll fit another linear model, as before. There is a correlation between dose and cholesterol. But it‚Äôs not linear, and therefore the model doesn‚Äôt detect it.\n\n# Fit linear model \nmodel1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol2 ~ Drug + BloodPressure + TreatmentGroup, data = data)\nbroom::tidy(model1)\n\n# A tibble: 5 √ó 5\n  term                  estimate std.error statistic p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)             0.101      0.155     0.653   0.515\n2 Drug                    0.0353     0.103     0.343   0.732\n3 BloodPressure           0.0574     0.103     0.557   0.579\n4 TreatmentGroupB        -0.247      0.243    -1.02    0.311\n5 TreatmentGroupPlacebo  -0.0997     0.251    -0.398   0.692\n\nbroom::glance(model1)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0142       -0.0273  1.01     0.343   0.848     4  -141.  293.  309.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nIt is always possible to try a more complex model to see if there might be patterns that a linear or other simple model can‚Äôt detect. Here, we are trying a random forest model, which can detect more complicated correlations between predictor variables and output. Random forest models don‚Äôt produce the standard p-values, but one can look at something called variable importance to see which variables most impact the outcome. And we see that it correctly identifies drug as the most important variable. The other two should really be at zero importance, since they are not correlated with the outcome. But this model is flexible enough to fit to possibly spurious patterns.\n\n# fit a random forest model \n# use the workflow approach from tidymodels\nrf_mod &lt;- rand_forest(mode = \"regression\") %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\")\n# the recipe, i.e., the model to fit\nrf_recipe &lt;- \n  recipe(Cholesterol2 ~ Drug + BloodPressure + TreatmentGroup, data = data) \n\n# set up the workflow\nrf_workflow &lt;- \n  workflow() %&gt;% \n  add_model(rf_mod) %&gt;% \n  add_recipe(rf_recipe)\n\n# run the fit\nmodel2 &lt;- rf_workflow %&gt;% \n      fit(data)\n\n# get variable importance\nimp &lt;- model2 %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\nprint(imp)\n\n\n\n\n\n\n\n\nThe take-home message from this is that our simulated data showed us that a linear model can‚Äôt pick up the pattern, and we need a different model.\n\n\nSummary\nModel testing is one of the most important applications for synthetic data. Since you generated the data and know everything about it, you know exactly what the analysis of the data should find. If your models can‚Äôt find the right patterns, it means you need to modify your analysis.\n\n\nFurther Resources",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Use synthetic data to test models"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html",
    "title": "Generating synthetic data with R",
    "section": "",
    "text": "In this unit, we look at a few examples of generating synthetic data with R.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup",
    "title": "Generating synthetic data with R",
    "section": "Setup",
    "text": "Setup\nWe‚Äôll start by loading the packages used in the code below. While we will not use packages that are specifically meant to generate synthetic data, we will still use several common packages that make data generation tasks easier. That said, you could also do all of this with base R and no additional packages.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(ggplot2)\nlibrary(here)\n\n\n# Set a seed for reproducibility\nset.seed(123)\n# Define the number of observations (patients) to generate\nn_patients &lt;- 100",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data",
    "title": "Generating synthetic data with R",
    "section": "Generating data",
    "text": "Generating data\n\n# Create an empty data frame with placeholders for variables\nsyn_dat &lt;- data.frame(\n  PatientID = numeric(n_patients),\n  Age = numeric(n_patients),\n  Gender = character(n_patients),\n  TreatmentGroup = character(n_patients),\n  EnrollmentDate = lubridate::as_date(character(n_patients)),\n  BloodPressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  AdverseEvent = integer(n_patients)\n)\n\n# Variable 1: Patient ID\nsyn_dat$PatientID &lt;- 1:n_patients\n\n# Variable 2: Age (numeric variable)\nsyn_dat$Age &lt;- round(rnorm(n_patients, mean = 45, sd = 10), 1)\n\n# Variable 3: Gender (categorical variable)\nsyn_dat$Gender &lt;- purrr::map_chr(sample(c(\"Male\", \"Female\"), n_patients, replace = TRUE), as.character)\n\n# Variable 4: Treatment Group (categorical variable)\nsyn_dat$TreatmentGroup &lt;- purrr::map_chr(sample(c(\"A\", \"B\", \"Placebo\"), n_patients, replace = TRUE), as.character)\n\n# Variable 5: Date of Enrollment (date variable)\nsyn_dat$EnrollmentDate &lt;- lubridate::as_date(sample(seq(from = lubridate::as_date(\"2022-01-01\"), to = lubridate::as_date(\"2022-12-31\"), by = \"days\"), n_patients, replace = TRUE))\n\n# Variable 6: Blood Pressure (numeric variable)\nsyn_dat$BloodPressure &lt;- round(runif(n_patients, min = 90, max = 160), 1)\n\n# Variable 7: Cholesterol Level (numeric variable)\n# Option 1: Cholesterol is independent of treatment\n#syn_dat$Cholesterol &lt;- round(rnorm(n_patients, mean = 200, sd = 30), 1)\n\n# Option 2: Cholesterol is dependent on treatment\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"A\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"A\"), mean = 160, sd = 10), 1)\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"B\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"B\"), mean = 180, sd = 10), 1)\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"Placebo\"), mean = 200, sd = 10), 1)\n\n# Variable 8: Adverse Event (binary variable, 0 = No, 1 = Yes)\n# Option 1: Adverse events are independent of treatment\n#syn_dat$AdverseEvent &lt;- purrr::map_int(sample(0:1, n_patients, replace = TRUE, prob = c(0.8, 0.2)), as.integer)\n\n# Option 2: Adverse events are influenced by treatment status\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"A\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"A\"), replace = TRUE, prob = c(0.5, 0.5)), as.integer)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"B\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"B\"), replace = TRUE, prob = c(0.7, 0.3)), as.integer)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"Placebo\"), replace = TRUE, prob = c(0.9, 0.1)), as.integer)\n\n\n# Print the first few rows of the generated data\nhead(syn_dat)\n\n  PatientID  Age Gender TreatmentGroup EnrollmentDate BloodPressure Cholesterol\n1         1 39.4 Female              B     2022-08-25         152.0       179.7\n2         2 42.7 Female              B     2022-06-14         128.7       192.2\n3         3 60.6 Female              A     2022-04-17         153.4       150.6\n4         4 45.7   Male              B     2022-02-02         131.1       171.7\n5         5 46.3 Female              A     2022-03-24         119.6       160.5\n6         6 62.2 Female              A     2022-12-20         156.5       154.3\n  AdverseEvent\n1            1\n2            0\n3            0\n4            0\n5            1\n6            1\n\n# Save the simulated data to a CSV and Rds file\nwrite.csv(syn_dat, here(\"data\",\"syn_dat.csv\"), row.names = FALSE)\n# if we wanted an RDS version\n#saveRDS(syn_dat, here(\"data\",\"syn_dat.Rds\"))",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data",
    "title": "Generating synthetic data with R",
    "section": "Checking data",
    "text": "Checking data\nTake a peek at the generated data.\n\nsummary(syn_dat)\n\n   PatientID           Age           Gender          TreatmentGroup    \n Min.   :  1.00   Min.   :21.90   Length:100         Length:100        \n 1st Qu.: 25.75   1st Qu.:40.08   Class :character   Class :character  \n Median : 50.50   Median :45.60   Mode  :character   Mode  :character  \n Mean   : 50.50   Mean   :45.90                                        \n 3rd Qu.: 75.25   3rd Qu.:51.92                                        \n Max.   :100.00   Max.   :66.90                                        \n EnrollmentDate       BloodPressure    Cholesterol     AdverseEvent \n Min.   :2022-01-08   Min.   : 91.3   Min.   :129.6   Min.   :0.00  \n 1st Qu.:2022-04-06   1st Qu.:110.7   1st Qu.:160.7   1st Qu.:0.00  \n Median :2022-06-25   Median :130.8   Median :176.3   Median :0.00  \n Mean   :2022-06-30   Mean   :128.0   Mean   :175.2   Mean   :0.29  \n 3rd Qu.:2022-10-04   3rd Qu.:147.4   3rd Qu.:188.7   3rd Qu.:1.00  \n Max.   :2022-12-30   Max.   :159.5   Max.   :223.7   Max.   :1.00  \n\ndplyr::glimpse(syn_dat)  \n\nRows: 100\nColumns: 8\n$ PatientID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ Age            &lt;dbl&gt; 39.4, 42.7, 60.6, 45.7, 46.3, 62.2, 49.6, 32.3, 38.1, 4‚Ä¶\n$ Gender         &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Male\", \"Female\", \"Female‚Ä¶\n$ TreatmentGroup &lt;chr&gt; \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"Placebo\", \"A\", \"Pla‚Ä¶\n$ EnrollmentDate &lt;date&gt; 2022-08-25, 2022-06-14, 2022-04-17, 2022-02-02, 2022-0‚Ä¶\n$ BloodPressure  &lt;dbl&gt; 152.0, 128.7, 153.4, 131.1, 119.6, 156.5, 139.6, 118.9,‚Ä¶\n$ Cholesterol    &lt;dbl&gt; 179.7, 192.2, 150.6, 171.7, 160.5, 154.3, 172.8, 189.2,‚Ä¶\n$ AdverseEvent   &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1‚Ä¶\n\n# Frequency table for adverse events stratified by treatment\ntable(syn_dat$AdverseEvent,syn_dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 28 19      24\n  1 15 11       3\n\n# ggplot2 boxplot for cholesterol by treatment group\nggplot(syn_dat, aes(x = TreatmentGroup, y = Cholesterol)) +\n  geom_boxplot() +\n  labs(x = \"Treatment Group\", y = \"Cholesterol Level\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis concludes our first example. This was a very simple setup, with rectangular data. You often encounter these kind of data in clinical trials. As you generate your data, you can build in any dependencies between variables you want to explore. Then later, when you use the data to test your analysis code, you can see if your analysis code can detect the dependencies you built in. We‚Äôll come back to that.\nYou could also add further complexities into your synthetic data, for instance you could set some values to be missing, or you could add some outliers. The goal is to generate data that has as the important features of your real dataset to allow you to test your analysis approach on data where you now the truth (since you generated it). If your analysis works on your generated data, there is hope it might also work on the real data (for which of course you don‚Äôt know the truth). We‚Äôll define in a later unit what we mean by ‚Äúyour analysis works‚Äù. But basically, you want to be able to recover the patterns/dependencies you built into your data with your analysis methods.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup-1",
    "title": "Generating synthetic data with R",
    "section": "Setup",
    "text": "Setup\nThe usual setup steps.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(here)\n\n\n# Set seed for reproducibility\nset.seed(123)\n# Number of patients in each treatment group\nnum_patients &lt;- 20\n# Number of days and samples per patient\nnum_days &lt;- 7\nnum_samples_per_day &lt;- 1",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data-1",
    "title": "Generating synthetic data with R",
    "section": "Generating data",
    "text": "Generating data\n\n# Treatment group levels\ntreatment_groups &lt;- c(\"Low Dose\", \"High Dose\")\n\n# Generate patient IDs\npatient_ids &lt;- rep(1:num_patients, each = num_days)\n\n# Generate treatment group assignments for each patient\ntreatment_assignments &lt;- rep(sample(treatment_groups, num_patients, replace = TRUE), \n                             each = num_days)\n\n# Generate day IDs for each patient\nday_ids &lt;- rep(1:num_days, times = num_patients)\n\n# Function to generate drug concentrations with variability\ngenerate_drug_concentrations &lt;- function(day, dose_group, patient_id) {\n  baseline_concentration &lt;- ifelse(dose_group == \"Low Dose\", 8, 15)\n  patient_variation &lt;- rnorm(1, mean = 0, sd = 1)\n  time_variation &lt;- exp(-0.1*day)\n  baseline_concentration * time_variation + patient_variation \n}\n\n\n# Generate drug concentrations for each sample\ndrug_concentrations &lt;- mapply(generate_drug_concentrations, \n                              day = rep(day_ids, each = num_samples_per_day), \n                              dose_group = treatment_assignments,\n                              patient_id = rep(1:num_patients, each = num_days))\n\n\n# Flatten the matrix to a vector\ndrug_concentrations &lt;- as.vector(drug_concentrations)\n\n# Generate cholesterol levels for each sample \n# (assuming a positive correlation with drug concentration)\ncholesterol_levels &lt;- drug_concentrations + \n  rnorm(num_patients * num_days * num_samples_per_day, mean = 0, sd = 5)\n\n# Generate adverse events based on drug concentration \n# (assuming a higher chance of adverse events with higher concentration)\n# Sigmoid function to map concentrations to probabilities\nadverse_events_prob &lt;- plogis(drug_concentrations / 10) \nadverse_events &lt;- rbinom(num_patients * num_days * num_samples_per_day, \n                         size = 1, prob = adverse_events_prob)\n\n# Create a data frame\nsyn_dat2 &lt;- data.frame(\n  PatientID = rep(patient_ids, each = num_samples_per_day),\n  TreatmentGroup = rep(treatment_assignments, each = num_samples_per_day),\n  Day = rep(day_ids, each = num_samples_per_day),\n  DrugConcentration = drug_concentrations,\n  CholesterolLevel = cholesterol_levels,\n  AdverseEvent = adverse_events\n)",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data-1",
    "title": "Generating synthetic data with R",
    "section": "Checking data",
    "text": "Checking data\nTake a peek at the generated data.\n\n# Print the first few rows of the generated dataset\nprint(head(syn_dat2))\n\n  PatientID TreatmentGroup Day DrugConcentration CholesterolLevel AdverseEvent\n1         1       Low Dose   1          8.462781        12.401475            0\n2         1       Low Dose   2          6.909660        10.754871            0\n3         1       Low Dose   3          6.327317         7.988330            0\n4         1       Low Dose   4          5.473243         0.431360            1\n5         1       Low Dose   5          4.296404         3.699141            0\n6         1       Low Dose   6          6.177406         4.775430            1\n\nsummary(syn_dat2)\n\n   PatientID     TreatmentGroup          Day    DrugConcentration\n Min.   : 1.00   Length:140         Min.   :1   Min.   : 2.081   \n 1st Qu.: 5.75   Class :character   1st Qu.:2   1st Qu.: 5.174   \n Median :10.50   Mode  :character   Median :4   Median : 7.056   \n Mean   :10.50                      Mean   :4   Mean   : 7.593   \n 3rd Qu.:15.25                      3rd Qu.:6   3rd Qu.: 9.738   \n Max.   :20.00                      Max.   :7   Max.   :14.933   \n CholesterolLevel  AdverseEvent   \n Min.   :-4.494   Min.   :0.0000  \n 1st Qu.: 3.844   1st Qu.:0.0000  \n Median : 7.234   Median :1.0000  \n Mean   : 7.855   Mean   :0.6571  \n 3rd Qu.:11.259   3rd Qu.:1.0000  \n Max.   :24.422   Max.   :1.0000  \n\ndplyr::glimpse(syn_dat2)  \n\nRows: 140\nColumns: 6\n$ PatientID         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3‚Ä¶\n$ TreatmentGroup    &lt;chr&gt; \"Low Dose\", \"Low Dose\", \"Low Dose\", \"Low Dose\", \"Low‚Ä¶\n$ Day               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4‚Ä¶\n$ DrugConcentration &lt;dbl&gt; 8.462781, 6.909660, 6.327317, 5.473243, 4.296404, 6.‚Ä¶\n$ CholesterolLevel  &lt;dbl&gt; 12.4014754, 10.7548711, 7.9883301, 0.4313600, 3.6991‚Ä¶\n$ AdverseEvent      &lt;int&gt; 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1‚Ä¶",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#exploratory-plot",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#exploratory-plot",
    "title": "Generating synthetic data with R",
    "section": "Exploratory plot",
    "text": "Exploratory plot\n\n# Plot drug concentrations over time for each individual using ggplot2\np1 &lt;- ggplot(syn_dat2, aes(x = Day, y = DrugConcentration, \n                      group = as.factor(PatientID), color = TreatmentGroup)) +\n  geom_line() +\n  labs(title = \"Drug Concentrations Over Time\",\n       x = \"Day\",\n       y = \"Drug Concentration\",\n       color = \"TreatmentGroup\") +\n  theme_minimal()\nplot(p1)\n\n\n\n\n\n\n\np2 &lt;- ggplot(syn_dat2, aes(x = as.factor(AdverseEvent), y = DrugConcentration, \n                           fill = TreatmentGroup)) +\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8), color = \"black\") +\n  geom_point(aes(color = TreatmentGroup), position = position_dodge(width = 0.8), \n             size = 3, shape = 16) +  # Overlay raw data points\n  labs(\n    x = \"Adverse Events\",\n    y = \"Drug Concentration\",\n    title = \"Boxplot of Drug Concentration by Adverse Events and Treatment\"\n  ) +\n  scale_color_manual(values = c(\"A\" = \"blue\", \"B\" = \"red\")) +  # Customize color for each treatment\n  theme_minimal() +\n  theme(legend.position = \"top\")\nplot(p2)\n\n\n\n\n\n\n\n\nThis dataset has a bit more complicated structure than the previous one, but it isn‚Äôt much harder to generate it with code. You‚Äôll want to generate your data such that it mimics the real data you plan on analyzing.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#save-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#save-data",
    "title": "Generating synthetic data with R",
    "section": "Save data",
    "text": "Save data\n\n# Save the simulated data to a CSV or Rds file\nwrite.csv(syn_dat2, here(\"data\",\"syn_dat2.csv\"), row.names = FALSE)\n# if we wanted an RDS version\n#saveRDS(syn_dat2, here(\"data\",\"syn_dat2.Rds\"))\n\nWe are saving the data as CSV and Rds files. CSV files are readable with pretty much any software and thus very portable. Rds files are R-specific, thus not as flexible. The advantage of Rds files is that they are generally smaller, and they retain information about the variables, e.g.¬†if they are factor or numeric variables. Either format works, sometimes one or the other might be better.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html",
    "title": "R packages to generate synthetic data",
    "section": "",
    "text": "This unit provides brief introductions of different R packages that can be used to generate synthetic/simulated/artificial/fake data.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "R packages for data creation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#r-packages-to-generate-synthetic-data",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#r-packages-to-generate-synthetic-data",
    "title": "R packages to generate synthetic data",
    "section": "R packages to generate synthetic data",
    "text": "R packages to generate synthetic data\n\nThe coxed package allows you - among other things - to generate longitudinal survival (time-to-event) data using its sim.survdata() function.\nThe admiral package, which is part of the pharmaverse, allows users to generate data in ADaM format. ADaM is a common data format in the pharmaceutical area.\nThe simstudy package allows users to define variables and relations between them, and then have synthetic data generated based on those specifications. It can also create data with potentially complex structures, such as longitudinal or hierarchical data.\nThe synthpop package can produce synthetic data that is very similar to the original data, including potential patterns/correlations.\nThe mice package is most often used to impute missing data. But one can also use it to generate synthetic data. This tutorial provides a good introduction.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "R packages for data creation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#some-r-helper-packages",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#some-r-helper-packages",
    "title": "R packages to generate synthetic data",
    "section": "Some R helper packages",
    "text": "Some R helper packages\n\nThe CRAN Task View: Probability Distributions lists packages that can be used to generate data following various probability distributions. This can be useful if you want to generate data that has a certain shape/distribution.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "R packages for data creation"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html",
    "href": "content/module-workflow/complete-data-analysis-workflow.html",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "",
    "text": "This very short unit re-visits all of the data analysis concepts and components we discussed so far, and provides a summary and big-picture discussion, meant to wrap up things.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-question",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-question",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The question",
    "text": "The question\nHaving a good question (hypothesis) that is interesting, important, new, and can be answered with the resources you have (data, skills, computing power) is the most crucial part of any project. You can do an analysis that is technically perfect, but if you don‚Äôt answer an interesting and relevant question, nobody will care. While I think one should use state-of-the art analysis approaches as much as possible, it is in my opinion more important to answer a good question. I believe that an important question analyzed with a simple model is almost always better than using a complicated model to answer a question that nobody cares about. Of course, the simple model still needs to be reasonable. If one uses a completely wrong model or performs a faulty analysis, the whole project/paper might also be meaningless.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-setup",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-setup",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The setup",
    "text": "The setup\nYou should do your whole analysis as automated, reproducible, and as well-structured and well-documented as possible. Your colleagues, your readers and your future self will thank you for it. We have used tools in this course (R/R Markdown/GitHub) which help in performing an analysis in such a way. Many other tools are available. While some tools are worse than others (e.g., Excel), in the end it doesn‚Äôt matter too much what tools you use, as long as you can do things automated, reproducible, and well-documented.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-wrangling",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-wrangling",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The wrangling",
    "text": "The wrangling\nAs you will likely appreciate by now, getting the data into a shape that can be analyzed is ‚Äì for almost any dataset ‚Äì time consuming and also incredibly important. Lots of mistakes happen at that stage. For a recent prominent example where things went wrong, see e.g.¬†Aboumatar and Wise 2018 published to JAMA, where a mis-coding of a factor variable led to one conclusion, and upon fixing it, the conclusion changed in the other direction, leading to retraction of the original study (and republication of the corrected study). This is an example where an error was found and fixed. Unfortunately, there are probably a lot of studies in the literature where mistakes during the wrangling process were made, wrong results published, and nobody noticed. It is impossible to fully prevent making mistakes. But there are ways to try and minimize those problems. To do so, follow these rules:\n\nDocument everything very well. Every step in the wrangling/cleaning process should be explained and justified (e.g.¬†if you drop observations with NA, what does it mean and why do you think it‚Äôs ok to do).\nAutomate things as much as possible. Manual tasks often introduces errors.\nMake everything reproducible. That helps you and others spot mistakes faster.\nCritically evaluate every step you take. If something is happening that doesn‚Äôt look quite right, or you get warning messages in your code, stop and figure out what is going on. Only proceed once you know exactly what is happening and are ok with it.\nTry different alternatives. For instance if you are unclear if you should remove missing observations, or remove a variable that has a lot of missing, or use imputation, why not try it all 3 ways? It usually doesn‚Äôt take much extra work to do a few alternatives. If each version of doing things gives you more or less the same results, it helps convince yourself and the readers that your finding might be robust to the details of the analysis. If different reasonable ways of doing the analysis lead to different results, you have learned something too, and it might be worth digging deeper to understand why results differ. You might find some new, unexpected and interesting bit of science lurking. It is important to report an any analysis you did, even if just briefly in the supplement. You are not allowed to run multiple analyses and then just report the one that gives you the answers you want (likely happens often, see p-hacking above.)",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-analysis",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-analysis",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The analysis",
    "text": "The analysis\nYou have learned that there are a lot of different analysis approaches out there, and which one to choose depends on many factors, such as the question (e.g.¬†do you care more about interpretability or performance), available resources, and many other considerations. All the rules listed above for wrangling hold for the analysis bit too. Make it reproducible, well documented, well explained and justified. Make sure you understand results at each step. If possible, try different alternative approaches. Some additional, analysis-specific considerations are the following:\n\nThink carefully about the performance measure you want to fit to. While the ‚Äòstandard‚Äô ones, like RMSE/SSR for continuous outcomes and accuracy for categorical outcomes are at times ok, often other measures might be more meaningful. E.g. for continuous outcomes, you might want to do RMSE not on the outcome but the log of the outcome. Or you might want to penalize with least absolute difference to better deal with outliers. Similarly, for categorical outcomes, especially when there is imbalance in the data and you have much fewer of one category than of others, using accuracy might not be best. Some other metric such as F1 score, or a custom performance measure might be better. Spend some time thinking about the best performance measure before you do all your fitting.\nOnce you picked your performance measure and are ready to fit/train your model, make sure to not evaluate performance on the data used for building the model. More complex models can always give improved performance on the data used to build the model, thus this metric is not meaningful! Instead, to evaluate model performance, ideally use some version of cross-validation, i.e.¬†fitting the model to some of the data and evaluating model performance on a part of the data that was not used for fitting. If this is not possible, e.g. because you don‚Äôt have much data or it takes too long to run, us AIC & Co.¬†as a backup option to determine model quality.\nCompare your model to baseline/null models and simple single-predictor models to get an idea for the improvement you can get. Try a complex model to estimate the upper bound of model performance. Then try a few reasonable models ‚Äúin between‚Äù the null model and the really complex model, and pick the one that works overall best for your purpose. That last step is subjective. That is ok, as long as you can explain and justify why you ended up going with the model you chose.\nOnce you have chosen your best model (or even before, for the purpose of picking your final model), perform model assessment. Look at uncertainty, investigate residuals, look at variable importance, etc. Poke your model in as many ways as possible to understand how it works and what its limitations might be.\nIf you have enough data, set some aside at the beginning (test data), and apply your model to that data at the very end. This gives you the most honest assessment of your model performance for new/unseen data.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow.html#the-reporting",
    "href": "content/module-workflow/complete-data-analysis-workflow.html#the-reporting",
    "title": "Revisting the Full Data Analysis Workflow",
    "section": "The reporting",
    "text": "The reporting\nYou almost never do an analysis just for fun and for yourself. Usually, there are other reasons. For instance in academia/science, we analyze data to better understand our system, to test hypotheses, to answer questions. Similarly in industry and other applied settings, we analyze data to come to actionable conclusions (e.g.¬†we determine which images show a likely cancer and therefore which patients need further tests or surgery). In all those situations, we want to communicate our findings to others. That can be through the peer-reviewed literature, in a meeting with our colleagues and bosses, as a report for patients, etc. Being able to report findings from a potentially complicated analysis in a way that it has an impact, and is appropriate for the right audience, is not easy. Some of the ideas listed above, as well as others are worth keeping in mind:\n\nPresent your findings in such a way that people can (but don‚Äôt have to) go deeper easily. Start with a short summary (often called executive summary in industry and abstract in academia). This short write-up should summarize your findings understandably and honestly. Do not spin/hype things that are not supported by your actual analysis. Also, focus on the main important finding(s) and their implications. The main deliverable (usually some form of written report or a paper), should present all the main findings and steps you took, nicely explained. Then provide additional information (e.g.¬†supplement, appendices) with more details. Finally, provide all the raw materials, i.e.¬†data and well-documented code, for others to look at. By layering content, different audiences can go into your findings in as much or little detail as they want.\nExplain and justify everything. It‚Äôs unlikely that everyone would have made exactly the same decisions you did during your analysis. But by explaining your rationale, readers can decide if they find what you did reasonable, and thus make an informed decision as to how much they trust your findings.\nReport results from multiple approaches: If you show how certain decisions during the analysis do or don‚Äôt affect the results, it makes things more transparent and can instill greater confidence in readers.\nAutomate things. As much of your final products as possible should be automated. That means that figures and tables should not be created by hand. This way, if you want or have to change things upstream (e.g.¬†you noticed a mistake in some analysis step or reviewers/your boss request changes), you can update everything as quickly, automated and seamless as possible.\nUse modern ways to report and disseminate your findings. The standard academic way is still to write peer-reviewed papers, or in industry, prepare a report. However, such documents are generally not too widely read and at times have only limited impact. As appropriate for your project, consider other sources of dissemination. For instance, make a website for your analysis. Turn it into a blog post. Tweet about it. Use interactive tools (e.g. the R Shiny package) to allow the audience to interact with your results. Be creative and think about the best ways to reach your intended audience.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html",
    "href": "courseinfo/course-communication.html",
    "title": "Course Communications",
    "section": "",
    "text": "The main course content can be found on this website.\nWe‚Äôll mostly communicate and discuss through Discord.\nThere will be completely optional Zoom sessions (aka office hours).\nSome communication/interaction will happen via GitHub. More details are provided in the relevant sections.\nQuizzes will be available from this website and submitted to an online system, instructions are provided.\n\n\n\n\nThis is the first time I‚Äôm using Discord for the class, previous classes used Slack. I decided to switch since the free version of Slack is very limited. If you find anywhere on this website a mention of Slack, just assume that I mean Discord. Also please let me know so I can fix it. (Or contribute the fix yourself, you‚Äôll soon learn how to do that through GitHub).\n\nThe main mode of communication will be through Discord. You should get an email invite to the Discord server for this class. If you haven‚Äôt received an email by Friday before classes start or can‚Äôt find the Discord invite link on eLC, let me know. We won‚Äôt use eLC, everything is going to happen on Discord or this website.\nIf you are unfamiliar with Discord, it is essentially a chat/discussion tool (similar to Slack or Teams or‚Ä¶). The main area for the class is called a server in Discord. Ours is called MADA-Spring-2024. If you use Discord for other purposes, you might be a member of additional servers. Each server has channels. Each channel corresponds to a topic. The names and descriptions of the different channels should be self-explanatory.\nIf you have requests for specific channels to be created, let me know.\nIf a channel is public, you can see it and participate in the discussion. For private channels you need an invite to see it and participate. For simplicity, all channels are public, though there will be channels which likely won‚Äôt be of interest to you. Specifically, you‚Äôll be placed into groups during the course and will likely only be interested in the channel for your group.\nInside a channel, you can write messages. To respond to someone‚Äôs message, you can either respond as thread or type your message just below the previous one. The good thing with responding as new message is this is that everyone sees it easily. The bad thing is that if there are a good number of messages being typed, it can be hard to figure out which ones go together. An alternative, and maybe better way is to reply in a thread.\nYou can also use Discord to send one or more individuals direct messages (DM). You can use that to communicate with me or each other.\nOverall, Discord is fairly intuitive, and hopefully much easier to use than the rather clunky discussion feature of eLC. But some features might need getting used to. If you have no previous Discord experience, I suggest you browse through their very comprehensive documentation to pick up a few more tips and tricks.\nOne useful thing to know is that people will get a specific notification if you either send them a direct message or use their handle in our message, e.g.¬†by writing @personname can you take a look at the figure below‚Ä¶. Specifically, if you want to get my attention, you need to do it that way. I have too many things pinging me all the time üòÅ. While I try to stay on top of all the messages in our class, I likely won‚Äôt. To ensure I don‚Äôt miss something directed at me, use the @ notation (or DM).\nI will use the announcements channel to post any important course related announcements. Check regularly.\nUse Discord widely to ask questions, to answer others‚Äô questions, to notify me and others of things that are unclear/wrong, to post links to interesting resources, etc. I hope we can build a community of learners where you help and support each other. So please don‚Äôt always wait for me to answer questions or provide feedback! Engage with each other. I will regularly visit and read the discussions and participate, but I hope that I will end up as one participant of many, not the only person providing answers.\nI prefer that you use Discord instead of email to ask questions. The reason is that if you have a question, it is likely that others have it too. By asking and answering online, everyone can see it. That said, there might be instances where the question only relates to you. In such cases, feel free to either send me a direct message on Discord or email me.\nYou might have already a Dischord user name or decide to choose a user name which is not your real name. In that case, please update your Discord profile for our server to your real name (and if you want, preferred pronouns and other information). Also, to make things more personal, add an image of yourself to the profile. It will help make things a bit more social, even if it‚Äôs not quite the same as in-person. You can do all that if you right-click your user name and choose ‚ÄúEdit Server Profile‚Äù.\n\n\n\nI will offer weekly synchronous meetings through Zoom. Those are completely optional. Whoever wants to join can do so, and we will use the meeting for discussions, answering questions, and whatever else you want to talk about. Think of those meetings a bit like virtual office hours or virtual in-class discussions. To ensure everyone will be able to join at least occasionally, I plan to alternate between two times each week.\nI want to emphasize that there is nothing covered in the meetings other than me addressing questions from individuals. I will not record these meetings to ensure students feel comfortable asking ‚Äúdumb‚Äù questions (they are rarely ‚Äúdumb‚Äù, but I don‚Äôt want anyone to feel limited out of fear of ‚Äúlooking stupid‚Äù in the recording). If any topic comes up that I think is worth sharing more widely, I will afterwards post a message and/or add content to the website. Therefore, if you can‚Äôt make these Zoom meetings, you are not missing anything (other than the option of asking me questions that might be tricky to discuss on Discord).\nTentative dates for Zoom meetings are listed in the Schedule document. Those are subject to change. If we need to move things around, I‚Äôll post an announcement and also update the Schedule document. At the designated times, I will start the meeting. I‚Äôll wait around for 15 minutes. If nobody connects, I‚Äôll end the meeting. If people connect, we‚Äôll talk until we covered everything, or until I or you need to move on to our next appointments. The information for the Zoom meeting login will be provided on Discord.\nAnd of course, if you have something you want to discuss outside of those scheduled times, send me a message and we can schedule a meeting.\n\n\n\nWe will also use Github for some specific communications, related to exercises and projects. More detailed instructions on that are provided at the time you are asked to use GitHub.\n\n\n\nWe will not be using eLC. I might occasionally use the email everyone feature in eLC to send out an email to the class in the first few weeks until everyone has settled into using Discord. After that, I do not plan on using eLC.\n\n\n\nIn some semesters, I have a TA for the class. If there is a TA the semester you are taking the class, they will be another point of contact for you to ask questions. The TA will be posting an introduction to Discord with details of their availability and preferred modes of communication (Discord/Zoom/etc.).",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#discord",
    "href": "courseinfo/course-communication.html#discord",
    "title": "Course Communications",
    "section": "",
    "text": "This is the first time I‚Äôm using Discord for the class, previous classes used Slack. I decided to switch since the free version of Slack is very limited. If you find anywhere on this website a mention of Slack, just assume that I mean Discord. Also please let me know so I can fix it. (Or contribute the fix yourself, you‚Äôll soon learn how to do that through GitHub).\n\nThe main mode of communication will be through Discord. You should get an email invite to the Discord server for this class. If you haven‚Äôt received an email by Friday before classes start or can‚Äôt find the Discord invite link on eLC, let me know. We won‚Äôt use eLC, everything is going to happen on Discord or this website.\nIf you are unfamiliar with Discord, it is essentially a chat/discussion tool (similar to Slack or Teams or‚Ä¶). The main area for the class is called a server in Discord. Ours is called MADA-Spring-2024. If you use Discord for other purposes, you might be a member of additional servers. Each server has channels. Each channel corresponds to a topic. The names and descriptions of the different channels should be self-explanatory.\nIf you have requests for specific channels to be created, let me know.\nIf a channel is public, you can see it and participate in the discussion. For private channels you need an invite to see it and participate. For simplicity, all channels are public, though there will be channels which likely won‚Äôt be of interest to you. Specifically, you‚Äôll be placed into groups during the course and will likely only be interested in the channel for your group.\nInside a channel, you can write messages. To respond to someone‚Äôs message, you can either respond as thread or type your message just below the previous one. The good thing with responding as new message is this is that everyone sees it easily. The bad thing is that if there are a good number of messages being typed, it can be hard to figure out which ones go together. An alternative, and maybe better way is to reply in a thread.\nYou can also use Discord to send one or more individuals direct messages (DM). You can use that to communicate with me or each other.\nOverall, Discord is fairly intuitive, and hopefully much easier to use than the rather clunky discussion feature of eLC. But some features might need getting used to. If you have no previous Discord experience, I suggest you browse through their very comprehensive documentation to pick up a few more tips and tricks.\nOne useful thing to know is that people will get a specific notification if you either send them a direct message or use their handle in our message, e.g.¬†by writing @personname can you take a look at the figure below‚Ä¶. Specifically, if you want to get my attention, you need to do it that way. I have too many things pinging me all the time üòÅ. While I try to stay on top of all the messages in our class, I likely won‚Äôt. To ensure I don‚Äôt miss something directed at me, use the @ notation (or DM).\nI will use the announcements channel to post any important course related announcements. Check regularly.\nUse Discord widely to ask questions, to answer others‚Äô questions, to notify me and others of things that are unclear/wrong, to post links to interesting resources, etc. I hope we can build a community of learners where you help and support each other. So please don‚Äôt always wait for me to answer questions or provide feedback! Engage with each other. I will regularly visit and read the discussions and participate, but I hope that I will end up as one participant of many, not the only person providing answers.\nI prefer that you use Discord instead of email to ask questions. The reason is that if you have a question, it is likely that others have it too. By asking and answering online, everyone can see it. That said, there might be instances where the question only relates to you. In such cases, feel free to either send me a direct message on Discord or email me.\nYou might have already a Dischord user name or decide to choose a user name which is not your real name. In that case, please update your Discord profile for our server to your real name (and if you want, preferred pronouns and other information). Also, to make things more personal, add an image of yourself to the profile. It will help make things a bit more social, even if it‚Äôs not quite the same as in-person. You can do all that if you right-click your user name and choose ‚ÄúEdit Server Profile‚Äù.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#zoom",
    "href": "courseinfo/course-communication.html#zoom",
    "title": "Course Communications",
    "section": "",
    "text": "I will offer weekly synchronous meetings through Zoom. Those are completely optional. Whoever wants to join can do so, and we will use the meeting for discussions, answering questions, and whatever else you want to talk about. Think of those meetings a bit like virtual office hours or virtual in-class discussions. To ensure everyone will be able to join at least occasionally, I plan to alternate between two times each week.\nI want to emphasize that there is nothing covered in the meetings other than me addressing questions from individuals. I will not record these meetings to ensure students feel comfortable asking ‚Äúdumb‚Äù questions (they are rarely ‚Äúdumb‚Äù, but I don‚Äôt want anyone to feel limited out of fear of ‚Äúlooking stupid‚Äù in the recording). If any topic comes up that I think is worth sharing more widely, I will afterwards post a message and/or add content to the website. Therefore, if you can‚Äôt make these Zoom meetings, you are not missing anything (other than the option of asking me questions that might be tricky to discuss on Discord).\nTentative dates for Zoom meetings are listed in the Schedule document. Those are subject to change. If we need to move things around, I‚Äôll post an announcement and also update the Schedule document. At the designated times, I will start the meeting. I‚Äôll wait around for 15 minutes. If nobody connects, I‚Äôll end the meeting. If people connect, we‚Äôll talk until we covered everything, or until I or you need to move on to our next appointments. The information for the Zoom meeting login will be provided on Discord.\nAnd of course, if you have something you want to discuss outside of those scheduled times, send me a message and we can schedule a meeting.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#github",
    "href": "courseinfo/course-communication.html#github",
    "title": "Course Communications",
    "section": "",
    "text": "We will also use Github for some specific communications, related to exercises and projects. More detailed instructions on that are provided at the time you are asked to use GitHub.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#elc",
    "href": "courseinfo/course-communication.html#elc",
    "title": "Course Communications",
    "section": "",
    "text": "We will not be using eLC. I might occasionally use the email everyone feature in eLC to send out an email to the class in the first few weeks until everyone has settled into using Discord. After that, I do not plan on using eLC.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#course-teaching-assistant",
    "href": "courseinfo/course-communication.html#course-teaching-assistant",
    "title": "Course Communications",
    "section": "",
    "text": "In some semesters, I have a TA for the class. If there is a TA the semester you are taking the class, they will be another point of contact for you to ask questions. The TA will be posting an introduction to Discord with details of their availability and preferred modes of communication (Discord/Zoom/etc.).",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-schedule.html",
    "href": "courseinfo/course-schedule.html",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Schedule",
    "section": "",
    "text": "This is the current course schedule for the class. Things might change, so check regularly for updates.\nThe Assessments column lists items that need to be submitted and that will be graded. Those are described in more detail in an Assessments page and in the assessment document for each module.\nNote: Withdrawal deadline is Thursday 3/21.",
    "crumbs": [
      "General Information",
      "Schedule"
    ]
  },
  {
    "objectID": "courseinfo/course-schedule.html#synchronous-zoom-meetings",
    "href": "courseinfo/course-schedule.html#synchronous-zoom-meetings",
    "title": "MADA ‚Äì Spring 2024 ‚Äì Schedule",
    "section": "Synchronous Zoom meetings",
    "text": "Synchronous Zoom meetings\nI will offer a weekly Zoom meeting (aka office hour). See the Communication section for some more details. As described in the Communication document, if you don‚Äôt join the Zoom meetings you are not missing any important information, these are just completely optional opportunities to ask for help/feedback etc.\nYou can find the Zoom link on Discord. The plan is to have weekly alternating times for those zoom meetings such that everyone is able to make the meeting at least every 2 weeks.\nThe current plan is to have Zoom meetings for odd module numbers on Wednesdays at 8am, and for even module numbers on Tuesdays at 8am. The first Zoom session will be on Wednesday, 1/10 at 8am ET.\nHopefully, at least one of those two times works for you, allowing you to join at least every other time if wanted. If you already know that neither times work for you, and you are keen on joining the Zoom meetings, let me know and we can adjust.",
    "crumbs": [
      "General Information",
      "Schedule"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Modern Applied Data Analysis (MADA)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course was developed and is maintained by Andreas Handel.\nThe following individuals have contributed to improving the course: Ian Bachli (@idbachli), Megan Beaudry (@MeganSarah), Zane Billings (@wzbillings), Amanda Blubaugh (@alblubaugh), Mia Carmichael (@mncarmic), Ellen Cheng (@ellenmonster), Paige Duffin (@paigeduffin), Robert Edenfield (@ClaytonEdenfield), Julia Frederick (@JuliaFrederick), Savannah Hammerton (@smhammerton), Megan Lott (@meganejlott), Yao Lu (@yl3362), Brian McKay (@bmckay), William Norfolk (@williamnorfolk), Trang Quach (@TrangQuach), Emily Rayens (@Rayense), Megan Robertson (@meganjro), Jadey Ryan (@jadeynryan), Amanda Skarlupka (@askarlupka), Jonathan Wilson (@JJWilson1991), Yang Zhang (@yz17970)\n\nThe course materials are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "project/project-examples.html",
    "href": "project/project-examples.html",
    "title": "Course Project Examples",
    "section": "",
    "text": "Overview\nTo showcase some of the work previous students have done, and as inspiration of future projects, below you can find links to a few past projects. They are of course posted with permission from the students.\n\nPlastic in the Urban Freshwater Environment by Amelia Foley - main report, supplement file, GitHub project repository.\nPredicting Federal Emergency Management Agency Funding for U.S. Disasters by Morgan Taylor - main report, supplement file, GitHub project repository.",
    "crumbs": [
      "Project",
      "Project Examples"
    ]
  },
  {
    "objectID": "project/project-review.html",
    "href": "project/project-review.html",
    "title": "Project Review Information",
    "section": "",
    "text": "You will be asked to review the projects of a few of your classmates twice, once for part 3, and once at the end (part 5).\nI have several goals for these project reviews:\n\nYou get feedback that helps you improve your project.\nYou get experience with giving and receiving reviewer feedback, like you would when you submit a paper for publication or review someone‚Äôs submitted paper.\nYou practice being on the ‚Äòreceiving end‚Äô of reproducible research, i.e.¬†you will have to be able to reproduce someone else‚Äôs project so you can properly critique it.",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-review.html#part-3-review-logistics",
    "href": "project/project-review.html#part-3-review-logistics",
    "title": "Project Review Information",
    "section": "Part 3 review logistics",
    "text": "Part 3 review logistics\n\nFind the projects you are assigned to in the Announcments channel. If you have been asked to review a private project, make sure you are added as collaborator.\nFollow the instructions provided in the repository to run all code and reproduce everything.\nUse the simple grading rubric shown in the part 3 section of the Project Details document.\nSubmit your review by filing one (or several) GitHub Issues in the project repository. Provide any feedback you think your classmate(s) might find useful to help them further improve their project and ensure they are on the right track. Also provide your score as indicated in the rubric.",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-review.html#part-5-review-logistics",
    "href": "project/project-review.html#part-5-review-logistics",
    "title": "Project Review Information",
    "section": "Part 5 review logistics",
    "text": "Part 5 review logistics\nFollow the instructions provided in the repository to run all code and reproduce everything. Review the whole project using this review template (right-click to download).\nWrite up a detailed review using and filling out the template. Once you are done, place your completed review document into the main folder of the repository you are reviewing, and send a pull request (or do a direct push) for each project you review.\n\nAs you reproduce the project, you will likely make a lot of changes to the repository (e.g., by re-creating figures/tables). If you and another reviewer do that both, you might run into conflicts when sending a pull request. If you encounter problems like this, you can provide the project owner your completed review in some other manner (e.g.¬†through email or Discord) and they‚Äôll add it to the repository. The important part is that at the end of the review process, there should be (at least) 2 completed review documents in the project repository (either the main folder, or make a reviews sub-folder for them.) I need to be able to find the reviews for grading purposes.",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-review.html#review-assessment-rubric",
    "href": "project/project-review.html#review-assessment-rubric",
    "title": "Project Review Information",
    "section": "Review Assessment Rubric",
    "text": "Review Assessment Rubric\nOk, now it gets maybe a bit complicated. I will review and assess the quality of your reviews. To that end, I‚Äôll use a fairly simple rubric, similar to the ones for the previous project submissions.\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nReviews are complete or fairly complete\n3\n\n\nSomewhat insufficient\nReviews are somewhat incomplete, lack useful/detailed feedback\n2\n\n\nInsufficient\nOnly one review was submitted, or submissions were very incomplete\n1\n\n\nAbsent\nNo reviews were submitted\n0\n\n\n\nYou get a full week to do those reviews, so I expect thorough and good quality work!",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-review.html#finals-steps",
    "href": "project/project-review.html#finals-steps",
    "title": "Project Review Information",
    "section": "Finals steps",
    "text": "Finals steps\n\nUse the peer feedback/reviews from your classmates to further improve your project. You can make any further changes you want to make.\nPush your final project to your GitHub repository by the specified deadline for final grading.\nI will assess your final project using the same categories in the template used during peer review.",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "resources/resources-general.html",
    "href": "resources/resources-general.html",
    "title": "More Data Science Resources",
    "section": "",
    "text": "I kept adding resources until things got too unwieldy and the Course Resources page was becoming too large üòÅ. So I decided to split things into two pages. The Course Resources page lists materials directly related to and used/mentioned in the course. This page lists a lot of other resources that are not heavily featured in the course, but that might be useful and interesting. Everything listed here is broadly related to the course topic, i.e.¬†the resources focus on Data Science/Stats/R Coding/GitHub/etc. For even more materials, see the links to various lists by others at the end of this document.\nMost materials described below are (should be) freely available online. For better or for worse, a lot of the resources I list below are dynamic and ever changing. That means occasionally links might not work, sites go offline, chapters in online books get re-arranged, etc. If any link does not work and you can‚Äôt access the materials for some reason, let me know so I can update this document.\nI placed them into categories according to main topic, but there is a lot of overlap. Many R coding resources focus on data analysis, and most data science resources I list focus on R.\nI am familiar with some, but not all of these resources. Sometimes I just took a quick glimpse to decide if it was worth including them here. If you find particular resources especially helpful or unhelpful (both listed and not listed), I‚Äôd love to receive feedback.",
    "crumbs": [
      "Resources",
      "General Resources"
    ]
  },
  {
    "objectID": "resources/resources-general.html#researcher-degrees-of-freedom-p-hacking",
    "href": "resources/resources-general.html#researcher-degrees-of-freedom-p-hacking",
    "title": "More Data Science Resources",
    "section": "Researcher degrees of freedom (p-hacking)",
    "text": "Researcher degrees of freedom (p-hacking)\n\nThe concept of Researcher degrees of freedom, which is related to Data Dredging and p-hacking are important ideas to keep in mind when doing a data analysis. Note that this issue is often cast in the language of p-values since those are still (unfortunately) the most common approach to statistical analyses. But the concept applies even if one doesn‚Äôt use p-values.\nYou can find a fun hands-on exploration of the potential problem of researcher degrees of freedom in this 538 visualization and another choose-your-own adventure story.\nFor further discussions of this general problem, see e.g.¬†this article from 538 (which goes with the hands-on example just mentioned) or Gelman‚Äôs and Loken‚Äôs article, with their closely related article from 2013.\nSimmons et al.¬†2011 provides a nice and easy to follow illustration how researcher degrees of freedom, combined with incomplete reporting, can lead to apparently nonsensical results. The study is a (fake) psychology study, but everything applies in general and it is easy to follow.\nNot surprisingly, xkcd has also covered the topic of p-hacking.",
    "crumbs": [
      "Resources",
      "General Resources"
    ]
  },
  {
    "objectID": "resources/resources-general.html#reproducible-research",
    "href": "resources/resources-general.html#reproducible-research",
    "title": "More Data Science Resources",
    "section": "Reproducible research",
    "text": "Reproducible research\n\nTrisovic et al. 2022 provides a nice glimpse at the problems that still exist when trying to reproduce/replicate prior studies by re-running the code.\nR Workflow is an online book describing how to do reproducible research using the R ecosystem and the still fairly new Quarto framework.\nFor more Quarto, the Awesome Quarto repository has a nice curated list of links to resources.",
    "crumbs": [
      "Resources",
      "General Resources"
    ]
  },
  {
    "objectID": "resources/resources-overview.html",
    "href": "resources/resources-overview.html",
    "title": "Resources Overview",
    "section": "",
    "text": "This section contains pages with brief descriptions and links to other resources.\nThe Course Resources page lists materials that are most relevant to the course. Some of this material will assigned as part of this course.\nThe General Resources page lists other resources in case you might find them helpful. All materials are (should be) available freely online.\nThe Glossary page provides brief definitions of terms used in this course.\nIf you come across any other good resources, or find some important term is missing from the glossary, please let me know (or contribute a pull request with your updates to the course website).",
    "crumbs": [
      "Resources",
      "Resources Overview"
    ]
  }
]