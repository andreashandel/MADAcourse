[
  {
    "objectID": "resources/resources-overview.html",
    "href": "resources/resources-overview.html",
    "title": "Resources Overview",
    "section": "",
    "text": "This section contains pages with brief descriptions and links to other resources.\nThe Course Resources page lists materials that are most relevant to the course. Some of this material will be assigned as part of this course.\nThe General Resources page lists other resources in case you might find them helpful. Unless specifically noted, these materials should be available freely online.\nThe Glossary page provides brief definitions of terms used in this course.\nIf you come across any other good resources, or find some important term is missing from the glossary, please let me know (or contribute a pull request with your updates to the course website).",
    "crumbs": [
      "Resources",
      "Resources Overview"
    ]
  },
  {
    "objectID": "resources/resources-general.html",
    "href": "resources/resources-general.html",
    "title": "More Data Science Resources",
    "section": "",
    "text": "The Course Resources page lists materials directly related to and used/mentioned in the course. This page lists a lot of other resources that are not heavily featured in the course, but that might be useful and interesting. Everything listed here is broadly related to the course topic, i.e.¬†the resources focus on Data Science/Stats/R Coding/GitHub/etc. For even more materials, see the links to various lists by others at the end of this document.\nMost materials described below are (should be) freely available online. For better or for worse, a lot of the resources I list below are dynamic and ever changing. That means occasionally links might not work, sites go offline, chapters in online books get re-arranged, etc. If any link does not work and you can‚Äôt access the materials for some reason, let me know so I can update this document.\nI placed them into categories according to main topic, but there is a lot of overlap. Many R coding resources focus on data analysis, and most data science resources I list focus on R.\nI am familiar with some, but not all of these resources. Sometimes I just took a quick glimpse to decide if it was worth including them here. If you find particular resources especially helpful or unhelpful (both listed and not listed), I‚Äôd love to receive feedback.",
    "crumbs": [
      "Resources",
      "General Resources"
    ]
  },
  {
    "objectID": "resources/resources-general.html#researcher-degrees-of-freedom-p-hacking",
    "href": "resources/resources-general.html#researcher-degrees-of-freedom-p-hacking",
    "title": "More Data Science Resources",
    "section": "Researcher degrees of freedom (p-hacking)",
    "text": "Researcher degrees of freedom (p-hacking)\n\nThe concept of Researcher degrees of freedom, which is related to Data Dredging and p-hacking are important ideas to keep in mind when doing a data analysis. Note that this issue is often cast in the language of p-values since those are still (unfortunately) the most common approach to statistical analyses. But the concept applies even if one doesn‚Äôt use p-values.\nYou can find a fun hands-on exploration of the potential problem of researcher degrees of freedom on this page and another choose-your-own adventure story.\nFor further discussions of this general problem, see e.g.¬†this article from 538 (which goes with the hands-on example just mentioned) or Gelman‚Äôs and Loken‚Äôs article, with their closely related article from 2013.\nSimmons et al.¬†2011 provides a nice and easy to follow illustration how researcher degrees of freedom, combined with incomplete reporting, can lead to apparently nonsensical results. The study is a (fake) psychology study, but everything applies in general and it is easy to follow.\nAnother interesting example is this ecology study where teams of analysts looking at the same data came up with many different answers.\n\nNot surprisingly, xkcd has also covered the topic of p-hacking.\nNice discussion of p-values\nAnother stab at p-values\nAnother good discussion of the problem with p-values",
    "crumbs": [
      "Resources",
      "General Resources"
    ]
  },
  {
    "objectID": "resources/resources-general.html#reproducible-research",
    "href": "resources/resources-general.html#reproducible-research",
    "title": "More Data Science Resources",
    "section": "Reproducible research",
    "text": "Reproducible research\n\nTrisovic et al.¬†2022 provides a nice glimpse at the problems that still exist when trying to reproduce/replicate prior studies by re-running the code.\nAnother blog post discussing (lack of) reproducibility in science\nR Workflow is an online book describing how to do reproducible research using the R ecosystem and the still fairly new Quarto framework.",
    "crumbs": [
      "Resources",
      "General Resources"
    ]
  },
  {
    "objectID": "resources/resources-course.html",
    "href": "resources/resources-course.html",
    "title": "Course Resources",
    "section": "",
    "text": "This page lists resources that this course draws on frequently or at least occasionally. For a more extensive list of data science resources, see the General Resources page.\nAll materials described below are (should be) freely available online. If you can‚Äôt get to them, let me know. Note that a lot of the listed resources are dynamic and ever changing. That means occasionally links might not work, sites go offline, chapters in online books get re-arranged, etc. If you notice anything that‚Äôs not quite right, please let me know.\n\nBooks and online tutorials\n\nIntroduction to Data Science (IDS is the book I‚Äôll refer to a good bit in the 1st part of the course. The link above takes you to the 1st edition of the book. Mentions of specific chapters of IDS refer to the 1st edition. However, there is now a new version of the book, which splits the old version into 2, Introduction to Data Science and Advanced Data Science. I assume the newer versions are improved, so you might want to read them instead of version 1. I just haven‚Äôt gotten around to updating references to version 2.\nR for Data Science (R4DS) (2nd edition). This book will also be mentioned frequently. It is a very gentle and good introduction to data science in R. It has a lot of good exercises and solutions. Note that most of the course material was written when only the 1st edition existed. I‚Äôve tried to update references to specific chapters/pages of R4DS on this website, but it could be that there is still the occasional pointer to something in the 1st edition. If you find such outdated references/links, please let me know so I can fix.\nThe Art of Data Science (ADS) has some good big-picture, introductory level chapters on different aspects of a data analysis. It is a pay what you want book with a minimum price of zero, which means you can get it for free (note that the book + video bundle has a non-zero minimum price). (I heard that sometimes, once you register, the email with the book link goes to the spam folder, make sure to check there.)\nAn Introduction to Statistical Learning (with R) (ISL) is a good introduction to the statistical and machine learning (i.e.¬†model fitting) part of the data analysis workflow. I‚Äôll refer to it frequently in the second part of the course. While we won‚Äôt work through the exercises/labs that are part of ISL, if you are interested in trying out some of those, Emil Hvitfeldt‚Äôs website does the labs using the tidymodels set of packages, which we will also be using in this course.\nHands-on Machine Learning with R (HMLR) covers somewhat similar material as ISL, but with different emphasis and a different approach. I refer to it in several of the later course modules.\nFeature Engineering and Selection (FES) focuses on a specific aspect of the data analysis workflow and is a good resource for those topics.\nhappygitwitR (HGR) is a good resource to learn some of the basics of Git/GitHub with R.\nPosit Recipes (PR) is a good source for short code examples to do common tasks. This is the successor to RStudio Primers, which was even better, and had interactive exercises. While Posit removed the Primers, they were fortunately licensed to allow recreation, and Andrew Heiss did just that with his version of R Studio primers.\n\n\n\nTools\n\nWe‚Äôll be using various software tools in this course. All tools are available for free. More information can be found in the Course ad Tools Introduction unit of the course.\n\n\n\nGeneral help\n\nThe main place to get any course specific help are our course discussion boards. Use them widely to ask questions, to answer others‚Äô questions, to post links to interesting resources, etc.\nMost questions you will have are likely not course specific, but will have to do with R/GitHub/RMarkdown/etc. For that, Google will be your best friend. Most of the time, someone had the same problem/question you do and someone else answered it. The only tricky thing is finding that post. Even after years of doing this, I probably google how to do something in R every day üòÉ.\nWhen you search online for help, quite often you land on some StackExchange site (often Stack Overflow). These are a collection of widely-used online question and answer sites covering all kinds of topics (including R, GitHub, Data Analysis, etc.). The majority of the time, you will find an answer, or at least something that gets you closer, on those sites. I rarely go to those sites directly, instead I type my query into Google and it often sends me to one of the StackExchange sites.\nThe Posit community is another good place to ask questions. It is not as widely used as StackExchange, but it seems to be more newcomer friendly. I haven‚Äôt used it much.\nAnother highly recommended resource is the R for Data Science learning community. They have a Slack workspace which you can join for free and ask questions about R, Data Science and related topics. They are very newbie-friendly. I have not used them much but they seem to respond fairly quickly and are helpful.\n\n\n\nMore resources\nThe General Resources page has a more extensive list of relevant resources.",
    "crumbs": [
      "Resources",
      "Course Resources"
    ]
  },
  {
    "objectID": "project/project-review.html",
    "href": "project/project-review.html",
    "title": "Project Review Information",
    "section": "",
    "text": "You will be asked to review the projects of a few of your classmates twice, once for part 3, and once at the end (part 5).\nI have several goals for these project reviews:\n\nYou get feedback that helps you improve your project.\nYou get experience with giving and receiving reviewer feedback, like you would when you submit a paper for publication or review someone‚Äôs submitted paper.\nYou practice being on the ‚Äòreceiving end‚Äô of reproducible research, i.e.¬†you will have to be able to reproduce someone else‚Äôs project so you can properly critique it.",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-review.html#part-3-review-logistics",
    "href": "project/project-review.html#part-3-review-logistics",
    "title": "Project Review Information",
    "section": "Part 3 review logistics",
    "text": "Part 3 review logistics\n\nFind the projects you are assigned to. If you have been asked to review a private project, make sure you are added as collaborator.\nFollow the instructions provided in the repository to run all code and reproduce everything.\nUse the simple grading rubric shown in the part 3 section of the Project Details document.\nSubmit your review by filing one (or several) GitHub Issues in the project repository. Provide any feedback you think your classmate(s) might find useful to help them further improve their project and ensure they are on the right track. Also provide your score as indicated in the rubric.",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-review.html#part-5-review-logistics",
    "href": "project/project-review.html#part-5-review-logistics",
    "title": "Project Review Information",
    "section": "Part 5 review logistics",
    "text": "Part 5 review logistics\nFollow the instructions provided in the repository to run all code and reproduce everything. Review the whole project using this review template (right-click to download).\nWrite up a detailed review using the provided template. Once you are done, place your completed review document into the main folder of the repository you are reviewing, and send a pull request (or do a direct push) for each project you review.\n\nAs you reproduce the project, you will likely make a lot of changes to the repository (e.g., by re-creating figures/tables). If you and another reviewer do that both, you might run into conflicts when sending a pull request. If you encounter problems like this, you can provide the project owner your completed review in some other manner (e.g.¬†through email or Discord) and they‚Äôll add it to the repository. The important part is that at the end of the review process, there should be (at least) 2 completed review documents in the project repository (either the main folder, or make a reviews sub-folder for them.) I need to be able to find the reviews for grading purposes.",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-review.html#review-assessment-rubric",
    "href": "project/project-review.html#review-assessment-rubric",
    "title": "Project Review Information",
    "section": "Review Assessment Rubric",
    "text": "Review Assessment Rubric\nOk, now it gets maybe a bit complicated. I will review and assess the quality of your reviews. To that end, I‚Äôll use a fairly simple rubric, similar to the ones for the previous project submissions.\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nReviews are complete or fairly complete\n3\n\n\nSomewhat insufficient\nReviews are somewhat incomplete, lack useful/detailed feedback\n2\n\n\nInsufficient\nOnly one review was submitted, or submissions were very incomplete\n1\n\n\nAbsent\nNo reviews were submitted\n0\n\n\n\nYou get a full week to do those reviews, so I expect thorough and good quality work!",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-review.html#finals-steps",
    "href": "project/project-review.html#finals-steps",
    "title": "Project Review Information",
    "section": "Finals steps",
    "text": "Finals steps\n\nUse the peer feedback/reviews from your classmates to further improve your project. You can make any further changes you want to make.\nPush your final project to your GitHub repository by the specified deadline for final grading.\nI will assess your final project using the same categories in the template used during peer review.",
    "crumbs": [
      "Project",
      "Project Review"
    ]
  },
  {
    "objectID": "project/project-examples.html",
    "href": "project/project-examples.html",
    "title": "Course Project Examples",
    "section": "",
    "text": "Overview\nTo showcase some of the work previous students have done, and as inspiration of future projects, below you can find links to a few past projects. They are of course posted with permission from the students.\n\nPlastic in the Urban Freshwater Environment by Amelia Foley - main report, supplement file, GitHub project repository.\nPredicting Federal Emergency Management Agency Funding for U.S. Disasters by Morgan Taylor - main report, supplement file, GitHub project repository.",
    "crumbs": [
      "Project",
      "Project Examples"
    ]
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Modern Applied Data Analysis (MADA)",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis course was developed and is maintained by Andreas Handel.\nThe following individuals have contributed to improving the course: Ian Bachli (@idbachli), Megan Beaudry (@MeganSarah), Zane Billings (@wzbillings), Amanda Blubaugh (@alblubaugh), Mia Carmichael (@mncarmic), Ellen Cheng (@ellenmonster), Paige Duffin (@paigeduffin), Robert Edenfield (@ClaytonEdenfield), Julia Frederick (@JuliaFrederick), Savannah Hammerton (@smhammerton), Murphy John (@murphyhjohn), Megan Lott (@meganejlott), Yao Lu (@yl3362), Brian McKay (@bmckay), William Norfolk (@williamnorfolk), Trang Quach (@TrangQuach), Emily Rayens (@Rayense), Megan Robertson (@meganjro), Jadey Ryan (@jadeynryan), Amanda Skarlupka (@askarlupka), Jonathan Wilson (@JJWilson1991), Yang Zhang (@yz17970)\nThe course materials are licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Linked and embedded materials are governed by their own licenses. I assume that all external materials used or embedded here are covered under the educational fair use policy. If this is not the case and any material displayed here violates copyright, please let me know and I will remove it."
  },
  {
    "objectID": "courseinfo/course-schedule.html",
    "href": "courseinfo/course-schedule.html",
    "title": "MADA ‚Äì Spring 2026 ‚Äì Schedule",
    "section": "",
    "text": "This is the current course schedule for the class. Things might change, so check regularly for updates.\nThe graded assessments for the course consist of quizzes, exercises, discussions and a project. Those are described in more detail on the Assessments page and in the assessment document for each module.\n\nUnless otherwise specified, the first posting of the discussion is due by end of Wednesday, the discussion replies are due by Friday 5pm. Quizzes and Projects are due by Friday 5pm. Exercises often have multiple parts with staggered deadlines. See each exercise for details.\n\n\n\n\n\n\n\n\n\n\n\n\nModule Topic\nStart and end dates\nQuiz\nExercise\nDiscussion\nProject\n\n\n\n\nModule 1 ‚Äì Course and tools introduction\n1/12 ‚Äì 1/16\nY\nPortfolio start\nIntroductions\n\n\n\nModule 2 ‚Äì Data analysis introduction\n1/19 ‚Äì 1/23\nY\nStarter analysis (group work)\nReproducible research example\n\n\n\nModule 3 ‚Äì READy workflows\n1/26 ‚Äì 1/30\nY\nData analysis and coding (group work)\nModule Reflection\n\n\n\nModule 4 ‚Äì Data introduction & Synthetic data\n2/2 ‚Äì 2/6\nY\nComplex or Synthetic data (solo work)\nNone\nPart 1 due\n\n\nModule 5 ‚Äì Data processing\n2/9 ‚Äì 2/13\nY\nData wrangling (group work)\nAI tools\n\n\n\nModule 6 ‚Äì Presenting data\n2/16 ‚Äì 2/20\nY\nFigures and Tables (solo work)\nExercise feedback\n\n\n\nModule 7 ‚Äì Statistical analysis overview\n2/23 ‚Äì 2/27\nY\nNone\nScientific Paper Evaluation\nPart 2 due\n\n\nModule 8 ‚Äì Model fitting\n3/2 ‚Äì 3/6\nY\nModel fitting (solo work)\nModule Reflection\n\n\n\nSpring Break\n3/9 ‚Äì 3/13\n\n\n\n\n\n\nModule 9 ‚Äì Model assessment\n3/16 ‚Äì 3/20\nY\nNone\nModule Reflection\nPart 3 due\n\n\nModule 10 ‚Äì Model improvement\n3/23 ‚Äì 3/27\nY\nModel performance and predictions (group work)\nNone\nProject reviews due\n\n\nModule 11 ‚Äì Machine learning models I\n3/30 ‚Äì 4/3\nY\nMachine Learning Models (solo work)\nProjects\n\n\n\nModule 12 ‚Äì Machine learning models II\n4/6 ‚Äì 4/10\nY\nNone\nMachine Learning methods\nPart 4 due\n\n\nModule 13 ‚Äì The complete analysis workflow\n4/13 ‚Äì 4/17\nY\nTidytuesday analysis (solo work)\nCourse feedback\n\n\n\nFinishing Project\n4/20 ‚Äì 4/24\nN\nNone\nNone\nPart 5 due\n\n\nProject Reviews\n4/27 ‚Äì 5/1\nN\nNone\nNone\nProject reviews due\n\n\nFinal project revision\n5/4 ‚Äì 5/6 (Wednesday)\nN\nNone\nNone\nPart 6 due\n\n\n\nNote: Withdrawal deadline is April 9.",
    "crumbs": [
      "General Information",
      "Schedule"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html",
    "href": "courseinfo/course-communication.html",
    "title": "Course Communications",
    "section": "",
    "text": "The main course content can be found on this website.\nWe‚Äôll mostly communicate and discuss through Discord.\nThere will be completely optional Zoom sessions (aka office hours).\nSome communication/interaction will happen via GitHub. More details are provided in the relevant sections.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#overview",
    "href": "courseinfo/course-communication.html#overview",
    "title": "Course Communications",
    "section": "",
    "text": "The main course content can be found on this website.\nWe‚Äôll mostly communicate and discuss through Discord.\nThere will be completely optional Zoom sessions (aka office hours).\nSome communication/interaction will happen via GitHub. More details are provided in the relevant sections.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#discord",
    "href": "courseinfo/course-communication.html#discord",
    "title": "Course Communications",
    "section": "Discord",
    "text": "Discord\nThe main mode of communication will be through Discord. You should have received an invite link to join the Discord server for this class. If you haven‚Äôt received an email by Friday before classes start or can‚Äôt find the Discord invite link on eLC, let me know.\nIf you are unfamiliar with Discord, it is essentially a chat/discussion tool (similar to Slack or Teams or‚Ä¶). The main area for the class is called a server in Discord. Ours is called MADA-Spring-2026. If you use Discord for other purposes, you might be a member of additional servers. Each server has channels. Each channel corresponds to a topic. The names and descriptions of the different channels should be self-explanatory.\nPlease set your Discord profile for our server to your full name (first and last). Also, add an image of yourself to the profile. It will help make things a bit more personal and social, even if it‚Äôs not quite the same as in-person. You can do all that if you right-click your user name and choose Edit Server Profile. Doing so will adjust things just for our server. If you use Discord for other purposes and don‚Äôt want to use your real name on other servers, make sure you change the per-server profile. In contrast, if you want to change your user name/image and other details for all your Discord servers, you would adjust it under Main Profile.\nIf a channel is public, you can see it and participate in the discussion. For private channels you need an invite to see it and participate. For simplicity, all channels are public, though there will be channels which likely won‚Äôt be of interest to you. Specifically, you‚Äôll be placed into groups during the course and will likely only be interested in the channel for your group. If you have requests for specific channels to be created, let me know.\nInside a channel, you can write messages. To respond to someone‚Äôs message you should reply as thread so it‚Äôs clear which messages go together.\nYou can also use Discord to send one or more individuals direct messages (DM). You can use that to communicate with me or each other.\nOverall, Discord is fairly intuitive, and hopefully much easier to use than the rather clunky discussion feature of eLC. If you have no previous Discord experience, I suggest you browse through their very comprehensive documentation to pick up a few tips and tricks.\nOne useful thing to know is that people will get a specific notification if you either send them a direct message or use their handle in our message, e.g.¬†by writing @personname can you take a look at the figure below.... Specifically, if you want to get my attention, you need to do it that way. I have too many things pinging me all the time üòÅ. While I try to stay on top of all the messages in our class, I likely won‚Äôt. To ensure I don‚Äôt miss something directed at me, use the @ notation (or DM).\nI will use the announcements channel to post any important course related announcements. Check regularly.\nUse Discord widely to ask questions, to answer others‚Äô questions, to notify me and others of things that are unclear/wrong, to post links to interesting resources, etc. I hope we can build a community of learners where you help and support each other. So please don‚Äôt always wait for me to answer questions or provide feedback, instead, engage with each other! I will regularly visit and read the discussions and participate, but I hope that I will end up as one participant of many, not the only person providing answers.\nIn general, I prefer that you use the questions-help Discord channel to ask questions. The reason is that if you have a question, it is likely that others have it too. By asking and answering online, everyone can see it. You might also get a quicker reply since everyone can answer. That said, there might be instances where the question only relates to you or is personal. In such cases, feel free to either send me a direct message on Discord or email me.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#zoom",
    "href": "courseinfo/course-communication.html#zoom",
    "title": "Course Communications",
    "section": "Zoom",
    "text": "Zoom\nI will offer a weekly Zoom meeting (aka office hour).\nThose are completely optional. To ensure everyone will be able to join at least occasionally, I plan to alternate between two times each week.\nThe current plan is to have Zoom meetings for odd module numbers on Thursdays at 8am, and for even module numbers on Wednesday at 8am. The first Zoom session will be on Thursday, 1/15 at 8am ET. You can find the Zoom link on Discord.\nAt the designated times, I will start the meeting. I‚Äôll wait around for 15 minutes. If nobody connects, I‚Äôll end the meeting. If people connect, we‚Äôll talk until we covered everything, or until I or you need to move on to our next appointments.\nI want to emphasize that the only use for these Zoom sessions is for me to be there and answer any questions you might have. There is no material covered, no announcements, or anything of that sort. Unless you have a question/topic that you want to discuss and that can‚Äôt be addressed by posting it to Discord, there should be no reason for you to come to the Zoom meetings.\nI will not record these meetings to ensure students feel comfortable discussing/asking anything they want to. If any topic comes up that I think is worth sharing more widely, I will afterwards post a Discord message and/or add content to the website. Therefore, if you can‚Äôt make these Zoom meetings, you are not missing anything other than the option of asking me questions that might be tricky to discuss on Discord.\nAnd of course, if you have something you want to discuss outside of those weekly office hours, send me a message and we can schedule a meeting.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#github",
    "href": "courseinfo/course-communication.html#github",
    "title": "Course Communications",
    "section": "GitHub",
    "text": "GitHub\nWe will also use GitHub for some specific communications, related to exercises and projects. More detailed instructions on that are provided at the time you are asked to use GitHub.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#elc",
    "href": "courseinfo/course-communication.html#elc",
    "title": "Course Communications",
    "section": "eLC",
    "text": "eLC\nWe will not be using eLC. I might occasionally use the email everyone feature in eLC to send out an email to the class in the first few weeks until everyone has settled into using Discord. After that, I do not plan on using eLC and instead will communicate via Discord.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "courseinfo/course-communication.html#course-teaching-assistant",
    "href": "courseinfo/course-communication.html#course-teaching-assistant",
    "title": "Course Communications",
    "section": "Course Teaching Assistant",
    "text": "Course Teaching Assistant\nIn some semesters, I have a TA for the class. If there is a TA the semester you are taking the class, they will be another point of contact for you to ask questions. The TA will be posting an introduction to Discord with details of their availability and preferred modes of communication (Discord/Zoom/etc.). If no such posting shows up on Discord, it means there is no TA.",
    "crumbs": [
      "General Information",
      "Communication"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html",
    "href": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html",
    "title": "Revisiting the Full Data Analysis Workflow",
    "section": "",
    "text": "This very short unit re-visits all of the data analysis concepts and components we discussed so far, and provides a summary and big-picture discussion, meant to wrap up things.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-question",
    "href": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-question",
    "title": "Revisiting the Full Data Analysis Workflow",
    "section": "The question",
    "text": "The question\nHaving a good question (hypothesis) that is interesting, important, new, and can be answered with the resources you have (data, skills, computing power) is the most crucial part of any project. You can do an analysis that is technically perfect, but if you don‚Äôt answer an interesting and relevant question, nobody will care. While I think one should use state-of-the art analysis approaches as much as possible, it is in my opinion more important to answer a good question. I believe that an important question analyzed with a simple model is almost always better than using a complicated model to answer a question that nobody cares about. Of course, the simple model still needs to be reasonable. If one uses a completely wrong model or performs a faulty analysis, the whole project/paper might also be meaningless.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-setup",
    "href": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-setup",
    "title": "Revisiting the Full Data Analysis Workflow",
    "section": "The setup",
    "text": "The setup\nYou should do your whole analysis as automated, reproducible, and as well-structured and well-documented as possible. Your colleagues, your readers and your future self will thank you for it. We have used tools in this course (R/R Markdown/GitHub) which help in performing an analysis in such a way. Many other tools are available. While some tools are worse than others (e.g., Excel), in the end it doesn‚Äôt matter too much what tools you use, as long as you can do things automated, reproducible, and well-documented.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-wrangling",
    "href": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-wrangling",
    "title": "Revisiting the Full Data Analysis Workflow",
    "section": "The wrangling",
    "text": "The wrangling\nAs you will likely appreciate by now, getting the data into a shape that can be analyzed is ‚Äì for almost any dataset ‚Äì time consuming and also incredibly important. Lots of mistakes happen at that stage. For a recent prominent example where things went wrong, see e.g.¬†Aboumatar and Wise 2018 published to JAMA, where a mis-coding of a factor variable led to one conclusion, and upon fixing it, the conclusion changed in the other direction, leading to retraction of the original study (and republication of the corrected study). This is an example where an error was found and fixed. Unfortunately, there are probably a lot of studies in the literature where mistakes during the wrangling process were made, wrong results published, and nobody noticed. It is impossible to fully prevent making mistakes. But there are ways to try and minimize those problems. To do so, follow these rules:\n\nDocument everything very well. Every step in the wrangling/cleaning process should be explained and justified (e.g.¬†if you drop observations with NA, what does it mean and why do you think it‚Äôs ok to do).\nAutomate things as much as possible. Manual tasks often introduces errors.\nMake everything reproducible. That helps you and others spot mistakes faster.\nCritically evaluate every step you take. If something is happening that doesn‚Äôt look quite right, or you get warning messages in your code, stop and figure out what is going on. Only proceed once you know exactly what is happening and are ok with it.\nTry different alternatives. For instance if you are unclear if you should remove missing observations, or remove a variable that has a lot of missing, or use imputation, why not try it all 3 ways? It usually doesn‚Äôt take much extra work to do a few alternatives. If each version of doing things gives you more or less the same results, it helps convince yourself and the readers that your finding might be robust to the details of the analysis. If different reasonable ways of doing the analysis lead to different results, you have learned something too, and it might be worth digging deeper to understand why results differ. You might find some new, unexpected and interesting bit of science lurking. It is important to report an any analysis you did, even if just briefly in the supplement. You are not allowed to run multiple analyses and then just report the one that gives you the answers you want (likely happens often, see p-hacking above.)",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-analysis",
    "href": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-analysis",
    "title": "Revisiting the Full Data Analysis Workflow",
    "section": "The analysis",
    "text": "The analysis\nYou have learned that there are a lot of different analysis approaches out there, and which one to choose depends on many factors, such as the question (e.g.¬†do you care more about interpretability or performance), available resources, and many other considerations. All the rules listed above for wrangling hold for the analysis bit too. Make it reproducible, well documented, well explained and justified. Make sure you understand results at each step. If possible, try different alternative approaches. Some additional, analysis-specific considerations are the following:\n\nThink carefully about the performance measure you want to fit to. While the ‚Äòstandard‚Äô ones, like RMSE/SSR for continuous outcomes and accuracy for categorical outcomes are at times ok, often other measures might be more meaningful. E.g. for continuous outcomes, you might want to do RMSE not on the outcome but the log of the outcome. Or you might want to penalize with least absolute difference to better deal with outliers. Similarly, for categorical outcomes, especially when there is imbalance in the data and you have much fewer of one category than of others, using accuracy might not be best. Some other metric such as F1 score, or a custom performance measure might be better. Spend some time thinking about the best performance measure before you do all your fitting.\nOnce you picked your performance measure and are ready to fit/train your model, make sure to not evaluate performance on the data used for building the model. More complex models can always give improved performance on the data used to build the model, thus this metric is not meaningful! Instead, to evaluate model performance, ideally use some version of cross-validation, i.e.¬†fitting the model to some of the data and evaluating model performance on a part of the data that was not used for fitting. If this is not possible, e.g. because you don‚Äôt have much data or it takes too long to run, us AIC & Co.¬†as a backup option to determine model quality.\nCompare your model to baseline/null models and simple single-predictor models to get an idea for the improvement you can get. Try a complex model to estimate the upper bound of model performance. Then try a few reasonable models ‚Äúin between‚Äù the null model and the really complex model, and pick the one that works overall best for your purpose. That last step is subjective. That is ok, as long as you can explain and justify why you ended up going with the model you chose.\nOnce you have chosen your best model (or even before, for the purpose of picking your final model), perform model assessment. Look at uncertainty, investigate residuals, look at variable importance, etc. Poke your model in as many ways as possible to understand how it works and what its limitations might be.\nIf you have enough data, set some aside at the beginning (test data), and apply your model to that data at the very end. This gives you the most honest assessment of your model performance for new/unseen data.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-reporting",
    "href": "content/module-workflow/complete-data-analysis-workflow/complete-data-analysis-workflow.html#the-reporting",
    "title": "Revisiting the Full Data Analysis Workflow",
    "section": "The reporting",
    "text": "The reporting\nYou almost never do an analysis just for fun and for yourself. Usually, there are other reasons. For instance in academia/science, we analyze data to better understand our system, to test hypotheses, to answer questions. Similarly in industry and other applied settings, we analyze data to come to actionable conclusions (e.g.¬†we determine which images show a likely cancer and therefore which patients need further tests or surgery). In all those situations, we want to communicate our findings to others. That can be through the peer-reviewed literature, in a meeting with our colleagues and bosses, as a report for patients, etc. Being able to report findings from a potentially complicated analysis in a way that it has an impact, and is appropriate for the right audience, is not easy. Some of the ideas listed above, as well as others are worth keeping in mind:\n\nPresent your findings in such a way that people can (but don‚Äôt have to) go deeper easily. Start with a short summary (often called executive summary in industry and abstract in academia). This short write-up should summarize your findings understandably and honestly. Do not spin/hype things that are not supported by your actual analysis. Also, focus on the main important finding(s) and their implications. The main deliverable (usually some form of written report or a paper), should present all the main findings and steps you took, nicely explained. Then provide additional information (e.g.¬†supplement, appendices) with more details. Finally, provide all the raw materials, i.e.¬†data and well-documented code, for others to look at. By layering content, different audiences can go into your findings in as much or little detail as they want.\nExplain and justify everything. It‚Äôs unlikely that everyone would have made exactly the same decisions you did during your analysis. But by explaining your rationale, readers can decide if they find what you did reasonable, and thus make an informed decision as to how much they trust your findings.\nReport results from multiple approaches: If you show how certain decisions during the analysis do or don‚Äôt affect the results, it makes things more transparent and can instill greater confidence in readers.\nAutomate things. As much of your final products as possible should be automated. That means that figures and tables should not be created by hand. This way, if you want or have to change things upstream (e.g.¬†you noticed a mistake in some analysis step or reviewers/your boss request changes), you can update everything as quickly, automated and seamless as possible.\nUse modern ways to report and disseminate your findings. The standard academic way is still to write peer-reviewed papers, or in industry, prepare a report. However, such documents are generally not too widely read and at times have only limited impact. As appropriate for your project, consider other sources of dissemination. For instance, make a website for your analysis. Turn it into a blog post. Tweet about it. Use interactive tools (e.g. the R Shiny package) to allow the audience to interact with your results. Be creative and think about the best ways to reach your intended audience.",
    "crumbs": [
      "Content",
      "13. Complete Analysis Workflow",
      "Complete Data Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html",
    "title": "R packages to generate synthetic data",
    "section": "",
    "text": "This unit provides brief introductions of different R packages that can be used to generate synthetic/simulated/artificial/fake data.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "R packages for data creation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#r-packages-to-generate-synthetic-data",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#r-packages-to-generate-synthetic-data",
    "title": "R packages to generate synthetic data",
    "section": "R packages to generate synthetic data",
    "text": "R packages to generate synthetic data\n\nThe admiral package, which is part of the pharmaverse, allows users to generate data in ADaM format. ADaM is a common data format in the pharmaceutical area.\nThe simstudy package allows users to define variables and relations between them, and then have synthetic data generated based on those specifications. It can also create data with potentially complex structures, such as longitudinal or hierarchical data.\nThe synthpop package can produce synthetic data that is very similar to the original data, including potential patterns/correlations.\nThe mice package is most often used to impute missing data. But one can also use it to generate synthetic data. This tutorial provides a good introduction.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "R packages for data creation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#some-r-helper-packages",
    "href": "content/module-synthetic-data/synthetic-data-r-packages/synthetic-data-r-packages.html#some-r-helper-packages",
    "title": "R packages to generate synthetic data",
    "section": "Some R helper packages",
    "text": "Some R helper packages\n\nThe CRAN Task View: Probability Distributions lists packages that can be used to generate data following various probability distributions. This can be useful if you want to generate data that has a certain shape/distribution.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "R packages for data creation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html",
    "title": "Generating synthetic data with R",
    "section": "",
    "text": "In this unit, we look at a few examples of generating synthetic data with R.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup",
    "title": "Generating synthetic data with R",
    "section": "Setup",
    "text": "Setup\nWe‚Äôll start by loading the packages used in the code below. While we will not use packages that are specifically meant to generate synthetic data, we will still use several common packages that make data generation tasks easier. That said, you could also do all of this with base R and no additional packages.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\nlibrary(purrr)\nlibrary(lubridate)\nlibrary(ggplot2)\n\n\n# Set a seed for reproducibility\nset.seed(123)\n# Define the number of observations (patients) to generate\nn_patients &lt;- 100",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data",
    "title": "Generating synthetic data with R",
    "section": "Generating data",
    "text": "Generating data\n\n# Create an empty data frame with placeholders for variables\nsyn_dat &lt;- data.frame(\n  PatientID = numeric(n_patients),\n  Age = numeric(n_patients),\n  Gender = character(n_patients),\n  TreatmentGroup = character(n_patients),\n  EnrollmentDate = lubridate::as_date(character(n_patients)),\n  BloodPressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  AdverseEvent = integer(n_patients)\n)\n\n# Variable 1: Patient ID\nsyn_dat$PatientID &lt;- 1:n_patients\n\n# Variable 2: Age (numeric variable)\nsyn_dat$Age &lt;- round(rnorm(n_patients, mean = 45, sd = 10), 1)\n\n# Variable 3: Gender (categorical variable)\nsyn_dat$Gender &lt;- purrr::map_chr(sample(c(\"Male\", \"Female\"), n_patients, replace = TRUE), as.character)\n\n# Variable 4: Treatment Group (categorical variable)\nsyn_dat$TreatmentGroup &lt;- purrr::map_chr(sample(c(\"A\", \"B\", \"Placebo\"), n_patients, replace = TRUE), as.character)\n\n# Variable 5: Date of Enrollment (date variable)\nsyn_dat$EnrollmentDate &lt;- lubridate::as_date(sample(seq(from = lubridate::as_date(\"2022-01-01\"), to = lubridate::as_date(\"2022-12-31\"), by = \"days\"), n_patients, replace = TRUE))\n\n# Variable 6: Blood Pressure (numeric variable)\nsyn_dat$BloodPressure &lt;- round(runif(n_patients, min = 90, max = 160), 1)\n\n# Variable 7: Cholesterol Level (numeric variable)\n# Option 1: Cholesterol is independent of treatment\n#syn_dat$Cholesterol &lt;- round(rnorm(n_patients, mean = 200, sd = 30), 1)\n\n# Option 2: Cholesterol is dependent on treatment\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"A\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"A\"), mean = 160, sd = 10), 1)\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"B\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"B\"), mean = 180, sd = 10), 1)\nsyn_dat$Cholesterol[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- round(rnorm(sum(syn_dat$TreatmentGroup == \"Placebo\"), mean = 200, sd = 10), 1)\n\n# Variable 8: Adverse Event (binary variable, 0 = No, 1 = Yes)\n# Option 1: Adverse events are independent of treatment\n#syn_dat$AdverseEvent &lt;- purrr::map_int(sample(0:1, n_patients, replace = TRUE, prob = c(0.8, 0.2)), as.integer)\n\n# Option 2: Adverse events are influenced by treatment status\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"A\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"A\"), replace = TRUE, prob = c(0.5, 0.5)), as.integer)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"B\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"B\"), replace = TRUE, prob = c(0.7, 0.3)), as.integer)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- purrr::map_int(sample(0:1, sum(syn_dat$TreatmentGroup == \"Placebo\"), replace = TRUE, prob = c(0.9, 0.1)), as.integer)\n\n\n# Print the first few rows of the generated data\nhead(syn_dat)\n\n  PatientID  Age Gender TreatmentGroup EnrollmentDate BloodPressure Cholesterol\n1         1 39.4 Female              B     2022-08-25         152.0       179.7\n2         2 42.7 Female              B     2022-06-14         128.7       192.2\n3         3 60.6 Female              A     2022-04-17         153.4       150.6\n4         4 45.7   Male              B     2022-02-02         131.1       171.7\n5         5 46.3 Female              A     2022-03-24         119.6       160.5\n6         6 62.2 Female              A     2022-12-20         156.5       154.3\n  AdverseEvent\n1            1\n2            0\n3            0\n4            0\n5            1\n6            1\n\n# Save the simulated data to a CSV and Rds file\nwrite.csv(syn_dat, \"syn_dat.csv\", row.names = FALSE)\n# if we wanted an RDS version\n#saveRDS(syn_dat, here(\"data\",\"syn_dat.Rds\"))",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data",
    "title": "Generating synthetic data with R",
    "section": "Checking data",
    "text": "Checking data\nTake a peek at the generated data.\n\nsummary(syn_dat)\n\n   PatientID           Age           Gender          TreatmentGroup    \n Min.   :  1.00   Min.   :21.90   Length:100         Length:100        \n 1st Qu.: 25.75   1st Qu.:40.08   Class :character   Class :character  \n Median : 50.50   Median :45.60   Mode  :character   Mode  :character  \n Mean   : 50.50   Mean   :45.90                                        \n 3rd Qu.: 75.25   3rd Qu.:51.92                                        \n Max.   :100.00   Max.   :66.90                                        \n EnrollmentDate       BloodPressure    Cholesterol     AdverseEvent \n Min.   :2022-01-08   Min.   : 91.3   Min.   :129.6   Min.   :0.00  \n 1st Qu.:2022-04-06   1st Qu.:110.7   1st Qu.:160.7   1st Qu.:0.00  \n Median :2022-06-25   Median :130.8   Median :176.3   Median :0.00  \n Mean   :2022-06-30   Mean   :128.0   Mean   :175.2   Mean   :0.29  \n 3rd Qu.:2022-10-04   3rd Qu.:147.4   3rd Qu.:188.7   3rd Qu.:1.00  \n Max.   :2022-12-30   Max.   :159.5   Max.   :223.7   Max.   :1.00  \n\ndplyr::glimpse(syn_dat)  \n\nRows: 100\nColumns: 8\n$ PatientID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ Age            &lt;dbl&gt; 39.4, 42.7, 60.6, 45.7, 46.3, 62.2, 49.6, 32.3, 38.1, 4‚Ä¶\n$ Gender         &lt;chr&gt; \"Female\", \"Female\", \"Female\", \"Male\", \"Female\", \"Female‚Ä¶\n$ TreatmentGroup &lt;chr&gt; \"B\", \"B\", \"A\", \"B\", \"A\", \"A\", \"B\", \"Placebo\", \"A\", \"Pla‚Ä¶\n$ EnrollmentDate &lt;date&gt; 2022-08-25, 2022-06-14, 2022-04-17, 2022-02-02, 2022-0‚Ä¶\n$ BloodPressure  &lt;dbl&gt; 152.0, 128.7, 153.4, 131.1, 119.6, 156.5, 139.6, 118.9,‚Ä¶\n$ Cholesterol    &lt;dbl&gt; 179.7, 192.2, 150.6, 171.7, 160.5, 154.3, 172.8, 189.2,‚Ä¶\n$ AdverseEvent   &lt;int&gt; 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1‚Ä¶\n\n# Frequency table for adverse events stratified by treatment\ntable(syn_dat$AdverseEvent,syn_dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 28 19      24\n  1 15 11       3\n\n# ggplot2 boxplot for cholesterol by treatment group\nggplot(syn_dat, aes(x = TreatmentGroup, y = Cholesterol)) +\n  geom_boxplot() +\n  labs(x = \"Treatment Group\", y = \"Cholesterol Level\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nThis concludes our first example. This was a very simple setup, with rectangular data. You often encounter these kind of data in clinical trials. As you generate your data, you can build in any dependencies between variables you want to explore. Then later, when you use the data to test your analysis code, you can see if your analysis code can detect the dependencies you built in. We‚Äôll come back to that.\nYou could also add further complexities into your synthetic data, for instance you could set some values to be missing, or you could add some outliers. The goal is to generate data that has as the important features of your real dataset to allow you to test your analysis approach on data where you now the truth (since you generated it). If your analysis works on your generated data, there is hope it might also work on the real data (for which of course you don‚Äôt know the truth). We‚Äôll define in a later unit what we mean by ‚Äúyour analysis works‚Äù. But basically, you want to be able to recover the patterns/dependencies you built into your data with your analysis methods.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#setup-1",
    "title": "Generating synthetic data with R",
    "section": "Setup",
    "text": "Setup\nThe usual setup steps.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(here)\n\n\n# Set seed for reproducibility\nset.seed(123)\n# Number of patients in each treatment group\nnum_patients &lt;- 20\n# Number of days and samples per patient\nnum_days &lt;- 7\nnum_samples_per_day &lt;- 1",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#generating-data-1",
    "title": "Generating synthetic data with R",
    "section": "Generating data",
    "text": "Generating data\n\n# Treatment group levels\ntreatment_groups &lt;- c(\"Low Dose\", \"High Dose\")\n\n# Generate patient IDs\npatient_ids &lt;- rep(1:num_patients, each = num_days)\n\n# Generate treatment group assignments for each patient\ntreatment_assignments &lt;- rep(sample(treatment_groups, num_patients, replace = TRUE), \n                             each = num_days)\n\n# Generate day IDs for each patient\nday_ids &lt;- rep(1:num_days, times = num_patients)\n\n# Function to generate drug concentrations with variability\ngenerate_drug_concentrations &lt;- function(day, dose_group, patient_id) {\n  baseline_concentration &lt;- ifelse(dose_group == \"Low Dose\", 8, 15)\n  patient_variation &lt;- rnorm(1, mean = 0, sd = 1)\n  time_variation &lt;- exp(-0.1*day)\n  baseline_concentration * time_variation + patient_variation \n}\n\n\n# Generate drug concentrations for each sample\ndrug_concentrations &lt;- mapply(generate_drug_concentrations, \n                              day = rep(day_ids, each = num_samples_per_day), \n                              dose_group = treatment_assignments,\n                              patient_id = rep(1:num_patients, each = num_days))\n\n\n# Flatten the matrix to a vector\ndrug_concentrations &lt;- as.vector(drug_concentrations)\n\n# Generate cholesterol levels for each sample \n# (assuming a positive correlation with drug concentration)\ncholesterol_levels &lt;- drug_concentrations + \n  rnorm(num_patients * num_days * num_samples_per_day, mean = 0, sd = 5)\n\n# Generate adverse events based on drug concentration \n# (assuming a higher chance of adverse events with higher concentration)\n# Sigmoid function to map concentrations to probabilities\nadverse_events_prob &lt;- plogis(drug_concentrations / 10) \nadverse_events &lt;- rbinom(num_patients * num_days * num_samples_per_day, \n                         size = 1, prob = adverse_events_prob)\n\n# Create a data frame\nsyn_dat2 &lt;- data.frame(\n  PatientID = rep(patient_ids, each = num_samples_per_day),\n  TreatmentGroup = rep(treatment_assignments, each = num_samples_per_day),\n  Day = rep(day_ids, each = num_samples_per_day),\n  DrugConcentration = drug_concentrations,\n  CholesterolLevel = cholesterol_levels,\n  AdverseEvent = adverse_events\n)",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data-1",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#checking-data-1",
    "title": "Generating synthetic data with R",
    "section": "Checking data",
    "text": "Checking data\nTake a peek at the generated data.\n\n# Print the first few rows of the generated dataset\nprint(head(syn_dat2))\n\n  PatientID TreatmentGroup Day DrugConcentration CholesterolLevel AdverseEvent\n1         1       Low Dose   1          8.462781        12.401475            0\n2         1       Low Dose   2          6.909660        10.754871            0\n3         1       Low Dose   3          6.327317         7.988330            0\n4         1       Low Dose   4          5.473243         0.431360            1\n5         1       Low Dose   5          4.296404         3.699141            0\n6         1       Low Dose   6          6.177406         4.775430            1\n\nsummary(syn_dat2)\n\n   PatientID     TreatmentGroup          Day    DrugConcentration\n Min.   : 1.00   Length:140         Min.   :1   Min.   : 2.081   \n 1st Qu.: 5.75   Class :character   1st Qu.:2   1st Qu.: 5.174   \n Median :10.50   Mode  :character   Median :4   Median : 7.056   \n Mean   :10.50                      Mean   :4   Mean   : 7.593   \n 3rd Qu.:15.25                      3rd Qu.:6   3rd Qu.: 9.738   \n Max.   :20.00                      Max.   :7   Max.   :14.933   \n CholesterolLevel  AdverseEvent   \n Min.   :-4.494   Min.   :0.0000  \n 1st Qu.: 3.844   1st Qu.:0.0000  \n Median : 7.234   Median :1.0000  \n Mean   : 7.855   Mean   :0.6571  \n 3rd Qu.:11.259   3rd Qu.:1.0000  \n Max.   :24.422   Max.   :1.0000  \n\ndplyr::glimpse(syn_dat2)  \n\nRows: 140\nColumns: 6\n$ PatientID         &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3‚Ä¶\n$ TreatmentGroup    &lt;chr&gt; \"Low Dose\", \"Low Dose\", \"Low Dose\", \"Low Dose\", \"Low‚Ä¶\n$ Day               &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4‚Ä¶\n$ DrugConcentration &lt;dbl&gt; 8.462781, 6.909660, 6.327317, 5.473243, 4.296404, 6.‚Ä¶\n$ CholesterolLevel  &lt;dbl&gt; 12.4014754, 10.7548711, 7.9883301, 0.4313600, 3.6991‚Ä¶\n$ AdverseEvent      &lt;int&gt; 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1‚Ä¶",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#exploratory-plot",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#exploratory-plot",
    "title": "Generating synthetic data with R",
    "section": "Exploratory plot",
    "text": "Exploratory plot\n\n# Plot drug concentrations over time for each individual using ggplot2\np1 &lt;- ggplot(syn_dat2, aes(x = Day, y = DrugConcentration, \n                      group = as.factor(PatientID), color = TreatmentGroup)) +\n  geom_line() +\n  labs(title = \"Drug Concentrations Over Time\",\n       x = \"Day\",\n       y = \"Drug Concentration\",\n       color = \"TreatmentGroup\") +\n  theme_minimal()\nplot(p1)\n\n\n\n\n\n\n\np2 &lt;- ggplot(syn_dat2, aes(x = as.factor(AdverseEvent), y = DrugConcentration, \n                           fill = TreatmentGroup)) +\n  geom_boxplot(width = 0.7, position = position_dodge(width = 0.8), color = \"black\") +\n  geom_point(aes(color = TreatmentGroup), position = position_dodge(width = 0.8), \n             size = 3, shape = 16) +  # Overlay raw data points\n  labs(\n    x = \"Adverse Events\",\n    y = \"Drug Concentration\",\n    title = \"Boxplot of Drug Concentration by Adverse Events and Treatment\"\n  ) +\n  scale_color_manual(values = c(\"A\" = \"blue\", \"B\" = \"red\")) +  # Customize color for each treatment\n  theme_minimal() +\n  theme(legend.position = \"top\")\nplot(p2)\n\nWarning: No shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\nNo shared levels found between `names(values)` of the manual scale and the\ndata's colour values.\n\n\n\n\n\n\n\n\n\nThis dataset has a bit more complicated structure than the previous one, but it isn‚Äôt much harder to generate it with code. You‚Äôll want to generate your data such that it mimics the real data you plan on analyzing.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#save-data",
    "href": "content/module-synthetic-data/synthetic-data-new-r/synthetic-data-new-r.html#save-data",
    "title": "Generating synthetic data with R",
    "section": "Save data",
    "text": "Save data\n\n# Save the simulated data to a CSV or Rds file\nwrite.csv(syn_dat2, \"syn_dat2.csv\", row.names = FALSE)\n# if we wanted an RDS version\n#saveRDS(syn_dat2, here(\"data\",\"syn_dat2.Rds\"))\n\nWe are saving the data as CSV and Rds files. CSV files are readable with pretty much any software and thus very portable. Rds files are R-specific, thus not as flexible. The advantage of Rds files is that they are generally smaller, and they retain information about the variables, e.g.¬†if they are factor or numeric variables. Either format works, sometimes one or the other might be better.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-model-testing-r/synthetic-data-model-testing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-model-testing-r/synthetic-data-model-testing-r.html",
    "title": "Use synthetic data to test models",
    "section": "",
    "text": "Overview\nIn this unit, we explore the use of synthetic data to test your models.\n\n\nLearning Objectives\n\nUnderstand the use of synthetic data for testing models.\nBe able to implement model testing on synthetic data in R.\n\n\n\nIntroduction\nAt the beginning of this module, I mentioned that one great use-case of synthetic data is the ability to see if whatever patterns you stuck into the data when you generated it can be recovered by your analysis. If your analysis fails to recover what you built, it means your model is likely not suitable and you need to modify it.\nOf course, keep in mind that just because your model can recover the patterns/correlations you stuck into the data doesn‚Äôt mean it can do the same for the real data, or that any patterns in the real data are the same. However, if you can NOT recover the patterns for your simulated data, it‚Äôs a strong sign to stop and reconsider your analysis approach. It will very likely also not produce useful results for the real data (but you won‚Äôt know, since you don‚Äôt know what processes generated the real data).\n\n\nAn example\nWe will revisit the first example from the Generating synthetic data with R unit. If you don‚Äôt remember, take a look at the code shown in that unit to see that we created a variable called BloodPressure which was randomly distributed, and a variable called Cholesterol which varied by treatment group.\nIf we fit a model, we expect to recover these patterns. We can for instance fit a linear model with Cholesterol as the outcome and BloodPressure and TreatmentGroup as predictors.\nHere‚Äôs code to do that (you can again find the full R script here).\nSetting things up\n\n# Load the readr package for reading csv data\nlibrary(readr)\nlibrary(dplyr)\nlibrary(broom)\nlibrary(parsnip)\nlibrary(here)\n\n# Read in the csv data \ndata &lt;- read_csv(\"syn_dat.csv\")\n\nWe‚Äôll do a little bit of processing before fitting\n\n# select variables of interest\n# not strictly needed, but can sometimes make for more robust code to only keep what's necessary\ndata &lt;- data %&gt;%  \n  select(Cholesterol, BloodPressure, TreatmentGroup)\n\n# Standardize BloodPressure and Cholesterol\n# Helps with interpretation of coefficients\ndata &lt;- data %&gt;% \n  mutate(\n    BloodPressure = scale(BloodPressure),\n    Cholesterol = scale(Cholesterol)\n  )\n# turn TreatmentGroup into a factor\ndata$TreatmentGroup &lt;- as.factor(data$TreatmentGroup)\n\n# check to make sure data looks ok before fitting\nsummary(data)\n\n        Cholesterol.V1              BloodPressure.V1       TreatmentGroup\n Min.   :-2.38863692699e+00   Min.   :-1.72011727192e+00   A      :43    \n 1st Qu.:-7.63161855038e-01   1st Qu.:-8.10951978975e-01   B      :30    \n Median : 5.87369736145e-02   Median : 1.35753042193e-01   Placebo:27    \n Mean   :-6.00000000000e-16   Mean   : 3.00000000000e-16                 \n 3rd Qu.: 7.05262485325e-01   3rd Qu.: 9.11183053336e-01                 \n Max.   : 2.53752102054e+00   Max.   : 1.48014455924e+00                 \n\n\nFitting the first model\n\n# Fit linear model \nmodel1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol ~ BloodPressure + TreatmentGroup, data = data)\ntidy(model1)\n\n# A tibble: 4 √ó 5\n  term                  estimate std.error statistic  p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)            -0.898     0.0822    -10.9  1.58e-18\n2 BloodPressure          -0.0822    0.0548     -1.50 1.37e- 1\n3 TreatmentGroupB         1.19      0.129       9.24 6.48e-15\n4 TreatmentGroupPlacebo   2.00      0.133      15.1  4.05e-27\n\nglance(model1)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.718         0.709 0.539      81.6 2.61e-26     3  -78.0  166.  179.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nWe find what we hope to find. The blood pressure variable shows no noticeable correlation with cholesterol, while treatment group does.\nWe can explore other models. Here is one with an interaction term between BloodPressure and TreatmentGroup. We know there is none, since we know how the data was generated. So to test our model, we fit it to confirm that this is what we get:\n\n# fit a model with interaction\nmodel2 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol ~ BloodPressure + TreatmentGroup + BloodPressure*TreatmentGroup , data = data)\ntidy(model2)\n\n# A tibble: 6 √ó 5\n  term                                estimate std.error statistic  p.value\n  &lt;chr&gt;                                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)                          -0.901     0.0825   -10.9   2.15e-18\n2 BloodPressure                        -0.139     0.0766    -1.82  7.18e- 2\n3 TreatmentGroupB                       1.19      0.131      9.10  1.52e-14\n4 TreatmentGroupPlacebo                 2.02      0.134     15.1   8.90e-27\n5 BloodPressure:TreatmentGroupB         0.0856    0.129      0.665 5.08e- 1\n6 BloodPressure:TreatmentGroupPlacebo   0.163     0.144      1.13  2.60e- 1\n\nglance(model2)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.722         0.708 0.541      48.9 1.09e-24     5  -77.3  169.  187.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nAnd we do see no evidence for interaction being present. So for this simple example, the model works, namely it recovers the patterns we stuck into the data. This means it might be ok to use this model on our real data.\nLet‚Äôs now look at an example where the model does not properly recover the pattern in the data.\n\n\nA failing example\nWe‚Äôll start with the same data as before, but now mess around with it. Specifically, I‚Äôm creating a new variable called Drug and a new cholesterol variable that depends on drug concentration in a nonlinear manner.\n\n# Load the readr package for reading csv data\nlibrary(readr)\nlibrary(dplyr)\nlibrary(here)\nlibrary(tidymodels)\nlibrary(vip)\n\n# Read in the csv data \nset.seed(123)\ndata &lt;- read_csv(\"syn_dat.csv\")\n\n\n# Make Cholesterol depend on blood pressure in a nonlinear manner\ndata$Drug &lt;- runif(nrow(data),min = 100, max = 200)\n# making up Cholesterol value as function of Drug\n# this equation makes it such that cholesterol depends on drug in a nonlinear manner\ndata$Cholesterol2 &lt;- sqrt(100^2 + 10* (mean(data$Drug) -  data$Drug)^2)\n# plotting correlation between Drug and new cholesterol variables\nggplot(data,aes(x=Drug, y=Cholesterol2)) + geom_point()\n\n\n\n\n\n\n\n\n\n# select variables of interest\n# not strictly needed, but can sometimes make for more robust code to only keep what's necessary\ndata &lt;- data %&gt;%  \n  select(Cholesterol2, Drug, BloodPressure, TreatmentGroup)\n\n# Standardize continuous variables \n# Helps with interpretation of coefficients\ndata &lt;- data %&gt;% \n  mutate(\n    BloodPressure = scale(BloodPressure),\n    Drug = scale(Drug),\n    Cholesterol2 = scale(Cholesterol2)\n  )\n# turn TreatmentGroup into a factor\ndata$TreatmentGroup &lt;- as.factor(data$TreatmentGroup)\n\n# check to make sure data looks ok before fitting\nsummary(data)\n\n       Cholesterol2.V1                 Drug.V1         \n Min.   :-1.22222514059e+00   Min.   :-1.747166617720  \n 1st Qu.:-9.20323907468e-01   1st Qu.:-0.888043951821  \n Median :-1.25708288800e-01   Median :-0.112943096895  \n Mean   : 1.00000000000e-16   Mean   : 0.000000000000  \n 3rd Qu.: 8.74199838460e-01   3rd Qu.: 0.901461486215  \n Max.   : 2.10805990267e+00   Max.   : 1.739364949530  \n       BloodPressure.V1       TreatmentGroup\n Min.   :-1.72011727192e+00   A      :43    \n 1st Qu.:-8.10951978975e-01   B      :30    \n Median : 1.35753042193e-01   Placebo:27    \n Mean   : 3.00000000000e-16                 \n 3rd Qu.: 9.11183053336e-01                 \n Max.   : 1.48014455924e+00                 \n\n\nNow we‚Äôll fit another linear model, as before. There is a correlation between dose and cholesterol. But it‚Äôs not linear, and therefore the model doesn‚Äôt detect it.\n\n# Fit linear model \nmodel1 &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  parsnip::fit(Cholesterol2 ~ Drug + BloodPressure + TreatmentGroup, data = data)\nbroom::tidy(model1)\n\n# A tibble: 5 √ó 5\n  term                  estimate std.error statistic p.value\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)             0.101      0.155     0.653   0.515\n2 Drug                    0.0353     0.103     0.343   0.732\n3 BloodPressure           0.0574     0.103     0.557   0.579\n4 TreatmentGroupB        -0.247      0.243    -1.02    0.311\n5 TreatmentGroupPlacebo  -0.0997     0.251    -0.398   0.692\n\nbroom::glance(model1)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1    0.0142       -0.0273  1.01     0.343   0.848     4  -141.  293.  309.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nIt is always possible to try a more complex model to see if there might be patterns that a linear or other simple model can‚Äôt detect. Here, we are trying a random forest model, which can detect more complicated correlations between predictor variables and output. Random forest models don‚Äôt produce the standard p-values, but one can look at something called variable importance to see which variables most impact the outcome. And we see that it correctly identifies drug as the most important variable. The other two should really be at zero importance, since they are not correlated with the outcome. But this model is flexible enough to fit to possibly spurious patterns.\n\n# fit a random forest model \n# use the workflow approach from tidymodels\nrf_mod &lt;- rand_forest(mode = \"regression\") %&gt;% \n  set_engine(\"ranger\", importance = \"impurity\")\n# the recipe, i.e., the model to fit\nrf_recipe &lt;- \n  recipe(Cholesterol2 ~ Drug + BloodPressure + TreatmentGroup, data = data) \n\n# set up the workflow\nrf_workflow &lt;- \n  workflow() %&gt;% \n  add_model(rf_mod) %&gt;% \n  add_recipe(rf_recipe)\n\n# run the fit\nmodel2 &lt;- rf_workflow %&gt;% \n      fit(data)\n\n# get variable importance\nimp &lt;- model2 %&gt;% \n  extract_fit_parsnip() %&gt;% \n  vip()\nprint(imp)\n\n\n\n\n\n\n\n\nThe take-home message from this is that our simulated data showed us that a linear model can‚Äôt pick up the pattern, and we need a different model.\n\n\nSummary\nModel testing is one of the most important applications for synthetic data. Since you generated the data and know everything about it, you know exactly what the analysis of the data should find. If your models can‚Äôt find the right patterns, it means you need to modify your analysis.\n\n\nFurther Resources",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Use synthetic data to test models"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html",
    "title": "Scrambling existing data - R",
    "section": "",
    "text": "In this tutorial, we discuss how to scramble existing data to make it ‚Äúnew‚Äù.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Scrambling existing Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#ai-help",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#ai-help",
    "title": "Scrambling existing data - R",
    "section": "AI help",
    "text": "AI help\nSince you are working with the real data, you probably don‚Äôt want to use AI for this, unless your AI tool operates in a secure environment (e.g., fully on your companies‚Äô servers).",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Scrambling existing Data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#example",
    "href": "content/module-synthetic-data/synthetic-data-existing-r/synthetic-data-existing-r.html#example",
    "title": "Scrambling existing data - R",
    "section": "Example",
    "text": "Example\nTime for a simple example. You can find the code shown below in this file.\n\nSetup\nFirst, we do the usual setup steps of package loading and other housekeeping steps.\n\nlibrary('dplyr')\nlibrary('ggplot2')\nlibrary('here')\n\n\n# setting a random number seed for reproducibility\nset.seed(123)\n\n\n\nData loading and exploring\nWe‚Äôll look at some real data from this paper. As is good habit (and should be the standard), the authors (which includes some of us) supplied the data as part of the supplementary materials, which can be found here.\nIf you want to work along, go ahead and download the supplement, which is a zip file. Inside the zip file, find the Clean Data folder and the SympAct_Any_Pos.Rda file. Copy that file to the location where you‚Äôll be placing your R script.\nFirst, we load the data. Note that the authors (that would be us üòÅ) used the wrong file ending, they called it an .Rda file, even though it is an .Rds file (for a discussion of the differences, see e.g.¬†here).\n\n\nThe data\n\n#assuming your R script is in the same folder\n#rawdat &lt;- readRDS('SympAct_Any_Pos.Rda')\n# this is for my setup\nrawdat &lt;- readRDS('SympAct_Any_Pos.Rda')\n\nNext, we take a peek.\n\ndim(rawdat)\n\n[1] 735  63\n\ndplyr::glimpse(rawdat)  \n\nRows: 735\nColumns: 63\n$ DxName1           &lt;fct&gt; \"Influenza like illness - Clinical Dx\", \"Acute tonsi‚Ä¶\n$ DxName2           &lt;fct&gt; NA, \"Influenza like illness - Clinical Dx\", \"Acute p‚Ä¶\n$ DxName3           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Fever, unspecified\"‚Ä¶\n$ DxName4           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Other fatigue\", NA,‚Ä¶\n$ DxName5           &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, \"Headache\", NA, NA, ‚Ä¶\n$ Unique.Visit      &lt;chr&gt; \"340_17632125\", \"340_17794836\", \"342_17737773\", \"342‚Ä¶\n$ ActivityLevel     &lt;int&gt; 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, ‚Ä¶\n$ ActivityLevelF    &lt;fct&gt; 10, 6, 2, 2, 5, 3, 4, 0, 0, 5, 9, 1, 3, 6, 5, 2, 2, ‚Ä¶\n$ SwollenLymphNodes &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, No, Yes, Y‚Ä¶\n$ ChestCongestion   &lt;fct&gt; No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ ChillsSweats      &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, ‚Ä¶\n$ NasalCongestion   &lt;fct&gt; No, Yes, Yes, Yes, No, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ CoughYN           &lt;fct&gt; Yes, Yes, No, Yes, No, Yes, Yes, Yes, Yes, Yes, No, ‚Ä¶\n$ Sneeze            &lt;fct&gt; No, No, Yes, Yes, No, Yes, No, Yes, No, No, No, No, ‚Ä¶\n$ Fatigue           &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ SubjectiveFever   &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, Yes‚Ä¶\n$ Headache          &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes‚Ä¶\n$ Weakness          &lt;fct&gt; Mild, Severe, Severe, Severe, Moderate, Moderate, Mi‚Ä¶\n$ WeaknessYN        &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ CoughIntensity    &lt;fct&gt; Severe, Severe, Mild, Moderate, None, Moderate, Seve‚Ä¶\n$ CoughYN2          &lt;fct&gt; Yes, Yes, Yes, Yes, No, Yes, Yes, Yes, Yes, Yes, Yes‚Ä¶\n$ Myalgia           &lt;fct&gt; Mild, Severe, Severe, Severe, Mild, Moderate, Mild, ‚Ä¶\n$ MyalgiaYN         &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Yes, Ye‚Ä¶\n$ RunnyNose         &lt;fct&gt; No, No, Yes, Yes, No, No, Yes, Yes, Yes, Yes, No, No‚Ä¶\n$ AbPain            &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, Yes, Yes, N‚Ä¶\n$ ChestPain         &lt;fct&gt; No, No, Yes, No, No, Yes, Yes, No, No, No, No, Yes, ‚Ä¶\n$ Diarrhea          &lt;fct&gt; No, No, No, No, No, Yes, No, No, No, No, No, No, No,‚Ä¶\n$ EyePn             &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, No, Yes, No, Ye‚Ä¶\n$ Insomnia          &lt;fct&gt; No, No, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Yes, Y‚Ä¶\n$ ItchyEye          &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, Yes,‚Ä¶\n$ Nausea            &lt;fct&gt; No, No, Yes, Yes, Yes, Yes, No, No, Yes, Yes, Yes, Y‚Ä¶\n$ EarPn             &lt;fct&gt; No, Yes, No, Yes, No, No, No, No, No, No, No, Yes, Y‚Ä¶\n$ Hearing           &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, No, No, No, No,‚Ä¶\n$ Pharyngitis       &lt;fct&gt; Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, No, Yes, ‚Ä¶\n$ Breathless        &lt;fct&gt; No, No, Yes, No, No, Yes, No, No, No, Yes, No, Yes, ‚Ä¶\n$ ToothPn           &lt;fct&gt; No, No, Yes, No, No, No, No, No, Yes, No, No, Yes, N‚Ä¶\n$ Vision            &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, ‚Ä¶\n$ Vomit             &lt;fct&gt; No, No, No, No, No, No, Yes, No, No, No, Yes, Yes, N‚Ä¶\n$ Wheeze            &lt;fct&gt; No, No, No, Yes, No, Yes, No, No, No, No, No, Yes, N‚Ä¶\n$ BodyTemp          &lt;dbl&gt; 98.3, 100.4, 100.8, 98.8, 100.5, 98.4, 102.5, 98.4, ‚Ä¶\n$ RapidFluA         &lt;fct&gt; Presumptive Negative For Influenza A, NA, Presumptiv‚Ä¶\n$ RapidFluB         &lt;fct&gt; Presumptive Negative For Influenza B, NA, Presumptiv‚Ä¶\n$ PCRFluA           &lt;fct&gt; NA, NA, NA, NA, NA, NA,  Influenza A Not Detected, N‚Ä¶\n$ PCRFluB           &lt;fct&gt; NA, NA, NA, NA, NA, NA,  Influenza B Not Detected, N‚Ä¶\n$ TransScore1       &lt;dbl&gt; 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5‚Ä¶\n$ TransScore1F      &lt;fct&gt; 1, 3, 4, 5, 0, 2, 2, 5, 4, 4, 2, 3, 2, 5, 3, 5, 1, 5‚Ä¶\n$ TransScore2       &lt;dbl&gt; 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4‚Ä¶\n$ TransScore2F      &lt;fct&gt; 1, 2, 3, 4, 0, 2, 2, 4, 3, 3, 1, 2, 2, 4, 2, 4, 1, 4‚Ä¶\n$ TransScore3       &lt;dbl&gt; 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3‚Ä¶\n$ TransScore3F      &lt;fct&gt; 1, 1, 2, 3, 0, 2, 2, 3, 2, 2, 0, 1, 1, 3, 1, 3, 1, 3‚Ä¶\n$ TransScore4       &lt;dbl&gt; 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4‚Ä¶\n$ TransScore4F      &lt;fct&gt; 0, 2, 4, 4, 0, 1, 1, 4, 3, 3, 2, 2, 2, 4, 3, 4, 0, 4‚Ä¶\n$ ImpactScore       &lt;int&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ ImpactScore2      &lt;int&gt; 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,‚Ä¶\n$ ImpactScore3      &lt;int&gt; 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, ‚Ä¶\n$ ImpactScoreF      &lt;fct&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ ImpactScore2F     &lt;fct&gt; 6, 7, 13, 11, 10, 11, 7, 6, 9, 6, 12, 16, 10, 12, 8,‚Ä¶\n$ ImpactScore3F     &lt;fct&gt; 3, 4, 9, 7, 6, 7, 3, 3, 6, 4, 7, 11, 6, 8, 4, 4, 5, ‚Ä¶\n$ ImpactScoreFD     &lt;fct&gt; 7, 8, 14, 12, 11, 12, 8, 7, 10, 7, 13, 17, 11, 13, 9‚Ä¶\n$ TotalSymp1        &lt;dbl&gt; 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1‚Ä¶\n$ TotalSymp1F       &lt;fct&gt; 8, 11, 18, 17, 11, 14, 10, 12, 14, 11, 15, 20, 13, 1‚Ä¶\n$ TotalSymp2        &lt;dbl&gt; 8, 10, 17, 16, 11, 14, 10, 11, 13, 10, 14, 19, 13, 1‚Ä¶\n$ TotalSymp3        &lt;dbl&gt; 8, 9, 16, 15, 11, 14, 10, 10, 12, 9, 13, 18, 12, 16,‚Ä¶\n\n\nSo it looks like these are 735 individuals (rows) and 63 variables (columns). A lot of them have names of symptoms and are coded as Yes/No.¬†Some variables are harder to understand, for instance without some meta-data/explanation, it is impossible to guess what TransScore3F stands for. Hopefully, your data came with some codebook/data dictionary/information sheet that explains what exactly everything means. For this specific data set, you can look through the supplementary materials to learn more. We won‚Äôt delve into it now, and just pick out a few variables to illustrate the data scrambling process.\n\n\nData processing\nFor simplicity, let‚Äôs assume we are interested in just a few of these variables, namely ActivityLevel, Sneeze, Nausea, and Vomit. We‚Äôll select those and look at the first 10 entries.\n\ndat &lt;- rawdat |&gt; dplyr::select(\"ActivityLevel\",\"Sneeze\",\"Nausea\",\"Vomit\")\nhead(dat,10)\n\n   ActivityLevel Sneeze Nausea Vomit\n1             10     No     No    No\n2              6     No     No    No\n3              2    Yes    Yes    No\n4              2    Yes    Yes    No\n5              5     No    Yes    No\n6              3    Yes    Yes    No\n7              4     No     No   Yes\n8              0    Yes     No    No\n9              0     No    Yes    No\n10             5     No    Yes    No\n\n\n\n\nData Scrambling\nNow we‚Äôll scramble the data. I‚Äôm doing this here with a simple loop. I‚Äôm looping through each variable, and I sample from the old values without replacement, which basically just rearranges them. There are computationally faster and more concise ways of doing this, but the loop makes it hopefully very clear what‚Äôs going on.\n\n# define a new data frame that will contain scrambled values\ndat_sc &lt;- dat\nNobs = nrow(dat) #number of observations\n# loop over each variable, reshuffle entries\nfor (n in 1:ncol(dat))\n{\n  dat_sc[,n] &lt;- sample(dat[,n], size = Nobs, replace = FALSE)\n}\n\nhead(dat_sc,10)\n\n   ActivityLevel Sneeze Nausea Vomit\n1              8     No     No    No\n2              5     No     No    No\n3              0    Yes     No    No\n4              3     No    Yes    No\n5              5     No    Yes    No\n6              5    Yes     No    No\n7              1    Yes     No    No\n8              3    Yes     No    No\n9              8    Yes     No    No\n10             6    Yes    Yes    No\n\n\nThe first 10 entries look different, so that‚Äôs promising.\n\n\nComparing old and new data\nNow let‚Äôs see if things worked. First, we summarize both the old and the new data. We should see that they are the same, since we just re-arranged the values across individuals. This is indeed the case.\n\nsummary(dat)\n\n ActivityLevel    Sneeze    Nausea    Vomit    \n Min.   : 0.000   No :340   No :477   No :656  \n 1st Qu.: 3.000   Yes:395   Yes:258   Yes: 79  \n Median : 4.000                                \n Mean   : 4.463                                \n 3rd Qu.: 6.000                                \n Max.   :10.000                                \n\nsummary(dat_sc)\n\n ActivityLevel    Sneeze    Nausea    Vomit    \n Min.   : 0.000   No :340   No :477   No :656  \n 1st Qu.: 3.000   Yes:395   Yes:258   Yes: 79  \n Median : 4.000                                \n Mean   : 4.463                                \n 3rd Qu.: 6.000                                \n Max.   :10.000                                \n\n\nWe can also look at correlations between variables. Here is where we run into the above-mentioned problems. Correlations that might exist in the original data can be wiped out. We see that here. In the original data, more individuals (approximately 63% + 9%) reported either absence or presence of both nausea and vomiting. In the scrambled data, this dropped to around 58% + 4%. We would expect that these 2 symptoms are somewhat related, and the scrambling removed it. Similarly, the original data showed lower activity levels for those with vomit as symptom. This pattern is gone in the scrambled data.\n\n# cross-tabulation of 2 symptoms\ntb1=table(dat$Nausea,dat$Vomit)\nprop.table(tb1)*100 #as percentage\n\n     \n             No       Yes\n  No  62.993197  1.904762\n  Yes 26.258503  8.843537\n\ntb2=table(dat_sc$Nausea,dat_sc$Vomit)\nprop.table(tb2)*100\n\n     \n             No       Yes\n  No  58.095238  6.802721\n  Yes 31.156463  3.945578\n\n# looking at possible correlation between activity level and Vomit\np1 &lt;- dat |&gt; ggplot(aes(x=Vomit,y=ActivityLevel)) + geom_boxplot()\nplot(p1)\n\n\n\n\n\n\n\np2 &lt;- dat_sc |&gt; ggplot(aes(x=Vomit,y=ActivityLevel)) + geom_boxplot()\nplot(p2)\n\n\n\n\n\n\n\n\nThat means any statistical conclusions based on the scrambled data are not valid. This kind of data is just useful at testing the overall workflow and making sure everything can run, but one can‚Äôt conclude anything from it.\nIt is of course possible to try to scramble while preserving potential correlations, but that gets tricky and at this stage one might maybe just re-create the data based on some of the concepts discussed in the previous unit.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Scrambling existing Data"
    ]
  },
  {
    "objectID": "content/module-stats-overview/statistical-terminology/statistical-terminology.html",
    "href": "content/module-stats-overview/statistical-terminology/statistical-terminology.html",
    "title": "Statistical Modeling Terminology",
    "section": "",
    "text": "Overview\nThe field of data analysis has, in recent years, seen a lot of progress and changes. While Data Science or Data analysis was long synonymous with Statistics, in past years, other fields such as Computer Science, Engineering, and Business have all started to contribute toward the overall progress in data analytics. With a lot of old and new players engaged in this topic, a lot of new terminologies have emerged. Some of this terminology can be quite confusing, especially if different terms are used for the same concept. Throughout this course, I try to mention alternative names for any concept whenever I introduce it. The following is a brief discussion of the main names given to the task of analyzing data.\nOne of the hot ‚Äì and also confusing ‚Äì terms in recent years is Machine Learning (and the related terms of Data Mining, Deep Learning, and Artificial Intelligence), and how they relate to Statistics. There is a lot of debate about what exactly those different areas are. To get a bit of an idea of what people are talking about, skim through Frank Harrell‚Äôs blog post. As you can see from the post and the many other sources he cites, there is no real agreement on what exactly these terms mean.\nIn my view, it is not worth spending too much time trying to come up with a clear definition. But it‚Äôs good to have some frame of reference so when you see all these terms, you know what they mean. So here are my ‚Äì arguably fuzzy, but hopefully still somewhat useful ‚Äì thoughts on how to distinguish those topics. While making those distinctions can be at times useful, the reality is that the terminology is not clearly defined and all over the place.\nFor additional attempts at defining terms related to data analysis, see the Glossary page ‚Äì and certainly feel free to contribute!\n\n\nStatistics, Machine Learning and Artificial Intelligence\nStatistics is the classical machinery driving data analysis. Depending on the type of data, many different approaches have been developed (parametric vs.¬†non-parametric methods, longitudinal analysis, time-series analysis, and many more). Models are, in general, simple and interpretable, and the goal is to understand how inputs (predictors) relate to outcomes. Statistics was developed when data was sparse, computers didn‚Äôt exist, and mainly scientists interested in a deep understanding of their data used it. Because of this, statistical models tend to be simple and work well on small datasets. Most of classical statistics focuses on associative/exploratory/inferential analysis types.\nIn Machine Learning (ML), the models tend to be more complex, and the goal is mainly on getting a ‚Äúpowerful‚Äù model, i.e., a model that is good at prediction. Understanding how different inputs lead to different outcomes is of secondary importance. Data is often abundant, so more complex models can be used.\nArtificial intelligence (AI) can be considered a type of machine learning. The types of complex models that dominate AI these days are generally based on neural nets. Neural nets sound fancy, and they are certainly quite complex, but technically speaking, you can think of them as a large collection of logistic regression type models combined together.\nAs mentioned before, the terminology is fuzzy. Thus, a fairly simple model like a linear or logistic regression model could be considered a ‚Äúclassical‚Äù statistical model, while a more complex support vector machine or random forest (we‚Äôll visit those later) are generally considered machine learning models. However, the terminology is poorly-defined, and you will see pretty much any approach can be given any label. Often it makes sense to think of the two terms as describing more or less the same thing, and that is applying some kind of mathematical or computational model to gain insight from the data. For some similar ideas, and a bit more details, read Joshua Ebner‚Äôs blog post.\n\n\nRelated Terms\nThe term Data Mining is often used interchangeably with Machine Learning. It might sometimes indicate a ‚Äúfishing‚Äù approach of combing through data to look for patterns, without pre-defined hypotheses to be tested. As such, results from Data Mining explorations need to be confirmed with independent data. Exploratory Data Analysis or Secondary Data Analysis mean pretty much the same thing, though the latter are often done using smaller (Statistics) models as opposed to larger (Machine Learning) models. Again, terminology and use of the words is fuzzy.\nThe term Deep Learning is generally used when one class of machine learning models, namely neural nets, are applied to a data analysis problem. The ‚Äúdeep‚Äù part comes from the fact that the neural net models often have many stacked layers (it has nothing to do with deep as in especially insightful).\nThe use of deep learning and related approaches applied to ‚Äúcomplex‚Äù problems is often labeled Artificial Intelligence.\nNeural nets (and similar complex methods) usually need a lot of data to perform well. Thus, the term Big Data often shows up together with these other terms.\nIn general, there is currently a lot of hype around these topics, and people ‚Äì especially in the business, but also the research community ‚Äì use the words Deep Learning and Artificial Intelligence quite liberally, even if all they do is fit a linear model to data.\nWe‚Äôll discuss these topics a bit more at the end of the course, but since most scientific inquiry focuses on understanding patterns (i.e., questions tend to be inferential/causal/mechanistic) and amounts of data ‚Äì while growing ‚Äì still tend to be on the small side. So we won‚Äôt spend much time on it.\n\n\nTerminology abuse\nIn the last several years, there has been a tremendous interest among industry in anything related to Data Science. Most notable, the huge successes of AI in some areas and for some companies (Google, Amazon, Uber, ‚Ä¶) has meant that suddenly everyone wants AI and everyone sells AI, with a lot of players not having a clue what it is and can/can‚Äôt do. This has led to a lot of marketing hype and another host of acronyms. (For instance, I was recently at an online industry workshop with the title (quoting from memory) ‚ÄúFrom AI over BI to CI‚Äù - which apparently stand for Artificial Intelligence, Business Intelligence and Continuous Intelligence.) Most often, these terms can mean whatever you want them to mean. So some companies who have been doing data analysis by running simple linear or logistic regression models are now calling what they are doing AI. In the end, the only way to know what anyone is actually doing is to look at the detailed description of their methods. And if those are not provided, be very skeptical. Most of all, don‚Äôt be unduly impressed by all these big words. There is good data analysis, and there is bad data analysis, and just because someone uses a deep neural net AI model doesn‚Äôt mean what they are doing is any good. Critical and careful thinking about your data and the question you want to answer always trumps any new fancy modeling approach.\n\n\nFurther reading\nThis course has an optional module on Deep Learning, AI and Big Data. This is a very brief introduction you can check out if you are interested. For a broader introduction, the (non-free) book Artificial Intelligence: A Guide for Thinking Humans is a very good non-technical overview book. And of course, topics like ML and AI are so ‚Äúhot‚Äù these days, tons of free resources are available online‚Äìthough be sure to look at the writer‚Äôs credentials and think critically when reading online blogs.",
    "crumbs": [
      "Content",
      "7. Statistical Analysis Overview",
      "Some Statistical Model Fitting Terminology"
    ]
  },
  {
    "objectID": "content/module-stats-overview/descriptive-analysis/descriptive-analysis.html",
    "href": "content/module-stats-overview/descriptive-analysis/descriptive-analysis.html",
    "title": "Descriptive and preliminary analysis",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss descriptive and simple, preliminary statistical analyses.\n\n\nLearning Objectives\n\nKnow what descriptive analysis is\nUnderstand the value of doing preliminary statistical analyses\n\n\n\nIntroduction\nYou usually want to analyze your data with one or several models that are tailored to the question you are trying to address. These models might often include multiple variables and are potentially complex. Instead of starting with those complex models, it is often a good idea to summarize the data and fit simple models to it to gain a better understanding of what to expect from your full analysis.\n\n\nDescriptive analysis\nBefore any statistical analysis, you should summarize and describe your data. Only some of that will end up in a finished product (e.g., a paper or a report), but it‚Äôs important that you do it thoroughly and exhaustively to ensure you fully understand your data and thus know what statistical approaches are and are not appropriate.\nDescriptive analyses generally consist of making tables and figures that explore and summarize the data. Generally, you start with one variable at a time, and likely focus on the most important ones first. That means first you look at your outcomes of interest, then your main predictors/exposures of interest, then as much as feasible at all other variables. For continuous variables, you can use summary statistics (e.g., the summary command in R) or histograms (e.g.¬†the hist command). For categorical variables, you can look at tables (e.g.¬†the table command) or barplots (e.g.¬†the barplot command). The idea is to see if anything unusual or interesting is going on. You might notice problems with the data (e.g., a person with a negative weight) that need cleaning. You might find that you have a categorical variable with a lot of categories that contain only a few entries and only a few categories with a lot of entries (e.g., religion could be such a variable). You might want to flag that for further processing (we‚Äôll talk about that in the next unit).\nOnce you‚Äôve explored each variable by itself, you can start looking at combinations. For instance plots with each outcome of interest on the y-axis and each main predictor on the x-axis could be of interest. Depending on the type of variable for the outcome or predictor (continuous, categorical, etc.), you might want different types of plots, as discussed in the visualization module. Tables are possible too.\nFurther figures of interest might be correlation plots which show correlations between all predictors in your data and might indicate potential issues you need to address before fitting statistical models. For instance if you have height, weight, and BMI in your data, you know that BMI does not have independent information (it‚Äôs a combination of height and weight), and thus these variables are perfectly correlated. Many ‚Äì though not all ‚Äì statistical models don‚Äôt like such strongly correlated variables, so you might need to decide to remove some strongly correlated data beforehand. We‚Äôll talk about that more.\n\n\nPreliminary/exploratory statistical analysis\nSometimes, it is useful to start running some simple models on your data. Sometimes, such models will become part of the main analysis, but often they are just used to help explore the data. For instance if you have two continuous variables and make a scatterplot, adding a linear regression line through the data might be helpful to see if there is a pattern. That is easily done if you make your figure with ggplot2, as you can just add a stats geom to it.\nYou could also explore your data by running simple bivariate statistical models, e.g.¬†by fitting each predictor individually to the outcome of interest. We won‚Äôt talk much about standard regression approaches in this class, but I‚Äôm sure you are familiar with standard statistical tests that can be applied to two variables, depending on the types of variables (linear regression, t-test, etc.).\nA note of caution: It is ok (in my opinion) to use standard statistical tests and readouts (e.g., p-values) to guide main model choices and as suitable, to report them in a final product. It is however only ok to interpret p-values in the hypothesis testing sense if you predefined the hypothesis and the full data generation and analysis protocol. Outside a clinical trial, that‚Äôs almost never done, thus pretty much any reported p-value you see in the literature is improperly used/interpreted. Everyone does it, and pretty much everyone does it wrong. The state of affairs when it comes to statistical literacy, even among PhD holders, is still sad‚Ä¶\n\n\nSummary\nWhile your eventual goal is likely to use tailored, potentially complex models for your analysis, you should always explore the data through a descriptive analysis and fitting of simple (e.g.¬†bi-variate) models. The insights gained doing so will help you refine your main models.\n\n\nFurther Resources\n\nBoth the IDS and R4DS books provide good coverage of descriptive and simple analyses.",
    "crumbs": [
      "Content",
      "7. Statistical Analysis Overview",
      "Descriptive and preliminary analysis"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-tools/ready-tools.html",
    "href": "content/module-ready-workflow/ready-tools/ready-tools.html",
    "title": "Tools for a READy workflow",
    "section": "",
    "text": "Overview\nThis unit covers some tools that can help with a READy workflow.\n\n\nLearning Objectives\n\nBe familiar with some tools that can help with a READy workflow.\n\n\n\nIntroduction\nThere are many different ways to make a data analysis READy. The following is a brief discussion of some common tools.\n\n\nVersion control software\nVersion control software, such as Git/GitHub, is used for tracking project changes and is discussed in the Project Management unit.\n\n\nQuarto\nQuarto is tool that allows for easy scientific writing in a reproducible manner, by seamlessly integrating code with text. We discussed Quarto previously.\n\n\nJupyter\nJupyter is another platform that supports combining code with text. I‚Äôm not very familiar with it. As far as I understand it, it doesn‚Äôt support the generation of different outputs that Quarto does. Since Quarto supports the same languages as Jupyter, I am not sure what the advantages are of using Jupyter. There might be some üòÅ.\n\n\nPluto\nFor the Julia programming language, there is Pluto.jl, which has similarities to Jupyter and Quarto. Again, I don‚Äôt have experience with it, so can‚Äôt say what ‚Äì if any ‚Äì advantages it has over Quarto. But if you do some coding in Julia, it might be worth checking out.\n\n\nSweave and such\nThere are some older tools, such as Sweave that can also help in reproducible work. You might still occasionally run into old projects that use it. But in my opinion it is essentially obsolete and you should use Quarto or another of the tools listed above.\n\n\nPipeline tools\nQuarto and similar programs generally chain together a series of commands to get from the initial result to the final product. For instance for a Quarto file, when you render it, it first runs any code, then processes the markdown file, then turns it into the final product.\nMore generally, tools that allow you to automatically run such chains of steps are often referred to as pipeline tools. In R, there is the targets package. Outside of R, Unix/Linux type systems support Make, which lets you combine multiple steps into often complicated workflows. If you find yourself needing a workflow that executes multiple different steps that go beyond what Quarto can execute, the targets package might be worth checking out.\n\n\nOther tools\nIn general, any software tool you use that allows you to document each step - usually in the form of code and comments - should allow others to reproduce your results. Many common software packages and languages such as Mathematica, Matlab, Julia, Python, etc. allow you to add documentation to your code and therefore support reproducibility.\n\n\nHall of shame\nThere is a long list of software that is commonly used for research and data analysis, but which one should stay away from. This encompasses any software that does not contain a full documentation of all steps taken to allow reproducibility.\nExcel and similar spreadsheet software is a major culprit. Using such software for data entry and storage is perfectly fine, as long as one follows guidelines that make later analysis easy, as for instance described in this paper.\nHowever, spreadsheet software is a poor choice for analysis tasks (though nevertheless widely used). The problem with standard spreadsheet software like Excel is that if you make a change to the data and save it, there is usually no track record of what happened. Therefore, any data processing that happens is not reproducible.\nStatistical and data analysis software that relies on graphical user interfaces (GUI), such as e.g.¬†Graphpad Prism or SPSS are equally problematic. Some of these software tools can record the commands you perform through a GUI and you can save those at the end. This does allow for reproducibility. But if that step is missing or not implemented, the analysis is again not reproducible.\nSimilarly, any software to produce graphics by hand (e.g., Photoshop) should not be used. It is ok to use it make schematics, but not to edit any results.\nSimilarly, results tables should be generated automatically and not produced by copying and pasting numbers from one location to another. Again, that means no Excel.\n\n\nSummary\nThere are many useful tools that help you implement a READy workflow. The important part is that things are scripted, automated and repeatable, as well as documented. Any tools that support such a workflow are useful. Stay away from tools that require manual steps.\n\n\nFurther Resources\n\nGood enough practices in scientific computing is a paper that provides a nice discussion of the approaches, and some tools, that are useful for doing work in a way that follows READy guidelines.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Tools for a READy workflow"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-project-management/ready-project-management.html",
    "href": "content/module-ready-workflow/ready-project-management/ready-project-management.html",
    "title": "Project Management",
    "section": "",
    "text": "For this unit, we will discuss ways to make sure working collaboratively on a project is efficient and that project progress is tracked.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-project-management/ready-project-management.html#file-sizes",
    "href": "content/module-ready-workflow/ready-project-management/ready-project-management.html#file-sizes",
    "title": "Project Management",
    "section": "File sizes",
    "text": "File sizes\nGitHub has a problem with large files. Anything above 100MB will lead to a failure which is annoying and tricky to resolve. In general, files above 50mb will work, be will be slow and give you a warning. Unfortunately, once you have a failed attempt to sync a large file, it can be quite tricky to fix the repository to work again. Therefore keep large (&gt;100MB) files out of your GitHub repositories. If you have to work with such files, try to reduce them first before placing into the GitHub repository. As alternative, place those files in another sync service (e.g.¬†Dropbox, OneDrive, GoogleDrive) and load them from there. Fortunately, there are some nice R packages that can usually make retrieving those files reproducible, even if they are in a file syncing service outside of your repo. Finally, your overall repo must be less than 5GB in size, so consider storing your huge data files and audio/video files somewhere else.\nThere is an option to have larger files with GitHub using Git Large File Storage. This can be a bit tricky, if you think it‚Äôs worth trying, see the documentation here. GitHub Pro users (including students using the Student Developer Kit) have free access to limited LFS features.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-project-management/ready-project-management.html#confidentiality",
    "href": "content/module-ready-workflow/ready-project-management/ready-project-management.html#confidentiality",
    "title": "Project Management",
    "section": "Confidentiality",
    "text": "Confidentiality\nBy default, GitHub repositories are public. Since you might not want to share your analysis publicly, sometimes using a private repository is useful. I think you generally don‚Äôt need to worry about getting scooped. Everyone is so busy with their own stuff, nobody really cares what you are working on üòÅ. However, you might often have data or other information as part of your project that you don‚Äôt want to share publicly.\nTherefore, for most data analysis projects, it is probably good practice to start with a private repository, so you can have potentially confidential information in the repo. You can add collaborators to the private repository. This way only those individuals who you want to give access will be able to see, and potentially contribute to the project.\nEventually, to ensure your project is publicly reproducible, you want to share some version of the data and the code. Be aware that if you make a repository public, others can also see the full history. This can be important if you want to make your final results public - after you made sure everything is anonymous enough for sharing. If at any time you worked in the repository with the full dataset and tracked it, others can go back in your GitHub/Git history and see it. The best option is likely to start a new public repository and copy only the elements you want to share into that repository, while keeping your original repository private.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html",
    "title": "Project organization",
    "section": "",
    "text": "For this unit, we will discuss general principles for organizing your project in a READy way.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#project-structure",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#project-structure",
    "title": "Project organization",
    "section": "Project structure",
    "text": "Project structure\nYou want to set up your analysis in such a way that it makes sense to you and others and allows for a good and efficient workflow. The main components of your analysis will be data, code, results (tables, figures, etc.) and products (reports, interactive apps, slides, etc.).\nYour project could be a GitHub repository ‚Äî a good idea ‚Äî or not. If you use a software that supports the concept of projects, you should use it. For instance in R, R projects are defined by having a folder structure with an .Rproj file at the top level. Most of the time making use of such project support is recommended.\nBy having any project you work on as an R project, relative paths will always be relative to the main directory in which your .Rproj file is located. As long as someone loads the project by clicking on the .Rproj file, and you only use relative paths, things should work well on any computer.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#folder-structure",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#folder-structure",
    "title": "Project organization",
    "section": "Folder structure",
    "text": "Folder structure\nIn general, the different components of your project should be mapped to folders inside your main project folder.\nInside your folders, you can have subfolders, e.g., separate folders for figures and tables. Or you could have subfolders for different types of analyses. There is no one correct way to set up things, but you should think of a logical and consistent structure before you start your project.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#file-structure",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#file-structure",
    "title": "Project organization",
    "section": "File structure",
    "text": "File structure\nYou will want different files that do specific tasks. You‚Äôll likely have files that contain data in raw and processed form, scripts with code (such as .R files), scripts with results (such as Quarto .qmd files), files that contain images, files that contain the content for tables, etc.\nThe number and type of scripts and other files depends on your project and your preference. In general, keeping things modular is useful. If you had one file that did the analysis and created a report, that would be ok for a small project. But then if you wanted to make a set of slides based on your results, you‚Äôll have to find a way to include the code in those slides. It would be easier to have code produce and save results such as figures, which can then be included in both a report/manuscript and slides.\nAnother consideration is computational time. For simple projects, your code will likely run fast. Once your analyses or data become large, parts of your code might run very long. You will then want to structure your scripts such that the computationally-intensive part is only run when absolutely needed. You definitely want to avoid a scenario where you have to wait minutes or hours as you play around with a figure to make it look the way you want. A good way to split this is to have files/code that does the various pieces (cleaning, processing, exploratory analysis, etc.) that happen before your main analysis (and usually run fairly quickly), then have one or multiple scripts that perform the - potentially computationally expensive - main analysis and save the outcomes from that analysis, and then further scripts that take these results and turn them into good-looking figures and tables. Finally, you‚Äôll have one or a few files that produce manuscripts or reports in an automated fashion by pulling in the different results you generated.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#setting-file-and-folder-paths",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#setting-file-and-folder-paths",
    "title": "Project organization",
    "section": "Setting file and folder paths",
    "text": "Setting file and folder paths\nIt is important to ensure that everything can be reproduced not just on your computer, but on the systems of others.\nA common problem that prevents others to reproduce your work is the use of paths that are specific to only your computer. Do not set paths or load files from paths that only exist on your machine! Instead, you should only use relative paths. A relative path is a file path that is relative to some directory.\nAs an example, loading a file from location C:/myusername/mydesktop/myproject/data/rawdata.csv is a bad idea, because someone else will not have that setup. They will have a different user name, they might be on a Mac or Linux machine, they might have different names for their hard drives, etc.\nInstead, you should load data relative to your main project directory. So for instance reference data/rawdata.csv and use a tool that automatically looks for that folder inside your project folder (in this example, myproject). With this, as long as someone else has a copy of your project and its subfolders, everything should work, no matter where they store your project.\nFor nice video tutorial, see this video on relative versus absolute paths. The setting for this is development of websites, instead of data analysis. So he talks about html files, but that doesn‚Äôt really matter, everything that is explained about the paths applies in general.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-organization/ready-organization.html#here-r-package",
    "href": "content/module-ready-workflow/ready-organization/ready-organization.html#here-r-package",
    "title": "Project organization",
    "section": "here R package",
    "text": "here R package\nThe here R package and its associated here() command in R is very useful for working with paths and loading/saving files. It works nicely with R projects. If you use the here command to load a file, it always looks for the file relative to the main project directory. So you can load a file with a command like dat &lt;- read.csv(here::here('data','rawdata,csv')). Malcolm Barrett‚Äôs blog post is a great, short explanation of why one should use the here package and how to do it.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Organization"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-introduction/ready-introduction.html",
    "href": "content/module-ready-workflow/ready-introduction/ready-introduction.html",
    "title": "READy workflow introduction",
    "section": "",
    "text": "This unit provides an overview of the ideas for a data analysis that follows Reproducible, Efficient, Automated, and Documented (READy) workflow principles.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "READy Introduction"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-introduction/ready-introduction.html#footnotes",
    "href": "content/module-ready-workflow/ready-introduction/ready-introduction.html#footnotes",
    "title": "READy workflow introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nREADy is an acronym I invented. If you look around online, you‚Äôll find lots of discussions on the topic of reproducible or automated data analysis, and why documentation is important, but you‚Äôll likely not find anything calling it READy.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "READy Introduction"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/assessment-ready-workflow.html",
    "href": "content/module-ready-workflow/assessment-ready-workflow.html",
    "title": "Assessment - READy Workflow",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "3. READy Workflow"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/assessment-ready-workflow.html#group-setup",
    "href": "content/module-ready-workflow/assessment-ready-workflow.html#group-setup",
    "title": "Assessment - READy Workflow",
    "section": "Group setup",
    "text": "Group setup\nAs in the previous exercise, assign each member in your group a different, arbitrary number (I‚Äôm calling them M1, M2, ‚Ä¶). Make sure you are teamed up with a different person this time. For this exercise, everyone will first work on their own and finish this part by Wednesday. Then M1 will contribute to M2‚Äôs repository, M2 will work on M3, etc. The last person (M3/M4/M5, based on the number of people in your group), will work on M1‚Äôs repository. This way, everyone will work on their own and one group member‚Äôs repository.",
    "crumbs": [
      "Assessments",
      "3. READy Workflow"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/assessment-ready-workflow.html#part-1",
    "href": "content/module-ready-workflow/assessment-ready-workflow.html#part-1",
    "title": "Assessment - READy Workflow",
    "section": "Part 1",
    "text": "Part 1\n\nAdding data\nYou will be using another project template for this exercise (this is the same template you will be using for your class project). Go to the GitHub repository for the data-analysis-template and read the information in the README.md file (shown at the bottom of the repository website). You can ignore the bits about Package Management.\nThen start a new repository using this template by clicking the green Use this template button on the top right of the repository page, then chose Create a new repository. Name the repository FirstnameLastname-ready-exercise. Make sure the repository is public. Add a description. Then create the repository. Once it has been created on GitHub.com, clone it to your local computer.\nOne you have a local version, open it in Positron and look at the different folders, files, and readme.md comments that you find inside the folders. You should be able to run all the R and Quarto scripts. By exploring the repo, you should get some familiarity with the whole setup to prepare you for the next steps.\nNow go ahead and open the exampledata.xlsx file (with Excel or a similar program) located in the raw-data folder, and add 2 more columns to the data. One column should be something numeric, the other can be something consisting of (a few) different categories. As a (boring) example, you could add eye color and waist size. Feel free to come up with more creative attributes/variables to add. Also add descriptions of your new variables to the Codebook sheet/tab. Once done, save the new data as exampledata2.xlsx.\nThen find report.qmd in the products/report folder, open it, and add a few sentences explaining what you added to the data into the Description of data and data source section.\nCommit your changes (write a meaningful commit message) and push them to GitHub.com.\n\n\nCollaborating\nInstead of using the fork + pull-request workflow, we will explore a different collaborative approach. In this approach, you and your collaborator work on the same repository. To that end, you need to add your classmate as collaborator. Go to GitHub.com, find the repository for this exercise (FirstnameLastname-ready-exercise). Then go to Settings, then Collaborators. Choose Add People and add your classmate (based on their GitHub account name). Your classmate should receive an invitation, which they need to accept. With this, they are now able to directly push and pull to your repository, without them needing to create a fork. (You can remove them after this exercise if you don‚Äôt want them to be able to continue having write access to your repository).\nTo avoid any potential merge conflicts, once your classmate takes over, you shouldn‚Äôt make further changes to the repository.\nYou need to have this done by Tuesday evening.\n\n\nPart 2\nYou should have received an invitation to be a collaborator on your classmate‚Äôs repository. Accept it, then directly clone - not fork the repository to your local computer.\nLook at the exampledata2.xlsx file and information in Codebook, as well as the description in report.qmd to understand what new variables your classmate created.\nFind processingfile-v1.qmd inside /code/processing-code/, make a copy of the file. Update the code such that it now loads the new data file called exampledata2.xlsx. Take a look at the new data. Add code to clean the new data as needed. Have the code save the updated data to processeddata2.rds.\n\nIf you prefer, you can instead make copies and edit processingcode.R and processingfile-v2.qmd instead of processingfile-v1.qmd. The first option is a bit easier since you have to only edit one file, but the second option of working in a separate R script can become more efficient once you are doing larger projects with more code.\n\nNext, repeat those steps for the exploratory data analysis (EDA) code. Make a copy of the relevant files using either approach (single qmd file or R-script + qmd file), add code to the new file to create a violin-plot with the new categorical variable (whatever it is) on the x-axis, and height on the y-axis. Also create a scatterplot with weight on the x-axis and the new numerical variable on the y-axis. Save both figures to files into the appropriate results sub-folder.\nMake sure everything works, then commit and push your changes to GitHub.com. Let your classmate know that you are done with your part.\nNote that you are now directly pushing to the original repo which is owned by your classmate. This is easier, you don‚Äôt need to do a fork and pull request (PR). It‚Äôs also more dangerous, since you could potentially mess up your classmate‚Äôs repo. So make sure things work before committing and pushing.\nYou need to have this done by Thursday evening.\n\n\nPart 3\nEveryone goes back to their own repository. This should now contain the content your classmate contributed. Pull the changes from GitHub.com to your local computer.\nFind statistical-analysis.R inside /code/analysis-code/. Make a copy. Edit the code such that it fits a third linear model with Height as outcome and the 2 new variables as predictors. Save the result into resulttable3.rds into the appropriate results folder.\n\nI‚Äôm sure you noticed that for this last part, we switched from Quarto files to R scripts. I‚Äôm doing this to show you that either are fine. Sometimes it‚Äôs better to combine code and text in one file. Sometimes just having code by itself is easier/better. In any case, you should add lots of comments and documentation to the Quarto or R file.\n\nNow we are ready to include the new findings. Open report.qmd inside the products/report folder.\nAt the top of the file, list yourself and your classmate as authors. Remove everything else from the example that‚Äôs irrelevant for this exercise.\nWrite the appropriate text into the relevant Methods and Results sections. This will include code to pull in the new figures and the new table.\nAdjust sections as you see fit, just make sure you explain everything that is happening in this - very basic - analysis. Also delete everything in this report that is not relevant/applicable to your exercise.\nOnce all is done, render the report. Find the report.html file, open it on your browser and make sure everything looks ok. If all is good, commit and push to make sure your local repo is in sync with your remote.\n\n\nPart 4\nNow continue in your portfolio website repository.\nOpen the portfolio repo in Positron. Create a new folder inside the main folder called ready-exercise. Copy the report.html file you just created into that folder, and rename it ready-exercise.html.\nNow open _.quarto.yml. Find the menu section and copy/paste the 2 lines from ‚ÄúStarter Analysis Exercise‚Äù with ‚ÄúREADy Exercise‚Äù, adjusting the path and the text accordingly. Make sure to point to ready-exercise.html NOT .qmd. Save the file.\nFully rebuild/render your portfolio website. Make sure everything works and the new exercise shows up in the menu and links to the correct page. Commit and push your changes. Go to your website URL to make sure it also looks ok on your website. I will only be checking the entry on your portfolio website, so you need to make sure it is all there.\nYou need to have this done by Friday evening.\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it. I will assess both the contribution of the repository owner and the classmate who added to this.\n\nSome more comments on GitHub workflows:\nIn general, if you work closely with someone on a project, it might make sense to add them as collaborator, and as needed coordinate with them to avoid merge conflicts. Otherwise, telling someone to contribute by forking and sending a pull request is the safer approach, and you have control if you want to accept their changes or not.\nThere is yet another common way to use GitHub, namely collaborators working in the same repository, but with different branches. Think of a branch like a fork, but it happens inside the repository. Work can occur independently on branches, and at some point one can merge branches. This allows people to work in a single repository, but minimizes possible merge conflicts. This approach is standard for larger projects with many collaborators. For this class, we won‚Äôt use branches, but note that they are useful and commonly used.",
    "crumbs": [
      "Assessments",
      "3. READy Workflow"
    ]
  },
  {
    "objectID": "content/module-model-ml-2/more-ml-models/more-ml-models.html",
    "href": "content/module-model-ml-2/more-ml-models/more-ml-models.html",
    "title": "More Machine Learning Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover some further points related to ML that didn‚Äôt really fit into any other sections.\n\n\nLearning Objectives\n\nBe familiar with the idea of ensemble models.\nKnow about several unsupervised modeling approaches.\n\n\n\nEnsemble methods/models\nYou learned previously that one can combine many trees into a larger model (e.g., a random forest or a boosted regression trees model), and that those models often have better performance. In the many-tree units, I briefly mentioned that this is an example of an ensemble model.\nInstead of combining just the same kind of model (e.g., multiple trees), it is possible to build models which are combinations of different types of base models, e.g., combine a tree-based model with an SVM. Those approaches are known variously as ensemble methods/models or super learners or stacked methods/models. By combining different models in a smart way, it is often possible to increase performance beyond what can be achieved from any one individual model. The disadvantage is that fitting the model is more complex, takes more time, and results are even less interpretable (more black box) than any single model. And since each model has parameters that need tuning, more parameters means more data is needed for robust results. Nevertheless, if the only aspect that counts is predictive performance, and plenty of data is available, ensemble methods are often good choices.\nProperly fitting ensemble models is not easy, and requires usually a lot of data. In fact, so far I have never tried to fit an ensemble model for any of my research projects. Nevertheless, it is useful to know about them if you encounter them in the literature or if you have a problem/data where you think they might be helpful. For more on those models, check out the Stacked Models chapter of HMLR, the Ensembe Models chapter in TMWR and the stacks package, which integrates well with tidymodels.\n\n\nUnsupervised learning\nWhile I previously mentioned unsupervised learning here and there, we haven‚Äôt focused much on it in this course. The reason is that most data analysis problems deal with supervised learning, i.e.¬†with data that has a specific outcome of interest. However, there are situations where data without a given outcome needs to be analyzed. For instance images or customers might need to be sorted into categories. This analysis approach is also called clustering. Sometimes, unsupervised learning is also used as a preparatory step in a supervised learning setting. For instance it can be used to reduce the number of predictors. This is called dimension reduction. It is common in areas where one measures lots of variables but the observations are small, e.g.¬†genetic information on a few hundred individuals, with 1000s of genetic markers measured for each person. In such a case, one can reduce the number of predictor variables into a set of combinations of the original predictors such that the new set contains the most important information. Then one can use that reduced set to perform supervised learning.\nMethods like K-means clustering or Hierarchical clustering are ‚Äì as the name suggests ‚Äì used for clustering of unlabeled data. Partial least squares (PLS) and Principal component analysis (PCA) are methods for dimension reduction. Since for unsupervised learning, a performance measure like RMSE or Accuracy does not exist, other metrics are used to define the quality of model results. Different algorithms use different ways to perform the unsupervised learning task.\nThe Unsupervised Learning chapter of ISL discusses several unsupervised learning methods. So do the Dimension Reduction and Clustering sections of HMLR and the Clustering chapter of IDS. For an R implementation, check out the tidyclust package.\n\n\nFurther reading\nSee the references provided in the sections above, as well as the general references provided in the ML Introduction page.",
    "crumbs": [
      "Content",
      "12. Machine Learning Models II",
      "Some More ML models"
    ]
  },
  {
    "objectID": "content/module-model-ml-2/assessment-ml-models-2.html",
    "href": "content/module-model-ml-2/assessment-ml-models-2.html",
    "title": "Assessment - Machine Learning Models 2",
    "section": "",
    "text": "Quiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\n\n\nProject\nSubmission of part 4 is due. Make sure your updated project is on GitHub.\n\n\nDiscussion\nWrite a post in this week‚Äôs discussion channel that answers this question:\nWhich of the machine learning methods we covered (or any we didn‚Äôt cover but you are aware of) do you find most exciting/interesting and why? And which ML method seems the most confusing/pointless to you, and why? Are there any ML approaches you want to learn more about and need more resources beyond those provided on the website?\nPost by Wednesday, then reply to each other by Friday.\n\n\nExercise\nSince this week is another week with a project submission deadline, there‚Äôs no exercise. You can practice some of the models we covered this week in next week‚Äôs exercise.",
    "crumbs": [
      "Assessments",
      "12. Machine Learning Models II"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/ml-models-overview.html",
    "href": "content/module-model-ml-1/ml-models-overview.html",
    "title": "Machine Learning Models - Overview",
    "section": "",
    "text": "Overview\nIn this brief unit, I make a few general comments about machine learning models, before we look at several types of those models.\n\n\nLearning Objectives\n\nKnow what ML models are.\n\n\n\nIntroduction\nBy now, I am sure you have picked up on the fact that there is a whole zoo of different models out there one can use to analyze data. You have also likely picked up that the terminology is fuzzy. We could call a standard linear or logistic regression model a machine learning (ML) model, and people do. Or we can reserve the term ML for more complex models - however fuzzily we define ‚Äúcomplex‚Äù. In the end, getting hung up about terminology is not that useful and important.\nWhat is important is that you are at least a bit familiar with some of the more common models. Most of the time, you do not need to understand every detail of a model. You do however want to understand enough about how the model works, its strengths and limitations, to determine if a certain model is useful for a given situation or not. This way, when you read someone else‚Äôs modeling results, or try your own, you have an idea of what‚Äôs going in.\nWe will do a whirlwind tour of several classes of common models. There‚Äôs no way you can learn them in detail in the short amount of time we spend on them. However, you should be able to pick up enough about a type of model such that if you happen to work on a problem where you think a certain kind of model might be useful, you will then want to revisit that model and learn more.\n\n\nGeneral ML considerations\n\nI want to make an important point here: The statistics/model fitting topics we have discussed so far, namely pre-processing data, choosing the right performance metric, using methods like train/test and cross-validation to minimize overfitting, and carefully evaluating your model results apply to pretty much any data analysis project, no matter if you fit a simple linear model or a complicated neural net.\n\nMost models that are considered ML models tend to be more complex than the GLM type models we discussed so far. More complexity usually means more predictive performance, but also an increased chance for overfitting and more difficulty understanding what is going on inside the model and how to interpret model results.\nML models often have parameters that need to be tuned as part of the fitting process. In a first pass, you can often do those tuning operations without knowing much about the models. Once you dig deeper, it is useful to understand enough about the model to get an idea of what tuning a specific parameter for a given model actually means. Either way, always critically evaluate what your models return. Just because complex models are often black boxes and it‚Äôs hard to understand everything that goes on inside doesn‚Äôt mean you can skip your critical thinking and accept as reasonable whatever the model gives you back.\nI think by now you have also picked up on the idea that there is no recipe for choosing a specific machine learning/statistical modeling approach. It depends on the data, the question, the overall goal, what others in the field are using, and potentially further factors. In general, the most thorough approach is to try both simple and complex models, and then decide based on model performance and other considerations such as simplicity, speed, usability, etc. on a specific model. There is always a level of subjectivity involved, i.e.¬†different analysts might favor different models. As long as the thought process behind choosing a specific model is well explained and justified, you should be allowed to choose the model that you think is overall best for a given situation. Since it is very easy to fit multiple different models and compare results, it is not a bad idea to do that. You can report the results from the main model you chose as the main findings, with results from other models as supplementary material.\nAnd with those general points out of the way, we‚Äôll look at several different statistical/ML models in the following units.\n\n\nFurther resources\nThe books we have been using throughout this course all cover many aspects of ML. Especially ISL, IDS and HMLR are very good starting points for learning more about different machine learning methods. I will point to specific chapters when we discuss specific models.\nThere are also tons of online resources on machine learning models, the quality varies widely, but it might be worth looking around a bit. TheInsaneApp.com‚Äôs 10 Most Used Machine Learning Algorithms in Python with Code is a nice overview. Note that at the time I‚Äôm writing this, when I did a brief read-through I noticed some inaccuracies. E.g., they claim that a logistic regression model predicts 0/1, which is not quite right, it predicts probability which then is usually converted to 0/1 by defining a threshold. In general, when you look at resources like that, they are rarely completely wrong but might occasionally not be fully accurate (I‚Äôm sure my course website is no different üôÑ). So it‚Äôs often good to cross-check with resources that are fairly certain to be right (e.g., textbooks or Wikipedia).\nIf you want to practice some more ML modeling using the tidymodels framework, check out this free online course by Julia Silge, one of the main tidymodels maintainers. It consists of 4 case studies that teach you both general ML ideas and how to do them with tidymodels. Another online course focusing on tidymodels is Allison Hill‚Äôs course.",
    "crumbs": [
      "Content",
      "11. Machine Learning Models I",
      "ML Models Overview"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/decision-trees/decision-trees.html",
    "href": "content/module-model-ml-1/decision-trees/decision-trees.html",
    "title": "Decision Trees",
    "section": "",
    "text": "Overview\nIn this unit, we will cover a class of models generally referred to as decision trees.\n\n\n\nAn interesting classification tree. Source: xkcd.\n\n\n\n\nLearning Objectives\n\nUnderstand what tree-based methods are and how to use them\nKnow the difference between regression and classification trees\nKnow when it might be good to use a tree-based method\nUnderstand the advantages and disadvantages of single tree model\n\n\n\nIntroduction\nSimple GLMs have the main limitation of capturing only overall smooth patterns relating inputs and outcomes. Higher order terms or interactions must be manually specified. The polynomial and spline models we discussed allow for more flexible patterns, at the cost of more potential overfitting and also reduced interpretability. Another class of models that allow non-linear relation between inputs and outcomes are based on decision trees. In general, a decision tree is a tree-like structure that helps you make decisions. In machine learning, there are specific methods to build those trees. Decision trees can be used for continuous outcomes, in which case they are called regression trees or for categorical outcomes, in which case they are called classification trees. The umbrella term Classification and regression trees - or CART for short is also often used to mean the same as decision trees.\nBasic tree models have the advantage of being fairly easy to understand and use, even by non-statisticians. Trees can deal with nonlinear relations between predictors and outcome, they easily accommodate more than two categories for classification, and they can handle predictors that have missing values.\n\n\nTree Examples\nThe following shows a tree from Afonso et al.¬†2012, which is a study led by one of my colleagues Mark Ebell to determine if there are simple decision rules that can determine if a person has an influenza infection or not.\n\n\n\nSource: Alfonso et al.¬†2012\n\n\nAs you can see, this is a very simple model that is very intuitive and could be easily understood by physicians and other laypeople. The tree is used here for classification, with a binary outcome (presence/absence of flu).\nThe next example is a tree used for a regression problem to predict/model a continuous outcome.\n\n\n\nSource: Hess et al.¬†1999\n\n\nIn this study, the authors tried to determine the survival time of patients who had cancers with certain characteristics. The tree was used to group individuals, and for each group the tree/model predicts median survival.\n\n\nBuilding trees\nThe way one computes a tree is a little bit similar in concept to forward selection in a linear model. We start by looking at each predictor. We take the tree, which, when the outcome is split at some value of that predictor, leads to the best (cross-validated) performance increase in the model (e.g., lowest SSR/RMSE or highest Accuracy/F1 score). Let‚Äôs say we want to predict BMI, and our performance measure is RMSE of predicted value from the true outcome. The null model using no predictors gives us some value for the RMSE. We then try each predictor (let‚Äôs say we have exercise, calories, age, sex) and find that if we use exercise and assign everyone who exercises more than 23min per day one BMI value, and everyone who exercises less than 23min another BMI value, we get the best reduction in RMSE we could get (across all predictors and possible cut-off values). This is our first split using the exercise predictor at 23min. We now have a tree with one root and 2 nodes (in this case, they are leaves, which is the name for the terminal nodes).\nWe now take each leaf and run through all predictors again including the one we just used and see which predictor split at some value will further improve model performance. Let‚Äôs say we find that for those exercising more than 23min, we can split by sex and get 2 new BMI values, one for males exercising &gt;23min and one for females exercising &gt;23min, which gives the maximum reduction in RMSE. Similarly, we find that for the &lt;23min exercise people, splitting at 1500 calories a day is best. We now have a tree with a root, 2 internal nodes, and 4 terminal nodes.\nWe keep building the tree in this way until some criterion is met (e.g., no more than n observations in a given leaf, no further increase in performance). Note that in this procedure, some predictors may never enter the model. In that way, a tree performs an automatic subset selection, i.e., the algorithm might decide to not include predictor variables it doesn‚Äôt find useful. Also, any predictor could be used more than once.\nAlgorithms that implement the tree building routine differ in their details, which is why you will see many different options in both mlr and caret. In general, if you have a good signal in your data, any algorithm should pick up on it.\nSince trees have a tendency to overfit, it is common to regularize them by including a penalty for tree size. For instance, if we were to minimize SSR/RMSE, we would add a penalty to the cost function, C, to get\n\\[C = SSR + \\lambda T, \\]\nwhere T is the number of leaves of the tree. More leaves means a larger tree, which is being penalized. The tuning parameter \\(\\lambda\\) needs to be determined using parameter tuning or model training. Reducing a tree in this way is also called pruning.\n\n\nFast and frugal trees\nWhile trees are fairly simple to understand, sometimes, especially if there are many branching points, a complex decision tree might still not be suitable in practice. Further, even with regularization, trees might overfit. There is a type of tree, called fast and frugal tree (FFT), which can potentially help with both aspects. The difference between a regular tree and a FFT is that for the FFT, at each split/decision point, at least one of the splits needs to end in a terminal node. Check e.g.¬†the example diagrams shown in the FFT Wikipedia article to see what this means. This constraint makes the trees often simpler and thus easier to implement in practice (e.g.¬†by a doctor) and they might also be more robust, i.e.¬†their performance on future data might be better than a larger tree. Disadvantages of FFT are that sometimes they might be too simple and thus not perform as well as a full tree, and they only work for binary outcomes.\nA nice R package called FFTrees implements FFT (and automatically compares their performance to regular trees and some of the tree based methods discussed below).\n\n\nAdvantages and disadvantages of tree models\nA great feature of trees is that they are relatively quick and easy to build and especially easy to understand. They can easily be communicated to and used by non-experts (e.g., doctors, other decision-makers). As such, trees (sometimes called decision diagrams or other names) are common in many fields. As mentioned, trees are also able to handle missing data in predictors, and they are often reasonably robust in the presence of collinear or near-zero-variance predictors since trees tend to use one of the variables and ignore the others. Tree models also tend to excel at inferring the presence of nonlinear interactions between variables. Often trees don‚Äôt need predictors to be standardized either.\nThe main disadvantage of trees is that they usually have reduced performance compared to other models. Thus, if a simple, interpretable model is the primary goal, trees are ideal. If instead, a high-performance predictive model is the goal, trees are rarely the best choice.\n\n\nFurther information\nR2D3‚Äôs excellent interactive tutorial gives a very nice, visual introduction to machine learning in general and trees in particular. It covers and nicely illustrates some of the topics discussed here, as well as topics discussed in previous units. I strongly recommend you check it out, even if you just skim through it. It is fun and informative!\nFor more on tree models, see the first part Tree-based Methods chapter of ISLR, the Decision Trees chapter of HMLR.",
    "crumbs": [
      "Content",
      "11. Machine Learning Models I",
      "Decision Trees"
    ]
  },
  {
    "objectID": "content/module-intro-tools/tools-reference-management/tools-reference-management.html",
    "href": "content/module-intro-tools/tools-reference-management/tools-reference-management.html",
    "title": "Reference Management",
    "section": "",
    "text": "Overview\nIn this short unit, you will learn how to manage your references within the R/Quarto/Markdown stack of tools.\n\n\nLearning Objectives\n\nKnow how to do reference management that plays nicely with Quarto/Markdown\n\n\n\nReference management with Quarto/R Markdown\nFor almost any data analysis, especially if it is meant for publication in the academic literature, you will have to cite other people‚Äôs work and include the references in your work. In this class, possibly for some of our exercises and definitely for the class project, you need to include references and cite other people‚Äôs work like in a regular research paper.\nI assume you are familiar with how citing references works, and hopefully, you are already using a reference manager. You might be using EndNote or Mendeley or PaperPile or‚Ä¶\nTo have something that plays well with Quarto/R Markdown, you need references in a format called bibtex. This is essentially a large file of structured plain text. You don‚Äôt ever want to edit the bibtex file yourself. Instead, you use a reference manager that is bibtex friendly. I mostly use Zotero, which is free. If you use Zotero, you should use the Better BibTeX extension, which allows you to keep your bib file in sync as you update references in Zotero.\nYou could probably use others. I don‚Äôt have much experience with any others. Whichever reference manager you use, the important part is that you need to be able to save or export your references as a bibtex file.\nTo use references in your Quarto or RMarkdown files, specify the name and location of your bibtex file in the YAML header. You can also specify a CSL style file there. This is a file which controls how the references are formatted. Styles for almost any journal are available for download from Zotero‚Äôs styles page. When you create your bibtex file, each reference gets a key, which is a shorthand that is generated by the reference manager or you can create yourself. For instance, I use a format of lower-case first author last name followed by 2 digit year for each reference. If I cite a paper by the same first author that was published in the same year, then a lower case letter is added to the end. For instance, for a paper that I wrote as 1st author in 2010, my bibtex key might be handel10 or handel10a. You can decide what scheme to use, just pick one and use it forever.\nIn your Quarto document, you can then cite the reference by adding the key, such as ...in the paper by Handel et al. [@handel10].... When you process the file, it looks for a reference with the specified tag in your bibtex file and places all references at the end of your document using the formatting you specified.\nFor references with Quarto, see this Quarto documentation.\nSome more details on citations and references in R Markdown/bookdown can be found in the RMarkdown Cookbook bibliography section and citations section.\nYou will see a few examples of bibtex use together with Quarto/R Markdown in the course, so when it‚Äôs time for you to use it (e.g.¬†for the class project), you should know how to go about doing it.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Reference Management"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-course/introduction-course.html",
    "href": "content/module-intro-tools/introduction-course/introduction-course.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Introduction\nWelcome to the class Modern Applied Data Analysis (MADA)!\nThe course is listed as EPID8060E/BIOS8060E. It doesn‚Äôt matter if you are enrolled under the EPID or BIOS label. I will generally refer to the course as MADA.\n\n\nModule Overview\nThis first module provides a brief introduction to the course, the tools we will be using, and the topic.\n\n\nLearning Objectives\nThe specific learning objectives for this unit are:\n\nKnow what this course is all about.\nKnow how this course is set up and what you are expected to do.\n\n\n\nCourse Goals\nThe main goal for this course is for you to learn the whole process of performing a data analysis project. The focus is on applied analysis of real world, messy data.\nA second goal is to introduce you to some modern analysis approaches commonly referred to as Machine Learning.\nA related goal is to introduce you to a set of tools that allow for a modern, reproducible workflow of your analysis.\nFor more detailed learning objectives, see the Syllabus.\n\n\n\nIn this course, I randomly switch back and forth between singular and plural. Source: phdcomics.com.\n\n\n\n\nCourse Philosophy\nHere are my goals, promises, and expectations for this course.\n\nI expect you to be self-motivated and committed to learning the material by putting in the effort needed to succeed.\nI will try to maximize the rewards you get by hopefully teaching methods that are useful to you, and I will try to provide as much help as needed to maximize your learning.\nThis class strives to be challenging but non-threatening. As such, I‚Äôll make you work hard, and expect you to do the assigned tasks by the deadlines, but in the end, I usually don‚Äôt grade hard - unless you fail to keep up your end of the agreement and don‚Äôt put in the work.\nThis class is open everything. You can use the internet, AI, ask your classmates, myself and others, get help from wherever you can. I trust you will find the right balance of getting help when you need it while still putting in enough effort to experience real learning.\nI will not perform any policing to try to prevent you from taking shortcuts (i.e., not doing work yourself). The class contains graded assessments with deadlines, but those are meant to help you stay on track. If you somehow cheat - and cheating will be easy - you are mainly cheating yourself out of learning.\n\nOverall, I hope this course is going to be useful, interesting, challenging and also interactive. Online courses are always a bit tricky with interaction/participation. I hope we can create something online that feels like a classroom. Please participate, ask questions, etc. The more you engage in the course, the more you‚Äôll get out of it.\n\n\nCourse Setup\n\nThe course is split into modules. Each module will usually be covered in a week. The Schedule document gives an outline. The schedule is not fully finalized and will change, so check frequently.\nEach module consists of one or several units/documents containing a mix of things I wrote, and writings or videos by others. They are listed in the order you should go through them.\nAll material for the course can be accessed through this course website. Some material might not be available yet and will be unlocked as the course proceeds.\nThere is generally a lot of material for each unit. You are expected to go through the main components at a level that allows you to get the big picture and be able to answer the quiz questions. Once you get the overall idea, consider the materials to be resources you can visit on demand, e.g., when you are working on the exercises.\nYou will be placed in groups throughout the course. The hope is that this will create a support group of classmates where you can help each other. Assignments often require interaction among group members (like in a real-world team), and I hope you will interact with your group members outside the assignments as well. You are welcome and encouraged to interact with anyone else in the course.\n\n\n\nAssessments\nFor details on assessments, see the Assessments page.\n\n\nCourse Resources\nWe‚Äôll be drawing on a lot of different resources. I compiled a list with the ones we‚Äôll use and others you might find helpful in the Resources section of the class website.\n\n\nGetting help\nI do not expect you to figure it all out yourselves. You will get stuck and are encouraged to seek help. You can ask for help from your classmates or your instructor. Also, use the wider community online. For specific places to get help, see the Resources pages.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Course Introduction"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html",
    "href": "content/module-data/data-types-complex/data-types-complex.html",
    "title": "Complex types of data",
    "section": "",
    "text": "For this unit, we will discuss different types of data and how data type influences the analysis approach.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#text",
    "href": "content/module-data/data-types-complex/data-types-complex.html#text",
    "title": "Complex types of data",
    "section": "Text",
    "text": "Text\nWorking with and analyzing larger sections of text is becoming increasingly important. Complex and powerful AI tools (e.g., ChatGPT) have become rather good at working with text. The analysis of text often goes by the term natural language processing. Such text analysis will continue to increase in importance, given the increasing data streams of that type. If you are interested in doing full analyses of text data, the tidytext R package and the Text mining with R book are great resources. A short introduction to this topic is the Text Mining chapter (27) of IDS.\nOf course, with the rise of text-based LLM AI tools and similar such offerings, this whole area of data analysis has grown very large on its own. If you want to really dig deep into this, check out some of the course and training offerings from DeepLearning.AI.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#audio",
    "href": "content/module-data/data-types-complex/data-types-complex.html#audio",
    "title": "Complex types of data",
    "section": "Audio",
    "text": "Audio\nAudio recordings can contain text/speech, or other sound. In the former case, models need to be applied to transform audio into text. After that, any text analysis tool can be used. If the audio does not contain text, one likely wants to apply other analyses. For instance one might want to detect if a certain sound is present, such as the sound of a fire alarm going off. This generally requires partitioning and transforming the audio into pieces that can be turned into some form of numeric variables and analyzed. Audio has a time-series structure that needs to be taken into account when analyzing it.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#images",
    "href": "content/module-data/data-types-complex/data-types-complex.html#images",
    "title": "Complex types of data",
    "section": "Images",
    "text": "Images\nFor analysis purposes, one can think of an image is essentially a matrix of pixels, with each pixel having color and intensity values. This large amount of - usually very structured ‚Äî data needs to be processed.\nImages are generally converted into multiple matrices of values for different pixels of an image. For instance, one could divide an image into a 100x100 grid of pixels, and assign each pixel RGB (color) values and intensity. That means one would have 4 matrices of numeric values, each of size 100x100. One would then perform operations on those values. As you can imagine, that quickly leads to fairly large amounts of data. These days, most successful image analysis is done using some form of neural nets, which are generally considered an artificial intelligence (AI) method. (But recall that ML and AI terminology is somewhat fuzzy. The general current usage is that AI are a type of ML, specifically the neural net type.)",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#video",
    "href": "content/module-data/data-types-complex/data-types-complex.html#video",
    "title": "Complex types of data",
    "section": "Video",
    "text": "Video\nVideo is essentially a time-series of images with audio. As such, approaches that work for image and audio analysis can also be applied to video. Of course, the time-series nature of the images makes things more complicated. Neural-net/AI based methods are most often used.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#genetics-and--omics-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#genetics-and--omics-data",
    "title": "Complex types of data",
    "section": "Genetics and -omics data",
    "text": "Genetics and -omics data\nThe main genetic type of data is based on sequences. A lot of specialized tools exist to work with what is often fairly noisy data. Aligning sequences, comparing them, placing them on phylogenetic trees, and other such operations are so common and important that there is a large area of tools for those purposes.\nData that is often called -omics (e.g., metabolomics, glycomics) is often rectangular in structure, but often has distinguishing features, such as few individuals/rows and many observations/columns. Such data needs special treatment. Often, variable/feature reduction is a common step in the analysis workflow.\nThe bioconductor website is your source for (almost) all tools and resources related to genetics and omics-type data analyses in R.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#big-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#big-data",
    "title": "Complex types of data",
    "section": "Big data",
    "text": "Big data\nThis isn‚Äôt really a type of data in the same sense as the ones listed above. Still, it is its own type of data simply by the fact that it encompasses large amounts of data. Some of the complex data types above, e.g., videos, quickly become big data.\nWhile the term big data is a bit fuzzy, in general people mean any dataset that doesn‚Äôt easily fit into the memory of a regular computer (or cluster) and thus needs to be analyzed using special tools. Alternatively, big data might be defined as data that is so big that doing analysis on it takes too long using standard tools (e.g.¬†R on a regular computer) and instead requires special treatment. Of course this also depends on the type of analysis, not only the type of data. As computers keep getting faster and tools more flexible and integrated, the label big data is a moving target.\nGenerally, big data is stored somewhere in a database. SQL type databases are most common. You then want to access that database in a form that allows you to perform your analysis. There are different ways of dealing with big data. Most methods are general and apply independent of the programming language you use. This article describes a few general approaches and explains how they can be implemented in R. This webinar gives a bit more information and a nice description of the overall setup for big data. As you learn in that tutorial, R is often used together with other software to analyze big data. A tool that is often used for big data analysis is Spark. For R, there is the sparklyr package, which allows one to nicely interface with Spark.\nIn general, when you work with big data, you will have to carefully look at the data, the type of database it is stored, and the analysis goals. Based on that, you should use a stack of tools that allows analysis. The Databases task view gives a good overview of different R packages for specific types of databases. You will use R for your analysis, and R will then interface with other software. This interface is usually fairly seamless.\nBig data is commonly modeled using complex models, such as machine-learning or AI algorithms. The reason for that is that those models are very powerful but need a lot of data. Thus, if you have big data, you can use them. That said, you can analyze big data with any model you want, including simple GLM or similar such models.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#hierarchical-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#hierarchical-data",
    "title": "Complex types of data",
    "section": "Hierarchical data",
    "text": "Hierarchical data\nThis type of data is actually quite common. A hierarchy (also at times called multi-level or nesting structure) occurs when there is some type of grouping inherent. For instance, if you analyze heart attacks from different hospitals, there might be systematic differences between hospitals that are worth taking into account. You can then build models that take that into account. I think there is a strong case to be made that one should often start with a hierarchical modeling approach, and only drop it if one is convinced that there are no systematic differences in the data. Unfortunately, such hierarchical models are still a little bit harder to implement, and often a lot harder to understand and interpret. For such models, a Bayesian framework is often very useful. A good introductory resource for hierarchical modeling, and especially how to do it with R, is Statistical Rethinking (the second half of the book). There is also a CRAN Task view that lists R packages relevant to hierarchical modeling.\nAnother term you might hear is recursively nested data. While this type of data is a type of hierarchical data, when we discuss hierarchical data we often think about nesting in terms of statistical clusters. Computer scientists and web developers often use hierarchical data for storage and data transfer purposes, and often call it recursively nested data instead.\nRecursively nested data has become extremely common on the internet the past few years, and is essentially the default data format of APIs. While statisticians often store hierarchical data in one or more rectangular data sets, recursively nested data is stored in recursive lists. For example, consider the following list of Cars in Jay Leno‚Äôs Garage.\n\nMazda Miata\n\nYear: 1996\nOriginal MSRP: $18450\nColor: Red\nConvertible: True\n\nChevrolet Volt\n\nYear: 2011\nOriginal MSRP: $4100\nColor: Steel Metallic\nInterior: Black\nConvertible: False\nHybrid: True\n\nEtc.\n\nWhile we can imagine a world where this data is stored in a rectangular format (see mtcars for example), we would have to do some processing to get it in that format, and we would have to decide what to do for the fields that are not shared across all the entries. These data can also have lists nested within lists (nested within lists, to very high levels of list-recursion), which can be more challenging to ‚Äúrectangle‚Äù (that is, convert to a rectangular format).\nData stored in this format is usually stored in JSON or XML formats, which can be read into R using the rjson or xml2 packages, among others. The new edition of R4DS has a chapter on working with hierarchical and recursively nested data data (but does not cover the fitting/analysis part). The repurrrsive package, by Jenny Brian, contains some examples of this type of data.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#longitudinaltime-series-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#longitudinaltime-series-data",
    "title": "Complex types of data",
    "section": "Longitudinal/Time-series data",
    "text": "Longitudinal/Time-series data\nLongitudinal or time series are a specialized data type that are autocorrelated (i.e., each measurement is correlated with at least one measurement that came before it), and we can therefore get better predictions by using specialized models that take this data structure into account. A good resource is the free textbook Forecasting: Principles and Practice and a lot of the other work by Rob Hyndman.\nA very useful set of tools to allow times-series work in R is the set of packages called the tidyverts. The modeltime R package allows one to use the tidymodels framework to analyze time-series data.\nCRAN also has a Task View for Time Series Analysis. (A Task View on CRAN is a site that tries to combine and summarize various R packages for a specific topic). Another task view that deals with longitudinal/time-series data is the Survival Analysis Task View.\nNote that, somewhat confusingly, longitudinal data are common in epidemiology, and despite being measured over time, ‚Äútime-series methods‚Äù are not usually appropriate. Time-series and forecasting methods are mainly appropriate for data where the times are dense, which means many time points are measured somewhat close together. The term ‚Äúlongitudinal data‚Äù (also called panel data in some fields) is most commonly used to refer to sparse data collected over time, where each study unit is only measured a handful of times which may be spread apart. Most cohort studies therefore fall into this longitudinal framework, not into a time series framework.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-complex/data-types-complex.html#spatial-data",
    "href": "content/module-data/data-types-complex/data-types-complex.html#spatial-data",
    "title": "Complex types of data",
    "section": "Spatial data",
    "text": "Spatial data\nLike time series data, spatial data also feature autocorrelation, but typically in two dimensions. (i.e., latitude and longitude.) Spatiotemporal data, typically collected as repeated measurements of spatial data over time, is also somewhat common. One could also have an elevation coordinate and have 3D (or even 4D spatiotemporal data) spatial data, but most analyses focus on 2D spatial autocorrelation structures.\nWhile there is (to my knowledge) no current way to fit specific spatial models in tidymodels (i.e.¬†no modeltime analogue), one can use spatial resampling through the spatialsample package, which provides resampling methods that take spatial autocorrelation into account.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Complex Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html",
    "title": "Storing and Loading data",
    "section": "",
    "text": "For this unit, we will discuss different common formats for storing data.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Storage Formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#text",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#text",
    "title": "Storing and Loading data",
    "section": "Text",
    "text": "Text\nPlain text is the most portable and easy to use format. Most code, no matter what the programming language is, is written and stored as text. The Quarto files you have been working on are also text. Text can be read by any text editor and can be modified easily. The major disadvantage is that it‚Äôs not that suitable for storing potentially complex data.\nFixed-width files are an important special case of plain text data, which are becoming more rare as time goes on, but that you may still run into. These files have a set length for each variable (so column one may be 8 characters long, column 2 may be 3 characters long, etc.), so they are easy for the computer to read, but you will need to use an appropriate function and set the widths. While fixed width files are usable in most programs, we do not recommend them because they are (generally) more annoying to work with than other options.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Storage Formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#csv-and-delimited-formats",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#csv-and-delimited-formats",
    "title": "Storing and Loading data",
    "section": "CSV (and delimited formats)",
    "text": "CSV (and delimited formats)\nThe CSV (comma-separated values) format is really just a text format that‚Äôs very well suited for rectangular, spreadsheet-type data. In a CSV, you have rows and columns of entries, and the entries are separated by commas. CSV files are easily readable by most software. If you are storing information that contains commas as part of the values (e.g., if you are storing text in some columns), CSV files can be problematic. TSV files, which stands for tab-separated values, are a variation that uses a tab instead of a comma to separate entries, thus sometimes being more robust. Semicolons are also sometimes used to separate values.\nIn general these files are called ‚Äúdelimited‚Äù formats, and the separator character (comma for CSV, tab for TSV) is called the ‚Äúdelimiter‚Äù. A special case is space-delimited files, which are typically saved in DAT or TXT file types instead of a specialized format.\nDelimited storage formats are likely the most common data-sharing format, and are a good, all-purpose way to share data files across platforms.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Storage Formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#compressed-data",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#compressed-data",
    "title": "Storing and Loading data",
    "section": "Compressed data",
    "text": "Compressed data\nWhile CSV and text files are very easily readable formats, one major disadvantage of those files is that they can get relatively large. They don‚Äôt store information in the most efficient manner. CSV files also can‚Äôt store meta-information, for instance the fact that some variable encodes a date. In a CSV file, it‚Äôs all just text.\nThere are formats that are more space-efficient and can preserve some meta-information. Examples are the Rds and Rda formats in R. For differences between those two formats, see e.g.¬†this article. The disadvantage of this and similar formats is that they require specific software to read the data. As long as the data format and software needed is open and free, it‚Äôs not that big of an issue, though sometimes it can hinder portability.\nFormats like parquet and Arrow invented by Apache for their Arrow software are also very efficient and have become quite common in industry recently. The Arrow R package is a fairly intuitive interface for these file formats if you come across them in your data science work.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Storage Formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#other-open-formats",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#other-open-formats",
    "title": "Storing and Loading data",
    "section": "Other open formats",
    "text": "Other open formats\nDepending on the data type, there are other open formats for images, audio, video, web content, etc. As much as possible it is good to use formats that are open and follow some international standard. This makes it most likely that a wide variety of tools can work with these files, both in the present and the future.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Storage Formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#excel",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#excel",
    "title": "Storing and Loading data",
    "section": "Excel",
    "text": "Excel\nExcel has its own format for storing data. It is common enough that a lot of software can read Excel files. One problem with the Excel format is that the way the data is stored is not always how it looks when you open the file and look at the data, due to additional formatting that‚Äôs encoded. This means you think you might have the data in one format, but really it‚Äôs different once you load it into another software. Dates are good example where this can happen.\nIf possible, it is safer to store information in CSV files. If you get data in Excel files, you‚Äôll ideally want to read it into another software - such as R - as quickly as possible.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Storage Formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#sasspssstataetc.",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#sasspssstataetc.",
    "title": "Storing and Loading data",
    "section": "SAS/SPSS/STATA/etc.",
    "text": "SAS/SPSS/STATA/etc.\nMost commercial software has its own format of storing code and data. The problem with these formats is that you need the right software to read the data, and you often won‚Äôt know what you are getting unless you can open the file with the right program.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Storage Formats"
    ]
  },
  {
    "objectID": "content/module-data/data-storage-formats/data-storage-formats.html#databases",
    "href": "content/module-data/data-storage-formats/data-storage-formats.html#databases",
    "title": "Storing and Loading data",
    "section": "Databases",
    "text": "Databases\nAt times, the data is so complex or large that it can‚Äôt be stored in a single file. In such situations, data is often stored in databases. There are tons of types of databases in existence. SQL based databases are very common. Databases are a huge topic and definitely something worth learning about if you need to work with them. However, since there are so many types, it is best to learn ‚Äòas needed‚Äô about the specific database you plan on working with.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Storage Formats"
    ]
  },
  {
    "objectID": "content/module-data/data-generation-process/data-generation-process.html",
    "href": "content/module-data/data-generation-process/data-generation-process.html",
    "title": "Thinking about data generation",
    "section": "",
    "text": "Overview\nWe are generally in the business of analyzing data, not generating it (but see the synthetic data content). Nevertheless, it is very important to think about the processes that generated the data you are trying to analyze.\n\n\nLearning Objectives\n\nUnderstand why it‚Äôs important to think of the data generation process\nBe familiar with common features and structures in data\n\n\n\nIntroduction\nEvery dataset is generated by processes that can often be very complex. At times, seemingly similar data is generated by different processes. For instance, most of the text you read in this course was written by a single person. In contrast, a lot of the text you read in Wikipedia was co-authored and co-edited by many individuals. Thus, while both are pieces of text, the processes that generated them are different. Often, this difference in processes is reflected in patterns in the data, and needs to be accounted for. For any kind of analysis you do, it is useful to think about the likely processes that generated the data. This can influence your analysis approach.\n\n\nRandomness\nMany natural processes that lead to the generation of data have some inherent randomness associated with them. For instance if you want to analyze the effect of a drug on some health outcome, you expect to see variation among individuals. Some of the variation is due to individual characteristics (e.g., age, weight), but some variation is likely random. While one could discuss if true randomness exists, for the purpose of pretty much any analysis, randomness just means anything that can‚Äôt be measured in our system and that fluctuates randomly and contributes to variation. Being aware of which processes might be affected by more or less of such random variation is important. In the worst case, if you have too much random variation, it might swamp your signal and you won‚Äôt see any meaningful pattern in your data.\n\n\nMeasurements\nMeasurements that lead to data always have inherent limitations. For instance if you use a scale to measure someone‚Äôs weight, it might only report it to one decimal place. Similarly, if your scale has a minimum and maximum value it can record, anything that is lighter and heavier than that value will not produce a numeric result, just an indication that it‚Äôs too light (the scale might show zero) or too heavy (the scale might show some error message or the maximum value). It is important to consider these kind of measurement limitations and account for them if possible.\n\n\nInteractions\nOften, the components in our system of interest interact. The simplest interaction is a direct one between a predictor and an outcome. Say you give some kind of cancer drug to a patient and see if it reduces the size of their tumor. This is often the scientific question of interest. However, there might be other factors interacting with the variables of interest. For instance the drug might induce some immune response which could either help or hinder the ability of the drug to shrink the tumor. Let‚Äôs say the immune system synergistically helps the drug reduce the tumor in individuals that have blood type A-positive and it hinders the drug in everyone else. These might be important aspects that ‚Äî if you know them ‚Äî need to inform your model.\nThe whole field of causal modeling tries to - among other things - get at such situations and help you devise the best analysis approach given what you know or assume about the system. It‚Äôs worth learning causal modeling, but we can‚Äôt cover it in this class.\n\n\nData structures\nDepending on the processes that generate the data, it is common for data to have certain structures. Thinking about those structures, and as suitable accounting for them in your analyses, is important. The Complex Data unit discussed some important structure such as temporal/spatial/hierarchical. These structures will likely determine the details of your modeling approach, since you will likely want to use models that account for such structures.\n\n\nSummary\nOccasionally, you might do an analysis that tries to explicitly model (some of) the processes that lead to the observed data. More often, you will want to discover patterns and you don‚Äôt directly care about the processes that led to the data. Nevertheless, thinking about them is important since it might influence the way you go about analyzing the data, the specific questions you might try to answer, and the conclusions you can draw from your analysis. Therefore, as a good analyst, you want to understand as much as possible about the system and processes that led to the data you are analyzing.\n\n\nFurther resources\n\nA great source for causal modeling, and statistics in general, is Richard McElreath‚Äôs online course - some of the materials are free, though his book is not.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Generation"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html",
    "href": "content/module-data/assessment-data.html",
    "title": "Assessment - All about Data",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "4. Data Introduction & Synthetic Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#overview",
    "href": "content/module-data/assessment-data.html#overview",
    "title": "Assessment - All about Data",
    "section": "Overview",
    "text": "Overview\nYou have two main options for the exercise this week:\n\nOption 1: Try your hand at a bit of an exploration of some complex data type.\nOption 2: Generate and explore synthetic data.\n\nFor either option, I expect well-documented code and lots of explanatory text in the Quarto document.\nWhile this is an open-ended exercise and there are no specific requirements as to what you need to produce, I expect more than just a single figure or 10 lines of code. Since you are getting better each week, it should definitely be at least as comprehensive as last week‚Äôs exercise üòÅ.",
    "crumbs": [
      "Assessments",
      "4. Data Introduction & Synthetic Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#setup",
    "href": "content/module-data/assessment-data.html#setup",
    "title": "Assessment - All about Data",
    "section": "Setup",
    "text": "Setup\nThis exercise doesn‚Äôt have an existing folder in your portfolio. Make a new folder, call it either data-exercise. Copy one of the existing Quarto files into that new folder (e.g., the one called starter-analysis-exercise.qmd), rename it to data-exercise.qmd. Then open your portfolio as a project/workspace in Positron. Open the new .qmd file, delete everything apart from the YAML header. Now you are ready to start.",
    "crumbs": [
      "Assessments",
      "4. Data Introduction & Synthetic Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#option-1",
    "href": "content/module-data/assessment-data.html#option-1",
    "title": "Assessment - All about Data",
    "section": "Option 1",
    "text": "Option 1\nFind some data that falls into the categories described in the Complex data types unit. It could be some text or images or video or audio or any other complex data. Basically anything that‚Äôs not rectangular data with rows as observations and columns as variables.\nThen look for some suitable R packages that allow you to explore the data. You don‚Äôt need to do any statistical analysis at this stage. Focus on loading the data, processing as needed, and then do some descriptive/exploratory analysis, e.g.¬†by making plots or tables or‚Ä¶ Basically anything that you can think of.",
    "crumbs": [
      "Assessments",
      "4. Data Introduction & Synthetic Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#option-2",
    "href": "content/module-data/assessment-data.html#option-2",
    "title": "Assessment - All about Data",
    "section": "Option 2",
    "text": "Option 2\nWrite code that generates a synthetic dataset. This dataset should have multiple variables, and there should be some associations between variables.\nThen write code that explores the data by making plots or tables to confirm that your synthetic data is what you expect it to be.\nThen fit a few simple models to the data. For instance, use the lm or glm functions to fit a linear or logistic model. Make sure your model can recover the associations you built into the data. Explore if and how different models might be able to capture the patterns you see.\nTake some inspiration from the examples shown in the Synthetic Data module.",
    "crumbs": [
      "Assessments",
      "4. Data Introduction & Synthetic Data"
    ]
  },
  {
    "objectID": "content/module-data/assessment-data.html#wrap-up",
    "href": "content/module-data/assessment-data.html#wrap-up",
    "title": "Assessment - All about Data",
    "section": "Wrap-up",
    "text": "Wrap-up\nOnce you got your exercise done, you need to add it to your website. To that end, edit the _quarto.yml file as you did previously. Basically, copy the menu: entry for the Starter Analysis exercise, and adjust so it has the right title and points to the right file for this week‚Äôs exercise. Remember to be careful to have exactly the same number of empty spaces at the beginning of the line as all the other lines.\nWhile you are in that file, if you haven‚Äôt done so, update other parts, e.g.¬†such as the title and the github URL. See the instructions for the first exercise if you need a reminder.\nOnce all done, render your website to make sure everything looks ok and your new exercise shows up. Once things looks fine on your local computer, push your updates to GitHub and check that they also look ok online.\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it.",
    "crumbs": [
      "Assessments",
      "4. Data Introduction & Synthetic Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/missing-data/missing-data.html",
    "href": "content/module-data-processing/missing-data/missing-data.html",
    "title": "Missing Data",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss missing data and what to do about it.\n\n\nLearning Objectives\n\nBe familiar with the problem of missing data.\nBe able to apply approaches to handle missing data.\n\n\n\nIntroduction\nIn almost any dataset, there are some missing entries. Data can be missing for different reasons. It could have not been asked/recorded, the person refused to provide it, the variable is not applicable (e.g.¬†for a non-smoker, the variable ‚Äúnumber of cigarettes per day‚Äù might be left blank.) Understanding why data are missing is important, so we can form an action plan based on that.\nThere are different ways to deal with missing data, here are some common ones.\n\n\nRemove missing data\nMany analysis approaches do not allow for missing data. In this case, you need to reduce your dataset such that nothing is missing.\nThe easiest approach is to remove all observations with missing data (this is called ‚Äúlistwise deletion‚Äù). That can get problematic if you have a lot of variables, and each variable has some missing values; you might be left with almost no observations that have complete data. Another approach is to exclude them in analyses where they have missing data in variables of interest, but include then in analyses where they have information for all variables that are being considered (this is called ‚Äúpairwise deletion‚Äù). Both of these methods can lead to bias in different ways and should be used with careful consideration.\nAnother option is to remove all variables with missing data from further analysis. Unfortunately, it is common that at least some values are missing for each variable, which means you would be left with no variables. Also, if the variable is important for the question you are trying to answer, you obviously cannot remove it.\nAnother option is to use a combination of removing variables and observations. You could start removing variables with missing values above some threshold, e.g.¬†any variable that has more than 10% or 20% (or some value you pick) missing. There is no rule for this, and you need to justify it. Then once all variables with missing values above some threshold are removed, you remove any remaining observations that still contain missing data. This mix of removing variables and observations might preserve the most amount of data.\nThere are two problems when removing data. One is the obvious fact that you lose data, and thus statistical power. The other problem is if the data are not missing completely at random. In that case, by removing those observations with missing data, you introduce bias into your dataset. Again, it is important to understand why and how data are missing so you can have an understanding of potential problems such as introducing bias.\n\n\nImpute missing data\nInstead of removing rows and columns (observations and variables) until you have no more missing entry, you can also impute the missing values. Basically, you make an educated, data-driven guess as to what the missing value might have been and stick that value into the missing slot. In principle, any regression and classification method that you can use to estimate and predict an outcome can be used by temporarily thinking of the variable you want to impute as your outcome and the other variables as predictors, and then predicting the missing values. Methods such as k-nearest neighbors or random forest, which we discuss later in the course, are useful for this. Imputation adds uncertainty since you made guesses for the missing values, and often, the estimated/guessed values are randomly drawn from a distribution. Multiple imputation creates several different imputed datasets, and you can then run your analysis on each of those imputed datasets, hopefully with similar results for each. In R, the recipes package, which is part of the fairly new set of tidy modeling tools called tidymodels, and others such as the mice package allow for imputation. We‚Äôll try some of those in a later unit.\n\n\nFeature engineering\nAs the creating new data/variables unit discusses, it is possible to create new variables/data from the existing ones. This can sometimes help with missing values.\nFor instance, you could create new variables/predictors/features with less missing. As an example, if you have data that records if a person drinks beer (yes/no), wine (yes/no), or hard liquor (yes/no) and each of those variables has some missing, maybe you can create a new variable labeled any alcohol and code it as yes/no. If a person has a yes for at least one of the 3 original variables, they would be coded as yes in the new one. If they have all no, they would be coded as no. For anyone left, you do need to decide what to do with missing values in the original variables, i.e.¬†if you interpret them as yes or no. You could either stick with one, e.g.¬†if you had some additional knowledge that suggests anyone who doesn‚Äôt have that value recorded is more likely a no. For categorical data, you could also treat missing values as their own category (this can be useful when missingness provides information about the value of the response).\n\n\nKeep missing data\nWhile many standard statistical models, such as linear and generalized linear models don‚Äôt work with missing values, there are some statistical algorithms that can handle missing values. For instance tree-based methods (e.g., random forest, boosted regression trees) can take predictors that have missing values. If you know that the method you plan on using can handle missing values, you can decide to not get rid of them but keep them in the data. Though it‚Äôs still useful to think carefully about your missing data and possibly process it. You might get better results from your models, even from those that can handle missing values.\n\n\nAvoid missing data\nAs you can tell, missing values can cause headaches. It is therefore very important if you design and collect data to try to do it in such a way as to minimize missing values. Of course, if you analyze data collected by someone else, there is not much you can do, and you have to decide how to deal with missing values. For that, having a good understanding of what the data mean and how they were collected is essential.\n\n\nMissing data in R\nIn R, missing values are coded as NA. When you read data into R and that data for instance codes missing as 99, you should recode to NA. NA in R is a bit tricky, since any operation on NA returns NA. The tidyverse functions tend to be pretty good in dealing with NA, but for base R code you often have to be more careful. The function is.na() is often useful. Some functions, e.g.¬†mean() and sum() can deal with NA if you tell them what to do. Sometimes, functions just deal with NA in some built-in way. You need to check that this is what you want. Always perform careful checks when handling missing values! The tidyverse packages tidyr and dplyr have tools for dealing with missing values during the wrangling process.\n\n\nSummary\nIn pretty much any real dataset, you‚Äôll encounter missing data. You should understand as much as possible why these entries are missing. Based on that, and your analysis goals, you should formulate a good approach of handling the missing data.\nOften, it‚Äôs good to do things more than one way and show that your findings are robust. For instance if you either remove or impute some missing data and you get different results, that means the missing data is influential and you need to be very careful about the conclusions you draw. On the other hand, if you get more or less the same result, you can be somewhat reassured that the missing data might not matter too much.\n\n\nFurther Resources\n\nThere is a chapter on missing data in the book Feature Engineering and Selection: A Practical Approach for Predictive Models.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Dealing with missing data"
    ]
  },
  {
    "objectID": "content/module-data-processing/feature-engineering/feature-engineering.html",
    "href": "content/module-data-processing/feature-engineering/feature-engineering.html",
    "title": "Feature engineering",
    "section": "",
    "text": "For this unit, we will discuss the idea of processing existing variables to potentially improve model performance. This is often called Feature Engineering.1",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "content/module-data-processing/feature-engineering/feature-engineering.html#removing-variables",
    "href": "content/module-data-processing/feature-engineering/feature-engineering.html#removing-variables",
    "title": "Feature engineering",
    "section": "Removing variables",
    "text": "Removing variables\nIt is often the case that we have more data than we need to answer a scientific question. For almost any analysis, one therefore needs to remove some variables before starting the statistical model fitting process. A simple example is an identifier (ID) variable or the names of subjects. Often this information is not needed for the modeling process and can thus be removed before fitting. Other examples are instances where the data was collected for some purpose other than the planned analysis. In this case, it is likely that there are variables in the data which are irrelevant for our analysis.\nSuch removal of variables is done on scientific grounds, based on expert opinion. Ideally, you should report in enough detail which parts of the data you included and excluded in that way to allow the reader to make an informed decision if they agree with what you did. And of course, you should ‚Äì as much as possible ‚Äì also provide the raw data and the R scripts which do and document removal of specific variables ‚Äì such that someone who doesn‚Äôt agree with your choice could re-run the analysis with different inclusion/exclusion criteria.\nSometimes, you might have variables that could, in principle, be useful, but the reported values show little diversity and thus contain little information. For instance, if you had a sample of several hundred individuals and only 3 of them were smokers, then it might not be useful to include the smoking variable for the analysis of this dataset, even if, in general, it might be worth considering. Such variables that do not contain much information are called ‚Äúnear-zero variance‚Äù variables. Some models perform better if those variables are removed. Other modeling approaches do not care since they have built-in mechanisms to remove variables that are not useful in predicting the outcome.\nAnother instance where removing variables might be useful is if predictors are strongly collinear/correlated. A trivial example is if you have height reported as both inches and centimeters in your data. Obviously, one of them should be removed. Other variables might not be so obviously containing the same information, but might be related enough (collinear) that including both makes the model performance worse. An example might be age and height among children.\nAnother example we already discussed are variables with missing values. In this case, you might want to remove the variable, or the observations, or a mix of them.\nAnother important consideration are variables that could bias your outcome. This is a very broad and complex topic. The best current approach is to take a causal modeling analysis perspective, and based on your causal diagram decide which variables to include. It is a bad idea to just add all variables into your model, even after you removed obviously useless ones. Unfortunately, while a very important topic, we can‚Äôt cover causal modeling here. If you want to learn more, the best resource I‚Äôm aware of is Statistical Rethinking. An alternative are some papers by Judea Pearl. Some of his work is very technical, but he has several readable introductions.\n\nAll of these inclusion/exclusion decisions are based on expert (that‚Äôs you!) judgment. There is no magic recipe you can follow. Some statistical methods can help (see below), but they are only so useful. If someone on the internet tries to sell a magical way of knowing how to process your data before fitting, run away!",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "content/module-data-processing/feature-engineering/feature-engineering.html#footnotes",
    "href": "content/module-data-processing/feature-engineering/feature-engineering.html#footnotes",
    "title": "Feature engineering",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFeature is another word used for predictor or independent or input variable. The term feature is especially common in the machine learning literature.‚Ü©Ô∏é\nOf course, sometimes exceptions apply. But you need to be able to fully justify why for your situation it is ok to use one of these generally bad approaches. And ‚Äúeveryone else is doing it‚Äù is not a good justification üòÅ.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Feature Engineering"
    ]
  },
  {
    "objectID": "content/module-data-processing/data-processing-introduction/data-processing-introduction.html",
    "href": "content/module-data-processing/data-processing-introduction/data-processing-introduction.html",
    "title": "Processing Data",
    "section": "",
    "text": "Overview\nThis unit provides a brief introduction to the topic of data processing.\n\n\nLearning Objectives\n\nBe familiar with the topic of data processing\nUnderstand the importance of proper processing for data analysis\n\n\n\nIntroduction\nI want to remind you of this diagram:\n\n\n\nData analysis workflow\n\n\nDifferent data-analysis related tasks rarely happen in a linear sequence. Instead there is a general stage before statistical analysis where you mess with your data (that‚Äôs a technical term üòÅ) to get ready for the main formal analysis. And often, as you start that formal analysis, you‚Äôll have to go back and do some more cleaning/exploration.\nWhile one can distinguish the different tasks of Cleaning, Exploring and Processing, one can also more generally think of them as all being a part of data processing. Alternative terms often used instead of processing are data wrangling or tidying. Cleaning, munging.\nAs with any part of the data analysis process, processing should be automated, reproducible, and well-documented. No ‚Äúfixing by hand‚Äù!\n\n\nData processing tasks\nAlmost all datasets require some processing before they are in a format that can be used with statistical models. For any data analysis, the majority of time is likely spent in the data processing and exploration steps before the main model fitting. Some folks suggest that this processing part is as much as 80% of the time spent on the whole project. In my experience, this is about right.\nWhile every dataset is different, there are certain tasks that are common. Those involve missing data, recoding variables, and dealing with outliers. Further ones are merging and reshaping of the data. We will discuss some of them in further units.\n\n\nData processing and exploratory data analysis (EDA)\nAs you process your data, you need to explore your data to figure out what needs processing. As such, exploratory data analysis (EDA) and data processing generally go together.\nEDA generally relies on making figures and tables, and we‚Äôll talk about that in a separate module. There is no clear definition for exploratory analysis. Beyond figures and tables, it can sometimes involve simple statistical computations, for instance you might compute correlations among predictors.\nYou can think of EDA as any process that happens before you start building and fitting your main models. Again, it often happens concurrently with the processing of the data.\n\n\nSummary\nData processing is a major and crucial part of any data analysis project. It can be tedious, but with practice and good tools, you might even reach a point where you enjoy the process somewhat üòÅ.\n\n\nFurther resources\n\nChapters 4 and 5 of The Art of Data science (ADS) discuss some processing and exploration tasks and examples. if you want to work along, see the Some practice section of the Data Analysis Overview page for some details on how to do it.\nThe whole R4DS book focuses on the early stages of data analysis, including processing and exploration. Definitely keep checking this book if you are looking for more information on a specific topic.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Data Processing Introduction"
    ]
  },
  {
    "objectID": "content/module-data-processing/censored-data/censored-data.html",
    "href": "content/module-data-processing/censored-data/censored-data.html",
    "title": "Title",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss\n\n\nLearning Objectives\n\nBe familiar with\n\n\n\nIntroduction\nSometimes data are censored, i.e., you know that the value is ‚ÄúX or greater/less‚Äù but not the exact value. This is common in longitudinal (survival or similar) data. It is also common in experimental data, where you might have a measurement procedure with lower or upper limits of detection. The way to deal with censored data depends on what kind of censored data you have, and what you want to do with it. In R, packages that are used to analyse survival data, e.g.¬†the survival package, can deal with some censored data. Otherwise, you will likely need to google around to figure out how you should deal with the censored data scenario you have and how to do it in R.\n\n\nSummary\n\n\nFurther Resources"
  },
  {
    "objectID": "content/module-data-presentation/visualization-introduction/visualization-introduction.html",
    "href": "content/module-data-presentation/visualization-introduction/visualization-introduction.html",
    "title": "Visualization Overview",
    "section": "",
    "text": "Overview\nIn this unit, we will discuss visualization (a.k.a. making figures), what they are good for, and how to do them in R.\n\n\n\nFlowcharts are sometimes, but not always good visualization tools. Source: xkcd.com.\n\n\n\n\nObjectives\n\nUnderstand when and how to use graphs.\nBecome familiar with major types of graphs and when to use them.\nLearn how to implement high-quality graphs in R.\n\n\n\nMotivating Examples\nThe topic of visualization is enormous, and impossible to cover thoroughly in a week (like pretty much any topic we talk about in this course üòÉ). To get you started, the two following TED talks provide some examples of interesting and inspiring data visualizations.\nIn the first video Mona Chalabi talks about good ways to present data and statistics and how good graphs can help in understanding.\n\n\n\n\nIn the second video Chris Jordan discusses how one can take abstract facts and make them much more tangible through visualization.\n\n\n\n\nIf you are interested, you can find more interesting talks on visualization in this TED topics site and this TED Studies collection.\n\n\nVisualization uses\nFigures and graphs are useful at any stage of your analysis. Use them early to explore your data, to get an idea for what you have, to figure out what cleaning you might need to do, and to check what analysis approaches might be useful. Use visualizations once you have done your analysis to check your results and interpret them. Use them at the end to communicate your results. While tables can be useful tools of communication as well, they tend to be much harder to understand and are ‚Äì in my opinion ‚Äì overused in science. Good figures are, most of the time, easier to understand and a more effective way to look at data.\n\n\nTypes of graphs\nI am sure you are familiar with several common types of graphs, such as scatter plots, boxplots, and bar charts. Many more chart types exist, some are much better at conveying potentially complex information than the simple standard ones. An excellent resources is the From Data to Viz website. It lists a lot of different graphs based on the kind of data you want to plot, and for each graph it gives a brief description and often a link to the R Graph Gallery which shows examples with code. Sometimes you start with data and are looking for the best way to plot it. Other times you see a great graph and want to know what it is called. These 2 websites are great resources for either situation.\nAnother great source that provides an overview of different graphs is the Directory of Visualizations chapter of the excellent Fundamentals of Data Visualization book by Claus Wilke. Further chapters in his book describe specific types of plots in more detail, and also provide general best practices for making plots. If you are interested in specific types of plots and general concepts of visualization, skim through his book.\n\n\nFurther visualization resources\nSee the General Resources page for some additional sources.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Visualization Overview"
    ]
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r/tables-in-r.html",
    "href": "content/module-data-presentation/tables-in-r/tables-in-r.html",
    "title": "Tables in R",
    "section": "",
    "text": "In this unit, we will discuss how to make publication-ready tables of data in R, and when making tables is appropriate (compared to visualizations).\n\n\n\nSome data are best in tables‚Ä¶but that means we have to learn how to make footnotes too.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Tables in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r/tables-in-r.html#simple-markdown-tables",
    "href": "content/module-data-presentation/tables-in-r/tables-in-r.html#simple-markdown-tables",
    "title": "Tables in R",
    "section": "Simple markdown tables",
    "text": "Simple markdown tables\nVisualizations are somewhat easy for Quarto to use, because they are always some type of image file (PNG, JPEG, etc.). However, tables are not so easy ‚Äì because they consist of just text and grid lines (usually), they need to be converted into a specific format for Quarto to use, and the best format to use depends on what output you need Quarto to produce. Remember that Quarto runs on Markdown, so the simplest option is to use a Markdown table.\nAlthough you can make those by hand, you shouldn‚Äôt! There are a few functions that can make simple Markdown tables for you, including knitr::kable(), and pander::pandoc.table(). Both of these are decently customizable and work well for a lot of cases. Because they generate Markdown text from code, they should work with Quarto regardless of the output format you use.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Tables in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r/tables-in-r.html#tables-with-complex-formats",
    "href": "content/module-data-presentation/tables-in-r/tables-in-r.html#tables-with-complex-formats",
    "title": "Tables in R",
    "section": "Tables with complex formats",
    "text": "Tables with complex formats\nHTML is the most flexible format for generating tables. Any table you can see on the internet is made with HTML (and sometimes CSS and JavaScript). R has many packages for generating HTML tables, including the easy-to-use kableExtra. The kableExtra package is basically an extension of kable to have a lot of Extra features for making nicer tables. HTML has the special power to generate interactive tables using packages like [DT] and [Reactable].\nIf you are using a PDF format (common in math, physics, theoretical statistics and other math-heavy fields), you can write LaTeX code directly in Quarto, which means you can write tables using any of the LaTeX packages you want. The kableExtra package is also compatible with LaTeX.\nWord is the most complicated format, and in Epidemiology the most common. Many table packages do not work with Word, or have limited functionality. In my experience, the best package for word output is flextable, which was designed by the creator of the officer package specifically to work with Word outputs. Any table making package that outputs raw Markdown should be compatible with Word output. Quarto specifically will attempt to convert any HTML tables into the correct output format. What this means in real life is that many HTML tables will work in Word output, but if they are too complicated they may not look how you expect, or may not work. As an alternative, I also know several researchers who like huxtable, although I personally prefer flextable.\nThe package tables deserves an honorable mention as (one of) the oldest table-making package on CRAN. If you want to generate an HTML, LaTeX, or plain text table, or use a table for further calculations in R, tables can probably do it. However, this package is very old and in my opinion is quite difficult to learn and quite clunky to use ‚Äì the interface is (unsurprinsingly) quite old-fashioned and different from tidyverse-style code. A relatively new addition to the tablemaking scene is tinytable. The tinytable package provides a modern interface for tables and notably has very few dependencies, which can be helpful for reproducibility.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Tables in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/tables-in-r/tables-in-r.html#modern-table-packages",
    "href": "content/module-data-presentation/tables-in-r/tables-in-r.html#modern-table-packages",
    "title": "Tables in R",
    "section": "‚ÄúModern‚Äù table packages",
    "text": "‚ÄúModern‚Äù table packages\nFor better or worse, the table package ecosystem in R has largely coalesced around three major families of packages. Each of these packages has pros and cons, and can generally make whatever table you need to make, and they are all very good packages.\nThe first of these sets of packages is the previously-mentioned knitr::kable() and kableExtra. These packages are relatively easy to use, but are not quite as flexible as the other two options. They will suffice for most tables if you like using them.\nThe second of these is the also-previously-mentioned flextable. The flextable package is notable for its consistency across many formats ‚Äì for several years, flextable was the only table package that could guarantee your table would look the same in HTML, PDF, and Word. While Flextable certainly has a learning curve, the detailed manual with several examples written by the author is fairly approachable. It‚Äôs a package worth checking out.\nThe final family is the gt family of packages. The gt family, which stands for ‚Äúgrammar of tables‚Äù is developed by Posit, and intended to be the table version of ggplot2. This family revolves around the gt package. The syntax for making tables in this package is designed to be similar to tidyverse-style syntax, and is therefore probably the easiest to learn as part of this course. There are also many, many examples and tutorials showing how to use gt to make gorgeous tables. In the old days, gt was not a viable option for us because it didn‚Äôt work with Word, but that has been remedied and now gt is the official table package supported by Posit, which means there are a lot of resources showing how to use gt. The gt package can do almost everything, but if you need it, the gtExtras package provides even more options. There is currently no ‚Äúgt extension gallery‚Äù like there is for ggplot2, but in time I think there will be.\nIn short, while all of these packages are great, I highly recommend that new users start practicing with the gt package, since this package will have the most learning resources and the most consistent development support for the foreseeable future.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Tables in R"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html",
    "href": "content/module-data-presentation/assessment-presenting-results.html",
    "title": "Assessment - Presenting Results",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#find-a-graph-to-reproduce",
    "href": "content/module-data-presentation/assessment-presenting-results.html#find-a-graph-to-reproduce",
    "title": "Assessment - Presenting Results",
    "section": "Find a graph to reproduce",
    "text": "Find a graph to reproduce\nFind an interesting graph from a news website. The main requirements are that the original graph must be interesting and good looking (a basic scatter plot is not enough), AND that the graph is freely available online, not behind a pay-wall. Ideally, the data that went into making the graph should be available too. If it is not, you will have to go through the extra step of extracting the data from the graph using tools like DataThief or WebPlotDigitizer.\nOne suggestion is to look at graphs from FiveThirtyEight. They are known to produce high-quality graphs, and for some of their stories and figures, they also provide the original data source. Good starting points for a chart from FiveThirtyEight might be their annual summaries of weirdest charts, which you can find here for 2015, 2016, 2018, 2019, 2020, 2021, 2022, 2023. (No, I don‚Äôt know why there‚Äôs no 2017.) Or you can go straight to their data, and see what graphs they produced with it and find a good one.\nIf you can‚Äôt find a graph on FiveThirtyEight that you like and would like to reproduce, you can also check out other major news outlets (USA Today, NY Times, Wall Street Journal, The Guardian, etc.).",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#setup",
    "href": "content/module-data-presentation/assessment-presenting-results.html#setup",
    "title": "Assessment - Presenting Results",
    "section": "Setup",
    "text": "Setup\nThis exercise will be part of your portfolio and will live in the presentation-exercise folder. Create that folder and place your data (see next) in that folder. Then create a file called presentation-exercise.qmd, which will contain your code and explanation for this exercise.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#get-the-data",
    "href": "content/module-data-presentation/assessment-presenting-results.html#get-the-data",
    "title": "Assessment - Presenting Results",
    "section": "Get the data",
    "text": "Get the data\nAs mentioned, you either need to have the data in a file, or you need to extract it from the graph and place it into a file. I recommend a CSV file. Place it into the presentation-exercise folder.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#ask-ai-to-re-create-the-original-graph",
    "href": "content/module-data-presentation/assessment-presenting-results.html#ask-ai-to-re-create-the-original-graph",
    "title": "Assessment - Presenting Results",
    "section": "Ask AI to re-create the original graph",
    "text": "Ask AI to re-create the original graph\nAsk an AI of your choice (or try several) to re-create the original graph. I‚Äôm not sure what the best prompt is. You probably want to feed it the data and instructions on what kind of plot to make. I‚Äôm not sure if giving it the original plot, either as uploaded file or as a URL, will help. You can try. You will likely have to go through multiple iterations of asking the AI to update the code/figure.\nYou will likely want to use ggplot2 & friends, but if you want to use base R, lattice or another plotting approach that‚Äôs ok too, as long as everything happens with R code.\n\nIf you already feel comfortable making high-quality graphs with ggplot2 and want to try something different, feel free to create an interactive graph using one of the many options for that (e.g., Shiny, plotly). You can also recreate a previous static graph and make it interactive (e.g., allowing people to turn on/off specific parts).\n\nDocument (some of) your AI prompts in the visualization-exercise.qmd Quarto file. Also show the code that the AI produced. Depending on how many iterations you do, you don‚Äôt need to show prompts and code for all intermediate steps. The AI will likely not get you all the way to where you want to get. Manually edit your code to try and get as close as possible to the original graph.\nYou should end up with a Quarto file that shows your AI prompts, possibly intermediate code that the AI returned, and finally a chunk of code that loads/takes the data and re-produces the original plot as closely as possible. Also show the original plot in youe Quarto file next/below your reproduction for comparison.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#create-a-publication-quality-table",
    "href": "content/module-data-presentation/assessment-presenting-results.html#create-a-publication-quality-table",
    "title": "Assessment - Presenting Results",
    "section": "Create a publication quality table",
    "text": "Create a publication quality table\nFor this part, the easiest approach is to use the same data you used for the graph (or a subset of the data) and create a nice looking table.\nAlternatively, find a good-looking table somewhere online and try to recreate it. Of course, you need all data necessary to re-create the table.\nYour table should be appropriately formatted to display some type of potentially interesting result. Your table should take up at most one standard letter-sized page if printed and should summarize information in a useful way (not just list the raw data, but for instance show summary statistics such as mean, percentage, etc.).\nYour table must contain an example of each of the following:\n\nFormatted text which can be bold, italic, etc.\nColored text or highlighted cells\nAdditional text outside of the regular table structure like spanning column headers or correctly attributed footnotes\nA descriptive caption explaining the useful information summary that your table represents.\n\nAdditionally, your table must contain at least one of the following advanced features:\n\nA column with some type of image for the values, which could potentially be a data visualization (e.g.¬†sparklines).\nAutomatically calculated summary statistics (summary statistics that you calculate manually do not count towards this objective)\nAutomatically applied (not manual) conditional formatting (potentially overlapping with formatted or colored text/cells)\nInteractive features\n\nAs always you are welcome and encouraged to use an AI tool to get started. It is up to you which package you use to make the table. No matter what R package you use to make the table, it should look professional, visually pleasing, and nicely convey the information. For inspiration, you can for instance check out this R table gallery. The R graph gallery also has a section on tables.\n\nAs for the graphs, if you want to challenge yourself, you can try to make your table interactive. There are several good R packages that allow you to create interactive tables inside a Quarto document. Interactivity only works if your output format is HTML, it won‚Äôt work for pdf or word. But in your case it‚Äôs part of your website, so it‚Äôs HTML and can include interactivity.\n\nAdd the code and explanatory text (e.g., your AI prompts) into the same presentation-exercise.qmd Quarto document. You should end up with code that takes some data and generates a nice-looking table.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-presentation/assessment-presenting-results.html#finish-up",
    "href": "content/module-data-presentation/assessment-presenting-results.html#finish-up",
    "title": "Assessment - Presenting Results",
    "section": "Finish up",
    "text": "Finish up\nOnce all done, edit the _quarto.yml file to include an entry for this exercise. Re-build your website and make sure that your data visualization exercise shows up nicely and is reachable through the main menu. Then, post the link to your online page showing your visualization into the discussion channel for this module. Do that by Wednesday so your classmates can take a look and comment by Friday.",
    "crumbs": [
      "Assessments",
      "6. Presenting Data and Results"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-overview.html",
    "href": "content/module-data-analysis/data-analysis-overview.html",
    "title": "Data Analysis Overview",
    "section": "",
    "text": "Overview\nIn this unit, we will talk about the whole data analysis workflow from a general, big-picture perspective. We‚Äôll then revisit the different components in more detail later.\n\n\nLearning Objectives\n\nUnderstand the elements of a data analysis.\nKnow the role and importance of each component of a data analysis.\n\n\n\nData Analysis Workflow\nThe following figure provides a conceptual illustration of the whole data analysis workflow.\n\n\n\nData analysis workflow\n\n\nThe spiral is meant to represent that while the different steps in an analysis are rarely linear, things should improve over time until you have the data and code in a form that everything fits and you can produce final results.\nSome things that might send you around in circles (or hopefully spirals) are:\n\nExploring data indicates that more cleaning needs to happen.\nExploring data indicates that the current question can‚Äôt be answered.\nAn analysis fails and indicates that cleaning/pre-processing wasn‚Äôt adequate.\nA change in analysis method requires different kinds of cleaning and pre-processing.\nEvaluating results indicate that something ‚Äòisn‚Äôt quite right‚Äô and needs to be fixed.\nNew analysis based on a better understanding of the data and how to best analyze the data and present results.\n\nIn the introductory chapter of their great book R for Data Science (R4DS), Garrett Grolemund and Hadley Wickham have a similar diagram. Check out the figure and read through the introductory chapter.\nNote that the process of getting the data ready for analysis goes by different names often used slightly differently by different people. In my figure above, I separated cleaning and pre-processing, while in the R4DS chapter and figure, they distinguish between tidying and transforming and together call them wrangling (some people also say munging). Sometimes, these terms mean a specific part of the cleaning process, sometimes they are used interchangeably. You need to guess from the context what is meant, but often you can think of these terms as all having similar meanings.\nLet‚Äôs look at the different components of an analysis in a bit more detail. The following sections are meant as a conceptual overview. Apart from the Question and Data Match discussion, we will get into each of the other steps and how to do them in more detail later in the course.\n\n\nQuestion & Data Match\nMaybe the most important challenge of a good scientific project (and thus an interesting and useful data analysis) is to find a combination of questions and data that lead to a project that is both interesting and doable. One could argue that having a good question and suitable data to answer it is part of science/research but not strictly part of the analysis. However, for your data analysis to be meaningful, it is critical to get the question-data match right.\nIf you start with a boring (or dumb) question or with data that is essentially garbage/noise, or you have a bad question/data match, no part of your analysis matters!\nThere are different approaches to finding a question-data match. The classical one is to have a specific question or hypothesis, then design a study to collect data, and analyze the collected data to address that specific question.\nIf you have a question, but you do not have the resources to collect data specifically to answer your question, you can look for existing data that might help you answer your question. You rarely find data that allows you to answer your initial question exactly. But you might find data that enables you to answer a very similar question. You might need to iterate a few times by modifying your question and changing the datasets you use before you have a good match, i.e., a good question and data that allows you to answer it.\nAnother approach that is becoming more common is to start with the data. You might come across one or multiple datasets that interest you. You can then investigate the data to see what questions you might be able to answer with it. Since the data was not collected to answer your specific question, you will have to be somewhat flexible with the question and let the data guide your analysis. This approach can be very powerful since more and more data are becoming available for analysis.\nAny of these different approaches for coming up with a good data-question match are valid. You should always keep in mind that unless you start with the question and analysis plan, then collect the data, then follow your plan exactly, the usual statistics (e.g., p-values and such) do not really apply.\nBy fiddling with the data or adjusting data and question, you are using what is sometimes called investigator‚Äôs degrees of freedom or less kindly, data dredging or data torturing. Essentially, if you fiddle with your data and question long enough, you will eventually find something statistically significant. You cannot interpret this in the same way as a statistically significant result obtained by pre-specifying the question, collecting data specifically to answer the question, and analyzing the data to answer the initial question.\nThe (former) news site 538 had a nice example, which is not available anymore, but fortunately, the prolific Andrew Heiss, recreated it. Visit this page to Hack Your Way To Scientific Glory. The website gives a nice interactive illustration on how you can adjust your question and data in very reasonable ways so you can get whatever statistically significant result you want. This is researchers‚Äô degrees of freedom in action, and the main reason why p-values are most often meaningless.\nThat doesn‚Äôt mean you can‚Äôt do and publish interesting work based on such a secondary data analysis approach! But it does mean that whatever you find should be considered preliminary: hypothesis generating, not hypothesis testing.\nAgain, unless you fully pre-specify your hypothesis and your exact analysis plan (which is almost never done outside of clinical trials that seek regulatory approval), statistical measures such as p-values are largely meaningless. Unfortunately, to get published, you often need to include those. It‚Äôs a sad state of affairs that a lot of published work comes with junk statistics. It will take a good bit of time (if it ever happens), before things become more reasonable (and we will get rid of p-values in most situations).\nFor some more thoughts on this topic, read chapters 1-3 of The Art of Data Science (ADS). It is a pay what you want book with a minimum price of zero, which means you can get it for free (note that the book + video bundle has a non-zero minimum price). Chapter 2 has a figure similar to the one above.\n\nWhile I am generally in favor of supporting creators of books (or other content) and thus paying for their products, in this case since I‚Äôm making you get/read the book, and also since students have very limited budgets, I think it is entirely acceptable for you to get the book for free.\n\n\n\nGetting and loading data\nIn some sense, the best, but also the most expensive and time-consuming way of getting data, is to collect it yourself. Doing so ensures that the data can answer the question you have. You can also control the quality of the data that is being collected ‚Äì at least to some extent. However, it is often not feasible to collect your own data. You might not have funding or time or interest to do so. The next best source of data is directly from someone who collected it. The advantage of such data is that you can ask someone for clarification. Also, the person who collected the data is likely a subject matter expert, who can help you generate better questions. Finally, you can get data that is generally available, e.g., in some online resource. This gives you access to a lot of different datasets. The drawback is that the data was not collected to answer your specific question, and there is usually nobody you can ask for clarification.\nNo matter the source, you need to get the data into your favorite data analysis system (in our case, that will be R). Sometimes, you get data in a format that can be read in easily, e.g., a comma-separated CSV file without any strange formatting that messes up the import. At other times, you might get data in a collection of terribly formatted Excel spreadsheets, or you get it by scraping data from some online source. We‚Äôll look at ways of getting data into R in a later module.\n\n\nExploring and cleaning data\nOnce you have the data loaded into your analysis software, such as R, you want to explore the data to get an overall idea of what is going on. At the same time, you‚Äôll likely have to engage in data cleaning.\nData exploration can and should be done both graphically and by looking at text output of various summaries describing your data, e.g., summary tables for all variables. We will get into that in more detail later. As you explore, you will likely find that your data is dirty, i.e., there are things wrong with it. The amount of data cleaning (also called tidying or wrangling) you need to do depends very much on your data.\nA general rule is that you should never directly edit your raw data. If you can, load the raw data into R and do all cleaning inside R with code, so everything is automatically reproducible and documented. Sometimes, you need to edit the files in the format you got before you are even able to load them into R. In those cases, you might have to make modifications in a software other than R. If you can‚Äôt directly read the data into R and need to make some changes before, make copies of your raw data and ONLY EDIT THOSE COPIES. Also, write down the edits you made.\nThere are certain cleaning actions you will need to do with almost any dataset. You will have to find and deal with data entry errors. If the mistake is obvious, you can fix it. If you know the value is wrong but don‚Äôt know what it should be, you might need to discard that data. Sometimes you get data that seems way outside the range of all other values, but it is not obviously wrong. For instance, if you had a group of individuals and their weights, a person with a weight of 8000 pounds is obviously not possible. Someone with an 800 pound weight seems wrong, but it‚Äôs not impossible. You will have to decide how to deal with such outliers, either during the cleaning or the analysis stage.\nIn almost any dataset, some values will be missing. First, you need to figure out how missing values are coded and what missing means. Sometimes an empty entry means it was not recorded. At other times, it could be that it is not applicable. For instance on a survey that asks if someone ever smokes, and the next question is for how long, if they say they never smoked, the ‚Äúhow long‚Äù question is moot and might be empty, so it is not really a missing value even though nothing is written down. For your analysis, you might want to code it as a duration of 0. We‚Äôll talk more about missing values later in the course.\nAs you do all the cleaning, you need to get your data into a shape that is ready for analysis. If you apply more than one analytic method, you might need different clean versions of your dataset.\n\n\nPause and ponder\nAt pretty much every stage of your analysis workflow, you should include short pauses where you sit back and briefly contemplate what it all means and what is going on. Careful thinking and reflection is the most important (and unfortunately most under-used) component of a data analysis.\nSomewhere during the exploring and cleaning process, you might want to take a longer break. You should have come to a stage where you will be fairly sure if your project is feasible or not, i.e., if you can use the data to answer your question. If not, alter or abandon. Ideally, you will want to alter. That means either changing your question to one that is similar (or sometimes quite different) to your original question, and that can be answered with the data. Or, if feasible, it is to change the data, i.e., get a different dataset or another dataset that contains information which, when combined with your first dataset, lets you answer the question.\nSometimes, you might find that changing the question or data is not possible or will lead to a project that is of no interest to you. In that case, you should abandon your analysis. This is hard, but the honest thing to do.\nDon‚Äôt keep going hoping against all hope that ‚Äúmagic might happen‚Äù and you‚Äôll get a good final product after all. If you keep going, you will get some results. Often, you might even be able to publish them. Unfortunately, they will be useless and wrong - which is a common occurrence in the academic literature - and probably even worse outside of the peer-reviewed academic literature.\n\n\nPre-process Data\nAdditional data processing could be considered part of the cleaning process, or you can think of it as a separate step. Depending on the analytic method you use, some further data processing might be suitable. For instance, some methods work better if you center and scale your predictors. Some methods, such as linear models and related approaches, are not able to handle missing values, so you need to decide how to deal with those. Sometimes it might be good to not include all variables in your model, or to use the existing variables to create new ones (e.g., instead of including height and weight in your model, you might decide to compute BMI and use that variable in your model instead.) The specific pre-processing steps that need to happen depend on the statistical modeling approach(es) you plan on using.\n\n\nAnalyzing Data\nYou finally reached the step where you can apply actual statistical analysis to your data. This topic takes up the majority of time in most (bio)statistics courses. While it is obviously very important to get the statistical analysis right, for any real data analysis you will likely spend &lt;25% of your time on this task. The majority of your time (though not computing time) goes into the other tasks.\nAt this stage in the process, you should know your data well enough to apply statistical methods and approaches that are suitable for your data and question. You can either use a single approach or explore multiple different ones. As you will learn in this course, trying more than one approach is quite easy. If you try multiple models, you need to be careful to not overfit by only picking and reporting the models and analyses that give you the results you want. If you analyze data in multiple ways, report all results.\n\n\nEvaluating and interpreting results\nOnce you have your results, and before you decide on how (or if) to report them, you should take another break and carefully contemplate what your findings mean. Do the results make sense? Occasionally, there is some new and interesting finding that goes against everything we think we know. However, most of the time if you find something that doesn‚Äôt ring true (e.g., increased exercise shortens your lifespan), it probably isn‚Äôt. You need to think carefully about what your result actually means and how to interpret results. We can certainly think of a way to get a result that suggests increased exercise shortens lifespan. For instance, we could look at a group of people who practice some kind of extreme sport that not infrequently leads to death, and compare that group to people who do not practice this sport. You might find that those with increased exercise live less long in that particular scenario. But of course, that doesn‚Äôt mean you have overall shown that normal exercise shortens lifespan. For a real epidemiology example, you can read about the birth weight paradox, where researchers found that smoking during pregnancy lead to lower infant mortality ‚Äì of course this was false and was later explained by sampling bias.\nUnfortunately, the scientific literature is full of examples where the interpretation of the results are likely not right. A good and amusing example was published in the New England Journal of Medicine, a very prestigious journal, by Messerli 2012. The showed that chocolate consumption and Nobel prizes were correlated. While there is no arguing with the data, the interpretation that ‚Äú‚Ä¶it seems most likely that in a dose-dependent way, chocolate intake provides the abundant fertile ground needed for the sprouting of Nobel laureates‚Ä¶‚Äù is of course nonsense, see also this article for a more general discussion. You can find more rebuttals online. Unfortunately, since the original study was in a prestigious journal, the nonsense was widely reported in the media. You can see Forbes, BusinessInsider, Time Magazine, and some other news outlets reporting the finding fairly uncritically. To this day, I do not know if this article was meant as a satire or for real.\n\n\nReporting and communicating your results\nUnless you do an analysis just for yourself, you will want to communicate your findings to others. In academia, this is usually done with peer-reviewed publications, as well as presentations at conferences in the form of talks (for which most people use slides) and posters. In those settings, the readers or listeners are usually your peers.\nOutside of academia, you might also need to present your work in written or oral form, but it might have a different target audience. For instance, you might want to communicate your findings to a lay audience, to your superiors, to groups of stakeholders, to colleagues and team members, etc.\nWhile you will need to tailor your products and way of communication to your situation and audience, there are components that are common. Figures, tables, and descriptive text are almost always used.\nYou want to make as much of your reporting automated as possible. For that, all figures, tables, and numeric results should be auto-generated by code. Similarly, for an efficient and automated workflow, you will want as many aspects of the products (reports, slides, etc.) as possible to be automated. Of course, you have to write the text yourself. But results should be loaded automatically, and text layout should be automated, as should formatting of references. You also want to use a system that is as flexible as possible. Quarto is ideal because you are writing in plain text and can easily send the text into different output formats, e.g., reports, slides, posters, websites, blogs, etc. This is much harder to do with other common software. E.g., going from Word to Excel to Powerpoint is much less seamless and certainly not automated.\nYou want automation for both efficiency and reproducibility purposes. Especially if you design an analysis that might be repeated often (maybe not so common in academia, but very common in other settings), you want to be able to automatically re-run and regenerate as much of the whole process as possible.\n\n\nFurther reading\nAbove, I gave you my thoughts related to the various steps of the data analysis workflow. It‚Äôs good to hear it from multiple sources. To that end, go back to The Art of Data Science (ADS) and read through chapters 4 and 9-11.",
    "crumbs": [
      "Content",
      "2. Data Analysis Introduction",
      "Data Analysis Overview"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html",
    "href": "content/module-data-analysis/assessment-data-analysis.html",
    "title": "Assessment - Data Analysis",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#group-setup",
    "href": "content/module-data-analysis/assessment-data-analysis.html#group-setup",
    "title": "Assessment - Data Analysis",
    "section": "Group setup",
    "text": "Group setup\nFind your fellow group members and organize yourself. You can find group assignments in the important-information channel. There is also a dedicated channel for each group. Get in touch with your group members. You will need to exchange GitHub user names. Assign each group member an (arbitrary) number (I‚Äôm calling them M1, M2, ‚Ä¶).\nYou will start working in your portfolio repository and finish this part by Wednesday. Then M2 will contribute to M1‚Äôs repository, M3 will work on M2s, etc. M1 will contribute to the last person in the group (M3/M4/M5, based on the number of people in our group). This way, everyone will work on their own and one other group member‚Äôs repository.\nBecause there are multiple parts to this exercise, the due dates are adjusted.",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#part-1",
    "href": "content/module-data-analysis/assessment-data-analysis.html#part-1",
    "title": "Assessment - Data Analysis",
    "section": "Part 1",
    "text": "Part 1\nPart 1 is due by Tuesday.\n\nGetting started\nOpen your portfolio website repository/project. Then open the - currently empty - file for this exercise, called starter-analyis-exercise.qmd.\n\nDocumenting well is very important! Add lots of comments to your code/file. I suggest that your code should be more than half comments. For every block of code, you have a few lines of comments at the beginning explaining what the code block does, and then each line of code gets its separate line of comment with more details. Comment on both the how and why of your code. This much commentary might seem overkill initially. But as your code gets more complex, it will be very useful. Both your collaborators, and your future self looking at the code you wrote several weeks ago will be incredibly thankful for your comments!\nIf you write R code, your comments will be lines that start with #. For Quarto or R Markdown files, you can either add comments as Markdown text above/below your code, and/or add comments inside your R code chunks. Both is ideal. Positron allows you to quickly turn sections of a document into comments or un-comment them. Just select the part you want to change, and hit Ctrl/Cmd + /. That can be useful for turning on/off code during testing, or hiding some parts of text that‚Äôs just meant for you but not for the reader.\n\n\n\nLoading and checking data\nWe‚Äôll look at and play with some data from the dslabs package. Write a code chunk using the library() function that loads the package (install the dslabs package first if you don‚Äôt have it yet).\n\nIt is good practice to load all packages at the beginning of your code. So if you are using some R package, instead of loading it with library just before you use it, place all your library commands at the beginning of your R script or Quarto file. Also, add a short comment explaining why you are loading a certain package. For a complex project, it might even make sense to list all packages you use in a readme file. You can also use something like the renv package which keeps track of all your packages and makes sure someone running your project at a later time gets exactly the same packages you use. While renv needs a bit of getting used to, and it‚Äôs not required for the course, I encourage you to check it out and if you want to, use it.\n\nAll the code you write for this (and any other) project should be written into an R or Quarto file, not in the R console. The reason for that is that you want a permanent record of what you did, and the ability to modify and re-run your analysis easily. For this exercise, the best option is to write the code directly into the Quarto document.\nWe‚Äôll look at the gapminder dataset from dslabs. Once you have installed and loaded the dslabs package, the dataset is available. I.e., different than datasets you get from external sources, those that come with R packages are available right after you load the package. If you want to learn more about the dataset, you can run help(gapminder) or ?gapminder in the R console - this is one instance where an R call should only be done interactively, not inside a file since you don‚Äôt want the help page to open when you run your code in an automated/scripted manner.\nWrite code to use the str() and summary() functions to take a look at the data. Use the class() function to check what type of object gapminder is. To illustrate how that should look, you should have something like these lines of code and R output so far for this exercise.\n\n# load dslabs package\nlibrary(\"dslabs\")\n\nWarning: package 'dslabs' was built under R version 4.5.2\n\n# only run the next command interactively, not in a script\n# help(gapminder)  \n# get an overview of data structure\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\n# get a summary of data\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\n# determine the type of object gapminder is\nclass(gapminder)\n\n[1] \"data.frame\"\n\n\n\n\nProcessing data\n\nYou can accomplish the next steps (and pretty much anything) with just basic R commands and not use additional packages. However, things are often easier with packages. For data processing tasks, the packages from the tidyverse are very useful. You can do the following tasks with any commands/packages you like.\n\nWrite code that assigns only the African countries to a new object/variable called africadata. Run str and summary on the new object you created. You should now have 2907 observations, down from 10545. Depending on how you do this, you might also notice that all the different categories are still kept in the continent (and other) variables, but show 0. R does not automatically remove categories of what in R is called a factor variable (a categorical variable) even if they are empty. We don‚Äôt have to worry about that just now, but something to keep in mind, it can sometimes lead to strange behavior.\nTake the africadata object and create two new objects (name them whatever you want), one that contains only infant_mortality and life_expectancy and one that contains only population and life_expectancy. You should have two new objects/variables with 2907 rows and two columns. Use the str, and summary commands to take a look at both. Make sure you add comments into your code to explain what each line of code is doing, and as needed, also add additional explanatory Markdown text to your Quarto file.\n\nI find it the least confusing to call things which store values in R objects (e.g., x is an object here: x &lt;- 2 + 2) and reserve the word variable for a data variable, i.e., usually a column. However, it is common in programming to also refer to an object as a variable. Because of that, I sometimes use that terminology (inadvertently) too. So if I talk about a variable, you need to determine from the context if I mean a certain variable in the data (e.g.¬†height or weight), or a variable in R (e.g.¬†x or result) that stores some content.\n\n\n\nPlotting\nUsing the new variables you created, plot life expectancy as a function of infant mortality and as a function of population size. Make two separate plots. Plot the data as points. For the plot with population size on the x-axis, set the x-axis to a log scale.\nYou should see a negative correlation between infant mortality and life expectancy, which makes sense. You should also see a positive correlation between population size and life expectancy. In both plots, especially the second one, you will see ‚Äòstreaks‚Äô of data that seem to go together. Can you figure out what is going on here? Take another look at the africadata data we generated, which should give you a hint of what‚Äôs happening. Add descriptive text into your Quarto file to explain what you see and why.\n\n\nMore data processing\nI‚Äôm sure you realized that the pattern we see in the data is due to the fact that we have different years for individual countries, and that over time these countries increase in population size and also life expectancy. Let‚Äôs pick only one year and see what patterns we find. We want a year for which we have the most data. You might have noticed that in africadata, there are 226 NA (i.e., missing values) for infant mortality. Write code that figures out which years have missing data for infant mortality. You should find that there is missing up to 1981 and then again for 2016. So we‚Äôll avoid those years and go with 2000 instead. Create a new object by extracting only the data for the year 2000 from the africadata object. You should end up with 51 observations and 9 variables. Check it with str and summary.\n\n\nMore plotting\nLet‚Äôs make the same plots as above again, this time only for the year 2000. Based on those plots, there seems to still be a negative correlation between infant mortality and life expectancy, and no noticeable correlation between population size and life expectancy. Let‚Äôs apply some statistical model to this data.\n\n\nSimple model fits\nUse the lm function and fit life expectancy as the outcome, and infant mortality as the predictor. Then repeat, now with the population size as the predictor variable. (Use the data from 2000 only.) Save the result from the two fits into two objects (e.g.¬†fit1 and fit2) and apply the summary command to both, which will print various fit results to the screen Add comments into your Quarto file to explain what you did and found.\n\n\nSending updates to GitHub\nOnce you are done with your exercise, render/re-build your portfolio website. Make sure no error messages show up. A preview should show up, check that the page for this exercise looks the way you want it to. Once you are happy with how everything looks, commit your changes and push to the remote repository on GitHub.com. Check your portfolio website online to make sure you can now see the newly created R exercise document (in addition to your previously created About page).\n\n\nHanding it off to your classmate\nBased on the group setup you did above, tell the classmate who will be working on your project that it‚Äôs ready and where to find it. Assuming you are M1, you would tell M2 that things are ready. At the same time, you should be notified by another classmate that their repository is ready for you (again, if you happen to be M1, it would be the last person in the group, say M4 or M5).",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#part-2",
    "href": "content/module-data-analysis/assessment-data-analysis.html#part-2",
    "title": "Assessment - Data Analysis",
    "section": "Part 2",
    "text": "Part 2\nPart 2 is due by Thursday.\n\nAdding to each other‚Äôs work\nOnce you‚Äôve done your first part, you‚Äôll contribute to another group member‚Äôs project.\nYou will work on your classmate‚Äôs repository using what is called the fork and pull-request workflow. The basic idea is as follows. First, you make a copy of someone‚Äôs GitHub repository. In GitHub terminology, that is called doing a fork of their repository. You can do that for any public repository.\nNext, you implement your improvements in the forked repository. Once you are done, you ask the owner of the original repository to incorporate the updates you made in the fork into their main repository. This last part is called issuing a pull request. You are requesting that the other person pull your changes into their repository, hence the at times confusing (at least for me) terminology. I prefer to think of them as merge requests or sync requests, i.e.¬†you are requesting that they merge or sync your changes into their repository. You‚Äôll find the terminology merge request is used at times. If the person who controls the main repository likes your changes, they will merge your fork into the main branch. And just like that, you have contributed to some project becoming better! We will practice this fork and pull flow now.\nFind the repository of the team member you will contribute to and fork their repository on GitHub.com. This places a copy of their repository into your online GitHub account. This fork is now your own repository, you have it forever, even if the person who owns the original repository deletes theirs.\nNext, clone your fork to your local machine, as you have done previously with your own repositories. Once you finished cloning to your local machine, open the repository in Positron. Once you have the repository open, find the file for this exercise. Open it, make sure it runs/renders. Then add your part at the bottom of the file.\nStart off by adding a comment that says something like This section contributed by YOURFULLNAME. This needs to be there for me to be able to grade your contribution.\n\n\nMore data exploration\nPick another dataset from dslabs, whichever one you want. Unfortunately, the dslabs package doesn‚Äôt have a nice website. But you can go to its official CRAN entry and click on Reference Manual. The pdf lists the various other datasets and for each provides a brief explanation.\nOnce you chose one of the datasets, write R code to go through similar steps as above. Specifically, do the following:\n\nExplore the dataset.\nDo any processing/cleaning you want to do.\nMake a few exploratory figures. Optionally, also some tables.\nRun some simple statistical model(s). Your choice.\nReport the results from the model(s).\nFor each step, add plenty comments to the code and explanatory Markdown text to the Quarto file.\n\n\n\nSending a pull request (PR)\nOnce you are done with your additions, make sure the whole website renders. Then commit and push your updates to your fork on GitHub. Note that this pushes to your fork (i.e.¬†copy) of the repository.\nNow it‚Äôs time to offer your contribution to your classmate to integrate into their repository.\nThis is called issuing a pull request. You are requesting that the other person pull your changes into their repository, hence the at times confusing (at least for us) terminology. Maybe thinking of it as merge requests or sync requests is better, i.e.¬†you are requesting that they merge or sync your changes into their repository. You‚Äôll find the terminology merge request is used at times. If the person who controls the main repository likes your changes, they will merge your fork into the main branch. And just like that, you have contributed to some project becoming better!\nTo open a pull request, go to your fork of the repo on GitHub. At the top, you should see something like This branch is N commits ahead of NNN:main. and next to it a Contribute button. Click on that buttion and choose Open pull request.\nYou will be taken to a page where you can provide a title and description of your changes. Be clear about what you changed and why.\nHopefully, you‚Äôll see a green check-mark that says able to merge. If your classmate made changes to the same files you did, it could have created a merge conflict. Hopefully, this won‚Äôt be the case. If it is, you might want to put the file that has been edited by both into a safe location outside of your repository, then pull the latest version of the original repository into your local copy, re-apply your changes, commit and push again.\nIn either case (merge conflict or not), you can click the green button and Create a pull request. This should send a notification to the owner of the repository that a pull request was created.",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#part-3",
    "href": "content/module-data-analysis/assessment-data-analysis.html#part-3",
    "title": "Assessment - Data Analysis",
    "section": "Part 3",
    "text": "Part 3\nPart 3 is due by Friday.",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-data-analysis/assessment-data-analysis.html#merging-the-pr",
    "href": "content/module-data-analysis/assessment-data-analysis.html#merging-the-pr",
    "title": "Assessment - Data Analysis",
    "section": "Merging the PR",
    "text": "Merging the PR\nOnce you receive a pull request notification from your teammate, go to the GitHub site for your repository. Click on Pull requests, then click on the request. Take a look. On the first page, it shows you their message and if there are conflicts with your version of the repository. Hopefully, you didn‚Äôt change things around while they did, so there shouldn‚Äôt be any conflicts. Click on the Files changed button, which will show you an overview of the code they changed. Removals are red, and additions are green.\nOn the main pull request site, you can do various things. If you don‚Äôt like the suggested edits, you can write a comment and close the pull request without merging their changes into your repository. If you like most of what they did, but there is something they need to adjust, write a comment and let them know. Close the pull request and ask them to send a new one. If you are ok with their changes (hopefully, this is the case here), you can merge the pull request and close it. Their updates are now part of your repository.\nOnce you finished the merging of their updates into your repository online, make sure to pull the latest version to your local computer. Make sure everything looks fine and everything runs.\nMake any further needed changes/updates. Re-render everything. Once everything is good, push your updated portfolio repository to GitHub. Check the website to make sure everything looks good. I‚Äôll be looking at the content shown on the website for assessment/grading purposes.\n\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it.",
    "crumbs": [
      "Assessments",
      "2. Data Analysis Introduction"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html",
    "title": "Subset Selection",
    "section": "",
    "text": "In this unit, we will cover the idea of subset selection.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#forward-selection",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#forward-selection",
    "title": "Subset Selection",
    "section": "Forward selection",
    "text": "Forward selection\nIn forward selection, you start with the null model (no predictors), then evaluate all single-predictor models. The best-performing 1-predictor model is chosen, and you build all 2-predictor models on top of that chosen 1-predictor model. You choose the best 2-predictor model, then go on to add a third, and do that until adding any further predictors does not lead to a model that performs better (again, measured by CV on the test set) than the smaller model. E.g., you might find that adding any of the remaining 3rd predictors does not lead to a better-performing model than the 2-predictor model you already have. Then you stop here and pick the 2-predictor model as your best.\nLet‚Äôs consider an example. Say you want to predict BMI based on age, sex, calorie intake, and exercise level. Since BMI is continuous, we can consider a linear regression model, and we might try to minimize RMSE between model-predicted BMI and actual measurements.\nWe start by computing RMSE for the null model. Then we compute RMSE for all 1-predictor models. Assume that all 1-predictor models have RMSE lower than the null model, and the lowest RMSE (all computed using cross-validation) of the 1-predictor models is the one that includes calorie intake. Next, you try all 2-predictor models that include calorie intake. Among those, a model with calorie intake + exercise level has the lowest RMSE, and it‚Äôs also lower than the model with calorie intake only. Next, you try all 3-predictor models that include the 2 chosen predictors. In this example, only 2 of those 3-predictor models are possible, namely calorie intake + exercise level + age and calorie intake + exercise level + sex. Let‚Äôs assume both of those models have RMSE that is larger than the 2-predictor model. Note, it is important to compute RMSE through cross-validation since the RMSE evaluated on the data used for fitting will always be lower for the bigger model. We thus found our best model, namely one that includes the 2 predictors calorie intake + exercise level.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#backward-selection",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#backward-selection",
    "title": "Subset Selection",
    "section": "Backward selection",
    "text": "Backward selection\nBackward selection is essentially the reverse to forward selection: You start with the model with all predictors, then evaluate all models with one predictor dropped. The smaller model with the best performance is your choice. You then drop each predictor at a time from that model. You continue until dropping predictors does not improve performance anymore. Then you keep the model with the best performance. Note that this method does not work on datasets that have more predictors than observations (e.g., most -omics type data).\nLet‚Äôs consider the above example again. We now start with a model that includes all 4 predictors and compute cross-validated RMSE. Next, we drop each predictor and try all 3-predictor models. Let‚Äôs say the one that dropped exercise level is the 3-predictor model with the lowest RMSE, lower than all other 3-predictor models, and lower than the full model. Next, we drop each variable from the current best candidate 3-predictor model, which is age + sex + calorie intake. All of the 2-predictor models have RMSE larger than the 3-predictor model. We thus found our final model, with the 3 predictors age + sex + calorie intake.\nNote that it is also possible to do ‚Äúforward-backward‚Äù selection: you can choose to start with the full or reduced model, but the method is allowed to add/remove predictors at every step depending on what improves performance most.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#exhaustive-subset-selection",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#exhaustive-subset-selection",
    "title": "Subset Selection",
    "section": "Exhaustive subset selection",
    "text": "Exhaustive subset selection\nYou might have noticed that in my example, forward selection and backward selection do not necessarily produce the same model. The reason for that is that neither forward- or backward-selection try all possible models (predictor combinations). Both approaches use what is called a greedy strategy. Once they included or excluded a predictor, it can never be dropped or included. Once you picked your 1-predictor model, all subsequent models will consist of that 1 predictor model - even if there might be a better performing 2-predictor model that doesn‚Äôt include the 1st predictor you picked. The same holds for dropping predictors in backward selection.\nAlso, as in my example, often (but not always), the final model determined through backward selection tends to include more predictors than a model determined through forward selection. In such a case, you have to decide which model to choose. Alternatively, you can try a different way of choosing the model. One approach is to try models with all combinations of predictors. This is called exhaustive subset selection, all subsets selection, or best subset selection (although note that ‚Äúbest subset selection‚Äù can also refer to a different method which still hasn‚Äôt gained much traction). If you have only a few predictors, you can try every combination of predictors. However, once you have more than 5-10 predictors (depending on the number of observations), trying every combination of sub-models would be too time-consuming. Therefore, most of the time, such an exhaustive feature selection is not feasible.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#other-subset-selection-methods",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#other-subset-selection-methods",
    "title": "Subset Selection",
    "section": "Other subset selection methods",
    "text": "Other subset selection methods\nThere have recently been a number of approaches that try to be somewhat smarter in finding the best model, without having to try every combination. The caret package has several methods, such as Genetic Algorithms and Simulated Annealing. Those methods often do a good job finding a very good sub-model in a reasonable amount of time. While the only guarantee that you found the single best model is to try them all using exhaustive subset selection, these modern approaches usually find a model that is either the best or at least close to the best while doing so rather efficiently. See sections 18-22 of the caret manual for more information if you are interested. The mlr/mlr3 packages also have similar such selection methods. tidymodels currently does not have any subset selection procedure implemented (see more below).",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#manual-subset-selection",
    "href": "content/model-improvement/model-improvement-subset/model-improvement-subset.html#manual-subset-selection",
    "title": "Subset Selection",
    "section": "Manual subset selection",
    "text": "Manual subset selection\nAll the approaches above use cross-validated performance of some metric to determine which predictors should be included in the model. Another, still commonly used strategy is to do subset selection manually. The analyst usually starts with a simple model containing just one predictor (the most important one), and then decides based on model results (e.g., systematic deviations in the residuals) and/or based on scientific knowledge, if adding additional predictors is justified. For a skilled analyst with a lot of subject matter expertise, this can lead to models that are all-around very good. They have good performance, make scientific sense, and are still simple. Unfortunately, this requires a lot of knowledge, time and careful analysis. Also, the analyst needs to honestly report any model they tried, not just the one they end up with. Otherwise they are using and not reporting researcher‚Äôs degrees of freedom, which leads to overfitting. Most analyses are not done that way.\n\nI mentioned this before in various places and it‚Äôs a theme throughout the course, that unless data is collected and analyzed using pre-specified protocols (e.g., a clinical trial for FDA approval) it is almost always the case that scientists have so much flexibility during the data collection and analysis stages that p-values and similar statistical measures are meaningless. The Researcher‚Äôs degrees of freedom issue is everywhere and essentially impossible to get rid of. For more on this topic, see the Pitfalls section on the General Resources page.\nAll of this applies to manual subset selection (which is why I‚Äôm thinking about it here and adding this note), but it is more general and applies to every step in a scientific project.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Subset Selection"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-overview.html",
    "href": "content/model-improvement/model-improvement-overview.html",
    "title": "Model improvement",
    "section": "",
    "text": "Overview\nAlmost always, your first model is not that great. You generally want to improve your models to find a good one.\nThis module discusses ways to improve your models.\n\n\nLearning Objectives\n\nBe familiar with methods to improve models.\nBe able to perform different model improvement tasks.\n\n\n\nContent\nThe module consists of the following units:\n\nModel improvement introduction\nSubset selection\nRegularization\nTuning",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Model Improvement Overview"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html",
    "href": "content/model-improvement/assessment-model-improvement.html",
    "title": "Assessment - Improving Models",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#setup",
    "href": "content/model-improvement/assessment-model-improvement.html#setup",
    "title": "Assessment - Improving Models",
    "section": "Setup",
    "text": "Setup\nYou‚Äôll continue working on the exercise you started previously. We‚Äôll also do some group work again, using the ‚Äì by now familiar ‚Äì M1, M2, ‚Ä¶ setup. Assign each other a number. As much as possible, do it such that you end up working with group members you have not (or not in a while) worked with. Use the ‚Äòcircular‚Äô setup such that everyone will work on their own repository and on one other person‚Äôs repository.\nThis exercise has 3 parts. Part 1 is due Wednesday morning, part 2 Friday morning, and part 3 Friday evening. Coordinate with your team member accordingly.\n\nThis exercise draws a good bit on the Getting Started tidymodels tutorial. If you haven‚Äôt yet, now would be a great time to at least skim through the tutorial and make sure you understand the overall steps.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#part-1",
    "href": "content/model-improvement/assessment-model-improvement.html#part-1",
    "title": "Assessment - Improving Models",
    "section": "Part 1",
    "text": "Part 1\nWe‚Äôll continue this exercise using the previous Quarto fitting-exercise.qmd file.\nAt this stage, I assume all the data wrangling and EDA code, as well as the model fitting code you worked on previously is present and fully functional. If there are still some issues that need to be resolved, go ahead and do so.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#setting-a-random-seed",
    "href": "content/model-improvement/assessment-model-improvement.html#setting-a-random-seed",
    "title": "Assessment - Improving Models",
    "section": "Setting a random seed",
    "text": "Setting a random seed\nThis exercise involves sampling. Sampling means that under the hood, the computer draws some random numbers and does certain actions based on the values of those numbers (for instance it might draw a random number between 0 and 1 and if it‚Äôs &gt;0.5 it does one thing, otherwise another thing).\nReally, for a computer, these random numbers are pseudo-random, and are a long sequence of random looking numbers, but if one starts at the same point in that sequence, one gets the same collection of random numbers. This is important for reproducibility. To ensure we get the same random numbers each time, we need to set a seed.\nThere are two options. One option is to set a single seed at the top of your script. This way, if you re-run the script with the same seed, everything should stay the same, if you change the seed, everything changes. The disadvantage of setting a single seed at the beginning is that if you add some other bits to your code that perform more operations that require random numbers, then you advance in the sequence of random numbers and everything downstream that depends on random numbers changes.\nThe other option is to set a seed before each operation you know involves random numbers. For that you do need to know which parts involve randomness and which don‚Äôt. It also means if you want to try a different seed - always good to ensure results are robust and don‚Äôt depend on random variation - then you have to change the seed multiple times in your code.\nYet another option, and the one I personally like best, is to specify a value for a seed at the start, and then re-set the seed to this value before every operation that involves random numbers. The drawback might be that some samples look too similar, but since almost always they are used internally in different ways, one should get enough variability.\nFor this exercise, we‚Äôll try the last method. At this point in our code, add a statement that sets a seed with rngseed = 1234. We‚Äôll use it shortly.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#data-prep",
    "href": "content/model-improvement/assessment-model-improvement.html#data-prep",
    "title": "Assessment - Improving Models",
    "section": "Data prep",
    "text": "Data prep\nWe are doing one more change to the data. Since the RACE variable has a few values that are coded weirdly (7 or 88, which probably means some sort of missing), we‚Äôll completely remove that variable for this exercise.\nSo for this exercise, we‚Äôll have a data frame with 120 observations and 6 variables, namely Y, DOSE, AGE, SEX, WT, HT.\nSince we are about to do some random sampling of our data, now is the time to set a seed with set.seed(rngseed). After you‚Äôve done that, write code that takes the data and splits it randomly into a 75% train and 25% test set, following for instance the example in the Data Splitting section of the Get Started tidymodels tutorial.\n\nWe only have 120 observations here. This is not much, so for a real project, it might not make a lot of sense to do a train/test split. Instead, one would probably use all data during the model fitting process, and then use cross-validation to try and estimate model performance on unseen data. But since this is an exercise, we‚Äôll do the train/test split.\n\n\nAlso note that we are doing a random train/test split. But for a case like we have here, one might want to split differently. For instance we could split by using everyone who got the lower doses as training data, and then testing if we can predict the higher-dose outcomes. This is a common question of interest in drug development.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-fitting",
    "href": "content/model-improvement/assessment-model-improvement.html#model-fitting",
    "title": "Assessment - Improving Models",
    "section": "Model fitting",
    "text": "Model fitting\nUse the tidymodels framework to fit two linear models to our continuous outcome of interest, (here, Y). The first model should only use DOSE as predictor, the second model should use all predictors. For both models, the metric to optimize should be RMSE.\nThis is the same as you did previously, but for this exercise, you should only use the training data set for fitting.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-performance-assessment-1",
    "href": "content/model-improvement/assessment-model-improvement.html#model-performance-assessment-1",
    "title": "Assessment - Improving Models",
    "section": "Model performance assessment 1",
    "text": "Model performance assessment 1\nCompute predictions for the two models on the training data (we‚Äôll use the test data later). Then use observed and predicted values to compute RMSE of the best-fitting model.\nAlso compute the RMSE of a null-model, one that would just predict the mean outcome for each observation, without using any predictor information. You can use the null_model() function from tidymodels for this (using the parsnip engine), or just do it ‚Äúby hand‚Äù.\nCompare the 3 RMSE values and discuss which model is performing the best (according to those metrics). You should have gotten values of 948, 702 and 627 for the null model and models 1 and 2 respectively. If your values are different, it‚Äôs likely that you didn‚Äôt set the random seed.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-performance-assessment-2",
    "href": "content/model-improvement/assessment-model-improvement.html#model-performance-assessment-2",
    "title": "Assessment - Improving Models",
    "section": "Model performance assessment 2",
    "text": "Model performance assessment 2\nSo far, we have assessed performance on the training set. But as you know by now, that is often not really of interest, we want to know how the model performs in general. We could use our test data to check, but we want to reserve that until the very end, to get a final ‚Äúfair‚Äù assessment of our best model.\nWe could use an information criterion, such as AIC, to try to estimate how well the models might perform on unseen data. Even better, we can use use cross-validation to compute performance on unseen data.\nSince CV includes sampling, and therefore random numbers, it‚Äôs time again to (re)-set the random number seed. We‚Äôll use the same value as before.\nFollow the Evaluate your model with resampling section of the tutorial and do a 10-fold cross-validation. Doing so, you fit the 2 models to the data 10 times. Each time, you use 90% to fit the model, and 10% to evaluate (compute the RMSE) of the model. If you take the average for the RMSE over the 10 folds, you get an estimate for the model performance on data not used for fitting. It‚Äôs the same idea as doing the initial train/test split, but now we use it as part of the model building and choosing part, while we keep the test data as a final evaluation of the model and won‚Äôt use it before then.\nCompute the RMSE for both models again. Of course nothing changes for the null model. Compare the new RMSE estimates obtained through CV with those obtained earlier. What did and didn‚Äôt change?\nAlso look at the standard error for the RMSE. Since you are now sampling, you not only get a single estimate for RMSE, but one for each sample, so you can look at the variation in RMSE. This gives you a good indication of how robust your model performance is.\nFinally, run the code again that creates the CV folds and does the fitting. This time, choose a different value for the random seed. The RMSE values for the CV fits will change. That‚Äôs just due to the randomness in the data splitting. If we had more data, we would expect to get less variability. The overall pattern between changes in the RMSE values for the fits to the training data without CV, and what we see with CV, should still be the same.\n\nIf you want to get more robust RMSE estimates with CV, you can try to set repeats to some value. That creates more samples by repeating the whole CV procedure several times. In theory this might give more robust results. You might encounter some warning messages. This is likely related that occasionally, by chance, data is split in a way that some information (e.g., a certain value for SEX in our data) is missing from one one of the folds. That can cause issues.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#wrap-up-part-1",
    "href": "content/model-improvement/assessment-model-improvement.html#wrap-up-part-1",
    "title": "Assessment - Improving Models",
    "section": "Wrap up part 1",
    "text": "Wrap up part 1\nMake sure everything runs and works as expected. Also make sure everything is well commented/documented/explained! Then commit, push and tell your classmate that they can take over. Finish this by Wednesday.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#part-2",
    "href": "content/model-improvement/assessment-model-improvement.html#part-2",
    "title": "Assessment - Improving Models",
    "section": "Part 2",
    "text": "Part 2\nFork and clone (or if you are added as collaborator, clone directly) your classmate‚Äôs repository. Open their fitting-exercise.qmd file.\nAdd a heading that says # This section added by YOURFULLNAME. I need this so I can grade accordingly.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-predictions",
    "href": "content/model-improvement/assessment-model-improvement.html#model-predictions",
    "title": "Assessment - Improving Models",
    "section": "Model predictions",
    "text": "Model predictions\nNothing beats a visual inspection of results. A very useful figure is one that plots predicted versus observed values. This can quickly show you if there are important deviations that suggest the model isn‚Äôt very good.\nPut the observed values and the predicted values from your 3 original model fits to all of the training data (the ones without the CV) into a data frame. Also add a label to indicate the model. Then use ggplot to create a figure that plots (as symbols) observed values on the x-axis and predictions (from each of the 3 models, including the null model) on the y-axis. Use a different color and/or a different symbol to differentiate between the 3 model predictions. Alternatively, you can use facets. Let both x and y axes go from 0 to 5000 and add a 45 degree line. For a good model, the points will fall along that line, namely observed and predicted values agree - with some scatter.\nYou should see that the ‚Äúpredictions‚Äù from the null model are a straight horizontal line. We expect that, since we predict exactly the same value (the mean) for each observation. For the model that only includes dose, you should the data fall along 3 horizontal lines. Can you understand why? If not clear, look at the DOSE variable and the values it takes. That should tell you why you only get 3 different predicted values for the outcome.\nThe model with all predictors looks the best, though there‚Äôs still a lot of scatter around the diagonal line. If that scatter is fairly random, it could mean that we captured all the ‚Äúsignal‚Äù with our model and the rest is noise. If there seems to be some pattern to the scatter ‚Äì as it seems to be here, with model predictions lower than observed values for high values ‚Äì it can indicate that there‚Äôs still a lot of the outcome pattern that our model can‚Äôt explain.\nA good way to see if there are patterns is to plot predicted versus residuals (the latter being just residuals = predicted-observed). Make a plot that plots that for model 2. Also add a straight line at 0. Make sure your y-axis goes the same amount into the positive and negative direction.\nFor a good model, there should be a cloud of data without any discernible pattern. Here‚Äô we again see that there‚Äôs some residual pattern, in genera there are more and higher negative values compared to positive ones. That could either be because we are missing important information, i.e.¬†we need more variables. Or it could be that the model is too simple, for instance it could be that the outcome depends on some variable in a nonlinear way.\nWe can‚Äôt do anything about the former problem (other than collecting more data), but we can try to explore if more complex models can capture the data better. We‚Äôll do that in an upcoming exercise.\nFor this exercise, we‚Äôll do one more exploration of our existing models.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#model-predictions-and-uncertainty",
    "href": "content/model-improvement/assessment-model-improvement.html#model-predictions-and-uncertainty",
    "title": "Assessment - Improving Models",
    "section": "Model predictions and uncertainty",
    "text": "Model predictions and uncertainty\nAs you learned, model predictions without a measure of uncertainty are usually not that useful. We generally always want to determine uncertainty in our estimates/predictions.\nWhile for a linear model it is possible to get uncertainty directly from the uncertainty of the parameter estimates, for more complex models, this is not possible. However, it is always possible to use the bootstrap method to sample the data, fit models to the data, and get uncertainty that way.\nWe‚Äôll give that a try. Since our plot above suggested that model 1 is not good, we‚Äôll focus here on model 2.\nFirst, we need to (re)-set the random seed again, we‚Äôll use the same value as above. Then, use the bootstraps function from the rsample package to create 100 bootstraps. These bootstraps should use the training data.\nNext, write a loop (or use a map or apply function) to fit the model to each of the bootstrap samples and make predictions from this model for the original training data. Record all predictions (e.g., in an array or a list).\nIf you are wondering how to get the individual bootstrap samples out of the object that bootstraps gave you back: Assuming you called the object that you got back dat_bs, you can get a single bootstrap sample with this code:\ndat_sample = rsample::analysis(dat_bs$splits[[i]])\nOnce you have all your predictions stored, compute the mean and confidence intervals. Assuming your predictions are stored in an array called pred_bs that has as many rows as samples and columns as data points, this bit of code computes median and 89% confidence intervals.\npreds &lt;- pred_bs |&gt; apply(2, quantile,  c(0.055, 0.5, 0.945)) |&gt;  t()\nFinally, make a figure that plots observed values on the x-axis, and point estimate (obtained from your original predictions on the training data), as well as median and the upper and lower bounds - obtained by the bootstrap sampling and stored in pred on the y-axis. You can for instance use black symbols for original predictions (the point estimate, which is the mean), and some colors to indicate median and lower and upper confidence limits. As above, make sure x- and y-axis are on the same and add a 45 degree line.\nInterpret what you see in this plot.\n\nThe bootstrap uses sampling, and CV uses sampling. The difference is that for bootstrap, we sample from all the (training) data with replacement. For CV, we randomly partitioned some of the data into the part used to fit, the other part was used to evaluate/assess. So the two approaches are conceptually similar in that they use a sampling-based method, but not the same.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#wrap-up-part-2",
    "href": "content/model-improvement/assessment-model-improvement.html#wrap-up-part-2",
    "title": "Assessment - Improving Models",
    "section": "Wrap up part 2",
    "text": "Wrap up part 2\nMake sure everything runs and works as expected. Then commit, push and if you forked the repo, initiate a pull request. Tell our classmate that its done.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#part-3",
    "href": "content/model-improvement/assessment-model-improvement.html#part-3",
    "title": "Assessment - Improving Models",
    "section": "Part 3",
    "text": "Part 3\nNow it‚Äôs back to you, the portfolio repository owner. Based on the work you and your classmate did, perform a final evaluation task and then summarize/discuss the models.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#final-evaluation-using-test-data",
    "href": "content/model-improvement/assessment-model-improvement.html#final-evaluation-using-test-data",
    "title": "Assessment - Improving Models",
    "section": "Final evaluation using test data",
    "text": "Final evaluation using test data\nLet‚Äôs do a final model evaluation, this time using the test data.\nUse the fit of model 2 using the training data you computed previously, and now use this fitted model to make predictions for the test data. This gives us an indication how our model generalizes to data that wasn‚Äôt used to construct/fit/train the model.\nThen make a plot that shows predicted versus observed for both the training data (which you did above) and in the same plot, also show predicted versus observed for the test data (using, e.g.¬†different symbols or colors).\nYou should see that the observed/predicted values for the test data are mixed in with the train data. That‚Äôs a good sign. If the test data points were systematically ‚Äúoff‚Äù, it would indicate a problem such as overfitting to the training data.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#overall-model-assessment",
    "href": "content/model-improvement/assessment-model-improvement.html#overall-model-assessment",
    "title": "Assessment - Improving Models",
    "section": "Overall model assessment",
    "text": "Overall model assessment\nLet‚Äôs critique all our models. Here are some points for discussion:\n\nWe want to make sure that any model we have performs better than the null model. Is that the case?\nDoes model 1 with only dose in the model improve results over the null model? Do the results make sense? Would you consider this model ‚Äúusable‚Äù for any real purpose?\nDoes model 2 with all predictors further improve results? Do the results make sense? Would you consider this model ‚Äúusable‚Äù for any real purpose?\n\nUse all the assessment bits you computed above to reason about the models. And I‚Äôm not really looking for ‚Äúthe right‚Äù answer here (if that even exists), I just want to see that you are thinking about what your fit results and thus underlying models mean, and that you can assess their strengths and weaknesses and interpret what you are doing and seeing.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#test-and-website-update",
    "href": "content/model-improvement/assessment-model-improvement.html#test-and-website-update",
    "title": "Assessment - Improving Models",
    "section": "Test and website update",
    "text": "Test and website update\nMake sure everything works. Make any further updates that you think need to be made. Then rebuild your portfolio website and make sure everything works and shows up as expected. Then commit and push.\nSince this is part of your portfolio site, you don‚Äôt need to post anything, I know where to find it.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-improvement/assessment-model-improvement.html#more-explorations",
    "href": "content/model-improvement/assessment-model-improvement.html#more-explorations",
    "title": "Assessment - Improving Models",
    "section": "More explorations",
    "text": "More explorations\nIf you want additional practice, you can re-do this exercise, but this time consider the categorical/binary variable SEX as your outcome and the others as the predictors. That does of course not make a lot of scientific sense, but as an exercise you might want to explore to get some practice with categorical outcomes.\nYour metric will change from RMSE to one of your choice that is suitable for categorical/classification problems. That also means you can‚Äôt do a scatterplot for observed versus predicted, but you can look at e.g.¬†a confusion matrix. Overall, it‚Äôs the same conceptual workflow. So if you want to try a categorical outcome, go ahead. This is completely optional.",
    "crumbs": [
      "Assessments",
      "10. Model Improvement"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-overview.html",
    "href": "content/model-fitting/model-fitting-overview.html",
    "title": "Model fitting",
    "section": "",
    "text": "Overview\nAt the heart of most data analyses is the step at which the data is brought into close contact with models. This is at times called the statistical analysis part, though often that term is also applied to the whole data analysis process. A more descriptive term is model fitting. The model is being fit to the data - what exactly that means and entails is the topic of this module.\n\n\nLearning Objectives\n\nBe familiar with steps involved in model fitting.\n\n\n\nContent\nThe module consists of the following units:\n\nModel fitting introduction\nFitting basic models\nModel fit evaluation\nModel fitting in R",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model Fitting Overview"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html",
    "title": "Model performance metrics",
    "section": "",
    "text": "This unit discusses the idea of assessing a model based on its performance through some metric.\n\nI use the term model performance here in a narrow sense, quantified through some metric.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#least-squares",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#least-squares",
    "title": "Model performance metrics",
    "section": "Least squares",
    "text": "Least squares\nYou are likely familiar with one of the most widely used functions for f, the least-squares method. If you have ever fit a linear model (e.g.¬†using lm() in R or an equivalent function in a different statistical software), chances are you used least squares as your objective function (maybe without knowing that you did). For least squares, we compute the squared difference between model prediction and data for each observation and sum it all up. In equation form:\n\\[C = f(Y_m,Y_d) = \\sum_i (Y_m^i - Y_d^i)^2\\] Again, \\(Y^i_m\\) are all the predictions made by the model for the outcome, and the \\(Y^i_d\\) are the actual outcomes from the data. The quantity \\(C\\) for this equation has many names. A common one is least squares error, or sum of square residuals (SSR), or residual sum of squares (RSS), or sum of squares (SS), or residual square error (RSE), and a bunch of similar names. You will usually be able to tell from the context what is being used as the performance metric.\nYou will often see a variation where one divides by the sample size, i.e.¬†\\(C\\) will look like\n\\[C = \\frac{1}{N} \\sum_i (Y_m^i - Y_d^i)^2\\]\nThis is called mean squared error (MSE). Of course, other names exist.\nDividing by the sample size has the advantage of allowing you to compare values across samples of different size from the same dataset (but it doesn‚Äôt really work for comparing across different datasets). For instance if you compare model performance on training and test data (to be discussed shortly), and each has different sample size, you need to make sure you standardize by it.\nIf you want to compare different models on the same dataset which might include some missing values, and one of your models can deal with missing data while the other cannot, you need to be careful. One option is to fit both models only to the data without missing values. If you decide to allow one model to use the observations that have some missing values, while the other model does not, you definitely need to standardize by the sample size. Even then, care is needed, since the samples with some missing data might be systematically different from those without and thus the datasets might not be equivalent anymore.\nAnother variant is a version where at the end you take the square-root, i.e.\n\\[C = \\sqrt{\\frac{1}{N} \\sum_i (Y_m^i - Y_d^i)^2}\\]\nwhich is called the root mean squared error (RMSE). The advantage of taking the square-root at the end is that now the units of \\(C\\) are the same as those of your outcomes. THis often makes interpretation of the results easier. In general, it is best to use MSE or RMSE. In tidymodels, the yardstick package has the rmse() metric built-in.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#coefficient-of-determination",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#coefficient-of-determination",
    "title": "Model performance metrics",
    "section": "Coefficient of determination",
    "text": "Coefficient of determination\nAn equivalent alternative to SSR is to use a quantity called Coefficient of determination, or more commonly \\(R^2\\) (R-squared). This quantity is defined as \\[R^2 = 1-RSS/TSS\\] where RSS is the residual sum of square introduced above and TSS is the total sum of squares.3\nThe latter is defined as\n\\[TSS =  \\sum_i (Y_{av} - Y_d^i)^2\\]\nwhere \\(Y_{av}\\) is the mean of the data. Therefore, the equation for \\(R^2\\) is\n\\[R^2 = 1-  \\frac{\\sum_i (Y_m^i - Y_d^i)^2}{\\sum_i (Y_{av} - Y_d^i)^2}\\]\nSince TSS is fixed for a given dataset, minimizing SSR is equivalent to maximizing \\(R^2\\). You might see \\(R^2\\) reported in papers, and it is highly likely that the fitting was performed by minimizing SSR (or MSE or RMSE). The SST is a useful quantity since it tells us the performance of a dumb/null model which uses no information from any predictor variables, but instead just predicts the mean of the outcomes. Any model you build that includes predictors needs to do better than this dumb null model.\nLeast squares fitting is simple and frequently used. A lot of standard routines in major statistical packages use least squares. It often makes good sense to penalize predictions that deviate from the actual value with the squared distance (and under certain conditions, this is equivalent to maximizing the likelihood). However, sometimes a different way to define the function f might be useful. We‚Äôll discuss a few of them briefly.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#beyond-least-squares",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#beyond-least-squares",
    "title": "Model performance metrics",
    "section": "Beyond least squares",
    "text": "Beyond least squares\nAn alternative to least squares is to penalize not with the distance squared, but linearly with the (absolute) distance. This metric is called (mean) absolute error (MAE) or (least) absolute deviation, and the model is\n\\[C = f(Y_m,Y_d) = \\sum_i |Y_m^i - Y_d^i|\\]\nThis approach can be useful if the data contains outliers (that are real, and one can‚Äôt justify removing them during cleaning). With a squared distance penalty, outliers have a strong impact on the model fit. With a linear penalty, such outliers carry less weight. Because of this, the linear difference approach is sometimes called a robust estimation.\nRobust estimation methods such as MAE are not as common as least squares, but most software has support for it. For instance, there are multiple packages in R that support MAE and other robust fitting methods.\nYou might wonder why we don‚Äôt just use those robust methods all the time, just to be safe. There are fundamental statistical reasons why optimizing the MSE is often more aligned with the question we want to address, so you should switch to a robust method only after you thought about it and decided it‚Äôs the right approach for your specific scenario. You can also always do it both ways and compare results to get an idea of the potential influence on your results from the outliers. Doing things more than one way ‚Äì and of course reporting each approach you tried ‚Äì is generally a great idea. This is often called performing a sensitivity analysis.\nAnother way to define f is with step functions. The idea is that as long as model and data are within some distance, the penalty is zero. Once model and data differ by some threshold, a penalty is given, e.g., a fixed value or a linear or quadratic penalty. Such types of schemes to define f are common in the class of models called Support Vector Machines, which we will look at later in the course.\nNo matter what scheme you choose, it might at times be useful to weigh data points. In the examples given above, each model-data pair was given the same importance. Sometimes it might be that you have some data points that should carry more weight. A common case is if you have measurements not on the individual level but some aggregate level. As an example, assume you have a fraction of heart attacks among all patients for some time period in different hospitals, and you want to fit that fraction. You don‚Äôt know the number of people who had heart attacks, only the fraction. But you do know something about the total number of beds each hospital has. You could then argue that hospitals with more beds have more patients and thus likely more heart attacks, and therefore the data from larger hospitals should get more weight, and you could e.g., multiply each term in the sum by bed size. Note that this is a scientific decision based on your expert knowledge of the data. Almost all fitting routines allow you to provide weights for your data and you can then perform weighted least squares (or a weighted version of whatever other approaches you choose).",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#accuracy",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#accuracy",
    "title": "Model performance metrics",
    "section": "Accuracy",
    "text": "Accuracy\nThe simplest way to determine performance of a model for categorical data is to count the fraction of times the model did (not) correctly predict the outcome. If we instead count the fraction of correct predictions made by the model, it is called accuracy. If we focus on the number of times the model got it wrong, it is called the (mis)classification error. The yardstick package has it as metric accuracy().\nWhile accuracy is often not a bad choice of metric, sometimes just counting the number of times a model didn‚Äôt get it right might not be the best idea. A common situation where accuracy (just counting how often the model prediction is right/wrong) is not very useful is for rare (but important) outcomes. Data of this type is often called unbalanced data. As an example, say we had some algorithm that tried to use brain images to predict if people have brain cancer. Fortunately, brain cancer is rare. Let‚Äôs say (I‚Äôm making this number up) that 1 in a million people who undergo this screening procedure actually have this cancer. Therefore, a model that predicts that nobody has cancer would be a very accurate model, it would only make one mistake in a million. However, missing that one person would be a very important mistake. We likely would prefer a model that flags 10 people as (potentially) having a cancer, including the person who really does and 9 false positives. This model has worse accuracy since it gets 9 out of 1 million wrong. But it‚Äôs likely more important to catch the one true case, even if it means temporarily scaring several other individuals, until further checks show that they are healthy. Of course this trade-off is very common. You likely know it in the context of balancing sensitivity and specificity of say a clinical test. Other areas where this happens is, e.g., predicting the (hopefully rare) errors when building a plane engine, flagging the (rare) credit card fraud, and many others of that type.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#beyond-accuracy",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#beyond-accuracy",
    "title": "Model performance metrics",
    "section": "Beyond accuracy",
    "text": "Beyond accuracy\nIn instances where accuracy/misclassification is not a good performance metric, other metrics are more helpful. A large variety of such metrics exist. Some only apply to the special (but very common) case of a binary outcome, and others apply more generally. Fairly common ones are: (Cohen‚Äôs) Kappa, Area under a Receiver operating curve (AUC ROC), Matthews correlation coefficient and F-score.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#the-confusion-matrix",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#the-confusion-matrix",
    "title": "Model performance metrics",
    "section": "The confusion matrix",
    "text": "The confusion matrix\nFor the common case of a binary outcome, one can construct what is called the confusion matrix (also known as 2x2 table in epidemiology). The confusion matrix tracks 4 quantities: true positives (TP, model predicts positive, and data is positive), true negative (TN, both model and data are negative), false positive (FP, model wrongly predicts positive) and false negative (FN, model wrongly predicts negative).\nThe confusion matrix comes with very confusing terminology since many of the quantities are labeled differently in different fields. For instance, epidemiologists tend to call the quantity TP/(TP+FN) sensitivity, while in other areas, such as machine learning, it is called recall. For a good overview of those different quantities and terminology, see this Wikipedia article.\nOne can of course also make a table for a case with more than 2 outcomes and track the true values versus the predictions. Some of the metrics generalize to such a scenario of more than 2 outcomes, but not all do.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/model-evaluation/model-evaluation.html#footnotes",
    "href": "content/model-fitting/model-evaluation/model-evaluation.html#footnotes",
    "title": "Model performance metrics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis business of varying the model in a smart way until you find the best one is an example of what‚Äôs called optimization. Developing routines that can quickly, efficiently, and reliably find the optimum is a huge area, and depending on the model and data you are trying to fit, having smart algorithms to do the optimization can be very challenging. You can make a whole career just working on optimization topics. Fortunately, for most purposes, we can generally use existing software and rely on their built-in optimizers.‚Ü©Ô∏é\nTechnically, for some simple models, it is possible to find the best parameter values other than by ‚Äòtrial and error‚Äô, but for many other models, this is more or less what is happening.‚Ü©Ô∏é\nAlso called SST, sum of squares total.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model performance metrics"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html",
    "href": "content/model-fitting/assessment-model-fitting.html",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#setup",
    "href": "content/model-fitting/assessment-model-fitting.html#setup",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Setup",
    "text": "Setup\nThis will be part of your portfolio site. Make a new folder called fitting-exercise. Also make a new Quarto document inside the folder and call it fitting-exercise.qmd.\n\nIn general, I suggest using a folder structure like the one from data-analysis-template and having separate pieces of code and Quarto files. For the purpose of this exercise, we‚Äôll keep it simple and do everything inside the fitting-exercise folder and fitting-exercise.qmd file.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#data",
    "href": "content/model-fitting/assessment-model-fitting.html#data",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Data",
    "text": "Data\nWe are going to use data on a drug candidate called Mavoglurant. This is The original paper describing and analyzing the data. You don‚Äôt need to look at it, unless you want to üòÅ. In case you do, since the paper might be behind a paywall, here‚Äôs a local copy of the pdf.\nThe data is part of an R package called nlmixr2data. You could get it by installing that package. But the easiest way to get the data is to download it from this GitHub repository, which is part of another paper you don‚Äôt need to read - but might want to peek at, see below.\n\nGo to the GitHub repository, download the Mavoglurant_A2121_nmpk.csv file and place it into the folder you just created.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#data-processing-and-exploration",
    "href": "content/model-fitting/assessment-model-fitting.html#data-processing-and-exploration",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Data processing and exploration",
    "text": "Data processing and exploration\nBefore we can do any analysis, we need to explore and process the data. The data we are looking at here has the ‚Äì very common ‚Äì feature that on first glance, it is confusing üôÑ. Variables have weird names and there seems to be a strange structure. In almost any real-world setting, this is the kind of data you‚Äôll be given. So let‚Äôs do some exploration and processing.\nUnfortunately, the data does not come with a codebook. The documentation of this dataset in the nlmixr2data package provides a bit of information. However, this documentation is very sparse and not enough. Another way to learn a bit more is to take a quick look at the papers that were published covering the data.\nThe second paper mentioned above is likely giving you a quicker overview compared to the first one. Take a look at the Data section and Figure 3, which shows the data as symbols. Based on that, you‚Äôll figure out that this is time-series data of drug concentrations.\nYou can get a quick visual idea of the data by plotting the outcome variable (which according to the documentation is called DV here) as a function of time, stratified by DOSE and using ID as a grouping factor.\n\nWrite code to load the data into R. Then write code to make a plot that shows a line for each individual, with DV on the y-axis and time on the x-axis. Stratify by dose (e.g., use a different color for each dose, or facets).\n\nAs you look closer, the formatting of the dataset still looks a bit strange. So let‚Äôs dig deeper. One thing you will notice is that there are some individuals that seem to have received the drug more than once, indicated by having both entries with OCC=1 and OCC=2. Since we are not sure what the difference is, and to keep things simple, we only keep one dataset for each individual. Therefore, remove all entries with OCC=2.\n\nWrite code that keeps only observations with OCC = 1.\n\nYou will also see that each individual has an entry at time 0 that has DV=0 and some non-zero value for AMT. This is the dosing entry for everyone. All the other entries are the time-series values for the drug concentration. We won‚Äôt do a time-series analysis here, so instead we‚Äôll compute the total amount of drug for each individual by adding all the DV values. Note that this is a pretty bad idea, since some individuals might have more or less data points. The proper way to do this would be to do some form of integration to get the area under the curve, e.g.¬†with a simple trapezoid rule, or to model the whole time-series with a function and then compute the AUC from that function. But to keep things simple, we‚Äôll go ahead - keeping in mind that in general, outside of a practice example, this is not a good idea.\n\nWrite code to exclude the observations with TIME = 0, then compute the sum of the DV variable for each individual using dplyr::summarize(). Call this variable Y. The result from this step should be a data frame/tibble of size 120 x 2, one column for the ID one for the variable Y. Next, create a data frame that contains only the observations where TIME == 0. This should be a tibble of size 120 x 17. Finally, use the appropriate join function to combine those two data frames, to get a data frame of size 120 x 18.\n\nFinally, we‚Äôll do a bit more cleaning. At this point, we don‚Äôt need most of these indicator variables anymore (e.g., OCC or EVID). We also want to convert RACE and SEX to factor variables.\n\nWrite code that converts RACE and SEX to factor variables and keeps only these variables: Y,DOSE,AGE,SEX,RACE,WT,HT\n\nCheck the data and make sure everything looks ok. As needed, perform further exploration or cleaning steps.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#eda-revisited",
    "href": "content/model-fitting/assessment-model-fitting.html#eda-revisited",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "EDA revisited",
    "text": "EDA revisited\nYou probably already produced some EDA (figures and tables) as part of the exploratory process above. Do a few more here, once the data is clean.\n\nMake some useful summary tables.\nShow some scatterplots or boxplots between the main outcome of interest (total drug, Y) and other predictors.\nPlot the distributions of your variables to make sure they all make sense.\nLook at some pair/correlation plots.\n\nThis is a fairly open exploration, I want you to play with the data enough to get a good idea of what you are looking at and what‚Äôs ‚Äúinside‚Äù the data.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#general-wrangling-and-eda-comments",
    "href": "content/model-fitting/assessment-model-fitting.html#general-wrangling-and-eda-comments",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "General wrangling and EDA comments",
    "text": "General wrangling and EDA comments\nAlways make sure you document well. Prior to each code piece that produces some output, you should add text describing what you are about to do and why. After you produced the result, add some text that comments on what you see and what it means. E.g. you could write something like Histogram for height shows two persons at 20in, everyone else else is above 50in. Checked to see if those are wrong entries or not. Decided to remove those observations.\nOnce you‚Äôve come this far in any kind of data analysis, you should have some code that explores and cleans the data, produces some figures and tables, and (in general, not required here) saves the cleaned data set to a file.\nYou should also have documentation, as comments in the code and as text in a Quarto document, as well as some notes in README file(s), that explain what the different parts of data and code are/do, what variables the data contain, what each variable means, which are the main outcome(s) and main predictor(s), etc. Providing all this meta-information is important so that someone else (or your future self) can easily understand what is going on.\nAny good data collection and analysis requires that data is documented with meta-data to describe what it contains, what units things are in, what variables are allowed, etc. Without good meta-data, analyses often go wrong. There are famous examples in the literature where the coding 0/1 was assumed to mean the opposite of what it was, so all conclusions were wrong. To guard against this, careful documentation is crucial. Also, giving variables easy to understand names and values is best. The data we have here is a good example of how NOT to do things. There‚Äôs no codebook, and we don‚Äôt really know what SEX 0/1 stands for (which ones are male/female). We also don‚Äôt know what the 4 entries 1/2/7/88 for RACE stand for. If we had a codebook, or some better labeling ‚Äì or ideally both ‚Äì, would be very helpful. Unfortunately, a lot of real-world data is rather poorly documented.",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-fitting/assessment-model-fitting.html#model-fitting",
    "href": "content/model-fitting/assessment-model-fitting.html#model-fitting",
    "title": "Assessment - Fitting Basic Statistical Models",
    "section": "Model fitting",
    "text": "Model fitting\nFinally, we‚Äôll do some model fitting. We‚Äôll keep it simple here. We‚Äôll use the tidymodels framework. I suggest you go to the build a model tutorial in the Get Started section on the tidymodels website. There, you‚Äôll see an example of fitting some data with a linear model. From that tutorial, and other sources in the tidymodels website, as well as the TMWR book and other online sources figure out how to do the steps below. Of course AI tools are also an option (that‚Äôs how I did part of it üòÅ).\nWrite code that does the following:\n\nFit a linear model to the continuous outcome (Y) using the main predictor of interest, which we‚Äôll assume here to be DOSE.\nFit a linear model to the continuous outcome (Y) using all predictors.\nFor both models, compute RMSE and R-squared and print them.\n\nRepeat the steps above, but now we‚Äôll consider SEX as the outcome of interest (that doesn‚Äôt make too much scientific sense, but we want to practice fitting both continuous and categorical outcomes).\n\nFit a logistic model to the categorical/binary outcome (SEX) using the main predictor of interest, which we‚Äôll again assume here to be DOSE.\nFit a logistic model to SEX using all predictors.\nFor both models, compute accuracy and ROC-AUC and print them.\n\nInterpret the results and add additional comments and thoughts to the document, so a reader can understand what you did, why you did it, and what it all means. For instance do you see any pattern between Y and DOSE or SEX and DOSE and does what you see make sense? How about the other predictors?\nIf you are extra ambitious or just enjoy playing around with tidymodels, add a k-nearest neighbors model fit to the continuous and/or categorical outcome.\n\nFor this exercise, it isn‚Äôt so important that the outcomes and predictors are of actual scientific interest. We won‚Äôt do exciting science here, just explore the analysis workflow. However, I want to remind you that having good/interesting question to answer (and data you can use to answer it) is the most important part of any research project. Rather a great question and basic stats than really fancy stats answering a meaningless question that nobody cares about!",
    "crumbs": [
      "Assessments",
      "8. Model Fitting"
    ]
  },
  {
    "objectID": "content/model-assessment/reality-based-assessment/reality-based-assessment.html",
    "href": "content/model-assessment/reality-based-assessment/reality-based-assessment.html",
    "title": "Reality-based assessment",
    "section": "",
    "text": "This unit discusses the idea of assessing a model based on expert knowledge.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Reality-based assessment"
    ]
  },
  {
    "objectID": "content/model-assessment/reality-based-assessment/reality-based-assessment.html#footnotes",
    "href": "content/model-assessment/reality-based-assessment/reality-based-assessment.html#footnotes",
    "title": "Reality-based assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm not the first one to come up with the map analogy. I‚Äôm not sure where I heard it first, but I know I didn‚Äôt invent it. Only the specific example shown here is from our previous paper.‚Ü©Ô∏é\nThis uses immunology as an example, but of course this could be any topic to make the same analogy.‚Ü©Ô∏é\nThis is not unique to mathematical/statistical models. If you do an experiment in a BALB/c mouse, you are using a model system. It doesn‚Äôt mean the results would be the same in another type of mouse, let alone in humans.‚Ü©Ô∏é\nThis paper shows a nice example in which the authors compared several influenza models. By comparing the models to each other and to data, they were able to discuss how models differed and which ones agreed with the data (spoiler alert: there was no one single best model, most of them were good at reproducing some data and not good at reproducing other data).‚Ü©Ô∏é\nYou might think that nobody would do something silly like that. You would be wrong. You can find a lot of extrapolations gone wrong in the literature. Often, it is subtle, but sometimes it is blatant. This is a good example of the latter.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Reality-based assessment"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-overview.html",
    "href": "content/model-assessment/model-assessment-overview.html",
    "title": "Model assessment",
    "section": "",
    "text": "Overview\nCritical thinking is important. That also applies to your (or someone else‚Äôs) models.\nThis module discusses ways to assess your models.\n\n\nLearning Objectives\n\nBe familiar with methods to assess models.\nBe able to perform model assessment on your own and others‚Äô models.\n\n\n\nContent\nThe module consists of the following units:\n\nModel assessment introduction\nReality-based assessment\nModel performance revisited\nModel evaluation with cross-validation\nModel evaluation with Information Criteria\nModel diagnostics\nModel uncertainty",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Overview"
    ]
  },
  {
    "objectID": "content/model-assessment/information-criteria-assessment/information-criteria-assessment.html",
    "href": "content/model-assessment/information-criteria-assessment/information-criteria-assessment.html",
    "title": "Information criteria based model assessment",
    "section": "",
    "text": "Overview\nThis unit discusses the idea of assessing model performance based on information criteria.\n\n\nLearning Objectives\n\nBe familiar with the idea of information criteria.\nKnow when to use information criteria.\n\n\n\nIntroduction\nThe previous units introduced the idea that you want to assess the performance of your models on external/new data. You then learned about the train/test and cross-validation approaches to try and get a more honest assessment of the quality of your models.\nHere, we‚Äôll cover alternative approaches that you can use if you are not able to use CV (e.g., because you don‚Äôt have enough data or your data/model take too long to run a CV analysis.)\n\n\nInformation criteria\nInformation criteria, such as AIC, BIC, DIC, WAIC and similar, compute a measure that is a trade-off between good fit to the data (low-cost function/high performance) and model complexity (number of parameters). Approaches based on such selection criteria essentially try to guess how the model would perform if it were to be fit to new data, without actually trying to do it (in contrast to CV).\nThe disadvantage is that these guesses as to how the model might perform on new data are not as reliable as actually evaluating model performance on such data through CV. The advantage is that no sampling is needed, which means these approaches are much less computationally intensive. For all those information criteria, things are set up that a model with a smaller value is considered better. These measures - thankfully! - do not have the arbitrary p&lt;0.05 value cut-off common in frequentist statistics. For AIC, a rule of thumb is that a difference of 10 between 2 models is meaningful. (Unfortunately, people seem to not be able to make their own decisions and need crutches, so arbitrary cut-offs for AIC/BIC/etc. have started to show up in the literature.)\nThere is a lot of math behind information criteria. It doesn‚Äôt hurt learning a bit more (see the further resources section). However, you can also just use these criteria without having to understand too much about them. A lot of statistical software can compute these criteria, and if you are not able to use CV approaches, using a information criterion can be a good idea. But beware of ‚Äúresearcher degrees of freedom‚Äù. You shouldn‚Äôt compute a lot of different criteria and then pick the one that gives you the answer you are looking for. If you compute more than one, you need to report that. And if they disagree (e.g., AIC prefers model 1, and BIC prefers model 2), you‚Äôll have to figure out what that means.\n\n\nSummary\nThis summarizes the whole idea of model performance evaluation. To repeat (again): We generally want to know how well a model performs in general and on new data - not the sample we fit it to. Testing/reporting model performance for the data the model was fit to very often leads to overfitting and optimistic/wrong conclusions about new/future data. To minimize overfitting, here is my recommended strategy (I‚Äôm sure it‚Äôs not the only one, so what matters most is that you clearly think about what each step in your analysis means/implies):\n\nIf you have enough data and care about predictive performance, set some data aside for a final validation/test. If you don‚Äôt have a lot of data, you might need to skip this split.\nIf you choose to use all your data for model fitting purposes, and don‚Äôt evaluate your model on data not used during model building/training, you need to interpret your findings as exploratory and hypothesis generating, and you need to be careful about trying to draw generalizable conclusions.\nIf you have enough data (&gt;100s observations) and CPU power, use cross-validation (CV) approaches to determine the best model. If for some reason (mainly computational time or small data) CV is not feasible, use AIC & Co.\nThink carefully about your cost function/metric! A model that is great at predicting the wrong outcome is useless!\nNo matter what approach you use, choosing a model based on performance alone is not enough.\n\n\n\nFurther Resources\n\nModel Selection and Multimodel Inference is a book that covers information criteria in a lot of detail.\nThe Statistical Rethinking book has - in my opinion - one of the best explanations of the concepts and ideas behind information criteria.\n\n\n\nTest yourself\n\n\nWhich of the following is NOT a common information criterion?\n\nVICBICDICAIC\n\n\n\nVIC is not a common information criterion.\n\nTrue\nFalse\nFalse\nFalse\n\n\n\n\n\nInformation criteria provide estimates of model performance on new data that are as good as those obtained from a train/test approach.\n\nTrueFalse\n\n\n\nFalse. They are generally less reliable than train/test approaches.\n\nFalse\nTrue\n\n\n\n\n\nYou should try many different information criteria and choose the one which gives the best results.\n\nTrueFalse\n\n\n\nFalse. You should not pick criteria based on desired results.\n\nFalse\nTrue\n\n\n\n\n\n\nPractice\n\nCheck publications in your area of interest to find an example of a data analysis that used an information criterion approach to choose between models. Assess if you think the authors implemented and reported their findings appropriately. Consider if the authors could/should have used a train/test and/or CV approach instead.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model evaluation with Information Criteria"
    ]
  },
  {
    "objectID": "content/model-assessment/diagnostics-assessment/diagnostics-assessment.html",
    "href": "content/model-assessment/diagnostics-assessment/diagnostics-assessment.html",
    "title": "Model diagnostics",
    "section": "",
    "text": "This unit discusses different diagnostics that are helpful to assess models.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "content/model-assessment/diagnostics-assessment/diagnostics-assessment.html#footnotes",
    "href": "content/model-assessment/diagnostics-assessment/diagnostics-assessment.html#footnotes",
    "title": "Model diagnostics",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWell, it can actually be quite tricky to interpret coefficients for anything but a linear model. For instance for a logistic model (and many other GLM), the impact of a change in a predictor on the outcome depends on the value of the predictor. Sometimes a 1-unit increase in some predictor (say drug dose) can lead to a strong change in the outcome (say cholesterol level), while for other values of that predictor, increasing it further by 1 unit might have almost no impact on the outcome. So one needs to be careful even when interpreting fairly simple GLM. However, the whole model is known so you can always figure out how one part relates to the other. That‚Äôs not the case any more for complex ML models.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "content/content-overview.html",
    "href": "content/content-overview.html",
    "title": "Content Overview",
    "section": "",
    "text": "This section contains almost all of the content, organized into modules. Each module has several units.\nAssessments for each module are in the separate Assessments section.\nThe Schedule page lists what content is covered and what assessments you need to do each week.",
    "crumbs": [
      "Content",
      "Content Overview"
    ]
  },
  {
    "objectID": "content/model-assessment/assessment-model-assessment.html",
    "href": "content/model-assessment/assessment-model-assessment.html",
    "title": "Assessment - Evaluating Models",
    "section": "",
    "text": "Quiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\n\n\nExercise\nWe definitely need to practice some of the topics we covered in this (and previous modules). But since at the end of the week there is another project deadline, I want you to focus on that. Thus no exercise this week, we‚Äôll practice the topics we covered in upcoming exercises.\n\n\nProject\nFinish (and make sure to push) part 3 of the project. There is no need to submit a link. Unless you renamed your repository, I, and your classmates, know where to find it based on your previous submissions. Make sure the link you posted previously to the project-related Discord channel is accurate. If you are using a private repository, add the students who will review your project as collaborators (see announcement channel).\n\n\nDiscussion\nWrite a post in this week‚Äôs discussion channel that answers this question:\nWhich of the concept(s) we covered in this module is/are the most interesting/surprising to you, and why? And which concept(s) or topic(s) from this module‚Äôs materials do you find the most confusing, and why/how so?\nPost by Wednesday, then reply to each other by Friday. I hope together we can reduce any remaining confusion and reinforce the ideas from this module.",
    "crumbs": [
      "Assessments",
      "9. Model Assessment"
    ]
  },
  {
    "objectID": "content/model-assessment/external-data-assessment/external-data-assessment.html",
    "href": "content/model-assessment/external-data-assessment/external-data-assessment.html",
    "title": "Model assessment using cross-validation",
    "section": "",
    "text": "Overview\nThis unit discusses the idea of using existing data to assess how well a model might perform on external data.\n\n\nLearning Objectives\n\nUnderstand the concept of cross-validation.\nKnow that advantages and disadvantages of cross-validation.\n\n\n\nIntroduction\nIn the previous unit, we discussed the idea that using the data that was used to build and fit the model gives an overly optimistic view of model performance. Overfitting is a big risk and often leads to a model that does not perform well on new data.\nHere, we‚Äôll discuss a useful approach that can help minimize overfitting.\n\n\nHonest performance evaluation\nMore data: The ultimate test is, of course, to check your model performance on newly collected data, or an existing, independently collected dataset. However, collecting or getting new data is not easy, and often you might not have a completely independent dataset that measures the same variables as the one you are trying to analyze/fit. Therefore, for the remainder of this discussion, we assume that getting more data is not an option. (Though if you have that option, certainly go for it!)\nReserving some data: If you have enough data, you can perform an evaluation of model performance that tries to imitate the idea of evaluating the model on newly collected data. What you do is that right at the start of your analysis, you set some of your data aside and do not look at it until the very end! A common fraction of data that is set aside is 20%-40%, but there is no clear rule. This data you set aside is called your test/validation data. You need to completely ignore that data and are not allowed to use it while doing any statistical model fitting. You then take the rest of your data, referred to as training data, and do all your model building and fitting and model choosing, etc.\nAt the end and only once at the very end do you run your model on the test data and compute the performance. The way you do this is to give your model the input variables (predictors) from the new data, ask the model to make predictions for the outcome, and use your metric/cost function to evaluate the performance of your model on that part of the data that your model has never seen before. This gives you an estimate of your model performance that is as honest as you can get without collecting new data. Be aware that even with this approach, you are likely still getting slightly better results than if you had evaluated your model on independently collected data (with somewhat different study design and data collection approaches). But in the absence of truly independent data, this is the best/most honest model performance evaluation you can do.\nIf you look at data analysis competitions such as Kaggle, this is how they operate. People need to submit their model predictions, and the model will be evaluated on a final dataset, only once, at the end of the competition. Whoever scores highest on that final dataset wins, no matter how well they did on the leaderboard.\nWhile such a train/test split is an excellent thing to do, there are some constraints. First, you need to have enough data. If your dataset is small (&lt;100-1000 observations, depending on the number of predictors and model complexity), then this approach will make you give away data that you need to build and fit your model reliably, and won‚Äôt provide robust insights. Second, you still need a way to decide which model to chose as your final model, and as mentioned above, using the one with the smallest cost function as evaluated on the data that was used to build the model is not good. An approach called cross-validation can help.\n\n\nCross-validation\nEven with the train/test approach just described, you still need to figure out how to evaluate which model is best during the model building process. To that end, an approach called cross-validation has become very widely used and is currently - in my opinion - the best method to evaluate and choose your model.\nCross-validation (CV) is a reasonably straightforward approach. It mimics the train/test idea just described, but is now applied during the model fitting process. The idea is that you take the data you set aside for the model building process (i.e., your training data), and during the model building procedure, you repeatedly split your data into a portion to which you fit our model, and a portion which you use to evaluate model performance. You repeat this many times to get several samples and always use some portion of the data to fit the model, and the remaining part to evaluate it.\n\nYou might not be surprised to learn that this is another place where terminology is confusing and inconsistent. The words training data and test/validation data are used both for the initial split described above, and the split done in the CV procedure. One terminology is to call the splits done by CV the analysis and assessment portions of the data. This is shown nicely in this schematic. I will try to use that terminology, but I might not be consistent. And other materials you see/read certainly do not follow that terminology, as you will see already below.\n\nThere are different ways to do the data splitting during cross-validation. A common setup is to randomly divide the data set into k blocks (called folds). You then use the data in all but one of the blocks to fit the model, then compute the cost function/performance metric (e.g., the RMSE) using the remaining block. You repeat that for all k blocks. The average value of the model performance over the k blocks is the target that you want to optimize. One usually chooses k = 5 or 10 blocks/folds, which has shown in simulation studies to give the best variance-bias trade-off. The figure shows an example of a 4-fold cross-validation for a categorical outcome (color of the balls). Note that this figure uses the train/test terminology for labeling the blocks, not the analysis/assessment terminology.\n\n\n\nSource: Cross Validation Wikipedia Page\n\n\nIn addition to the standard way of splitting the data, there are different sampling variants. For instance, if you have longitudinal data, or otherwise data with structure, you might want to sample in a different way (e.g., sample the earlier times and predict later times). One variant of the cross-validation approach is the leave-one-out (LOO) cross-validation, which removes each data point one at a time as the test data and therefore performs the train/test part N times. You will also often see repeated cross-validation, which just adds another round of sampling on top to get better distributions (e.g., a 10-fold CV 10 times repeated gives 100 samples, which will be used to compute model performance).\nCross-validation works for most situations and is quite robust. It is also fairly easy to implement. Thus CV is a good default strategy. However, sometimes it is not possible or desirable to use CV. Occasionally, datasets are so small that you need all the data just to fit your model. Alternatively, your dataset might be large and/or your model complex, which might mean running a CV method takes too long. (Often, one can speed up things by down-sampling the data, using faster algorithms, or running code in parallel on a computer cluster.)\n\n\nCross-validation and model complexity\nIf you are mostly fitting GLM type models or other fairly simple models, you might wonder what I mean when I say during the model building/fitting process, since for a given simple model, there is just one set of model parameters that fits best, and that‚Äôs it. However, many more complex models have what is often referred to as tuning parameters. These are not part of the model structure (e.g., they are not the coefficients in front of your predictor variables), but they influence model performance. A great example are AI models, many of which easily have millions of parameters that need to be tuned to get good model performance. For those models, it is critical to evaluate model performance on data that is somewhat independent from the sample that is used to fit/tune/optimize the model.\nTo provide an analogy for a GLM, the number of variables you put in your model is somewhat similar. You learned that larger models fit better, so sticking more predictors into your model will give better performance (e.g., lower RMSE). However, this might lead to worse performance on independent data. You can then use something like CV to determine how many predictors might be best for your model.\n\n\nSummary\nTrain/test data splitting and cross-validation are great approaches to more honestly assess the performance of a model. If your goal is to simply determine if there are patterns in the data you have, you might not need CV. But if you in any way have prediction as a goal, and want to understand if your model likely works in general - on data like yours, but not the one you used to fit the model - then you should use CV if you can.\n\n\nFurther Resources\nFrank Harrell‚Äôs blog post discusses some of the potential caveats for using methods such as cross-validation.\nChapters 11 and 12 of the Tidy Modeling with R book provide additional discussion of the sampling/CV approaches. Another discussion of CV is chapter 30 of IDS.\n\n\nTest yourself\n\n\nWhat‚Äôs the name commonly given to the data you do your final model evaluation?\n\nSplit DataTrain DataTest DataAnalysis Data\n\n\n\nTest data is used for the final model evaluation.\n\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\nOne needs to use at least 10 folds/blocks in CV to get a reliable answer.\n\nTrueFalse\n\n\n\nFalse. Other fold counts can be appropriate.\n\nFalse\nTrue\n\n\n\n\n\nUsing indepedently collected data to test model performance is better than using train/test or CV.\n\nTrueFalse\n\n\n\nTrue. Independently collected data is the most honest assessment.\n\nTrue\nFalse\n\n\n\n\n\n\nPractice\n\nCheck publications in your area of interest, or look online on websites like Kaggle, to find an example of a data analysis that used the train/test and/or the CV approach. Read how the authors did things and what they found and report.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model evaluation with cross-validation"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html",
    "title": "Model assessment introduction",
    "section": "",
    "text": "This unit provides an overview of the different ways one can and should assess models - both your own and those of others.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#agreement-with-reality",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#agreement-with-reality",
    "title": "Model assessment introduction",
    "section": "Agreement with reality",
    "text": "Agreement with reality\nFirst and foremost, you should ask yourself if a given model makes sense. If there‚Äôs an obvious conflict between the model structure or the model results and reality, something can‚Äôt be right about the model. This is where your subject matter expertise comes in. To be a good data analyst, you not only need statistical and data analysis skills, you should also know a fair bit about the system you are trying to study. If there‚Äôs disagreement between model and reality, reality wins. Let‚Äôs say, you used a model to explore pattern between children‚Äôs age and height. Your model predicts that a 30 year old person will be 20 feet tall. You know this is not possible, therefore your model is likely not a very good one. In this case, you maybe extrapolated outside the range of where the model might be useful - a common occurrence. For this example, the solution is to either only use the model in the range where it was calibrated, or if you want to predict the height of a 30 year old person, you need to change your model.\nSo always think critically about the model and its results and ensure they pass the ‚Äòreality test‚Äô. Only if they do does it make sense to move to more narrow ways of assessing the model.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#model-performance",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#model-performance",
    "title": "Model assessment introduction",
    "section": "Model performance",
    "text": "Model performance\nThis is generally the most clear-cut metric. After you define and quantify the metric you want to optimize, you can compare model performance among different models. We discussed some of those metrics before, such as RMSE or accuracy. Comparing those values among models can indicate which model is better. How exactly one should do that performance evaluation is something we‚Äôll discuss in later unit.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#predictions",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#predictions",
    "title": "Model assessment introduction",
    "section": "Predictions",
    "text": "Predictions\nThe model performance metric gives you a single quantity describing how well the model matches the data, given the metric you defined. That‚Äôs useful, but you generally want to dig deeper and look at how predictions from the model for individual observations compare to the actual data. You can do that best through various graphical methods, which we‚Äôll discuss in a later unit.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#uncertainty",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#uncertainty",
    "title": "Model assessment introduction",
    "section": "Uncertainty",
    "text": "Uncertainty\nAny estimate you obtain from a model comes with uncertainty. You need to look at that uncertainty and decide if it is acceptable. You might have trade-offs. For instance you might have a model that provides a better performance as measured by your metric (say RMSE), but when you look at the uncertainty around the estimate, it might be larger than what you get from a smaller model. Which model to choose in such a case is a scientific question. It depends on your goals.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#complexity",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#complexity",
    "title": "Model assessment introduction",
    "section": "Complexity",
    "text": "Complexity\nThere is often a trade-off between simple models that are easy to interpret but don‚Äôt perform quite as well, and complex models that give better performance but that can be hard to interpret. More complex models also tend to take longer to execute, and might not generalize that well since they might be overly tuned to the data you used to build the model. Again, it is up to you to decide which model is more suitable for your purpose.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#usefulness",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#usefulness",
    "title": "Model assessment introduction",
    "section": "Usefulness",
    "text": "Usefulness\nUsefulness is in some sense the combination of the categories above. Based on what you want a model to do, you choose one (or multiple) that are most useful for your purpose.\nFor instance, if you want to build a model that allows doctors to predict the chance that a patient has a certain disease, you might want to have a model that only uses the fewest (or easiest/cheapest to measure) variables to obtain good performance. So if you collect a lot of data, some based on checking patient symptoms and some on lab results, you might not want to use all those variables in your model. Let‚Äôs say that you had data on 10 predictors, 5 for easy to measure symptom variables (e.g., body temperature and similar), and 5 variables that come from different lab tests. You‚Äôll evaluate models with different predictors (performing e.g., subset selection or LASSO, which we‚Äôll discuss soon) and find that the best performing model retains 3 symptom variables and 2 lab tests. Let‚Äôs say its performance is 95% (I‚Äôm purposefully fuzzy about what that performance exactly is since it doesn‚Äôt matter). But you also find that another model that contains 4 symptom variables and no lab tests has 85% performance. Which do you choose? In this case, since you could get data on the 4 symptoms very quickly and cheaply, you might want to recommend that model for most doctors offices, and only use the better, but more time-consuming and expensive model with the 2 lab tests in settings such as high-risk populations in the hospital.\nIn contrast, if you are a bank that tries to predict fraud by having complicated models that constantly analyze various data streams, you might not care how complicated and big your model is, only that the performance in flagging fraudulent transactions is as good as possible.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#footnotes",
    "href": "content/model-assessment/model-assessment-introduction/model-assessment-introduction.html#footnotes",
    "title": "Model assessment introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe term evaluation is often used interchangeably with assessment. I‚Äôm trying to reserve evaluation when one has a clear performance metric that one can measure, for instance the mean squared error or the value of a likelihood function. I use the term assessment in a broader sense. But I might not be consistent, so if you see either word, just figure out what is meant based on context üòÅ.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model Assessment Introduction"
    ]
  },
  {
    "objectID": "content/model-assessment/model-performance-revisited/model-performance-revisited.html",
    "href": "content/model-assessment/model-performance-revisited/model-performance-revisited.html",
    "title": "Model performance revisited",
    "section": "",
    "text": "Overview\nThis unit discusses the idea of assessing a model based on how well it performs on external data.\n\n\nLearning Objectives\n\nUnderstand the concept of overfitting.\nKnow that a model should generally be assessed by its performance with independent data.\n\n\n\nIntroduction\nWe discussed how to specify a metric and use that metric to try to find the model with the best performance. We covered the idea of defining a single numerical value (cost function/metric) and optimizing (usually minimizing) it to find the best model. There is however a very big caveat to this. The main point is: It usually doesn‚Äôt matter how well your model performs on the data that you used to build and fit your model!\nThis is a very important point, and one that unfortunately a majority of scientific papers still get completely wrong! It is one area where modern machine learning is much more careful compared to the traditional way statistics is taught/used. In the machine learning field, it is very much recognized that it doesn‚Äôt matter a whole lot how well your model performs on the data that you used to build your model! What matters is performance on similar new data like the data that was used to fit the model.\nI‚Äôm going to repeat this and similar sentences a bunch of times throughout the rest of the course üòÅ. If you only take away 2 main points from this course, this would be one if them. The other is that doing data analysis in a reproducible (automated) manner is critical.\nSo let‚Äôs go into some more details regarding this important point.\n\n\nShould we really minimize the cost function?\nWe discussed that once we have chosen a cost function for our problem, we are trying to find a model that minimizes this cost function (e.g., minimizes the RMSE or the misclassification error), and models with smaller cost functions are better. The problem with this concept is that in general, a bigger model will be more flexible and thus able to fit the data better. However, when we do data analysis, we generally are not (only) interested in having a model that works well for the specific data sample we used to fit your model. Our main question/hypothesis usually does not concern the actual data we have/fit. Instead, we generally want to say something about ‚Äòthe larger world‚Äô.\nIf we are asking inferential questions, we are interested in what the data analysis teaches us about this system in general. E.g., if we analyze data to see if there is a correlation between levels of atmospheric pollutants and cases of asthma among our study population, we are usually really interested in knowing if such a correlation is real in general.\nIf we are asking predictive questions, we are interested in a model that can predict future observations, not the ones we already have. E.g., if we analyze data for a specific treatment, we are not very interested how well the model predicts the effect of the drug on the people for which we collected the data (we already know that). Instead, we want to make general predictions about the effectiveness of the treatment on future patients.\nIn either case, what we want is a model that is generalizeable (also sometimes called externally valid ), and that applies equally well to new and similar data beyond the data we already collected.\nWhat truly matters is how well our model can explain/predict other/future data, not just the data we already observed!\nIf we build a very complex model in an effort to match our existing data as closely as possible, what generally happens is that our model overfits. That means it becomes very good at modeling the data we use to build the model, but it won‚Äôt generalize very well to the general, broader context of other/future data. The reason for that is that there is noise (random variability) in any dataset, and if we have a model that is too flexible, it will not only match the overall signal/pattern (if there is any) but will also capture all the noise in our sample, which leads to worse performance on future data that have different amounts and types of noise/variability.\n\n\nBias-variance trade-off\nAs model complexity increases, models tend to perform/fit better to the data that is used to build the model. However, it is also generally the case that such complex models perform worse on future/new data compared to simpler models. This is an important general concept and is known in statistics as the bias-variance trade-off.\nBias describes the fact that a model that is too simple might get the data ‚Äúsystematically wrong‚Äù. A more restricted model like a simple linear model usually has more bias. Another way of saying this is that the model underfits, i.e., there are still patterns in the data that the model does not capture. More complex models generally reduce the bias and the underfitting problem..\nVariance describes how much a model would vary if it were fit to another, similar dataset. If a model goes close to the training data, it will likely produce a different fit if we re-fit it to a new dataset. Such a model is overfitting the data. More complex models tend to be more likely to overfit.\nWhile the concept sounds somewhat technical, you can get a very good and quick intuitive understanding by looking at the following figure.\n\n\n\nBias-variance tradeoff. Source: ISLR.\n\n\nIn the example shown in this figure, the data was produced by taking the black curve and adding some noise on top. This gives the data shown as circles. Three models are fit. A linear model (yellow) is too restrictive and misses important patterns. The next model (blue line) is more flexible and is able to capture the main patterns. The most complex model (green line) gets fairly close to the data. But you can tell that it is trying to get too close to the data and thus overfits. If we had another data sample (took the black line and added some noise on top), the green model would not do so well. This is shown on the right side, where the grey line plots the MSE for each model for the given dataset. As the model gets more complex/flexible, they get closer to the data, and the MSE goes down. However, what matters is the model performance on an independent dataset. This is shown with the red curve. Here, you can see that the blue model has the lowest MSE.\nThe same concept holds for categorical outcomes, and for models with multiple predictors. No matter what the model, there is always a sweet spot for model complexity somewhere ‚Äúin the middle‚Äù. This ‚Äúmiddle‚Äù depends on the data and the question. Often, linear models are as good as one can get, and more complex models will overfit. Even for linear models, we might have to remove predictors to prevent overfitting (we‚Äôll discuss that later). At other times, somewhat complicated models (e.g., neural nets) might perform best. In general, the more data (both quantity and richness), the less likely it is that a more complex model will lead to overfitting. However, we always need to check.\n\n\nOverfitting and machine learning\nIf you only fit simple models (e.g., a linear model), and maybe decide based on scientific knowledge which predictors need to be in the model, then your risk of overfitting ‚Äì while still present ‚Äì is not that large. However, in machine learning, you often have complex models with many components that can be adjusted/tuned (we‚Äôll get into that) to improve model performance. The danger is that if you have a very flexible model that can be finely tuned to perform well on the data, you have a very large risk of overfitting, namely of ending up with a model that is well tuned and performs very well on the data you use to build the model, but does not work so well on other data. Therefore, while overfitting is always something the be careful about, once you start using larger and more flexible models, you definitely need to guard against overfitting.\n\n\nDealing with overfitting\nNow that you learned that the model that performs best (using whatever metric you chose) is not necessarily the best one, how can we evaluate model performance in a better way? There are different options, which we‚Äôll discuss in the next units.\n\n\nSummary\nTo repeat (again): We generally want to know how well a model performs in general and on new data - not the sample we fit it to. Testing/reporting model performance for the data the model was fit to very often leads to overfitting and optimistic/wrong conclusions about new/future data. There are several good ways to minimize overfitting, which we‚Äôll cover next.\n\n\nFurther Resources\nChapter 2 ISL covers the bias-variance trade-off.\n\n\nTest yourself\n\n\nWhich of the following is NOT an important topic discussed in this unit?\n\nOverfittingVarianceConvergenceBias\n\n\n\nConvergence is not a main topic in this unit.\n\nFalse\nFalse\nTrue\nFalse\n\n\n\n\n\nOverfitting is only a problem for large machine learning models.\n\nTrueFalse\n\n\n\nFalse. Overfitting can occur with many model types.\n\nFalse\nTrue\n\n\n\n\n\nMost of the time, we want a model that performs well in general/on new data, not just for our sample.\n\nTrueFalse\n\n\n\nTrue. Generalization to new data is the key goal.\n\nTrue\nFalse\n\n\n\n\n\n\nPractice\n\nRevisit the papers you found for the previous unit‚Äôs exercise. Go through them again and specifically focus on the model structure and complexity, and if results make sense. Try, as best as you can, to critically evaluate if the authors made suitable choices and explained their choices well.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model performance revisited"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html",
    "title": "Model uncertainty assessment",
    "section": "",
    "text": "This unit discusses model uncertainty, how to compute and and how to assess it.\n\n\n\nA study with n=1 has large uncertainty. Source: SMBC.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#confidence-interval-ci",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#confidence-interval-ci",
    "title": "Model uncertainty assessment",
    "section": "Confidence interval (CI)",
    "text": "Confidence interval (CI)\nConfidence intervals (CIs) are a common way to report uncertainty. Those intervals capture uncertainty in the estimates for the model parameters and thus uncertainty in the mean outcome. CIs are very useful if we want to estimate the uncertainty in the population average outcome. For instance, going back to the example above, if we report an average drug-induced improvement in some quantity by 20%, with a CI (having some width, such as 89% or 95% or‚Ä¶) of 15% to 25%, we can conclude that if we gave the drug to another population like the one we studied, this is roughly the range of impacts we should expect to see in the new study.2",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#standard-error-se",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#standard-error-se",
    "title": "Model uncertainty assessment",
    "section": "Standard error (SE)",
    "text": "Standard error (SE)\nStandard errors (also often called standard error of the mean, SEM) are closely related to CI. For SE, you need to assume some kind of distribution for the quantity of interest. Almost always, that‚Äôs a normal distribution (which is often what you implicitly do when you optimize RMSE). In the common case that one looks at 95% CI and assumes a normal distribution, the relation is that CI = 1.96*SE. Thus, with SE, you get tighter bounds (they correspond to 68% CI). As for CI, an increase in data usually leads to more precise estimates of the mean outcome and therefore tighter bounds. It seems to me that SE are popular because the give the tightest bounds and thus the impression of preciseness üòÅ.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#prediction-interval-pi",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#prediction-interval-pi",
    "title": "Model uncertainty assessment",
    "section": "Prediction interval (PI)",
    "text": "Prediction interval (PI)\nIf we are interested in model predictions, we generally also want to know how much uncertainty is associated with those predictions. The difference between a CI and a prediction interval (for some reason, almost never abbreviated as PI - but I‚Äôm doing it here to make my typing life easier üòÅ) is that the latter also accounts for variation around the estimated mean.\nIt is important to be aware of the fact that prediction intervals are not the same as confidence intervals. The latter quantifies the uncertainty in model parameters, e.g., the bi in a regression model. Since those bi have uncertainty, the model predictions for the expected observation has uncertainty. However, each real observation has additional scatter around the expected value. This additional uncertainty needs to be factored in when trying to make predictions for individual outcomes. Thus, PI are always wider than CI - sometimes substantially so.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#standard-deviation-sd",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#standard-deviation-sd",
    "title": "Model uncertainty assessment",
    "section": "Standard deviation (SD)",
    "text": "Standard deviation (SD)\nStandard deviation is a measure that tries to assess the spread in the data itself. Thus, it is closely related to the prediction interval. SE and SD are related through SE = SD/\\(\\sqrt(N)\\), where \\(N\\) is the sample size. What that means in practice is that as you get more data, your estimates of the mean (or in general population average outcome) become more precise. However, the actual variability in the data will likely not change much, so the SD remains similar.\nThis is the same idea for CI and prediction intervals. As you get more data, your CI generally shrink. But your prediction intervals likely will not, since the spread in the data will probably not change much as you get more data.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#credible-interval",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#credible-interval",
    "title": "Model uncertainty assessment",
    "section": "Credible interval",
    "text": "Credible interval\nIf one uses Bayesian methods for data analysis, the equivalent of a confidence interval is a credible interval. Credible intervals are sometimes - unfortunately - also abbreviated as CI, sometimes CrI. A credible interval is essentially the Bayesian equivalent to a confidence interval. It informs you about the uncertainty in the population estimate.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#posterior-prediction-interval",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#posterior-prediction-interval",
    "title": "Model uncertainty assessment",
    "section": "Posterior prediction interval",
    "text": "Posterior prediction interval\nThe posterior prediction interval is the equivalent of the prediction interval discussed above, only computed in a Bayesian framework. It can be interpreted pretty much the same as a frequentist prediction interval.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#parametric-methods",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#parametric-methods",
    "title": "Model uncertainty assessment",
    "section": "Parametric methods",
    "text": "Parametric methods\nIf you make the - explicit or implicit - assumption that your outcome follows some distribution (often normal for continuous outcomes), you can use built-in routines to quickly calculate the above-described quantities. Almost all software that fits models will give you these estimates.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#bootstrapping",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#bootstrapping",
    "title": "Model uncertainty assessment",
    "section": "Bootstrapping",
    "text": "Bootstrapping\nFor more complex models or approaches, there might not be a readily available method to compute uncertainty measures. A general approach to produce confidence or prediction intervals is with a sampling method that is very similar in spirit to cross-validation, namely by performing bootstrapping.\nThe idea for bootstrapping is fairly straightforward. Bootstrapping tries to imitate a scenario in which you repeated your study and collected new data. Instead of having actual new data, the idea is that the existing data is sampled to form a new ‚Äúdataset‚Äù, which is then fit. Sampling is performed with replacement to obtain a sample the size of the original dataset. Some observations now show up more than once, and some do not show up. For each such sample, you fit your model and estimate parameters. You will thus get a distribution of parameter estimates. From those distributions, you can compute confidence intervals, e.g., the usual 95% interval. For each fit, you can also predict outcomes and thus obtain a distribution of prediction outcomes (see next).\nLike cross-validation, the bootstrap method is very general and can be applied to pretty much any problem. If your data has a specific structure, you can adjust the sampling approach (e.g., if you have observations from different locations, you might want to sample with replacement for each location.) Limitations for the bootstrap are that you need a decent amount of data for it to work well, and since you are repeating this sampling procedure many times, the procedure can take a while to run.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#p-values",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#p-values",
    "title": "Model uncertainty assessment",
    "section": "P-values",
    "text": "P-values\nSo what about the p-value? This quantity doesn‚Äôt measure uncertainty directly, though it is in some way related to it. p-values are sometimes useful for hypothesis testing. But in my opinion, p-values are overused, generally not too meaningful, and can most often be skipped (though sometimes one needs them just to make reviewers happy). I‚Äôm thus not talking about them further.",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#footnotes",
    "href": "content/model-assessment/uncertainty-assessment/uncertainty-assessment.html#footnotes",
    "title": "Model uncertainty assessment",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm sure you are wondering where that 89% number comes from, and why I‚Äôm not saying 95%. I‚Äôm following Richard McElreath‚Äôs example from his Statistical Rethinking book. The main point being that any number is arbitrary. 95% (and it‚Äôs associated p-value of 0.05) has emerged as a standard - and to avoid arguing with reviewers and colleagues, I do often use 95% in publications. But it‚Äôs a totally arbitrary standard, and there is no one right or wrong number. For some scenarios, 80% might be good enough, for others you might want to have 99%. You can of course use 95%, but be aware that there is nothing special about that number, other than that everyone else is also using it.‚Ü©Ô∏é\nI‚Äôm purposefully fuzzy here. The problem with CI in a frequentist framework is that their technical definition is rather confusing. According to Wikipedia: Given a confidence level X (95% and 99% are typical values), a CI is a random interval which contains the parameter being estimated X% of the time. You‚Äôll find equally confusing definitions in other places. Most folks actually interpret CI as Bayesian credible intervals (see below) when they talk about it, even if they do their statistical analysis in a frequentist framework.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "9. Model Assessment",
      "Model uncertainty"
    ]
  },
  {
    "objectID": "content/model-fitting/basic-statistical-models/basic-statistical-models.html",
    "href": "content/model-fitting/basic-statistical-models/basic-statistical-models.html",
    "title": "Basic Statistical Models",
    "section": "",
    "text": "Overview\nIn this unit, we will discuss a few basic, common models.\n\n\nLearning Objectives\n\nLearn about several types of basic models.\nUnderstand the relation between basic models and statistical tests.\n\n\n\nIntroduction\nThere are lots of models out there. It is fair to say that nobody is familiar with all of them. There are however several often-used types of models that everyone doing data analysis should be familiar with. These models are easy to implement and often surprisingly useful and powerful.\nYou are likely already familiar with these models. In fact, at some point you probably have taken - or should if haven‚Äôt - a course just on those models.\n\n\nSimple models for continuous outcomes (regression)\nThe most common model type applied to continuous outcomes is a linear regression model. This model is covered in many of the course materials we have been using (and of course other places).\nAdvanced Data Science ‚Äì which used to be part of the old IDS ‚Äì has a chapter on it. ISLR also has a chapter on the topic. So does HMLR.\nThis is such a fundamental type of model and is thus covered in a lot of resources. There are plenty more available, including the good old Wikipedia. Find some you like and read/learn enough about these models that you feel comfortable understanding conceptually what they are used for and how they work.\n\n\nSimple models for categorical outcomes (classification)\nFor categorical outcomes with 2 categories (also called binary or dichotomous outcomes), logistic regression is the most common approach. You might be wondering why this method is called logistic regression even though it is used for classification. This is because logistic regression predicts the probability ‚Äì a numeric value between 0 and 1 ‚Äì of having the outcome. These predicted probabilities are then turned into yes/no predictions (i.e., classifications) using a cut-off, usually (but not always) a probability of 0.5. Probabilities below that cut-off are classified as 0/not present/no and above that as 1/present/yes.\nTo learn about logistic regression, or refresh your knowledge if you have previously been exposed to those models, read through the Logistic Regression chapter in HMLR. This reading goes into maybe more detail than you want or need at this point. So just make sure you go through it to get the main points of what logistic models are all about. We‚Äôll come back to some of the other topics discussed there later.\nLogistic models are also covered in the Classification chapter in ISLR. Read through the first part of this chapter in enough detail to get the overall concepts. You can also go through the sections following the logistic regression section to learn about some additional methods for classification. We‚Äôll revisit some of those types of models later.\nOf course, you can also find lots of additional resources online describing logistic regression models. If you find a good one, let me know.\n\n\nGeneralized linear models\nBoth linear and logistic models belong to a class of models called generalized linear models (GLM). Those types of models allow you to fit outcomes of specific types, for instance, if you have outcomes that are counts (integers), you can use Poisson regression, or if you have continuous outcomes that are all positive, you could use a Gamma regression.\nGLMs all have the same structure: namely there is a linear combination of predictor variables (e.g., \\(b_1*age + b_2*height\\)) and those are connected to the outcome through what is called a link function.\nGLM are very commonly used. Among GLM, linear and logistic regression models are by far the most commonly used ones. Other models that assume specific distributions of the outcome, e.g.¬†a Poisson distribution, can be accommodated by choosing the appropriate link function, which connects a linear combination of predictor variables with the outcome. GLM models are fast and easy to fit (using e.g.¬†the glm function in R), fairly easy to interpret, and often provide performance that is hard to improve upon with more complicated models, especially if the data is on the small side. To prevent overfitting, variable/feature selection or regularization approaches are often used. GLM assume a specific relation between predictors and outcome (e.g.¬†linear, logistic) and as such might not perform well on data that does not show the assumed pattern.\nAt the end of the Classification chapter in ISLR, there is a nice brief discussion of GLMs, I suggest you read through that in enough depth to get the overall ideas, but there is no need to go through all the math.\n\n\nKNN - another simple model worth mentioning\nAnother model that you might have already encountered (e.g., in the ISLR reading) is one called k-nearest neighbors (KNN). The idea is very simple: For any new observation, you compare the values of the predictor variables with those in your data. You then predict the outcome of the new observation to be the average of the outcomes of the K observations whose predictors most closely resemble the predictors of the new observation.\nAs an example for a continuous outcome (regression), if you want to predict height as the outcome and you have age and weight as the predictors, you would take an observation, look at the age and weight values for that observation/individual and compare to the K individuals in your dataset with the closest values (we won‚Äôt go into detail how ‚Äúclose‚Äù is defined). Then you take the average height of those K closest individuals and that‚Äôs your prediction for the new observation.\nThe same idea can be applied to categorical outcomes (classification). Say you wanted to predict sex instead of height. You again looked at the K individuals whose predictor values (here age and weight) are closest, and use the majority to predict the outcome for the new observation. (Say K=5, and 3 of the 5 closest individuals are male. Then you would predict the new observation to be male.)\nKNN often perform well, but they are not very ‚Äúportable‚Äù. For other models, once you trained/fit a model, you can ‚Äútake it with you‚Äù and apply the model to new data, leaving the old data you used for model building behind. KNN are a somewhat strange since the data is the model. All the data is used to predict new observations by comparing them to existing ones and predicting outcomes based on closeness of predictor variables. Which means you always need to have the data to predict new outcomes, often making it not too useful in practice. You also don‚Äôt learn too much in terms of inferential insight. Still, it is a useful model to know about, and it works for both continuous and categorical outcomes. It is also commonly used for imputation of missing values. In that situation, you treat the predictor you want to impute as the outcome and use the remaining predictors as the data. KNN are described in the K-Nearest Neighbors chapter in HMLR and also show up in chapters 2 and 3 of ISLR and the Cross-validation and Examples of Algorithms chapters of IDS. Take a look at any of those resources if you want to learn a bit more about KNN.\n\n\nSimple models for no outcomes\nAs we discussed previously, if you have an outcome variable, your data is called labeled data and the methods you use are called supervised methods/models. Such data with one (or several) outcome variable(s) are most common. But as previously discussed, sometimes you might have data without clear outcomes (unlabeled data) and you still want to determine if there is some pattern in your data. This calls for unsupervised methods.\nThere isn‚Äôt really one single, standard go-to method for unsupervised learning ‚Äì in contrast to the very common linear and logistic models for supervised learning. There are often used methods, such as Principal Component Analysis (PCA) (which falls into a larger group of methods called dimensionality reduction methods) and a group of methods called clustering methods (the most common among these are probably k-means clustering and hierarchical clustering). Since most of you will work on labeled data using supervised methods most of the time, we won‚Äôt go much into unsupervised methods. If you want to learn a bit more, check out for instance the Unsupervised Learning chapter of ISLR. There are also chapters on PCA and different types of clustering in HMLR.\n\n\n\nChoosing K is an important task. 3 sometimes works. Source: xkcd.com.\n\n\n\n\nThe zoo of statistical tests\nIn some of your (bio)statistics classes, you likely came across a variety of statistical tests, e.g., t-test, Wilcoxon, Kruskal-Wallis, and others from the huge collection of tests. Each test makes certain assumptions and is most adequate for certain types of data. For instance you might remember hearing about parametric tests (which are most applicable if the underlying data has a certain distribution, usually a normal distribution) and non-parametric tests, which make fewer assumptions about the distribution of the data, but don‚Äôt have quite the same statistical power. Those more traditional statistical tests certainly have their uses (though are quite often overused and misused). We just don‚Äôt have time to cover them.\nPretty much all of those tests have equivalent formulations as multivariable models (either a GLM type as described here, or others that we‚Äôll cover later). The advantage of more general models, such as GLM, is that they easily allow for as many predictor variables as you want to add. Also, once you understand the general setup of GLM (and other models), this understanding transfers to other methods. For statistical tests, they are often taught as ‚Äúif the data look like this, then test like that‚Äù without any underlying fundamental and transferable understanding. Overall, I am not a big fan of the way many intro (bio)stats courses are still taught these days.\nBecause of that, and since this course is about modern data analysis, we will not cover any classical statistical tests and instead start with multivariable, GLM-type models and then move on to more complex/modern machine learning models.\nIf you never thought about the relation between classical statistical tests and GLM-type models, you might want to check out Common statistical tests are linear models by Jonas Kristoffer Lindel√∏v which explains the relation between the two in a good bit of detail. Statistical tests as linear models by Steve Doogue is also good, it is based on the original text by Lindel√∏v and covers some topics in more detail. Both resources show examples in R, and have links to further materials. At some point in your career as a data analyst, you might need to use or interpret classical statistical tests, and having a general idea that they can map to GLM-type and other models might be helpful.\n\n\nFurther reading\nChapters 3 and 4 of ISL discuss linear and logistic models. So do chapters 4 and 5 of HMLR.\nFor another source that discusses almost all the models just mentioned, each one very briefly, see the Examples of Algorithms chapter in IDS.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Fitting Basic Models"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-introduction/model-fitting-introduction.html",
    "href": "content/model-fitting/model-fitting-introduction/model-fitting-introduction.html",
    "title": "Model fitting introduction",
    "section": "",
    "text": "This unit provides a brief introduction and overview to the topic of fitting models to data.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model Fitting Introduction"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-introduction/model-fitting-introduction.html#footnotes",
    "href": "content/model-fitting/model-fitting-introduction/model-fitting-introduction.html#footnotes",
    "title": "Model fitting introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor some inexplicable, dumb reason, it is convention to capitalize Bayesian but not frequentist.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Model Fitting Introduction"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-r/model-fitting-r.html",
    "href": "content/model-fitting/model-fitting-r/model-fitting-r.html",
    "title": "Fitting (Simple) Statistical Models in R",
    "section": "",
    "text": "In this unit, we will discuss common approaches and packages that are useful for fitting statistical models in R.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Fitting Models in R"
    ]
  },
  {
    "objectID": "content/model-fitting/model-fitting-r/model-fitting-r.html#metrics-with-tidymodels",
    "href": "content/model-fitting/model-fitting-r/model-fitting-r.html#metrics-with-tidymodels",
    "title": "Fitting (Simple) Statistical Models in R",
    "section": "Metrics with tidymodels",
    "text": "Metrics with tidymodels\nThe yardstick package implements a lot of different metrics in the tidymodels framework. For details, see the yardstick package website and the Metric types vignette.\nYou can also define your own metrics, as described in this article on custom metrics.",
    "crumbs": [
      "Content",
      "8. Model Fitting",
      "Fitting Models in R"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-introduction/model-improvement-introduction.html",
    "href": "content/model-improvement/model-improvement-introduction/model-improvement-introduction.html",
    "title": "Model Improvement Introduction",
    "section": "",
    "text": "In this module, we will discuss ways one can try to improve model performance.\n\n\n\nSource: xkcd.com",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Model Improvement Introduction"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-introduction/model-improvement-introduction.html#footnotes",
    "href": "content/model-improvement/model-improvement-introduction/model-improvement-introduction.html#footnotes",
    "title": "Model Improvement Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI‚Äôm again using the term performance in the narrow sense of a model that does well (however quantified) on the metric you chose (based on scientific considerations).‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Model Improvement Introduction"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html",
    "title": "Regularization",
    "section": "",
    "text": "In this unit, we will cover the concept of regularization. We‚Äôll also briefly mention a few related approaches.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#ridge-regression",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#ridge-regression",
    "title": "Regularization",
    "section": "Ridge regression",
    "text": "Ridge regression\nOne way to choose the function that penalizes the predictors is to weigh each predictor by the predictor‚Äôs coefficient squared. Choosing the penalty term as the square of the coefficient is called ridge regression (AKA L2 regularization, Tikhonov regularization, weight decay, and potentially lots of other names). This leads to the cost function:\n\\[C = SSR + \\lambda \\sum_j^p b_j^2.\\]\nThe parameter \\(\\lambda\\) decides the balance between the goodness of fit (low SSR) and the penalty for having large coefficients. Instead of trying different subsets as above and picking the best based on lowest CV performance, we now try different values of \\(\\lambda\\) and pick the model with the lowest (cross-validated) value for our performance measure, C. The parameter \\(\\lambda\\) is often referred to as the tuning parameter or the penalty. Sometimes \\(\\lambda\\) is also called a hyperparameter of the model, which just means that the best value of \\(\\lambda\\) cannot be found by fitting the model one time only.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#lasso",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#lasso",
    "title": "Regularization",
    "section": "LASSO",
    "text": "LASSO\nAn alternative is to penalize the coefficients by their absolute value, namely using this cost function:\n\\[C = SSR + \\lambda \\sum_j^p |b_j|.\\]\nThis method is called L1 regularization or the Least Absolute Shrinkage and Selection Operator (LASSO). One nice feature of LASSO (which ridge regression does not have) is that coefficients may go to 0. That means the predictor has been dropped from the model, similar to the subset selection approach described previously. One can think of the LASSO as an efficient approach for performing subset selection. It is not quite equivalent though, since, in the LASSO, the predictors that remain might have been shrunk in their impact due to the regularization penalty.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#elastic-net",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#elastic-net",
    "title": "Regularization",
    "section": "Elastic net",
    "text": "Elastic net\nOne can also combine ridge regression and LASSO into an approach called elastic net, which has a cost function that is the combination of the previous two, namely:\n\\[ C = SSR + \\lambda \\left( (1-\\alpha) \\sum_j^p b_j^2 + \\alpha \\sum_j^p |b_j|\\right).\\]\nNow one needs to try different values for \\(\\lambda\\) (called the penalty parameter) and \\(\\alpha\\) (called the mixture parameter) to determine the model with the best (cross-validated) performance. \\(\\lambda\\) determines the overall weight given to the penalty factor, while \\(\\alpha\\) determines how the penalty should be distributed between the 2 alternative terms. There are also a few variants of this method, such as relaxed elastic net or adaptive elastic net which you can look into if you are interested but we won‚Äôt discuss here.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#tuning-a-regularization-model",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#tuning-a-regularization-model",
    "title": "Regularization",
    "section": "Tuning a regularization model",
    "text": "Tuning a regularization model\nDepending on the kind of regularization model you fit, you have to determine 1 or 2 extra parameters (\\(\\lambda\\) and \\(\\alpha\\)). These parameters are called tuning parameters (or hyperparameters) and it is the first time we see a model that has them. Most complex machine learning models have such tuning parameters, and determining good values for those is part of the model fitting/training process. We‚Äôll talk about that in the next unit.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#bayesian-regularization",
    "href": "content/model-improvement/model-improvement-regularization/model-improvement-regularization.html#bayesian-regularization",
    "title": "Regularization",
    "section": "Bayesian Regularization",
    "text": "Bayesian Regularization\nIf one fits models in a Bayesian framework, one needs to supplies priors for each parameter. If one provides informative priors that constrain the possible values of the model parameters based on prior scientific information, it also leads to regularization and generally reduces overfitting.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Regularization"
    ]
  },
  {
    "objectID": "content/model-improvement/model-improvement-tuning/model-improvement-tuning.html",
    "href": "content/model-improvement/model-improvement-tuning/model-improvement-tuning.html",
    "title": "Model Tuning",
    "section": "",
    "text": "Overview\nIn this unit, we will cover the concept of parameter/model tuning and training.\n\n\nLearning Objectives\n\nLearn what parameter tuning/model training are.\nBe able to implement training/tuning in R.\n\n\n\nIntroduction\nWe discussed at length that one (but not the only) goal of model fitting is to find a model that has good performance when applied to new/different data.\nFor linear, logistic, and similar models (generalized linear models), we discussed the idea of subset/feature selection that can help determine a good model. For different sub-models, we fitted the model to some of the data and evaluated model performance on another part of the data using cross-validation. The model with the best cross-validated performance across all sub-models is then designated as the best (at least with regard to our chosen performance metric).\nWe then discussed regularization, which tries to solve a problem similar to subset selection, namely preventing a model that is too complex and thus overfits. In the regularization approach, one does not compare sub-models with different predictor variables. Instead, all predictors are present, and one (or two) parameters (the regularization or penalty parameters, we called them \\(\\lambda\\) and \\(\\alpha\\)) are varied to influence model complexity. For each value of that parameter, the model is evaluated through cross-validation, and the \\(\\lambda\\) which produces the model with the best performance is chosen (or sometimes one picks a somewhat larger \\(\\lambda\\) to further prevent potential overfitting).\nThis approach of taking a model parameter and evaluating models for different parameter values is called model/parameter tuning. These model-specific tuning parameters are often also referred to as hyperparameters. Simple models, like linear or logistic regression, do not have any parameters that can be tuned. However, more complicated models, which we will discuss soon, generally have one or more tuning parameters. Very flexible models, such as neural nets used in artificial intelligence tasks, can have thousands or even more parameters that need tuning. For any models with tuning parameters, it is essential to tune the parameters/train the model. Without it, the model will likely not perform very well.\n\n\nModel training/parameter tuning recipe\nTraining the model by tuning its parameters follows a general approach that is conceptually the same for all models. You need to go through these steps:\n\nSelect some values for your tuning parameters.\nUsing cross-validation, fit model to a part of the data (the analysis portion), evaluate model performance on the remainder of data (the assessment portion).\nSelect new values for your tuning parameters, and repeat step 2.\nKeep going until you hit some stopping criterion, e.g., you tried all parameter combinations you wanted to try, you hit the maximum number of tries, or you hit the maximum amount of time you allocated for this parameter tuning. (Or you found the absolute best tuning parameter values, but that‚Äôs only likely for simple models with few tuning parameters).\nPick your best model as the one with the parameter values that produced the overall best model performance.\n\nYour final model consists of both the type of model and the values of the parameters.\n\nYou might (or might not) have wondered if this procedure of repeatedly refitting the model for different values of the tuning parameters (hyperparameters) and re-fitting the model for different values of the model parameters (e.g., for the coefficients \\(b_i\\) for a linear or generalized linear model) isn‚Äôt more or less the same. Both conceptually and in practice, those approaches are quite similar. For GLM, there are differences in how things happen, namely the \\(b_i\\) can be determined in a single step, without the need for trial and error. But for other models, such as some of the ML models we‚Äôll explore, tuning parameters and internal parameters associated with the model might both need to be determined by iterative procedures. Thus, while one can try to distinguish between model and tuning parameters (see e.g., Jason Brownlee‚Äôs blog post, which discusses that point a bit more), this is often fuzzy. And the usual caveat applies: The terminology is not consistent, and what some people might call a model parameter might be called a tuning parameter by others. The good news is that in practice it doesn‚Äôt matter much what you call a specific parameter. Some can be tuned and you can chose to do so (or not and fix it), others cannot be tuned and are determined internally.\n\n\n\nSearching the tuning parameter space\nThe problem of trying a lot of different tuning parameter values to find the ones that lead to the best performance is very similar to the problem of trying to test a lot of different sub-models during subset selection to find the best model. Not surprisingly then, the procedures to perform the search over parameter space are similar to the ones one can use to search over subset/sub-model space. The most basic one for subset selection was to try every possible model (exhaustive search). This works in principle for tuning parameters as well, but only if the tuning parameters are discrete. For continuous tuning parameters (e.g., \\(\\lambda\\) in regularization), it is impossible to try all values. One instead chooses discrete values for the parameter between some lower and upper limits and then searches that grid of parameters. This is called grid search. The advantage of a grid search is that you know you tried every combination of parameters in your grid.\nFor instance, if you have 2 continuous tuning parameters and 1 categorical tuning parameter with 3 categories, if you chose 10 discrete values for the 2 continuous parameters each, your grid would be 10x10x3. In this case, you would need to evaluate the model for 300 different tuning parameter values. That‚Äôs not too bad. But you can see that one problem with this approach is that as the number of tuning parameters increases, or if you want to try many different discrete values (e.g., 100 instead of 10 for each continuous parameter), the number of times you need to run the model increases rapidly. That‚Äôs the same problem as the exhaustive search for subset selection.\nTo solve this problem, there are a few major methods:\n\nOne can efficiently choose parameter values to estimate ‚Äì instead of searching the entire grid, you can use a sampling method to find a smaller grid that covers the same space in the most efficient way. The most well-known algorithm for this is probably Latin hypercube sampling.\nMethods similar to those mentioned for subset selection can be applied, e.g., one can use Genetic Algorithms, Simulated Annealing, Racing Methods, or many other optimizer routines for tuning. While it is, in principle, possible to write your own code that implements the tuning procedure with whatever method you want to use, for most instances, it is easier to use pre-existing methods.\n\n\n\nTuning with R\ntidymodels, and its package tune currently have a few different algorithms for searching the tuning parameter space implemented. Grid search is the main one, but the package also implements Iterative Bayesian optimization. The finetune which is not yet on CRAN implements Simulated Annealing and Racing Methods. To learn more about those, see chapters 12-14 in Tidy Modeling With R.\nThe mlr3/mlr package I keep mentioning also has algorithms to tune parameters, including some that are not available (yet) in tidymodels. For tuning in mlr see e.g.¬†tuning tutorial and advanced tuning tutorial. For this course, we‚Äôll focus on what‚Äôs available in tidymodels, but if you ever need to do some major parameter tuning/model optimization, checking out mlr might be worth it (or implementing your own with tidymodels, which is possible.)\n\n\nFurther comments\nOnly if your model has very few tuning parameters and your data is manageable in size can you find the absolute best parameter values in a reasonable amount of time. More likely, you‚Äôll find parameter values that give you a close-to-optimal model.\nAt times, cross-validation might take too much time, and you might have to use a computationally faster method, such as AIC or similar, to try to estimate model performance on future data. That‚Äôs not ideal, you might want to consider other approaches first (fewer parameter evaluations, faster computer, running things in parallel‚Ä¶).\nAny process that adjusts the model repeatedly based on prior fits to data has the danger of overfitting, even if you try to guard against this using approaches such as CV. Thus, sometimes less tuning might actually give you a more robust/generalizable model.\nThe more tuning parameters in your model, the more data you need to be able to train the model properly. If you have a mismatch between the amount of data and model complexity, you are likely going to overfit. This is why complex models such as neural nets need vast amounts of data (millions or billions of observations).\n\n\nFurther information\nMost relevant and maybe good to visit next are chapters 12-14 in Tidy Modeling With R, which discuss the general tuning process, and then explain how to do grid search and iterative search using tidymodels.\nSection 2.5.3 of HMLR provides a very short section on tuning. ISLR mentions tuning in various places but doesn‚Äôt describe it in a dedicated section. IDS mentions it in the Machine Learning chapters but also does not have a dedicated section on the topic.",
    "crumbs": [
      "Content",
      "10. Model Improvement",
      "Model Tuning"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html",
    "href": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html",
    "title": "Motivating Data Analysis Examples",
    "section": "",
    "text": "We‚Äôll get into the details of what data analysis is and how to do it very soon. But to start out, I want to provide a few hopefully inspiring and motivating examples of data analysis and use. You‚Äôll be doing similar projects yourself soon! In fact, you will be reproducing several analyses below.",
    "crumbs": [
      "Content",
      "2. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#ted-talk-1",
    "href": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#ted-talk-1",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 1",
    "text": "TED talk 1\nArguably one of the best communicators of public health data was the late Hans Rosling. He has several great entertaining and enlightening TED talks. Here is one of them: A statistics talk that has been viewed millions of times!\n\n\n\n\nYou can watch more TED talks by Hans Rosling. The software he uses to illustrate data is called Gapminder and available online on the Gapminder website.",
    "crumbs": [
      "Content",
      "2. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#ted-talk-2",
    "href": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#ted-talk-2",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 2",
    "text": "TED talk 2\nIn this entertaining and empowering talk, Talithia Williams argues for the utility of careful assessment of data in your personal life.",
    "crumbs": [
      "Content",
      "2. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#ted-talk-3",
    "href": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#ted-talk-3",
    "title": "Motivating Data Analysis Examples",
    "section": "TED talk 3",
    "text": "TED talk 3\nIn this talk, Sebastian Wernicke gives some cautionary suggestions about trusting data too much and argues for critical assessment as an important part of any data analysis and decision process.",
    "crumbs": [
      "Content",
      "2. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#more-ted-talks",
    "href": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#more-ted-talks",
    "title": "Motivating Data Analysis Examples",
    "section": "More TED talks",
    "text": "More TED talks\nThe TED website has a large number of data related talks. Feel free to explore as much as you like. If you find especially good ones, post them on the class discussion board.",
    "crumbs": [
      "Content",
      "2. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#tidytuesday",
    "href": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#tidytuesday",
    "title": "Motivating Data Analysis Examples",
    "section": "TidyTuesday",
    "text": "TidyTuesday\nAn interesting initiative started by Thomas Mock is TidyTuesday, where every Tuesday a new dataset is released and individuals are encouraged to analyze it. Later in this course, you will be participating in some TidyTuesdays yourself. Tidy Tuesday has by now been running for many years.\nIndividuals who participate in TidyTuesday often post their projects online and link to it on social media. We will look at an example by David Robinson (aka drob). David does screencast recordings showing him analyze the data.\nYou don‚Äôt need to watch all of the almost 2 hour long recording üòÑ, but do watch the first few minutes to see him coding/analyzing in real-time. You might just get hooked and keep watching. You might not be familiar with some or most of the R coding, but you will still get the overall idea of how he does the analysis. And his thinking out loud is informative as well. See for yourself:\n\n\n\n\nIf you like the ability to look over his shoulder while he does data analysis, he has a bunch more of those screencast recordings.\nThere are others who live-stream data analysis, including competitions, on platforms such as Twitch. Check out Jesse Mostipak, aka Kiersi or Nick Wan if interested.\nDavid does his work reproducibly and shares the code as RMarkdown files. For specific files, see the links provided in the video description. For convenience, download the code for his analysis. Just save the file after it opens in your browser. It should be named 2021_05_18_salary_survey.Rmd. After you have it saved, you can open it in Positron (or RStudio).\nOnce the file is open, you might see at the top of the file a message suggesting that you need to install several packages to run the file. Do so. If you don‚Äôt get that auto-suggestion, you need to install the packages by hand. Every package that is called with the library() command needs to be installed first.\nThis is a good example of an almost works reproducible example. It‚Äôs quite rare that things work completely without adjustments on different machines. How to do that best is a huge topic in data analysis (e.g., for drug companies that need full reproducibility for licensing). In this example, I had to make the following adjustments:\n\nChange output: html_output to output: html_document in the YAML section.\nInstall the ranger package, which is not listed under the library statements but is needed.\n\nWith those changes, I was able to run the whole script. If you are still missing a package, you will get an error message. If things work out, the whole script will run a while (depending on the speed of your computer), and you should get a document/report which reproduces the complete analysis he did in the video! If things don‚Äôt work out, post your error message to Slack so someone can help troubleshoot. One potential problem could arise from the fact that David uses parallel computing to make things run faster. If you get an error message, you can try commenting out or deleting the line of code that says doParallel::registerDoParallel(cores = 4) and see if it works without it. It worked ok with that line of code on my Windows machine, but might not always work. (If on Windows, you might be asked about firewall network permissions. That‚Äôs a quirk of how parallel computing works on a single machine. Just say ‚Äúyes‚Äù to both public and private networks.)\nAgain, most or all of the R code might not make sense to you just yet. That‚Äôs ok. The main point is to show you a nice example of a data analysis done in a way that others (you) can reproduce with just a few clicks. As part of this class, you will be producing those kinds of reproducible R Markdown files (well, we‚Äôll be doing Quarto files, which is the improved successor of R Markdown).",
    "crumbs": [
      "Content",
      "2. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#research-project",
    "href": "content/module-data-analysis/data-analysis-motivation/data-analysis-motivation.html#research-project",
    "title": "Motivating Data Analysis Examples",
    "section": "Research Project",
    "text": "Research Project\nFully open and reproducible analyses are also becoming common in academic research, with authors publishing everything needed to reproduce their paper and results. In my research group, we try to adhere to this approach as much as possible. As an example, I‚Äôm sharing a project/paper by Dr.¬†Brian McKay, a former grad student in my group. You can read the published paper. All materials to reproduce the full project and all results are available as supplementary material. Take a look at the Usage Notes section on the website.\nNext, click on Download Dataset, and unzip the file. You should end up with a folder called ProcSocB Supplemental Material.\nInside that folder you can see subfolders containing all the data and code to fully reproduce the analysis and manuscript. Open the main folder as a project/workspace in Positron. To reproduce the manuscript, go into the Manuscript folder, open Manuscript.Rmd. You‚Äôll see some code at the top which loads various datasets containing results. Further down, you‚Äôll find the text of the manuscript, combined with simple R commands that dynamically place numbers and figures into the text. Re-create the manuscript by clicking on the knit button. There might be error messages related to missing packages. If that happens, install those packages (e.g.¬†‚Äòbookdown‚Äô). If you get stuck, post your problem to the discussion boards.\nFeel free to explore further by opening various R scripts and running them. The Usage Notes section on the website where you downloaded the zip file, or the Supplementary-Material.docx document explain what the different pieces do and how to use them.\nAs you can tell, this is more complex and complete compared to the single R Markdown file example above. The details on how you structure your project and folders/files are up to you, but they should provide a way to organize things so it‚Äôs easy for you and others to understand what is going on where.\nThis example hopefully gives you an idea how nice and easy it is to write a whole paper with just a few clicks of a button. It is a combination of R scripts and R Markdown files (in this case a version called bookdown), which allow a fully automated generation of a complete data analysis and manuscript, including all figures, tables, references, etc. This is an incredibly efficient workflow and one you‚Äôll learn and use in this class. For instance if there was a mistake somewhere in the data, instead of going through the manuscript and manually changing numbers, one could just re-run the whole code, and it will produce new results based on the updated data in a fully automated manner. It‚Äôs a bit more work upfront until you get the hang of it, but once you have it set up, it can save an enormous amount of time and avoid potential copy & paste mistakes.",
    "crumbs": [
      "Content",
      "2. Data Analysis Introduction",
      "Motivating Examples"
    ]
  },
  {
    "objectID": "content/module-data-analysis/data-analysis-structure/data-analysis-structure.html",
    "href": "content/module-data-analysis/data-analysis-structure/data-analysis-structure.html",
    "title": "Structure of a Data Analysis",
    "section": "",
    "text": "Setting paths\n\n\nFurther Resources"
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html",
    "href": "content/module-data-presentation/presenting-results-overview.html",
    "title": "Presentation Overview",
    "section": "",
    "text": "In this unit, we will discuss the main ways to show your data and results and discuss some ways to do that efficiently.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Presenting Results Overview"
    ]
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html#figures",
    "href": "content/module-data-presentation/presenting-results-overview.html#figures",
    "title": "Presentation Overview",
    "section": "Figures",
    "text": "Figures\nA well-done figure is able to convey a lot of complex information in an intuitive and understandable format. On the flip side, a poorly thought-out figure can be utterly confusing even if it is presenting something simple. Worse, figures can easily be (ab)used to mislead the reader. Your goal is to make sure that your figures are great-looking, easy to understand and not misleading. We‚Äôll go over some more details relating to figures in subsequent units.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Presenting Results Overview"
    ]
  },
  {
    "objectID": "content/module-data-presentation/presenting-results-overview.html#tables",
    "href": "content/module-data-presentation/presenting-results-overview.html#tables",
    "title": "Presentation Overview",
    "section": "Tables",
    "text": "Tables\nTables allow you to easily present summaries of your data. Many epidemiological and other papers have a Table 1 which summarizes the data by its characteristics. For instance if you had a human cohort, this summary table will likely list the number and percentages or ranges of individuals based on gender, age, BMI, smoking status, etc.\nTables are also often used to present results of your data analysis. In my opinion, figures are often better, but if the results are simple or there are a lot of numbers that need to be shown, tables can work better at times. We‚Äôll go over some more details on how to generate tables in R in a subsequent unit.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Presenting Results Overview"
    ]
  },
  {
    "objectID": "content/module-data-presentation/visualization-in-r/visualization-in-r.html",
    "href": "content/module-data-presentation/visualization-in-r/visualization-in-r.html",
    "title": "Visualization in R",
    "section": "",
    "text": "Visualization and graphics in R\nVisualization and graphics are strengths of R. There are currently three main ‚Äì and unfortunately incompatible ‚Äì ways of making graphs in R.\nThe first approach is to use the base R commands, with the plot function as the main workhorse, and related functions for more specialized plots (e.g.¬†boxplot). You can make a lot of very good graphs using base R, but it often takes many lines of code and a good bit of fiddling.\nThe second approach is using the lattice package, and extensions that build on this package. lattice seems to have largely ‚Äúlost‚Äù against option number 3 and I don‚Äôt see it used much anymore these days.\nThe third, and currently most popular approach is to make graphs using the ggplot2 package and various extensions to it. ggplot2 is one of the many R packages written by Hadley Wickham. ggplot2 and its extensions have by now become more or less the standard for good graphics in R. Note that while the package is called ggplot2, people interchangeably refer to it as ggplot or ggplot2. (No one uses ggplot ‚Äúone‚Äù anymore.) I‚Äôll do the same.\nMy recommendation for learning to make plots in R is to focus on ggplot2 and related packages.\nFor most graphs and figures you want for a publication, ggplot2 is probably your best choice. It, and its many add-on packages, product very high quality graphics with not too much code writing required. While you could use ggplot2 exclusively for any figures you make, it is also useful to know a little bit about how to make plots using base R, since they are often very good for quick and dirty plots, especially during the data exploration stage.\n\n\nPracticing ggplot2\nIf you are new to ggplot2, a good and gentle introduction is the set of tutorials in the Visualize Data Posit Recipes section. Similar material, though a little bit more advanced, is covered in a non-interactive form in the Data Visualization, Exploratory Data Analysis and Graphics for Communication chapters of R4DS. A nice step-by-step tutorial can be found in C√©dric Scherer‚Äôs blog post. Another good source are the chapters in the Data Visualization section of IDS. Especially the Data visualization principles chapter is worth a read.\nIf you want to learn a bit more about ggplot2 in general, the ideas behind it, and also how to use it, you can check out Hadley & co-author‚Äôs free online book ggplot2: Elegant Graphics for Data Analysis.\nFor coding/R in general, and making plots in particular, it is generally a learning by doing approach. The most common approach - and one I still use a lot - is to search online for ‚Äúhow do I do X in ggplot‚Äù, and usually you find an example and code that is close enough to what you want so you can adjust it. The tricky bit is knowing what to search for. If you don‚Äôt know the type of plot you want, it is useful to learn a bit about different plots and what they are good for (something that was discussed in the previous unit). A good reference source for how do I do X with ggplot2 is the R graphics cookbook, which is meant to give you quick recipes for common questions and plots.\nFor inspiration and ideas, there is also the R graphics gallery where you can see all kinds of plots made in R, and you can also see the code. They are nicely organized into categories. It‚Äôs definitely worth browsing around.\n\n\nBeyond static graphs\nWhile you will likely always need regular figures for papers, reports, presentation slides, etc., it is becoming increasingly common to have interactive and dynamic visualizations. R has pretty good facilities for that, some in rapid development. The ggvis package is meant to be similar to ggplot2 but allow for interactive and dynamic visualizations. The plotly package also works well with R and allows interactive graphs.\nAnother great, and related set of tools is the htmlwidgets package which allows you to make a lot of very nice, interactive web-based graphs using R and various R packages.\nFor even richer, interactive graphical operations and visualizations, including writing full graphical user interfaces, you can use the shiny package and extensions to it. The shiny gallery has examples of apps. Some of my R packages, e.g.¬†DSAIDE and DSAIRM use Shiny for the interactive graphical interface (and either ggplot2 or plotly to show graphs for different models).\nOften, getting professional-looking interactive figures, dashboards, and other widgets in R is not that difficult. However, since those are all specialized products, they usually require commands and concepts that are somewhat different from basic or tidyverse R, which generally takes some time to learn.\nInteractive graphs and apps are beyond what we‚Äôll cover in this class, but it is something that might be worth learning, and you are certainly welcome to make use of some of these packages and tools for your class project üòÉ.\n\n\nSummary\n\n\nFurther Resources\n\nThe Visualize Data Posit Recipes section has some good worked examples and code.",
    "crumbs": [
      "Content",
      "6. Presenting Data and Results",
      "Visualization in R"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html",
    "href": "content/module-data-processing/assessment-data-processing.html",
    "title": "Assessment - Data Processing",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#setup",
    "href": "content/module-data-processing/assessment-data-processing.html#setup",
    "title": "Assessment - Data Processing",
    "section": "Setup",
    "text": "Setup\nWe‚Äôll also use the familiar group setup. Assign each member in your group an (arbitrary) number (I‚Äôm calling them M1, M2, ‚Ä¶). The order should be different than before so you get to interact with a different group member. Everyone will first work on their own repository. Then M1 will contribute to M2‚Äôs repository, M2 will work on M3, etc. The last person, will work on M1‚Äôs repository. This way, everyone will work on their own and one group member‚Äôs repository.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#part-1",
    "href": "content/module-data-processing/assessment-data-processing.html#part-1",
    "title": "Assessment - Data Processing",
    "section": "Part 1",
    "text": "Part 1\nPart 1 is due by Wednesday.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#finding-data",
    "href": "content/module-data-processing/assessment-data-processing.html#finding-data",
    "title": "Assessment - Data Processing",
    "section": "Finding Data",
    "text": "Finding Data\nPreviously, you did a quick exploration of a dataset that came with an R package (gapminder data inside dslabs package). A lot of datasets can be found inside R packages. For instance, this page lists what is likely only a small fraction. The good and the bad about datasets that come with R packages is that they are often fairly clean/tidy. That‚Äôs unfortunately not how most ‚Äúreal world‚Äù datasets look like. Getting dirty and messy datasets and wrangling them into a form that is suitable for statistical analysis is part of most workflows and often takes a lot of time. We‚Äôll start practicing this here by getting data that might or might not be very clean.\nGo to the CDC‚Äôs data website. Browse through the site and identify a dataset of interest.\nWhich dataset you choose is up to you. I suggest you pick a dataset that has at least 100 observations with 5 different variables, and a mix of continuous and categorical ones. Often, 5 variables means 5 columns. That would be the case in properly formatted data. However, some of the data provided by the CDC is rather poorly formatted. For instance CDC‚Äôs dataset on traumatic brain injury has the same variable (age) in separate columns, and it is also discretized. As we‚Äôll discuss, these are two really bad things you can do to your data, so I recommend staying away from such datasets. There are plenty on that website, so I‚Äôm sure you‚Äôll find one that is suitable and interesting to you.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#getting-the-data",
    "href": "content/module-data-processing/assessment-data-processing.html#getting-the-data",
    "title": "Assessment - Data Processing",
    "section": "Getting the data",
    "text": "Getting the data\nTo get the dataset you selected, it is easiest if you download the file to your computer and place it inside your portfolio repository. Note that in general, you should make each data analysis (or other) project its own GitHub repository, and always use a structure like the one provided in the Data Analysis Template (or something similar). However, for this small exercise and for logistic reasons, you‚Äôll use your portfolio/website repository, and just a single folder. Make a new folder called cdcdata-exercise inside your portfolio repository. Place the data into that folder.\n\nRemember that GitHub doesn‚Äôt like large files. So if you pick a large data file (&gt;50MB), first place it somewhere outside your repository, then reduce it by e.g., writing some R code that selects only a portion of the data. Once it‚Äôs small enough, you can place it into the GitHub repository.\n\n\nWhile you should be able to find data for direct download from the CDC website, sometimes you need to write a bit of code to pull data from a source. This is usually done through an API. R has packages that make this relatively easy. If you ever encounter that situation, search online for instructions. Google/Stackoverflow are your friends to figure out what commands you need to write).",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#exploring-the-data",
    "href": "content/module-data-processing/assessment-data-processing.html#exploring-the-data",
    "title": "Assessment - Data Processing",
    "section": "Exploring the data",
    "text": "Exploring the data\nNow, write code that explores the data. Add a new Quarto document called cdcdata-exercise.qmd to the folder you just created.\nStart by providing a brief description of the data, where you got it, what it contains. Also add a link to the source.\nThen write code that reads/loads the data. As needed, process the data (e.g., if there are weird symbols in the data, or missing values coded as 999 or anything of that sort, write code to fix it.) If your dataset has a lot of variables, pick a few of them (at least 5).\nOnce you have the data processed and cleaned, perform some exploratory/descriptive analysis on this cleaned dataset. Make some summary tables, produce some figures. Try to summarize each variable in a way that it can be described by a distribution. For instance if you have a categorical variable, show what % are in each category. If you have a continuous variable, make a plot to see if it‚Äôs approximately normal, then try to summarize it to determine its mean and standard deviation.\nThe idea is that your descriptive analysis will provide enough information for your classmate to make synthetic data that looks similar, along the lines discussed in the synthetic data module.\nRemember to add both text to your Quarto file and comments into your code to explain what you are doing.\nIn a final step, update the _quarto.yml file and include a menu item for ‚ÄúData Analysis Exercise‚Äù pointing to the new file. Follow the format of the existing entries. Remember to be very careful about the right amount of empty space. Re-create your website and make sure it all works and the new project shows up on the website.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#github-collaboration",
    "href": "content/module-data-processing/assessment-data-processing.html#github-collaboration",
    "title": "Assessment - Data Processing",
    "section": "GitHub Collaboration",
    "text": "GitHub Collaboration\nIf everything works as expected, commit and push your changes to GitHub. Let your team member know that things are ready. We are using the fork + pull-request workflow again. If you forgot, go back to the previous exercise where we used this and follow those detailed instructions.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#part-2",
    "href": "content/module-data-processing/assessment-data-processing.html#part-2",
    "title": "Assessment - Data Processing",
    "section": "Part 2",
    "text": "Part 2\nPart 2 is due by Friday.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#making-synthetic-data",
    "href": "content/module-data-processing/assessment-data-processing.html#making-synthetic-data",
    "title": "Assessment - Data Processing",
    "section": "Making synthetic data",
    "text": "Making synthetic data\nGet a copy of your classmate‚Äôs repository using the fork+clone approach discussed previously.\nFind and open the cdcdata-exercise.qmd file. At the bottom, write a comment that says something like This section contributed by YOURNAME. This needs to be there for me to be able to grade your contribution.\nTake a look at the descriptive analysis your classmate did.\nNext, produce a new synthetic data set with the same structure as their cleaned/processed data. You are encouraged to use some LLM AI tools to help write the code. If you do, specify in the Quarto document or as comments the AI prompts you are using.\nWrite code that produces synthetic data, then summarizes/explores the data with a few tables and figures similar to those made by your classmate for the original data.\nAdd comments in the code and text into the Quarto document to explain what you did and how close your synthetic data is compared to the original data.\nMake sure everything works and the website renders ok.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#finishing-the-github-workflow",
    "href": "content/module-data-processing/assessment-data-processing.html#finishing-the-github-workflow",
    "title": "Assessment - Data Processing",
    "section": "Finishing the GitHub workflow",
    "text": "Finishing the GitHub workflow\nOnce all is done and works, commit, push to GitHub. Visit your fork of the repo on GitHub.com and initiate a pull request as you did in a previous exercise.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#part-3",
    "href": "content/module-data-processing/assessment-data-processing.html#part-3",
    "title": "Assessment - Data Processing",
    "section": "Part 3",
    "text": "Part 3\nThe original repository owner should receive a PR. Hopefully there are no conflicts and the updates can be merged. Pull the updates to your local machine and make sure it all works. Rebuild/render the whole website. Then push back. Check your website online to make sure everything shows up as it should and that all your exercises are reachable through the menu.\nSince this will be part of your portfolio site, and you already posted a link to that previously, you don‚Äôt need to post anything, I know where to find it.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/assessment-data-processing.html#another-fork-and-pull-exercise",
    "href": "content/module-data-processing/assessment-data-processing.html#another-fork-and-pull-exercise",
    "title": "Assessment - Data Processing",
    "section": "Another fork and pull exercise",
    "text": "Another fork and pull exercise\nThis is optional. You can do it at any time during this course (and more than once) üòÅ.\nHelp improve the course with your contributions! Find something wrong/unclear/worth improving with this course (e.g.¬†a typo, something confusing, a broken link, a suggestion for a new reference, or anything else).\nGo to the GitHub repository for this course or the adjacent MDS Tools website. Follow the steps outlined above: Fork the course to your personal account, clone it to your local computer, implement your updates, push it back to GitHub, then initiate a pull request. I will get a notification of your pull request. If things look ok and no conflicts exist, I will merge your improvements into the course. And just like that, you have contributed to improving this course! (And of course, you will be listed in the Acknowledgments section of the main course page.)\nAnother option for helping to improve the course website is to file a GitHub Issue. Feel free to do so any time during the course to let me know of anything that needs fixing.",
    "crumbs": [
      "Assessments",
      "5. Processing Data"
    ]
  },
  {
    "objectID": "content/module-data-processing/data-loading/data-loading.html",
    "href": "content/module-data-processing/data-loading/data-loading.html",
    "title": "Loading data into R",
    "section": "",
    "text": "Overview\nFor this unit, we will briefly discuss how to load your data into R.\n\n\nLearning Objectives\n\nBe familiar with ways to get data into R.\nKnow best practices for loading data.\n\n\n\nIntroduction\nNo matter the source, you need to your data into your favorite data analysis system (in our case, that will be R). Sometimes, you get data in a format that can be read in easily, e.g., a comma-separated CSV file without any strange formatting that messes up the import. At other times, you might get data in a collection of terribly formatted Excel spreadsheets, or you get it by scraping data from some online source. The following sections discuss a few common ways of getting data into R.\n\n\nLoading spreadsheet data\nQuite often, you‚Äôll have data in Excel or another spreadsheet format.\nBase R has different functions to read CSV and similarly formatted data, e.g.¬†read.csv() or read.delim(). Those are ok to use, but in general I recommend using similar functions from the readr package, such as read_csv(). Those functions are more flexible and usually more robust.\nFor Excel data, the readxl package is a good option. For most spreadsheet/tabular formats, you‚Äôll likely find an R package that can read the data.\n\n\nProprietary formats\nA lot of software packages, such as SAS, SPSS, or Graphpad Prism, have their own proprietary format. If you get data that has that format, you need to find a way to get it into R.\nFor SAS, SPSS and STATA files, there is the haven package which lets you read them into R easily. For other file formats, you need to see if someone wrote an R package that can load them.\nSometimes there‚Äôs no tool/package to read that proprietary format. Then you need to ask the person who gave you the software to save it in a different format, or if you have access to the proprietary software, load the data in that software and save it in a format you can read.\n\n\nLoading R data\nSometimes, the data already comes in R format. There are essentially just two formats, Rds and Rda. For differences between those two formats, see e.g.¬†this article. Those are easily loaded with the functions readRDS() (or readr::read_rds() which is basically the same) or load().\n\n\nProcessing data outside of R\nAs much as possible, you should never directly edit your raw data in some software that isn‚Äôt scripted/reproducible (e.g., Excel). If you can, load the raw data into R and do all cleaning inside R with code, so everything is automatically reproducible and documented. Sometimes, you might need to edit the files in the format you got before you are even able to load them into R. For instance, sometimes an Excel spreadsheet is so messy that you can‚Äôt even properly get it into R without first doing some cleanup in Excel.\nIn those cases, you might have to make modifications in a software other than R. If you can‚Äôt directly read the data into R and need to make some changes before, make copies of your raw data files and only edit those copies. Also, write down and document all the edits you made.\n\n\nChecking data\nIt is important that once you‚Äôve loaded your data, you do a quick check to make sure it looks as expected. For instance if you expect \\(N\\) observations and \\(M\\) variables, make sure that is what you get. Those checks are simple but important. You don‚Äôt want to start processing the data unless you know it looks as you expect it to look once loaded.\nAlso, if you load a data frame, sometimes the variables don‚Äôt have the format you expect them. For instance, it is common that a variable that is a factor is loaded as text. Recoding this is then part of the data processing task.\n\n\nSaving data\nAt some point in your pipeline ‚Äî possibly more than once ‚Äî you might want to save your data or results. That is useful if you have different scripts that perform different tasks. For instance your processing script might save the cleaned data. If you plan on sharing that data, a common format such as CSV might be best. If you plan on only loading it again with another R script, then an R format like Rds or Rda might be suitable, as it preserves some meta-information (e.g., a variable coded as a factor will keep that information, while if you save it as CSV it will become text and once you load it you have to re-code it again as factor).\n\n\nSummary\nIn general, loading data into R is not too hard. At times, you might get especially messy data and you need to do a few extra steps to get it into R.\n\n\nFurther Resources\n\nThe Loading and Saving Data in R chapter of Hands-On Programming with R has some additional discussions.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Data loading"
    ]
  },
  {
    "objectID": "content/module-data-processing/data-processing-overview.html",
    "href": "content/module-data-processing/data-processing-overview.html",
    "title": "Processing data",
    "section": "",
    "text": "Overview\nWhile statistical analysis is important and cool, you will spend the majority of time with your data in the pre-analysis stages.\nEstimates are that for any real-world analysis, up to 80% of your time is spent getting the data ready to do the actual statistical model fitting. This module focuses on that 80%.\n\n\nLearning Objectives\n\nBe familiar with the concept of data wrangling.\nKnow what to do with missing data.\nKnow how to handle outliers.\nBe aware of feature engineering.\n\n\n\nContent\nThe module consists of the following units:\n\nData processing introduction\nData loading\nDealing with missing data\nDealing with outliers \nCreating new data/variables\nMerging data",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Data Processing Overview"
    ]
  },
  {
    "objectID": "content/module-data-processing/merging-data/merging-data.html",
    "href": "content/module-data-processing/merging-data/merging-data.html",
    "title": "Merging data",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss merging of different data sets.\n\n\nLearning Objectives\n\nBe familiar with the idea of merging data.\nKnow how to do merging in R.\n\n\n\nIntroduction\nIt is not uncommon that your raw data comes in more than one file. It could be multiple spreadsheets of a single study, or it could be different datasets from different sources (e.g., a dataset on asthma cases combined with a different data source that records air pollution levels). Often, the most interesting questions need to be answered by combining data from different sources. If you have multiple data sets, you will likely need to combine the data.\n\n\nMerging in R\nThe dplyr package has a great set of _join() functions that let you do different types of joining of data.\nOften, before you can merge, you might need to reshape the data so the structure of the data you are about to merge is consistent. The tidyr package and especially the pivot_longer() and pivot_wider() functions are very useful for that.\n\n\nSummary\nIf you have only a single dataset, you generally don‚Äôt need to merge data.\n\n\nFurther Resources\n\nFor an alternative package that can help with merging, especially large data, see the data.table package.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Merging data"
    ]
  },
  {
    "objectID": "content/module-data-processing/outliers/outliers.html",
    "href": "content/module-data-processing/outliers/outliers.html",
    "title": "Dealing with outliers",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss how to deal with outliers and generally ‚Äòstrange‚Äô values in your data.\n\n\nLearning Objectives\n\nBe familiar with the concept of outliers.\nKnow what to do with outliers and other strange values.\n\n\n\nIntroduction\nIt is not uncommon to have values in your data that are strange. It could be that someone at data entry made a mistake. Or your codebook doesn‚Äôt properly explain the entries. Other times, it could be real data that is just an outlier. It can be hard to decide if a value is strange but real or a mistake.\nDealing with such entries is a judgment call. The best approach is to have a pre-written analysis plan that explains exactly what to do in any such cases. Outside of clinical trials seeking FDA approval, that pretty much never happens. We usually have to decide what to do with strange entries when we run into them. Some good rules are to be consistent, fully document and explain what you do, and if you are able, do it both ways (e.g.¬†do the analysis with the values as they are, and then again with them removed). Of course both ways can quickly turn into a million different ways and at some point, you probably have to stop. However, trying it more than one way can be reassuring if you get pretty much the same answer each time. If you do not get similar results, then you have to be more careful and should describe in your report/paper in detail why and how different approaches to your data cleaning lead to different results.\n\n\nFinding outliers\nIn introductory statistics classes, students are often taught arbitrary rules for finding outliers to remove. Typically they learn to remove all points outside of the limits of 1.5 times the interquartile range (IQR). However, there is no clear definition of what an outlier is.\nGenerally speaking, an outlier is a date value that is far away from the majority of the other values of a variable or dataset. If one assumed a distribution for the data, one could define an outlier statistically, e.g., every value that is some standard deviations away from the mean could be considered an outlier. In practice, we generally don‚Äôt know the distribution from which the data is a sample, and defining any kind of cut-off for outliers is also fairly arbitrary. Therefore, a more case-by-case approach is usually best.\nA great tool to find outliers is with figures. If you plot the raw data, and some values are far away from others, they deserve further inspection and might be considered outliers.\nOnce you have identified outliers, you should try to determine if you think that strange value is obviously (or at least very likely) a wrong entry, or if it could be real, just unusual. Based on this, you might deal with the value differently. Some approaches for dealing with outliers are described next.\n\n\nRemoving outliers\nOne option is to remove any observation that contains a variable with an outlier. This is usually straightforward. It is in some sense similar to removing observations with missing values. The problem is that this leads to less data you can analyze, and if the outliers were real - just strange - data, you might get biased estimates.\n\n\nReplacing outliers\nInstead of removing data with outliers, you could try to replace the outliers with something that makes more sense. This is similar to imputation for missing data, and you can use the same approaches.\n\n\nModeling with outliers\nYou can also decide to leave the outliers in the data. You can run your standard models. However, some common models will give the outliers a lot of importance, and therefore a few outliers can skew your results. In this case, you can use statistical methods that are not influenced by outliers as much. Those are sometimes called robust methods (although this term confusingly has many different meanings; not all things called robust methods are about dealing with outliers). We‚Äôll revisit some of those when we get to the statistical analysis and modeling modules.\n\n\nSummary\nIt is likely that your data has some strange values (outliers). Take a close look at them, then come up with a plan. If possible, try things multiple ways (also called doing a sensitivity analysis) and report results from the different versions you tried. Hopefully, you‚Äôll find that how you deal with outliers doesn‚Äôt matter much.\n\n\nFurther Resources\n\nFinney 2012 provides a nice further discussion regarding outliers. It‚Äôs very non-technical (no equations) and easy to read.",
    "crumbs": [
      "Content",
      "5. Data Processing",
      "Dealing with outliers"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html",
    "href": "content/module-data/data-formats-r/data-formats-r.html",
    "title": "Data formats in R",
    "section": "",
    "text": "For this unit, we will briefly discuss different types of variables and data formats in R.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#charactersstrings",
    "href": "content/module-data/data-formats-r/data-formats-r.html#charactersstrings",
    "title": "Data formats in R",
    "section": "Characters/strings",
    "text": "Characters/strings\nA string is a collection of characters. You will often hear the labels ‚Äúcharacter‚Äù and ‚Äústring‚Äù used interchangeably, though strictly speaking, a string is a collection of characters. Everything can be encoded as a character string. Unfortunately, you cannot do a lot of analysis with strings. For instance, if you have the numbers 2 and 3, you can add and subtract. But if you code those numbers as characters ‚Äú2‚Äù and ‚Äú3‚Äù, you can‚Äôt do much with them. Thus, transforming characters into other, more useful categories (if applicable) is a common task. It is common that you read in some data and there is a variable which should be numeric, but some entries are not (e.g., the original spreadsheet shows something like ‚Äú&lt;10‚Äù). In this case, R reads all of these variables as characters. You then have to go in, clean the ‚Äú&lt;10‚Äù value, and convert the rest to numeric. Sometimes you do want to work with strings directly. There are many tools and packages in R that are helpful, including base R command. The stringr package is particularly useful.\nIf you do any kind of analysis of text, you‚Äôll be dealing with strings/characters. But even if you are not planning to analyze text, it is quite likely that you will need to work with strings at some point during a data analysis, even if it is only to find specific values, clean up variable names, etc. Thus, learning more about this topic is a good idea. A very powerful, and also very confusing way to deal with strings is to use what are called regular expressions (or regex for short). This concept applies to any programming language, not just R. Being at least somewhat familiar with the concept of regular expressions is useful. The rebus package is particularly useful for building regular expressions without knowing the highly specific syntax. However, learning regex applies to many different languages and is an incredibly powerful tool in your data science repertoire.\nIf you have no experience manipulating strings, I suggest you work through the Strings chapter (14) of R4DS, and do the exercises. The string processing chapter (25) of IDS contains further good material that is worth working through. Another good source is the Character Vectors chapter in the STAT 545 book by Jenny Bryan. Take a look at those various sources, decide which one is right for your level and go through some of them. And/or consult them as needed.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#factors",
    "href": "content/module-data/data-formats-r/data-formats-r.html#factors",
    "title": "Data formats in R",
    "section": "Factors",
    "text": "Factors\nFactors is the name that R gives to categorical variables. They can be ordered/ordinal or not. You need to make sure variables that should be coded as a factor are, and that those that shouldn‚Äôt be aren‚Äôt. For instance, you might have a variable with entries of 0, 1, and 2. Those could be numeric values, e.g., the number of siblings a person has. Or it could be a factor coding for 3 types of ethnicity (unordered), or 3 levels of socioeconomic status (ordered). You need to make sure it is coded as factor or numeric, based on what you know about the variable. An excellent package to work with factors is the forcats package.\nTo learn some more about factors, you might want to go through the Factors chapter of R4DS, and do the exercises. Factors are the most complicated basic data format, so it is worth spending extra time making sure you understand a bit how factors work.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#logicals",
    "href": "content/module-data/data-formats-r/data-formats-r.html#logicals",
    "title": "Data formats in R",
    "section": "Logicals",
    "text": "Logicals\nYou can think of a logical variable as a type of categorical variable with 2 categories, TRUE and FALSE. Alternatively, in R, 0 is interpreted as FALSE and 1 as TRUE (and vice versa). R will sometimes treat logicals as numbers, and sometimes as categorical variables, so it is often more straightforward to code these variables as either numeric or categorical to be safe. You will use those logical values often when checking your data, e.g., if you want to see if your variable x is greater than 5, then the R command x &gt; 5 will return either TRUE or FALSE, based on the value of x.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#numeric-value",
    "href": "content/module-data/data-formats-r/data-formats-r.html#numeric-value",
    "title": "Data formats in R",
    "section": "Numeric value",
    "text": "Numeric value\nNumeric values can be integers or general numeric values (double). You generally do not need to care too much how exactly your numeric values are coded. Often, you can treat integers as general numeric value. (In R, a general numeric variable is called double ‚Äì there is no single.) You might rarely come across a case where some analytic method or other bits of code requires integers to be specified as such. In R, you can use the as.integer() function to convert general numeric values to integers. You don‚Äôt really need any other special packages in R to deal with numeric values. Note that when you type an integer value, e.g.¬†x &lt;- 2, into R, this is considered numeric by default. If you want to make sure it is treated as integer, add an L, e.g.¬†x &lt;- 2L.\nOne issue to be careful about is if you do comparisons. For instance if x and y each contain some number and you expect it to be the same, but there was some rounding going on, it could be that if you do x == y you get a FALSE because you had x  = 2 and y = 1.99999997 - the latter not being exactly 2 because some rounding was happening. If this is the case, the safer approach is to check for abs(x - y) &lt; 1-e10 or some other small number; or alternatively you can use the shortcut all.equal(x, y). Basically you are looking if x and y are the same within some rounding error.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#dates-and-times",
    "href": "content/module-data/data-formats-r/data-formats-r.html#dates-and-times",
    "title": "Data formats in R",
    "section": "Dates and times",
    "text": "Dates and times\nWhile dates and times are a type of continuous numeric variable, you should assign the date class explicitly in R, which allows you to do more with them. Dates can be trick to work with in base R (which usually calls them POSIX variables). The lubridate package is a good package to work with dates, and is more user friendly. Others exist.\nTo learn some more about dates and times in R, check out the Dates and times chapter of R4DS as well as the Parsing Dates and Times chapter of IDS.\nThe nice thing about having explicit date formats is that you can do things like subtract one date from another to get the days between the two dates and R will automatically account for things like leap-years.\n\nExcel is notoriously bad at storing dates. It has some internal format that‚Äôs different than what you see in the spreadsheet. That means that at times, if you read an Excel file into R, the date is messed up. In my experience, the most robust option is to explicitly format the date (and really every column) in Excel as text. Then you can read this into R and re-code as date/factor/etc.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#special-data-types",
    "href": "content/module-data/data-formats-r/data-formats-r.html#special-data-types",
    "title": "Data formats in R",
    "section": "Special data types",
    "text": "Special data types\nR has several symbols that represent a special type of object. Here‚Äôs a quick summary.\nTRUE and FALSE are words that are interpreted as logical values. The abbreviations T and F are also treated as logical values (so you should avoid overwriting those names). R also allows you to use 0 and 1 as logical (1=TRUE, 0=FALSE) but that can be confusing.\nNA (not available) is R‚Äôs default way of encoding a missing value. If something is missing, you can‚Äôt do useful things with it. For instance if you have this vector x1 &lt;- c(1,3,NA) and try to do mean(x1), you get NA again. For some functions, there is an ignore setting, e.g.¬†mean(x1, na.rm=TRUE) removes the NA, then takes the mean. You can check if you have NA with is.na(x1). You‚Äôll likely encounter NA frequently in your data analysis, and you need to be careful when you try to perform operations in the presence of NA.\nInf and -Inf can also show up when you do numerical operations, for instance divide by zero. If you perform further mathematical operations with infinity, you almost always end up with infinity again. Generally, if you end up with one of those, it‚Äôs an indication that you did some mathematical operation that you probably shouldn‚Äôt be doing (commonly divide by zero, or log of a negative number).\nNaN (Not a Number) happens when you do some mathematical operation that can‚Äôt be resolved to any number or infinity. For instance if you do 0/0 you‚Äôll get NaN. Like Inf, NaN will generally mess up any further computations, so if you encounter it you should figure out where it came from and figure out a fix.\nNULL indicates something that exists but is empty. For instance if you want to set up a new list object to hold some future results, but want to start it empty, you can do x &lt;- list(NULL). And then you can later add elements to that list. You‚Äôll likely not need to care too much about NULL initially, but if you start writing more code it is often a useful object.\nFor most of these special objects, there are functions is() and is.*(), where * is a specific data type, that let you check. For instance is.nan(x) will tell you if any element in x contains NaN.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#vectors",
    "href": "content/module-data/data-formats-r/data-formats-r.html#vectors",
    "title": "Data formats in R",
    "section": "Vectors",
    "text": "Vectors\nVectors are a simple collection of elements in a single row or column. In R, the easiest way to create vectors is with the concatenate command, c(). An example is x1 &lt;- c(3,12,5). A single vector can contain only one element type (e.g., all characters or all numeric). If you try to mix and match, everything ends up as a character. Type x1 &lt;- c(3,12,5) and x2 &lt;- c(6, 5, 'h') into R and apply the class() command to both x1 and x2 and note the difference. It is worth mentioning that vectors can be of length 1. (Unlike some other languages, there is no separate concept of a scalar in R, a scalar is just a length one vector.) For instance if you define x3 &lt;- 1 and then check it with is(x3) you‚Äôll see that is is classified as a numeric vector ‚Äî of length 1, which you can find out with length(x3).",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#matrices",
    "href": "content/module-data/data-formats-r/data-formats-r.html#matrices",
    "title": "Data formats in R",
    "section": "Matrices",
    "text": "Matrices\nA matrix is a collection of elements in rows and columns. A matrix can contain only one element type. You can think of a matrix as a collection of horizontal vectors stacked on top of each other or vertical vectors next to each other.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#data-frames",
    "href": "content/module-data/data-formats-r/data-formats-r.html#data-frames",
    "title": "Data formats in R",
    "section": "Data frames",
    "text": "Data frames\nA data frame has the same shape as a matrix, i.e., it is a collection of elements in rows and columns. The critical difference is that each column of a data frame can contain elements of different types. This makes it ideal for storing data, with each row and observation and each variable in a column, and different columns potentially with different data types. E.g., column 1 could be age and numeric, and column 2 could be gender and be categorical, etc. Data frames also have a name for each column. As the name might imply, a data frame is a very common way to store data. It is worth mentioning a version of a data frame called a tibble that is often used in the tidyverse. It is similar but not quite the same as the basic data frame. Most of the time, you can use one format or the other. You can read more about tibbles on its package website.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#lists",
    "href": "content/module-data/data-formats-r/data-formats-r.html#lists",
    "title": "Data formats in R",
    "section": "Lists",
    "text": "Lists\nLists are the most flexible data types in R. You can combine different elements as in data frames. Further, each element can be of varying length. For instance, you could have the first list element contain a person‚Äôs name, the second list element their age, the third their address. You can even have other elements inside lists, for instance, you could have a data frame as a list element containing the names and ages of the person‚Äôs parents. Lists are very flexible, and if you get deeper into data analysis, you‚Äôll be working with them. The downside is that because they are more flexible, they can also be a bit more confusing to work with. With enough practice, you‚Äôll figure it out. Also note that almost every function in R that returns something a bit more complicated to you (e.g., the result from a linear fit), returns it as a list.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-formats-r/data-formats-r.html#other-data-structures",
    "href": "content/module-data/data-formats-r/data-formats-r.html#other-data-structures",
    "title": "Data formats in R",
    "section": "Other data structures",
    "text": "Other data structures\nThe ones described above are the most important ones, but there are many others. Often, specific R packages come with their own structures. You are usually told how to use specific data structures, and there are generally versions of the structures just described.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Formats in R"
    ]
  },
  {
    "objectID": "content/module-data/data-overview.html",
    "href": "content/module-data/data-overview.html",
    "title": "All about data",
    "section": "",
    "text": "Overview\nYou need data to do a data analysis ‚Äî surprise!\nThis module discusses data. What it is, where to get it, how to load and store it, and some other basics of that sort.\n\n\nLearning Objectives\n\nBe familiar with basic data types.\nBe aware of data structure.\nKnow how to work with various data formats in R.\n\n\n\nContent\nThe module consists of the following units:\n\nBasic data types\nComplex data types\nData storage formats\nData formats in R\nThe data generation process",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Data Overview"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html",
    "href": "content/module-data/data-types-basic/data-types-basic.html",
    "title": "Basic types of data",
    "section": "",
    "text": "For this unit, we will discuss basic data types that you will come across frequently.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Basic Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html#quantitative-variables",
    "href": "content/module-data/data-types-basic/data-types-basic.html#quantitative-variables",
    "title": "Basic types of data",
    "section": "Quantitative variables",
    "text": "Quantitative variables\nThis data type, sometimes called interval scale data, generally allows one to do certain mathematical operations, e.g., subtraction or addition. Some quantiative variables are also ratio scale data, which means they have a true zero, or lowest value ‚Äî for these data, multiplying, dividing, and taking ratios makes sense.\nDifferent subcategories exist:\n\nContinuous: Can, in principle, be any number. Examples are height, weight, age, etc.\nDiscrete: Can only take discrete (integer) values, e.g., the number of siblings a person has.\nFraction/Proportion: Continuous, but between 0-1.\nSometimes other special forms (e.g., only positive, only in some range).",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Basic Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html#qualitative-variables",
    "href": "content/module-data/data-types-basic/data-types-basic.html#qualitative-variables",
    "title": "Basic types of data",
    "section": "Qualitative variables",
    "text": "Qualitative variables\nBroadly speaking, qualitative variables are those that do not allow one to perform any mathematical operations such as subtraction or addition. Qualitative data which has no intrinsic order is also called nominal (scale) data. Types of such data are:\n\nDescriptive: e.g., free text data from participant interviews. This data typically has very different values for each individual and requires extra steps to aggregate into something that can be analyzed.\nCategorical: e.g., hair color, ethnicity. No ordering is possible. These are also called polytomous variables.\nDichotomous: A special and common case of categorical data is data with exactly 2 categories, e.g., yes/no, dead/alive, diseased/healthy, exposed/unexposed. These are also commonly called binary variables.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Basic Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html#ordinal-variables",
    "href": "content/module-data/data-types-basic/data-types-basic.html#ordinal-variables",
    "title": "Basic types of data",
    "section": "Ordinal variables",
    "text": "Ordinal variables\nThis data type is usually considered a type of categorical variable, but it is worth thinking about it as something on its own. Ordinal data fall in between being strictly quantitative or strictly qualitative. For instance, if a question asks a person to rank their level of a pain on a scale from 1-10, a 7 is clearly higher than a 6, and a 6 higher than a 5. But it‚Äôs unclear if the difference between 5 and 6 is the same as 6 and 7. Thus it is not clear if one can do operations like subtraction (to get a difference of 1 in each case). Another example is level of education, which a survey might collect in categories of ‚Äòno high school‚Äô, ‚Äòhigh school‚Äô, ‚Äòsome college‚Äô, ‚Äòcollege degree‚Äô, ‚Äògraduate degree‚Äô. We could code that with numbers 1-5, and in some sense these items are ordered, but it‚Äôs unclear if one is justified in considering the difference between ‚Äòhigh school (2)‚Äô and ‚Äòsome college (3)‚Äô the same as ‚Äòsome college (3)‚Äô and ‚Äòcollege degree (4)‚Äô. Typically ordinal data is more complicated to deal with than purely qualitative or quantitative data.",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Basic Data Types"
    ]
  },
  {
    "objectID": "content/module-data/data-types-basic/data-types-basic.html#footnotes",
    "href": "content/module-data/data-types-basic/data-types-basic.html#footnotes",
    "title": "Basic types of data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLogistic regression, which you might be familiar, is used for classification. However, the underlying model predicts a quantitative outcome (a value between 0 and 1 usually interpreted as a probability), which is then binned to make categorical predictions. This is true of many, but not all, classification approaches.‚Ü©Ô∏é",
    "crumbs": [
      "Content",
      "4A. Data Introduction",
      "Basic Data Types"
    ]
  },
  {
    "objectID": "content/module-intro-tools/assessment-course-tools-introduction.html",
    "href": "content/module-intro-tools/assessment-course-tools-introduction.html",
    "title": "Assessment - Course and Tools Introduction",
    "section": "",
    "text": "Each module comes with assessments. Those are things you need to do that will be graded. The table on the Schedule page lists the assessments for each module.\nIf you haven‚Äôt done so, please read through the Assessments Overview page so you know how each type of assessment should be done and how grading works.\nEach module has an Assessments page, like this one, which describes what the assessments are and what you should do. Often, there is a good bit of information for the exercises.",
    "crumbs": [
      "Assessments",
      "1. Course and Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/assessment-course-tools-introduction.html#workload-note",
    "href": "content/module-intro-tools/assessment-course-tools-introduction.html#workload-note",
    "title": "Assessment - Course and Tools Introduction",
    "section": "Workload Note",
    "text": "Workload Note\nFor those of you who are new to GitHub/R/Quarto, this first project will make you do a lot of things that you are not familiar with. I know that this will be time-consuming and also might feel a bit intimidating. It‚Äôs partly unavoidable and partly on purpose. You need to learn how to quickly get up to speed with all kinds of new tools that come your way. So practicing it is a good idea. You are welcome to draw on any sources for help that you want (online, classmates, instructor, etc.). I‚Äôm confident with a bit of trial and error you‚Äôll get it to work.\nIf you get stuck (quite possible), check the materials listed in the Resources section of the course, or use Google. Also use the discussion boards to ask questions. You may use any method to get help (Google, R4DS Slack channel, AI, colleagues/friends/relatives with R/Quarto/Markdown experience, etc.). Still, try to do as much as possible yourself. We‚Äôll use all the bits you are learning here repeatedly during this course.",
    "crumbs": [
      "Assessments",
      "1. Course and Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/assessment-course-tools-introduction.html#pre-requisites",
    "href": "content/module-intro-tools/assessment-course-tools-introduction.html#pre-requisites",
    "title": "Assessment - Course and Tools Introduction",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nYou should have gone through all content in this module and all relevant courses on the MDS Tools site.",
    "crumbs": [
      "Assessments",
      "1. Course and Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/assessment-course-tools-introduction.html#instructions",
    "href": "content/module-intro-tools/assessment-course-tools-introduction.html#instructions",
    "title": "Assessment - Course and Tools Introduction",
    "section": "Instructions",
    "text": "Instructions\nTo execute this exercise, follow the instructions in the Portfolio Creation Course on the MDS website.\n\nOnce your website is up and running, don‚Äôt forget to post the link into the introductions channel!",
    "crumbs": [
      "Assessments",
      "1. Course and Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-tools/introduction-tools.html",
    "href": "content/module-intro-tools/introduction-tools/introduction-tools.html",
    "title": "Tools Introduction",
    "section": "",
    "text": "This unit provides a brief introduction to the various course tools. The main materials to get started with these tools is found on a separate website, as described below.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-tools/introduction-tools.html#introduction",
    "href": "content/module-intro-tools/introduction-tools/introduction-tools.html#introduction",
    "title": "Tools Introduction",
    "section": "",
    "text": "This unit provides a brief introduction to the various course tools. The main materials to get started with these tools is found on a separate website, as described below.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-tools/introduction-tools.html#learning-objectives",
    "href": "content/module-intro-tools/introduction-tools/introduction-tools.html#learning-objectives",
    "title": "Tools Introduction",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe specific learning objectives for this unit are:\n\nKnow what tools we are using in this course.\nKnow where to learn about these tools.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-intro-tools/introduction-tools/introduction-tools.html#course-tools",
    "href": "content/module-intro-tools/introduction-tools/introduction-tools.html#course-tools",
    "title": "Tools Introduction",
    "section": "Course Tools",
    "text": "Course Tools\nEvery data analysis project requires a set of tools to get the job done. In this course, we will be using a variety of tools to help us perform data analyses in a modern, reproducible way.\nSince the use of these tools is an important part of the course, it is essential that you get familiar with them early on. The first assignment this week will ask you to use a good number of these tools.",
    "crumbs": [
      "Content",
      "1. Course and Tools Introduction",
      "Tools Introduction"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html",
    "title": "Assessment - Machine Learning Models 1",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#setup",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#setup",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Setup",
    "text": "Setup\nThis is a solo-exercise, and it‚Äôs part of your portfolio.\nSince the previous Quarto document was getting long, we‚Äôll set this up as a different one. Make a new folder in your portfolio repo and call it ml-models-exercise.\nIf you haven‚Äôt done so yet, update your previous code to save the clean data (with the variable SEX included) from the last exercise into an Rds file. Copy that Rds file into the newly created folder. Also in this new folder, start a new quarto file called ml-models-exercise.qmd.\nOpen the new quarto file you just created, and we should be ready to go.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#preliminaries",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#preliminaries",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Preliminaries",
    "text": "Preliminaries\nWrite code to load your packages. You already know you‚Äôll need tidymodels and probably ggplot2. You can add other package loading commands later (but remember that is is good practice to add them to the top of your script). Also define a random number seed. Set it to the value 1234. Then load the data.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#more-processing",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#more-processing",
    "title": "Assessment - Machine Learning Models 1",
    "section": "More processing",
    "text": "More processing\nJust to practice, we‚Äôll do a bit more data wrangling.\nEarlier, we dropped the variable SEX. Now we‚Äôll decide we actually want to keep it. Let‚Äôs go on a ‚Äòtreasure hunt‚Äô. Try to find out what the values used to encode race stand for. This is the kind of task one has to do at times for any real-world data analysis. Spend a few minutes to see if you can figure out the meaning of the numbers. (Hint: you can look at the manuscript from our first exercise with these data.)\nIf you can‚Äôt find out the meaning, that‚Äôs ok for the purpose of the exercise. What we‚Äôll do is to combine the two sparse groups into a single category, so that we are left with race 1, 2, and 3, where the latter is a re-coding of everyone with a 7 and 88.\n\nWrite code that changes the RACE variable such that it combines categories 7 and 88 and puts them in a category called 3.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#pairwise-correlations",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#pairwise-correlations",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Pairwise correlations",
    "text": "Pairwise correlations\nNext, let‚Äôs make a pairwise correlation plot for the continuous variables. If we were to find any very strong correlations, we might want to remove those.\n\nWrite code to make a correlation plot for the continuous variables.\n\nYou‚Äôll see that while some variables are somewhat strongly correlated, nothing seems excessive (e.g., above an absolute value of 0.9). So looks like we won‚Äôt have too much of a problem with collinearity.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#feature-engineering",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#feature-engineering",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Feature engineering",
    "text": "Feature engineering\nAs you learned, feature engineering is a fancy word for creating new variables (or more broadly, doing stuff to existing variables, e.g., one could interpret our recoding of RACE above as feature engineering).\nLet‚Äôs become a feature engineer üòÅ. Our data contains height and weight for each individual (HT and WT). There‚Äôs usually some correlation between those ‚Äì which you likely noticed above when you did the pairwise correlation. It‚Äôs not too strong here, but maybe it would still be good to combine these variables into a new one. Let‚Äôs do that by computing BMI.\n\nLook up the formula for BMI. Then add a new variable to the data frame called BMI, computed from the HT and WT variables. Be careful with the units! Since we don‚Äôt have a codebook that explains the variables and their units, you have to guess.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#model-building",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#model-building",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Model building",
    "text": "Model building\nWe are finally done with our additional processing bits and are ready to fit some models. Specifically, we will explore these 3 models:\n\nFor the first model, we‚Äôll revisit the one we had previously, namely a linear model with all predictors.\nFor our second model, we‚Äôll use LASSO regression.\nFor our third model, we‚Äôll do a random forest (RF).\n\nAs we discussed, sometimes it‚Äôs useful to do a train/test split, so that we can do a final model assessment at the end. Depending on your question and setup, that might or might not be useful. For this exercise we‚Äôll skip this split. Instead, we‚Äôll use all the data and perform cross-validation to try and get an ‚Äúhonest‚Äù assessment of the performance of our models.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#first-fit",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#first-fit",
    "title": "Assessment - Machine Learning Models 1",
    "section": "First fit",
    "text": "First fit\nWe‚Äôll start by performing a single fit to the training data, without any CV and model tuning.\nAs always, we should compare to a null model that includes no predictors. Since we already did that previously, we don‚Äôt need to repeat here. (Although keep in mind that you should almost always fit a null model when you do a real analysis!)\nSome algorithms, such as random forest, do at times use random numbers. So to ensure reproducibility, set a seed to the value given above.\n\nCreate tidymodels workflows/recipes and fit the 3 models specified above to the data. The outcome is Y, predictors are all other variables. Use the glmnet engine for the LASSO and the ranger engine for the random forest. Ignore any kind of tuning for the models for now. For the LASSO model, set the penalty to 0.1. We‚Äôll tune it later. For the random forest, we‚Äôll keep all the defaults as they are and don‚Äôt specify any for now. To ensure reproducibility for the random forest model, you also need to specify the random seed as an engine argument: set_engine(\"ranger\", seed = rngseed).\n\n\nYou‚Äôll likely have to install some further R packages to do LASSO and random forest, namely the glmnet and ranger packages.\n\nOnce you fit each model, use the model to make predictions on the entire dataset, and report the RMSE performance metric for each model fit. Also make a observed versus predicted plot for each of the models.\nYou should find that linear model and the LASSO give pretty much the same results. Can you explain why that is?\nYou should also find that the random forest performs best, with a RMSE of around 362 and predictions that are overall closer to the observations (closer to the 45 degree line) in the observed versus predicted plot.\nThis isn‚Äôt entirely surprising, because random forest models are very flexible and thus can capture a lot of the patterns seen in the data. The danger is that they easily overfit.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#tuning-the-models",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#tuning-the-models",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Tuning the models",
    "text": "Tuning the models\nNow we‚Äôll do something that you should never do in the real world!\nThe LASSO and RF both have tuning parameters. Tuning them will often improve model performance. So let‚Äôs tune them now.\nThe stupid thing we‚Äôll be doing is tuning them by repeated fitting to the data without cross-validation. This means model performance is evaluated on the data used for tuning. This pretty much guarantees that we‚Äôll overfit our model. This is a bad idea, but it is instructive to see what happens, so let‚Äôs do that.\n\nThe Tune model parameters section of the Tidymodels tutorial and this blog post might be useful to look at.\n\nWrite code that tunes the LASSO and RF models (there is nothing to tune for the linear model, because it doesn‚Äôt have any hyperparameters).\nFor LASSO, define a grid of parameters to tune over that range from 1E-5 to 1E2 (pick 50 values linearly spaced on a log scale). Note that here, our ‚Äúgrid‚Äù is just a vector of numbers. But as you‚Äôll see below, for models that have multiple tuning parameters, it does become a grid.\nUse tune_grid() to tune the model by testing model performance for each parameter value you defined as the search grid.\nTo create an object that only contains the data, and that you can use as resamples input for tune_grid(), use the apparent() function.\nOnce you have done the tuning, you can take a look at some diagnostics by sending your object returned from the tune_grid() function to autoplot(). For instance if you tuned the tree and saved the result as tree_tune_res, you can run tree_tune_res %&gt;% autoplot(). Depending on the model, the plot will be different, but in general it shows you what happened during the tuning process.\nFor the LASSO, you‚Äôll see RMSE as function of penalty parameter. You should see that the LASSO does best (lowest RMSE) for low penalty values and gets worse if the penalty parameter increases. At the lowest penalty, the RMSE is the same as for the linear model.\nExplain why you see this behavior. What are you doing here, what happens as the penalty parameter goes up? Why does the RMSE only increase and does not go lower than the value found from the linear model or the un-tuned model above? You‚Äôll likely need to re-read the LASSO section on the course website to really understand what is going on here.\nNow, let‚Äôs repeat with a random forest. Different ML algorithms have different tuning parameters. It‚Äôs usually a good idea to look them up to understand at least a bit what they mean and do. You can check this for the ranger algorithm by looking at its help file. There‚Äôs a bunch we could tune. We‚Äôll tune over the parameters mtry and min_n and fix trees at 300. Everything else we keep at their defaults. Update your model/workflow accordingly.\nThen set up a tuning grid with the grid_regular() function. Do a range for mtry between 1 and 7, min_n between 1 and 21, and 7 levels for each parameter. (That means we‚Äôll be trying 7 x 7 tuning parameter combinations).\nThe tuning might take several seconds. Once done, use the autoplot() function to look at the results. You‚Äôll see a plot showing how RMSE changes as you change the tuning parameters. You‚Äôll see that a higher value of mtry and a lower one for min_n lead to the best results. This model isn‚Äôt as easily understandable as the linear and LASSO models, so it‚Äôs not quite clear why those values for the tuning parameters should give the best performance. This is one of the drawbacks of complex ML/AI models, they are harder to interpret. We could probably understand this if we thought more about it. But for now, let‚Äôs move on.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#tuning-with-cv",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#tuning-with-cv",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Tuning with CV",
    "text": "Tuning with CV\nNow it‚Äôs time to do the tuning properly, by using CV to evaluate model performance during the tuning process.\nRepeat the steps you did above, for tuning both the LASSO and RF. But now, instead of creating silly resamples with the apparent() function that weren‚Äôt actually samples but just the data, we‚Äôll now create real samples. Let‚Äôs do 5-fold cross-validation, 5 times repeated. (There‚Äôs no specific reason to do this 5x5 pattern, other than to show you that there are different ways to pick the sample, and that I want you to not use the default.) Use the vfold_cv() function to create a resample object with these specifications. Since this uses samples, re-set the random number seed.\nThen redo the tuning of both models. You can keep everything as before, the only thing that needs to change in the tune_grid() function is to replace the resamples input, now with real samples generated by the vfold_cv() function.\nRunning this code will now take time. It‚Äôs basically 5x5 = 25 times as long as what you ran above. So it might take several minutes.\nOnce done, look at the results from both the LASSO and the RF again using the autoplot() function.\nCompare what you see for the RMSE here with the results above when we didn‚Äôt use CV to evaluate our model. You should find that the LASSO still does best for a small penalty, the RMSE for both models went up, and the LASSO now has lower RMSE compared to the RF. Explain why you are seeing what you do.\nBased on what you find here, which model performs better?\nWe should also compare the results to the linear model we had initially. However, this is not really necessary, since the LASSO with a small penalty is essentially the linear model. If this is not clear why, revisit the discussion of LASSO on the course page.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#conclusion",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#conclusion",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Conclusion",
    "text": "Conclusion\nI hope this exercise showed you a few things, the main ones to me are:\n\nComplex models need tuning and with tuning they can get close to the data.\nBecause complex models can fit data well, it‚Äôs important to not evaluate their performance on the data that was used to tune/train them. Otherwise one will make overly confident conclusions and the models will likely perform poorly on new data.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#ml-and-speed",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#ml-and-speed",
    "title": "Assessment - Machine Learning Models 1",
    "section": "ML and speed",
    "text": "ML and speed\nThis is probably the first exercise where things started slowing down. Once you do bigger models with more data, and extensive tuning and CV, things slow down. It is possible and generally not too hard to parallelize some code. For instance for CV, you could run each sample on a different core. This can often give you a lot of speedup. We won‚Äôt go into this here.\nIf you need to do a lot of hyperparameter tuning, there are also more advanced methods for tuning available in the tune and finetune packages that can help you reduce the number of hyperparameters you need to check.\nYou‚Äôll also find that as things slow down, including computationally intensive code into your Quarto documents can be annoying, since each time you re-render the code might run and take a lot of time. Some options to deal with this are:\n\nMove to a mix of R scripts for computation and Quarto for showing the results.\nSet specific code chunks to not execute during rendering. Instead, run those code chunks manually, save the results to a file, and in your Quarto document, you don‚Äôt execute the code chunk that does the computation but instead just load the previously computed code chunk.\nQuarto also lets you set a freeze option (either for specific Quarto documents in the YAML header, or at the project level in the _quarto.yml file) that you can set to only re-render a file when it changed. You might want to use this in combination with the ideas above.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/assessment-ml-models-1.html#final-test-and-website-update",
    "href": "content/module-model-ml-1/assessment-ml-models-1.html#final-test-and-website-update",
    "title": "Assessment - Machine Learning Models 1",
    "section": "Final test and website update",
    "text": "Final test and website update\nMake sure your analysis and all results are nicely documented and everything runs/renders correctly. Then, add the newly created Quarto document as an entry into your _quarto.yml file. Call it Machine Learning. Recompile your portfolio website and make sure everything works and shows up as expected. Then commit and push.\nSince this is part of your portfolio site, you don‚Äôt need to post anything, I know where to find it.",
    "crumbs": [
      "Assessments",
      "11. Machine Learning Models I"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/many-tree-models/many-tree-models.html",
    "href": "content/module-model-ml-1/many-tree-models/many-tree-models.html",
    "title": "Many-Tree Based Models",
    "section": "",
    "text": "Overview\nIn this unit, we will cover an extension of simple decision tree models, namely models that combine multiple trees for improved performance (but reduced interpretability).\n\n\nLearning Objectives\n\nBe familiar with several tree-based methods\nUnderstand the advantages and disadvantages of single trees versus tree ensembles\n\n\n\nMany trees\nAs discussed in the last unit, trees have many good properties, such as being interpretable, able to work for both regression and classification, deal with missing values, and auto-select predictors.\nThe main disadvantage of single-tree models is that their performance is often not that great‚Äìand complicated trees which perform better are vulnerable to overfitting. However, building models which are combinations of many single trees is possible. Usually these so-called ensemble models (the general name for an ML model that gets an aggregate result by combining many other models) sacrifice some interpretability for performance. The idea behind more sophisticated tree-based methods is to take multiple individual trees and add/average them together in a smart way to end up with an ensemble of trees that often performs much better than a single tree. Some of the most common approaches are Bagging (Bootstrap aggregation), Random Forests, and Boosting. We‚Äôll cover each one very briefly.\n\n\nBagging\nThe main goal of bagging is to reduce variance, which is the main factor that generally negatively affects the performance of a single tree (even if one uses pruning or cross-validation). Remember that if we have N independent observations, each with the same variance/standard deviation, SD, the total variance is SD/N. If we had M datasets for the same process, each with N observations, and we built a model for each dataset, we could average over the models and thereby reduce the variance by 1/M. We don‚Äôt have M different datasets, but we can re-sample our existing data (using bootstrapping), and build a model for each sample. We can then, in the end, average over the different models to reduce variance. Here, each model is a tree, but we could also apply this approach to other models. The final model is the average of all the trees/individual models. Since the bagging procedure reduces the variance, individual trees are not pruned. Bagging leads to models with less variance and thus generally better model performance/predictive power. What is lost now is the ease of interpretation, since our final model is now the sum of a (possibly large) number of individual trees.\n\n\nRandom forest\nThe random forest model/algorithm is similar to bagging. The difference is that as we split each tree, instead of considering all possible predictors, we pick a random subset of predictors and only split on the best among those. Since we are artificially limiting ourselves, we obtain many trees that don‚Äôt perform too well on their own. However, this random choosing of subsets of predictors leads to more ‚Äúdiversity‚Äù in the tree structure (i.e., it avoids the greedy nature of the standard tree building algorithm). This helps in de-correlating the trees when we sum across them in the final model. Often this further improves model performance. The cost is the same as for bagging, namely the final model is a sum of trees which is hard to understand and interpret.\n\n\nBoosting\nBoosting takes a somewhat different approach than the other two. In this case, instead of averaging over full trees, we build many small trees. The procedure starts by building a tree with a specified, often small number of splits (this number is a tuning parameter). This small tree is added to the model, the change in performance is computed (e.g., reduction in RMSE or misclassification error) and a new tree is built that tries to reduce the leftover (residual) errors. In this form, many small trees are added, each one trying to take care of the under-performance produced by previous trees. In the end, one ends up with a sum of many small trees as the final model. This tree ensemble is expected to perform much better than each of the individual trees. Again, the final model is somewhat hard to interpret.\n\n\nAdvantages and disadvantages of many-tree models\nAll the advantages mentioned for a single tree model apply to many-tree models. The main additional advantage is that these models often have (much) improved performance compared to a single tree. It is often possible to use an algorithm that implements one of these many-tree models and with little adjustment and tuning, obtain a model with excellent performance. It is, however, still important to tune/train the model.\nI already mentioned the main disadvantage: These models are hard to interpret. If the goal is to have a user-friendly model that could be used by humans, trees are best. If the user is ok with typing values into a computer (or smartphone) and letting some algorithms run in the background and produce answers, then more complex models might be ok.\nAnother possible disadvantage of many-tree models is that they generally take longer to train, so if speed is important, one might not be able to use all of these model variants to their full extent.\n\n\nAdditional comments\nAs mentioned, bagging and boosting are approaches that can be applied to methods other than trees. For instance, one can bag/boost linear models, GAMs, etc.\nThe many-tree methods described above are examples of what is often called ensemble methods. Boosting is an example of using an ensemble of weak learners, i.e., a combination of models that individually don‚Äôt perform that well, but when combined, they often have outstanding performance. The combination of different individual models/learners (weak or not) is often called model averaging or model stacking, and such methods lead to some of the best-performing models in machine learning. For more on this, see e.g., the Stacked Models chapter of HMLR. We will not further cover these models, but if you have a situation where you need the best model performance available, such models might be worth looking at.\n\n\nImplementing tree models in R\nThe tidymodels framework allows access to many different types of packages/algorithms that implement tree-based models. You can see the full list on the parsnip model search page.\nMany of these methods and packages are similar and differ in the details of the model implementation, the speed at which they run, if one can parallelize them, etc. Some common ones that people use are rpart for individual trees, ranger for random forests, and xgboost for boosting. Many others are available too, e.g., the bonsai package has several other methods. Often it doesn‚Äôt matter much which package you use. Sometimes you might need specific features that only a certain package can give you, so you need to go with that one.\nSince tidymodels is still under development, there are many other packages for tree-based and related model fitting that can‚Äôt yet be accessed through it. If you absolutely need one that‚Äôs not yet supported, you can either try to implement it through tidymodels or just write code that uses the model directly.\n\n\nFurther information\nFor more details on the many-tree methods described here, see chapter 8 of ISLR and chapters 10-12 of HMLR.",
    "crumbs": [
      "Content",
      "11. Machine Learning Models I",
      "Many-Tree Models"
    ]
  },
  {
    "objectID": "content/module-model-ml-1/polynomial-spline-models/polynomial-spline-models.html",
    "href": "content/module-model-ml-1/polynomial-spline-models/polynomial-spline-models.html",
    "title": "Polynomial and Spline Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover some statistical models that provide more flexible extensions to generalized linear models.\n\n\nLearning Objectives\n\nBe familiar with polynomial and spline models.\nUnderstand advantages and disadvantages of these models.\nKnow when to use them and how to minimize overfitting.\n\n\n\nGLM review\nSo far, we‚Äôve focused on generalized linear models (GLM), and mainly on the two workhorses of statistical analysis: linear models for regression modeling of continuous outcomes, and logistic models for classification modeling of categorical (binary) outcomes.\nAs mentioned, what GLMs have in common is that the predictor variables show up as a linear combination, which is then mapped to the outcome with some function. In general, one can write a GLM like this:\n\\[f(Y)=b_0 + b_1X_1 + b_2X_2 + \\ldots + b_nX_n,\\] where \\(Y\\) is the outcome, the \\(X_i\\) are the predictors, and the choice of the function \\(f(Y)\\) (called the link function) determines the specific model.\nFor a standard linear model, it‚Äôs simply \\(f(Y) = Y\\) and the model is \\[\\textrm{linear model:} \\qquad Y=b_0 + b_1X_1 + b_2X_2 + \\ldots + b_n X_n.\\] For a logistic model, it is \\(f(Y)=\\log \\left( \\frac{Y}{1-Y} \\right)\\) (this function is called the logit function) and the whole model becomes \\[\\textrm{logistic model:} \\qquad \\log \\left( \\frac{Y}{1-Y} \\right) = b_0 + b_1X_1 + b_2 X_2 + \\ldots + b_nX_n.\\] You can re-write this such that there is just \\(Y\\) on the left and exponentiated terms of the \\(X\\) on the right, you might have seen either notation for the model before in one of your statistics classes.\nIf, instead, your outcome was integer counts of something, you could use a Poisson model, with \\(f(Y) = log(Y)\\), and the model would be\n\\[\\textrm{Poisson model:} \\qquad \\log \\left( Y \\right) = b_0 + b_1X_1 + b_2 X_2 + \\ldots + b_n X_n.\\]\nSimilar GLMs for outcomes that are distributed in other ways are possible, see e.g.¬†the list on Wikipedia. In R, if you use the glm() function, you can set the link function by specifying the family argument. If you use tidymodels, there are similar ways to set the link function, depending on the underlying fitting function you use.\nThis model structure is easy to understand, one can test the impact of individual predictor variables by taking them in and out of the model (subset selection) or alternative approaches (e.g.¬†regularization), and one can see in the final model which predictor variables survive, and what their impact is, as given by the coefficients in front of the variables.\n\nIf you have a model that has a mix of categorical and continuous input variables, and you want to compare their impact by looking at the values of their coefficients, you need to standardize your continuous variables so they are on a similar scale as your categorical variables.\n\nBasic linear and generalized linear models (LMs and GLMs) such as logistic or poisson regression impose a specific structural relation between your inputs/predictors and your outcomes. This means that if you have more complex patterns relating your inputs with your output(s), basic GLMs might not capture it fully. For instance if you had a scenario where you measured the activity/movement of some animal and how that might be impacted by temperature, it is quite feasible to expect a curved relationship. At low temperatures, the animals try to stay warm and don‚Äôt move much. At high temperature, the animals also don‚Äôt move much and instead try to stay cool. At some intermediate temperature, their movement is largest. So if you had data for animal movement as outcome, and temperature as your predictor variable, a linear model would not be good. You wanted a model that allows the pattern to curve. If you start with a linear model and look at the plot of your residuals, or an outcome-prediction plot, you would see systematic deviations, telling you that your model is not missing important patterns in the data and you are therefore underfitting.\nIf that‚Äôs the case, you might want to move on to more flexible models.\n\nEven if you suspect (or know) that the true relation between input and output is not linear - and in most cases, that is true, - it is still often good to start with a linear model. Often, things are approximately linear, and if you don‚Äôt have a lot of data, have a good bit of noise, and are trying to capture the relation between multiple input variables and the outcome, it is quite possible that the data doesn‚Äôt allow you to go beyond checking for linear relations. So it‚Äôs generally good to start with a linear model, then explore the model results (e.g., residual plots) and if you see indications for underfitting, start extending your model.\n\n\n\nPolynomial models\nA natural extension of GLM is to include higher-order terms for the predictors, either with itself or with other predictor variables (the latter case is called an interaction term). For instance for a simple linear model, this model includes second-order terms for \\(X_1\\) and \\(X_2\\) and also an interaction term between the two variables:\n\\[Y=b_0 + b_1X_1 + b_2X_2 + b_3 X_1^2 + b_4 X_2^2 + b_5 X_1X_2\\]\nSuch models with higher order terms (\\(X^2\\), \\(X^3\\), ‚Ä¶) are often called polynomial models. Note that statisticians still call this a linear model, since the coefficients show up in linear combinations, even though the variables do not. (Statisticians or statistics books will often say that a linear model is ‚Äúlinear in the parameters.‚Äù This means the same thing as having a linear combination of coefficients.) It took me years to figure out that terminology, since a physicist/engineer would call this a non-linear model!\nThe advantage of polynomial models is that they can more flexibly capture patterns in the data. A major shortcoming of polynomial models is that as the predictor values get very large or small, the predicted outcomes go ‚Äúthrough the roof‚Äù (a technical statistical term üòÅ). Let‚Äôs say you had data for the animal movement-temperature example that had temperature up to 30 degrees Celsius and showed a marked decline in movement going from 20C to 30C. A 2nd order polynomial (also called a degree 2 polynomial‚Äìthe number is the highest power that the dependent variable is raised to in the equation) that curved down as temperature went up did a good job at modeling the data. It is quite likely that if you try to predict higher temperatures, say 35C or 40C, movement might turn negative! That of course can‚Äôt happen, an animal can‚Äôt move less than not at all. These are general problems with polynomial functions, they impose an overall structure on the data and generally therefore behave poorly for values outside the range of the data. Thus, while easy to use, those models are generally not the best choice, the ones we discuss next are generally better.\n\n\nSpline models\nSpline models are similar to polynomial models, inasmuch as they allow higher order terms of the predictor variables to show up in the model, and thus can capture patterns that go beyond linear relations. They try to deal with the potential problem of imposing a global structure done by polynomial models by applying polynomial combinations of predictors only to parts of the data, with connection points (knots) between those parts. The end result is a smooth function that allows for capturing of potentially non-linear patterns without the need to impose a global structure. This often improves the quality of the fits. Though even for these kinds of functions, extrapolation outside the range of observed/measured predictor values can be tricky and often lead to wrong results (in fact, a general problem for models). These models are often more difficult (or at least more complicated) to interpret as well.\n\n\nLocal regression\nWhen using ggplot2, you likely came across a local regression curve. The default smoothing for small data if you use geom_smooth() is a LOESS (LOcally Estimated Scatterplot Smoothing) model, which is a type of local regression. You can think of it as being conceptually very similar to spline models, though the details of how it‚Äôs implemented differ. It is perfectly fine to use smoothing functions without making any adjustments during data exploration. But if you want to use any of these for actual statistical fitting, you have to tweak/tune them as discussed below.\n\n\nStep functions\nA version of the models just discussed are those that allow non-smooth jumps in the outcome as a predictor varies. As a simple example, say you want to model current time as a function of geographical location. Since humans divided the world arbitrarily into time zones, as you move from one zone to another, there is an abrupt change in the outcome (time) with a small change in the predictor (longitude). A bad way of modeling that is to discretize your predictors, since most often you don‚Äôt know where the cut-offs are. Maybe the example I just gave is a bad one, since this is a rare situation where we do know the cut-offs, because they are human made üòÑ. But say we assume some outcome changes somewhat abruptly with age. It is unlikely that we know exactly what age(s) we should set the cut-off(s) at. By using a model, we can let the computer decide where to set the cut-off (see below). Other model types that are good at handling rapid changes in outcome for a change in a predictor are tree-based models, which you‚Äôll be learning about shortly.\n\n\nGeneralized additive models (GAM)\nGeneralized additive models (GAMs) are in some sense (though not completely, see below) a general form of the different models just discussed. The preserve the feature that each predictor enters the model additively. But now, each predictor can be related to the outcome with a ‚Äì potentially complicated, potentially unknown ‚Äì function. In equation form, a GAM can be written as:\n\\[g(Y) = x_0 + f_1(x1) + f_2(x2) + \\ldots + f_n(x_n).\\] Here, the \\(f_n\\) are functions of the precictor \\(x_n\\), and \\(g()\\) can again be some overall link function.\nFor instance one could have a GAM where some predictor is sinusoidally related to the outcome, e.g., \\(f_1(x_1) = sin(x_1)\\). GAMs allow for more flexible relations between predictors and outcome than GLMs, and are still fairly easy to fit and interpret (but not quite as easy). Also, to provide good functions to map predictors to outcome, one needs to know something about the underlying system. If that‚Äôs not the case, one often uses spline function, and let the data determine the exact shape of the splines. One restriction of GAM is the same as all additive models: having each predictor enter the model separately can lead to interactions being missed.\n\n\nFitting polynomial/spline models\nThe models just discussed are more flexible and allow capturing more intricate patterns in the data. Of course, the downside of this is that they might not only capture patterns that are due to real signals in the data, but might also try to capture spurious patterns that are entirely due to noise/randomness in the data. In other words, these more flexible models are easily prone to overfitting.\nBy now, you have learned what you can do to try and minimize overfitting. Most importantly, you can build your models and decide how complex to make them by cross-validation. You can think of model parameters that determine if you should have a 2nd/3rd/4th/etc. order spline functions as a tuning parameter, and you let the cross-validation procedure decide if a 2nd order or 3rd order model is better ‚Äì based on model performance during the cross-validation process. As you will see, using the tidymodels framework allows you to easily do this model training/tuning.\n\n\nModel fitting in R\nA lot of separate packages exist to fit the models discussed here. Not all, but several of the most important ones, can be accessed through the tidymodels framework. We will explore some of them in the exercises so you get a general idea of how this works. You can then explore more models as interested or as needed for your specific research questions.\n\n\nFurther information\nThis short summary closely followed the information in the Moving Beyond Linearity chapter of ISL. Check out that chapter for further information on the models covered here, as well as a few additional variations. HMLR also discusses several models of this type. See for instance the Multivariate Adaptive Regression Splines chapter of HMLR. The Smoothing chapter of IDS also covers the topic discussed here. For specific ways of using some of these models through tidymodels, check out some of the tutorials in the tidymodels ‚Äúlearn‚Äù section.\nThe references above for each class of models provide further reading. Those 3 sources, namely ISL, IDS and HMLR are very good starting points for learning more about different machine learning methods. The Resources section of the course provides some further pointers to additional material, and of course there is a lot of other, often free, information available online. You should be able to find more details on any of these methods with just a few online searches.",
    "crumbs": [
      "Content",
      "11. Machine Learning Models I",
      "Polynomial and Spline Models"
    ]
  },
  {
    "objectID": "content/module-model-ml-2/da-models/da-models.html",
    "href": "content/module-model-ml-2/da-models/da-models.html",
    "title": "Discriminant Analysis Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover discriminant analysis, which is a set of machine learning/statistical models that is very useful for classification.\n\n\nLearning Objectives\n\nBe familiar with discriminant analysis models.\nUnderstand advantages and disadvantages of these kinds of models.\n\n\n\nIntroduction\nWhile logistic regression is the most widely used approach for classification, it has several limitations. For one, it doesn‚Äôt easily generalize to outcomes that have more than 2 categories. Also, for certain types of data, logistic regression might not perform very well. Other methods like tree-based models, KNN or SVM can be used for multi-category outcome classification and often provide good performance. Another option are Discriminant Analysis (DA) methods. Those approaches are fairly quick and easy to implement and fit. At times, they provide better performance than logistic regression and might do as well as more complicated models.\n\n\nGeneral Idea behind DA\nDA make assumptions about the distributions for each predictor, and then uses those distributions to model and predict the outcome. This is called a generative approach.\nThere are different forms of DA. Linear Discriminant Analysis (LDA) is the simplest approach, it assumes that predictors can be modeled with a multivariate normal distribution. Nonlinear discriminant analysis approaches, e.g., quadratic DA, allow for a more flexible distribution of the predictors. This makes the model more flexible, but also bigger (so possibly more prone to overfitting). One can show that LDA and logistic regression are mathematically rather similar (see e.g.¬†the ISLR book). Other nonlinear variants of DA exist, e.g.¬†regularized, mixture, flexible, ‚Ä¶\nAs for any model, there is always a bias-variance/underfitting-overfitting trade-off between less flexible approaches like LDA and more flexible approaches like some of the nonlinear DAs.\n\n\nStrengths and Weaknesses of DA\nA nice feature of DA is that it is one of the algorithms that allows for classification with more then 2 outcomes. For two outcomes, there are situations where logistic regression struggles (if the outcomes are well-separated), and DA might perform better.\nDA models are not as common as logistic regression, thus it might require a bit more explanation to your audience if you decide to use them. Also, DA models make assumptions about the distributions of the predictors. If those assumptions do not hold, DA might not perform well. In situations where logistic regression works well, it usually performs better. Like logistic regression, DA is used for classification, not regression.\n\n\nDA in tidymodels\ntidymodels has the discrim package, which implements several forms of discriminant analysis.\n\n\nFurther Resources\nChapter 4 of ISLR discusses discriminant analysis. So does the Generative Models section in the Examples of Algorithms chapter in IDS.",
    "crumbs": [
      "Content",
      "12. Machine Learning Models II",
      "Discriminant Analysis Models"
    ]
  },
  {
    "objectID": "content/module-model-ml-2/svm-models/svm-models.html",
    "href": "content/module-model-ml-2/svm-models/svm-models.html",
    "title": "Support Vector Machine Models",
    "section": "",
    "text": "Overview\nIn this unit, we will very briefly cover support vector machines (SVM), another useful type of machine learning/statistical model.\n\n\nLearning Objectives\n\nBe familiar with SVM.\nUnderstand advantages and disadvantages of SVM.\n\n\n\nIntroduction\nSVM were developed in the computer science and engineering communities and were among the best-performing types of models for a while. They are not that exceptional anymore (now the big trend is neural networks), but still often perform rather well, especially sometimes for data where other models don‚Äôt perform quite as well. So SVM are often a good idea to try.\n\n\nGeneral Idea behind SVM\nSVM have cost/objective functions that do not penalize discrepancy between true and predicted outcome if the two are within a certain range, and do penalize outside that range. This is different than penalties based on basic least squares, which implement a penalty for any difference between model and data. SVM divide the predictor space into areas using multi-dimensional hyperplanes until observations within the same ‚Äúvolume‚Äù are sufficiently accurately predicted (a bit like trees).\nSVM can come in different flavors, for instance depending on how discrepancies between data and model are penalized (absolute distance, quadratic, etc.) or how the boundaries of the hyperplanes are defined.\n\n\nStrengths and Weaknesses of SVM\nSVM can be used for both classification and regression. They are very flexible and often have very good predictive performance. They are often well suited for data that contain outliers.\nThe math behind SVM is not that easy to follow, and since SVM models are somewhat complex, they tend to be a bit of a black box and the model itself is hard to interpret.\n\n\nSVM in tidymodels\ntidymodels/parsnip has several SVM models, such as svm_linear() and svm_poly(). They can be used as part of the tidymodels workflow like any other ML model you have already seen. They also need tuning.\n\n\nFurther Resources\nTo learn a bit more about SVM, you can check out chapter 9 of ISL which discusses SVM. So does chapter 14 of HMLR.",
    "crumbs": [
      "Content",
      "12. Machine Learning Models II",
      "Support Vector Machine Models"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-documentation/ready-documentation.html",
    "href": "content/module-ready-workflow/ready-documentation/ready-documentation.html",
    "title": "Project documentation",
    "section": "",
    "text": "Overview\nFor this unit, we will discuss why and how to document the different parts of your project.\n\n\nLearning Objectives\n\nKnow why documenting everything is essential.\nKnow ways to efficiently document different aspects of your project.\n\n\n\nIntroduction\nIf you do not explain the why, what and how of your project, it is essentially impossible for someone else to understand what‚Äôs going on. They might be able to technically reproduce what you did by running code in the order you specified, but they will unlikely be able to understand what‚Äô going on in enough detail to be useful to them.\nThus, any practically meaningful reproducibility attempt requires ample documentation.\n\n\nDocument your project, folder and files\nTogether with a good structure for your project, you should provide documentation to orient users. The easiest way is to have readme text files. Plain .txt text files or simple.md Markdown files are best. There should be one for the overall project, and then as suitable for individual folders. Those files don‚Äôt need to be too detailed, but should explain what is to be found where, and what different files contain/do.\nAn important bit is to keep those readme files updated. If your documentation talks about files and folders that don‚Äôt exist because you renamed or deleted them, it is confusing. So if you do major (or even minor) updates, take a few seconds to update your readme files too. Definitely make sure they are up to date before you share it with others, e.g.¬†collaborators or reviewers or readers.\n\n\nDocument your process\nIn addition to explaining your structure and the basics of your files, you should document what you are doing. This is in some sense the equivalent to a lab notebook. Some of this information might later make it into a finished product, but most of it might not. An ideal format for documenting is text that can be formatted somewhat, such as the markdown format. Quarto is excellent since it allows you to combine your code with documentation in a seamless manner.\nThis documentation should explain why you are doing what you are doing, and what you are thinking as you look at results, plots, etc.\n\n\nDocument your code\nThere is overlap between documenting your process and your code, but they are not the same. For instance you might write some commentary explaining that you deal with missing values by removing a few variables that contain mostly missing values, and for the remaining variables, remove any observations that has at least one missing. This would be the general explanation. Then you write code to do that. It is generally useful to document each line of code to explain what it does. This might seem like too much. But trust me, it helps. Code can get complicated, something that was obvious to you when you wrote it will not be in a few weeks, or will not be clear to someone else. Therefore, a good rule of thumb is that your code should be 50% or more comments. Each line of code can use a comment, and then have additional blocks of comment to explain what certain chunks of code are doing.\nThere is almost never too much documentation in code, but there certainly often is not enough.\nCommenting also helps you think clearly about what you are doing. If you end up with code that might not have enough comments, you could also try to ask some AI tool to add comments for you.\n\n\nSummary\nDocument everything. A lot. Others and your future self will thank you. Remember to documentation how your code works and also why you wrote the code you did.\n\n\nFurther Resources\n\nThe organization Write the Docs has a website with many resources, including a brief guide for beginners.\nGoogle‚Äôs developer documentation style guide is a very comprehensive free resource.\nYou might be interested in the idea of documentation driven development (DDD) (see i.a. this blog post), which is the idea that you should write down what your code will do, how a user will do each task, and how the code will internally do what it is supposed to before you even write the code down. It is a somewhat trendy philosophy that is maybe not 100% practical but will result in good (and often revised) documentation, by forcing you to do the thinking critically about how your code should work instead of just trying things.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Project Documentation"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-open/ready-open.html",
    "href": "content/module-ready-workflow/ready-open/ready-open.html",
    "title": "Open Research",
    "section": "",
    "text": "Overview\nThis unit provides an introduction to the concept of open research/science.\n\n\nLearning Objectives\n\nUnderstand the general idea of open science\nExplain what makes research reproducible and/or replicable\nKnow what FAIR data standards are\n\n\n\nIntroduction\nWhile READy principles and openness are not necessarily the same, they are often connected.\nThe idea is that if you provide others access to your data, code and other components of analysis - the open part - it is generally only useful if this is done in a READy framework. Otherwise, it might become close to impossible for others to reproduce and further build on your work.\n\n\nThe general idea of open science\nThere are many ways of defining and describing ‚Äúopen science‚Äù. If you search online for ‚Äúwhat is open science,‚Äù you will likely find various conceptual diagrams and descriptions. For instance, the Wikipedia Open Science page has a pretty good overview and diagram.\nThe basic idea is that - as much as possible - data, code, and any other related product is provided publicly and free of charge, in a way that is easily findable, accessible, and usable by others.\nIn general, a reproducible workflow makes it easier for others to find/access/and reuse your work, thus the connection between reproducibility and open science.\n\n\nFederal open science requirements\nWhile the concepts of reproducible research and open science have been around for a while, they have recently increased in prominence. Federal agencies like the NIH have been requiring data sharing and public releases of papers (on PMC) for some time now, but these recommended measures are even stronger, and in my opinion, a very positive direction for science. It is quite likely that if you work with federal funding in the near future, you will be required to share your data and code publicly as soon as you decide to publish. The exact implementation of these protocols hasn‚Äôt been determined yet (nor has the degree to which these recommendations will be formally implemented), but the writing is on the wall, so to speak.\nIt is easier for you to get comfortable with reproducible research methods and open science now, rather than scrambling to learn it in the future!\n\n\nAccessibility and FAIR data\nOpen access publishing is a big topic, but for the purposes of this class I will just say that I think you should publish Open Access whenever possible. In the future, it seems likely that any federally funded projects in the US will require an open access fund as part of the budget, as publishing OA can often be prohibitively expensive. If you want to learn more, check out the Wikipedia page on Open Access, or if you really want to learn more, see Peter Suber‚Äôs book on OA.\nThe main purpose of this section is to talk about data sharing. Data sharing used to be a pretty contentious topic, but fortunately it is more normalized now than it has ever been before, and thanks to NIH (and other agency mandates), it will likely be a normal part of science before too long.\nOf course just dumping messy data without any documentation into a depository is not great for accessibility and reproducibility. As some of you noticed during this course, even data from the CDC or other reputable organizations is often very poorly formatted and does not have a lot of metadata or documentation to help you out. To somewhat solve this issue, the FAIR data standards were created. FAIR stands for:\n\nFindable: when someone goes to your paper or project, they should be able to easily get to the data source.\nAccessible: users need to be able to access the data, and data are stored in a way that users can feasibly get them.\nInteroperable: data should be in standardized formats, using standard vocabulary. You should be specific if your data is derived from another dataset or if the user will need other related datasets.\nReusable: data should be documented and version-controlled when you release it, with clear descriptions of what information is contained in the data, who can use the data, and for what purposes.\n\nThe FAIR standard was originally described in this 2016 publication. Another great resource is the GO FAIR initiative. It is worth taking a short look at both these resources.\nOf course dealing with things like data use agreements and individual privacy is paramount, so 100% achieving these goals all the time is not feasible. Sometimes you might need to use synthetic data based on your real data, or remove certain information from your data to protect privacy, or require a data use agreement before you can distribute the data to individuals. We should strive for data to be as FAIR as possible, but protecting individual privacy is equally important!\nIn general, a lot of authors still hide behind ‚ÄúI can‚Äôt share my data, it‚Äôs confidential‚Äù as an excuse for not having to share. Most of the time, it is possible to share de-identified data without confidentiality problems. Hopefully, the near future will make data sharing much more common and also easier.\n\n\nInclusivity\nThe topic of inclusivity in data science is heavily tied into the idea of data ethics ‚Äì critical thinking about the potential results and consequences of the products we create is crucial in data science. Many of you have some public health training, and are therefore likely familiar with the Belmont Report.\nRegardless of your scientific background, when working with data (especially human subjects data), the guiding principles of respect for persons, beneficence, and justice should be critical considerations for all of our research.\nFor some additional reading on inclusivity and data science ethics, see for instance these resources:\n\nElaine O Nsoesie, Sandro Galea, Towards better Data Science to address racial bias and health equity, PNAS Nexus, Volume 1, Issue 3, July 2022, pgac120, https://doi.org/10.1093/pnasnexus/pgac120\nO‚ÄôNeil, Cathy. Weapons of math destruction. Crown, 2016. ISBN: 978-0553418811.\nNoble, Safiya Umoja. Algorithms of Oppression: How Search Engines Reinforce Racism. NYU Press, 2018. ISBN: 978-1479837243.\nhttps://datapractices.org/manifesto/\nFloridi L, Taddeo M. What is data ethics?. Philos Trans A Math Phys Eng Sci. 2016;374(2083):20160360. doi:10.1098/rsta.2016.0360\n\n\n\nPractical considerations\nIt is all nice and good to tell people that their data needs to be FAIR, and that their science needs to be open. But the question is, why do it? Most people might agree that there are societal benefits. For instance having a resource like GenBank for genetic sequences has allowed many scientists beyond those who created the original sequence to use the data and answer important scientific questions. However, there generally need to also be benefits for individuals to entice them to spend time following Open Science standards. This is happening increasingly. On one hand, tools to do research in an Open Science framework are getting increasingly better. For instance the whole R + Quarto framework we‚Äôve been using in class makes it rather easy to do things automated and reproducible. Many other similar tools and resources are becoming available. Another potential benefit of Open Science is increased visibility. If others can use your data and models, they will likely cite your work. They might even want to collaborate with you. It seems for those individuals who are good at sharing, the benefits outweigh the perceived risks (such as ‚Äúbeing scooped‚Äù). And the final, ever increasing ‚Äúbenefit‚Äù is that publishers and funding agencies increasingly require it. So if you don‚Äôt follow Open Science standards, it might soon get hard to publish work or get funding. It‚Äôs less of a ‚Äúbenefit‚Äù and more of an ‚Äúavoid the penalties‚Äù thing, but regardless, it is an important practical consideration.\n\n\n\n\nSummary\nWhile open research is not the same as READy principles, there is a lot of overlap. With the increasing requirements for research (data as well as code) to be made openly accessible and FAIR, it is important to ensure your analyses are done in a way that it is easy to publicly share your results and for others to build on it. The READy principles will ensure this is the case.\n\n\nFurther Resources\n\nThe Center for Open Science, a nonprofit dedicated to promoting open science practices and community change, maintains a blog with several thought provoking readings.\nStatistician Andrew Gelman sometimes writes about open science, mostly from a statistical perspective and has some nice comments on replication. (1) (2)",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Open Research"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-overview.html",
    "href": "content/module-ready-workflow/ready-overview.html",
    "title": "Reproducible, efficient, automated, and documented (READy) workflows",
    "section": "",
    "text": "Overview\nThis module covers different aspects and topics related to data analysis projects done in a reproducible, efficient, automated, and documented (READy) manner.\n\n\nLearning Objectives\n\nKnow different components of a reproducible, efficient, automated, and documented (READy) workflow and approaches to implementing it.\n\n\n\nContent\nThe module consists of the following units:\n\nIntroduction to READy\nProject organization\nProject management\nSoftware management\nProject documentation\nTools for a READy workflow\nOpen research",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "READy Workflows Overview"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-software-management/ready-software-management.html",
    "href": "content/module-ready-workflow/ready-software-management/ready-software-management.html",
    "title": "Software management",
    "section": "",
    "text": "For this unit, we will discuss ways to make sure your software is managed in such a way that it allows future reproducibility.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Software Management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-software-management/ready-software-management.html#r-package-renv",
    "href": "content/module-ready-workflow/ready-software-management/ready-software-management.html#r-package-renv",
    "title": "Software management",
    "section": "R package renv",
    "text": "R package renv\nThe renv package (renv stands for reproducible environments) is a tool that allows you to install all packages you use for a given project as part of that project. It records the packages and exactly which version of the package is used. If you then give your project to someone else, they can install these exact versions of the packages on their machine, and therefore ensure that they are using the same versions you did.\nNote that this only holds for R packages, not R itself. But again, R doesn‚Äôt change that rapidly, so even if you run something under a newer or older version of R, most of the time it is not much of a problem.\nBecause reproducibility issues related to outdated packages are quite common, it is a good idea to use renv for all your data analysis projects. Once you got the hang of it, it is fairly easy to use. The renv website has a good tutorial.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Software Management"
    ]
  },
  {
    "objectID": "content/module-ready-workflow/ready-software-management/ready-software-management.html#renv-alternatives",
    "href": "content/module-ready-workflow/ready-software-management/ready-software-management.html#renv-alternatives",
    "title": "Software management",
    "section": "renv alternatives",
    "text": "renv alternatives\nThere is a package called packrat. It‚Äôs basically the older version of renv and not as good. There is also a package called capsule, which is supposed to be an renv alternative. I have no experience with it.\nPosit (formerly R Studio) has the Posit package manager, which is an alternative to renv. It‚Äôs more powerful, but also more complex and not free.\nThere are further, more specialized options. For instance the company Metrum Research Group has a tool called the Metrum Package Network. It is also fairly complex and not recommended for individual use.\nBasically, at the time of this writing, renv is pretty much the only useful option for individuals working on projects and wanting to manage R packages.",
    "crumbs": [
      "Content",
      "3. READy Workflows",
      "Software Management"
    ]
  },
  {
    "objectID": "content/module-stats-overview/assessment-statistical-analysis.html",
    "href": "content/module-stats-overview/assessment-statistical-analysis.html",
    "title": "Assessment - Statistical Analysis Overview",
    "section": "",
    "text": "Quiz\nGet the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.\n\n\nExercise\nThere is no exercise this week. Focus on part 2 of the project.\n\n\nProject\nFinish (and make sure to push) part 2 of the project. There is no need to submit a link. Unless you renamed your repository, I know where to find it based on your part 1 submission.\n\n\nDiscussion\nFind a scientific paper of your choice online that does a data analysis. Explain what types of analyses the authors did (based on the categories described in this module). Also discuss what the (stated or not stated) objective of the paper/study was. Comment on if you think the authors performed the right analysis given their goals, and state your opinion if they interpreted/presented their results properly. If you think they didn‚Äôt do their analysis right or didn‚Äôt present/interpret their results correctly, make suggestions for what they should have done.\nYou don‚Äôt need to go into modeling details, a big picture description is enough. For instance if the authors clearly did a secondary analysis collected for a different purpose, but present their findings as if they showed causal relations, point out this mis-representation (there‚Äôs no shortage of studies like that).\nPost your findings and commentary by Wednesday. Then read through and comment on some of the studies your classmates posted.",
    "crumbs": [
      "Assessments",
      "7. Statistical Analysis Overview"
    ]
  },
  {
    "objectID": "content/module-stats-overview/statistical-analysis-introduction/statistical-analysis-introduction.html",
    "href": "content/module-stats-overview/statistical-analysis-introduction/statistical-analysis-introduction.html",
    "title": "Statistical Analysis Overview",
    "section": "",
    "text": "Overview\nIn this Unit, we will take another look at the whole data analysis process. Here, we focus on the actual (statistical) analysis (which I also call model fitting) component of the data analysis process. A general conceptual understanding is useful before we jump into looking at and applying specific modeling/statistical approaches.\n\n\n\nSource: VALDO.com\n\n\n\n\nLearning Objectives\n\nBecome familiar with different types of analyses.\nLearn how the type of data determines the modeling approach and be able to choose the right methods.\nUnderstand the overall goals of fitting statistical models to data.\n\n\n\nIntroduction\nSo far in this course, you briefly encountered a few simple statistical models in some of the readings and exercises (e.g., applying a linear model), but we didn‚Äôt focus on it. Instead, we looked at a lot of other components that are important for a full data analysis project but are less often taught. You might have come across the statement (e.g., in R4DS) that 80% of the time spent on data analysis is in the non-modeling/non-statistical parts. From my experience, that is true. While the statistical modeling/analysis part is certainly important, it often takes a fairly small amount of the whole project time. And while it is certainly possible to use the wrong statistical model, it seems to me that the most important and consequential mistakes that might invalidate an analysis do not happen in the modeling part. Sure, people often use the wrong statistical method, but that has - in my experience - often surprisingly (and thankfully!) little impact on the results. Not properly cleaning and processing data (e.g., not realizing that weight is reported in pounds and not kilograms when computing BMI) often has a much bigger impact on results.\nNo matter what statistical analysis you end up using, you will need to do the steps of getting and cleaning/processing/wrangling the data. During that process, you also explore your data, e.g., through plots and tables. Doing those steps efficiently and accurately is crucial. After you are done with the preliminary steps of getting/cleaning/wrangling data, you can move on to your main goal, fitting models to data. Once you reach the stage where you are ready to fit models, most of the hard work is done.\nOnce you reach the state at which your data is ready for statistical analysis, you should have a good idea of the types of models that might be appropriate. The choice of model is partly driven by the data, and partly by the kind of question you want to answer. There are several reasons why you might want to apply statistical models to your data, we‚Äôll discuss them briefly.\n\n\nWhy fit models to data\nThere are several reasons why we might want to fit models. The following is a brief list. The next section goes into more detail.\n\nTo produce or test hypotheses (e.g., ‚Äúthere is a (linear) correlation between BMI and diabetes‚Äù)\nTo estimate parameters (e.g.¬†‚Äúa 1 unit increase of particulate matter leads to 2 more asthma attacks per person per year‚Äù)\nTo draw causal conclusions (e.g.¬†‚Äútaking statins causes reduction in cholesterol‚Äù)\nTo draw mechanistic conclusions (e.g.¬†‚Äúinterferon reduces HCV virus load by stopping production of new virus‚Äù)\nTo make predictions (e.g.¬†‚Äúexposure to N cholera bacteria leads to an infection with probability p‚Äù)\n\nDepending on the objective, you will likely be using different statistical approaches. Note however that there is no clear mapping. For instance you can use a linear model to come up with hypotheses (i.e., do an exploratory analysis), to test hypotheses (e.g., in a clinical trial), to estimate parameters, or make predictions. A lot of models can be used for different purposes. This might be initially confusing. Just keep in mind that while some models are better for some purposes (e.g.¬†a complex machine learning or artificial intelligence model might be good for predictions, but bad for causal conclusions), most models can be applied to more than one type of objective.\n\n\nData Analysis Types\nThe following way of categorizing types of data analyses follows The Art of Data Science (ADS) ‚Äì specifically, chapter 3. Now might be a good time to give that chapter another quick re-read. A very similar, shortened version of the ADA chapter 3 discussion can be found in Jeff Leek and Roger Peng‚Äôs article. The following figure from their paper gives a good summary of what follows.\n\n\n\nSource: Leek and Peng 2015\n\n\nThe most straightforward analysis is a descriptive one. At that stage, you process, summarize and present the data, and do not go further. You don‚Äôt need to fit any statistical models. A lot of data collected by surveillance systems or government agencies falls into the descriptive category. For most scientific projects, we often start with a descriptive presentation. E.g. Table 1 in a study often describes and summarizes the data source. Note that some authors have the bad habit of including model fitting quantities, such as p-values, in a descriptive table. Measures that involve fitting a model (such as p-values), go beyond a descriptive analysis and should therefore generally not be in a descriptive table.\nSometimes, a descriptive study is interesting and sufficient by itself. But often, we then want to go beyond the descriptive presentation of data. The most common analysis approach is associative. Here, we are looking for associations (i.e., patterns) in the data. We are interested in seeing if patterns exist (e.g., if there is a correlation between age and speed of solving mathematical problems) and what the shape of the pattern is (e.g., linearly increasing/decreasing or non-monotone). This is also called correlation analysis.\nDepending on the way we came up with our question and the data, the results from such an associative analysis can be interpreted as an exploratory or hypothesis-generating approach, or an inferential or hypothesis-supporting approach. In general, if you asked the question/posed the hypothesis first, then went out and collected the data and analyzed it to see if your hypothesis holds, you can interpret your findings as supporting or refuting your hypothesis. If you had data that were not collected to answer your question specifically, and you analyzed the data to see if you can find some interesting patterns, then your findings should be interpreted as hypothesis-generating.\nFor both exploratory and inferential settings, you are usually interested in understanding how specific inputs/predictors affect the outcome(s) of interest. For that reason, you generally want to keep your models fairly simple and easy to interpret.\nEssentially all standard statistical tests and approaches you are likely already familiar with (e.g., various statistical tests like t-tests, simple linear regression, or simple classification such as logistic regression) fall into these categories of associative, exploratory, or inferential. We will cover some of those modeling approaches in future units.\nOften, we would like to go from association to causation, i.e., we would like to say that not only does an increase in X correlate with an increase in Y, but that X causes that increase in Y. There are two ways of doing so. One is to collect the data in the right way, namely using a randomized clinical trial or equivalent lab science approach where all contributing factors but the input of interest, X, are controlled. This way, we can say that a change in X directly causes a change in Y. If the data does not come from such a study design, methods of causal inference (which we won‚Äôt discuss in this course) can sometimes help in trying to determine causality.\nClassical statistical models can get us as far as determining potential causal relations. If we want to go even further and not only try to answer if X causes Y, but how X causes Y, we will need to employ studies or models that are mechanistic. Such models explicitly include postulated mechanisms, and by comparing such models to data, one can often determine which mechanisms are more plausible. Mechanistic models are also beyond what we cover in this course (if you are interested in those, I teach two courses on mechanistic modeling in infectious diseases üòÉ).\nWhile the main goal of science is generally understanding a system as well as possible, outside of science, other goals are often more important. In applied/industry/commerce settings, one often does not care too much if or how exactly certain inputs influence outcomes of interest. Instead, the main purpose is to try and predict future outcomes given a set of inputs. In this case, the interpretability of your model is not that important. Instead, a predictive modeling framework is more important. That‚Äôs where complex models, such as machine learning and artificial intelligence (neural net) approaches come into play. Those models are too complex to allow much interpretation and understanding of the system, but often these kinds of models are very good at prediction and real world performance (e.g., differentiating cats from dogs in images).\n\n\nExamples of data analysis types\nAn example of an inferential analysis might be the question which, if any, immunological markers (e.g., cytokines in blood) are most influential for a given clinical outcome. Building a simple model here, e.g.¬†a linear model if the outcome is continuous (e.g., blood sugar level) or a logistic model if the outcome is binary (e.g., heart attack within 5 years or not), allows us to quickly and transparently investigate how each variable in our model affects the outcome and which variables (immunological markers) might be important to study further.\nA good example of a prediction model is the monitoring of credit cards by the issuing companies, who try to predict fraudulent transactions. To that end, they feed all the data they can get about you into their models, and if something happens that is unusual, you might get flagged, and your card denied, or you will have to call to confirm. In this case, the interest is not too much on how exactly all these data points about you and your behavior lead to the prediction of legitimate vs.¬†fraudulent, only that the accuracy of those predictions is high. Because of this, in situations where prediction is important, models tend to be more complex, and one is willing to trade simplicity and interpretability of a model for increased predictive performance.\n\n\nModeling methods based on data\nWhile the goal of the analysis will guide you toward choosing a general type of modeling approach, the data usually dictate in more detail what kinds of models are suitable. The main determinant of the model type to use is the outcome(s) one wants to analyze.\nFirst, is there even an outcome? While the majority of datasets have an outcome(s) of interest, that is not always the case. Data without a clear outcome are sometimes called unlabeled. For instance, we might have collections of images of cell types, and our question is if these images cluster into specific types of cells - without knowledge of what those types might be. Another example is a scenario where we might have a large dataset of customers and lots of information about each customer. We might want to know if those customers can somehow be grouped based on the data we have about them (with the goal to design a marketing campaign directed at specific groups). Methods for those tasks are generally called clustering methods or unsupervised learning methods. Examples are k-means clustering, principal component analysis, and neural networks. (Note that some of these methods can also be used with data that include outcomes.)\nThe more common data structure is one with a specific outcome(s) of interest. This is also referred to as labeled data. Since labeled data is the most common, we focus on it in this course. In this case, we use approaches referred to as supervised learning methods. Those can be further divided based on the type of outcome. If the outcome is continuous (or can be treated as such), we use regression approaches, or if the outcome is categorical, we use classification approaches.\nYou are likely already familiar with some of these approaches. Most basic statistical tests are simple models for regression or classification, i.e., they try to detect patterns in data with quantitative or categorical outcomes. Some other statistical methods are generalized linear models (which include the basic linear and logistic models), generalized additive models, trees, support vector machines, neural nets, k-nearest neighbors, linear discriminant analysis, and a lot of further methods, many of which are variants of others. Some, but not all, of the more complex methods can be applied to both quantitative and categorical outcomes. We will cover a few of these methods later in the course.\nNote that there is, unfortunately, no one method that is universally best for all data/questions. Both the type of question and the details of the data will influence the model choices. Often, there are several models that would be reasonable choices for a given setting, and in such instances it is often worthwhile (and fairly easy) to explore multiple alternative models.\n\n\nFurther reading\nFor some additional (as well as overlapping) information to what I wrote, read Chapters 1 and 2.1. An Introduction to Statistical Learning (ILS). You don‚Äôt need to work through it in detail and can skip over the math bits if you want to. But do try to get an overall idea of the concepts these chapters are trying to convey. Chapter 1 of HMLR is another good source that you should skim through. Again, try to get the main points (which will of course overlap with the text above and the other readings).\nThis recent paper A practical guide to selecting models for exploration, inference, and prediction in ecology provides a nice discussion of different modeling goals and what approaches one should use. We have not yet discussed some of these approaches, but will soon. You could skim through the paper now, then revisit later once we covered more of the topics discussed in there.",
    "crumbs": [
      "Content",
      "7. Statistical Analysis Overview",
      "Statistical Analysis Introduction"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "",
    "text": "In this unit, we explore the use of LLM AI tools to generate synthetic data with R.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Use synthetic data with AI tools"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#a-simple-example",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#a-simple-example",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "A simple example",
    "text": "A simple example\nUsing AI to generate code that creates synthetic data can be very efficient. Here is an example. I typed the following prompt into ChatGPT 3.5.\n\nWrite R code that generates a dataset of 100 individuals with ages from 18 to 49, BMI values from 15 to 40 and smoking status as yes or no. Assume that age and BMI are uncorrelated. Assume that smokers have a somewhat lower BMI. Then use the patchwork R package to generate a panel of ggplot2 plots. The first panel should show a violin plot with BMI on the y-axis and smoking status on the x-axis. The second panel should show a scatterplot with age on the y-axis and BMI on the x-axis. Add thorough documentation to your code.\n\nOnly the first part is really the data generation bit, but since it‚Äôs so quick and easy, I wanted some code that also explores the data so I can see if what it produced is what I had in mind.\nThe result was this fully-working code.\nSince the AI systems are continuously updated, it is possible that if you type the same prompt into ChatGPT, your code might look slightly differently, but hopefully it is still working.\nIn my experience, you usually don‚Äôt quite get what you want on the first try. But it‚Äôs easy to tell the AI to update the code until it does what you want it to, or at least is close and then you can do the rest by hand.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Use synthetic data with AI tools"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#one-more-example",
    "href": "content/module-synthetic-data/synthetic-data-ai-tools-r/synthetic-data-ai-tools-r.html#one-more-example",
    "title": "Using AI tools to generate synthetic data with R",
    "section": "One more example",
    "text": "One more example\nIt is possible to ask the AI to create more complex data. You might not always get exactly what you want, but it‚Äôs often worth a try. I gave this prompt to Mcirosoft Bing/Copilot in precise mode.\n\nWrite R code that creates data for N=100 individuals. Individuals ages are randomly distributed between 18 and 50 years. Assume that individuals belong to 3 treatment groups: placebo, low dose, and high dose. Individuals in each group receive either no drug, or 100mg of drug or 200mg of drug every week. Drug concentration is measured every other day. Assume that drug concentration follows an exponential decline between doses, with the decay rate being the same for the low dose and high dose groups.\nIn addition to drug concentration, cholesterol levels are measured daily. Assume that higher drug concentrations correlate with lower cholesterol levels.\nCreate a data frame that contains Patient ID, treatment group, age, time, drug concentration, and cholesterol level.\nFor function calls, specify explicitly the package from which the function comes from.\nAdd thorough documentation to the code so it is clear what each line of code does.\n\nThe code I got back did not quite work, so I provided this prompt:\n\nThe code above does not work. It gives this error :\nError in map(): In index: 1. With name: ID. Caused by error in patient$ID:\nPlease fix the code. Also add code at the end that plots the raw data for drug concentration as a function of time for all patients, stratified by treatment status. Also plot the mean for each group.\n\nThis is the code I got back.\nThe code runs and does some of the things I asked for, but not all. I specified that the drug is given weekly, but the code only contains a single dose at the start. I forgot to tell the AI for how long the data should be simulated. It picked 13 days, which means there should have been 2 doses of drug.\nOf course I could keep talking to the AI, telling it to update things such that dosing is weekly, maybe explicitly say that I want to run for 28 days, fix whatever else might not be quite right (I didn‚Äôt check the code too carefully). It‚Äôs so quick and easy, there‚Äôs no reason not to do a few more iterations and see how close the AI can get to what I want. Eventually, I will likely intervene and finish things off manually.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Use synthetic data with AI tools"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html",
    "title": "Why create and use synthetic data?",
    "section": "",
    "text": "In this unit, we discuss why you might want to create synthetic (simulated/artificial/fake) data as part of your analysis and what general approaches there are for doing so.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#knowing-the-truth",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#knowing-the-truth",
    "title": "Why create and use synthetic data?",
    "section": "Knowing the truth",
    "text": "Knowing the truth\nFor your simulated data, you know exactly how it is created (because you do it). Let‚Äôs say for your data analysis, you want to look at the relation between the daily dose of some statin drug a person takes, and if that has some association with their LDL cholesterol. Depending on the exact structure of your data, you might fit various simple or complex models and get some answers. You can ‚Äî and should! ‚Äî look carefully at the results to see if they make sense. But you never know for sure if the model ‚Äúgot it right‚Äù since you don‚Äôt know what the true (whatever that might mean) answer is. In contrast, if you created the data, you know exactly what the true answer should be. This allows you to test your models and see if they can recover what you know the truth to be. For instance, if you set up the data such that there is a linear relation between drug dose and outcome of interest, and you set the slope of this relation to some value, then when you fit a linear model you should expect to get that value back.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#no-problems-with-sharing",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#no-problems-with-sharing",
    "title": "Why create and use synthetic data?",
    "section": "No problems with sharing",
    "text": "No problems with sharing\nThere are often limitations (real or perceived) for sharing the actual data. An important reason might be confidentiality. This not a problem with simulated data. So if you want to be able to share your analysis with collaborators or readers when you publish, and for some reason you can‚Äôt share the actual data, you can share simulated data. This allows others to run all your code and reproduce your analysis workflow, even if they won‚Äôt get quite the same results since they are using simulated and not the real data. (In this case, you should also share the results you got on your simulated data, in addition to your real results, as a basis for comparison.)\nAnother important aspect of being able to share is the increasing use of AI models to help with data analysis. You can upload data to AI tools and request them to perform certain analyses and return the code that does the analysis. This can be very powerful and time-saving. However, you probably often do not want to give the AI your real data, as whatever you upload is often stored and reused by the companies owning the AI tool (this is part of what you agree to when you click that terms and conditions check box). What you can do instead is feed the AI synthetic data, ask it to write code to do the analyses you are interested in, then take the code and with probably only a few modifications, apply it off-line on your real dataset.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#easier-to-play-around-with",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#easier-to-play-around-with",
    "title": "Why create and use synthetic data?",
    "section": "Easier to play around with",
    "text": "Easier to play around with\nOften, real-world datasets are very messy and require a good bit of time to clean them up (wrangle them) to get data in a form that allows analysis. If you are not even sure if your idea/analysis makes sense, it would be a waste to spend a lot of time cleaning data that ends up not being useful. What you can do instead is simulate data that has similar structure and content to the real data, but since you create it, you can make sure it‚Äôs clean and easy to work with. Then you try your analysis ideas on that simulated data. If it works ok, there‚Äôs a chance (unfortunately no guarantee!) that it might also work for the real data. As you play around with your simulated data, you might realize that certain ideas/approaches you initially had won‚Äôt work, and you can modify accordingly. Once your explorations using the simulated data suggest that your analysis plan can work, then you can start the often arduous task of cleaning and wrangling your real data.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#make-it-all-up",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#make-it-all-up",
    "title": "Why create and use synthetic data?",
    "section": "Make it all up",
    "text": "Make it all up\nCompletely making up data is the most flexible approach. You can try to produce synthetic versions of the full dataset, or just parts that are of interest. Let‚Äôs say in the statin-cholesterol example above, you real data set also contains information on the age and sex of the individuals, and if they have any pre-existing heart condition. Maybe in a first pass, you don‚Äôt want to look at those and just explore the main statin-cholesterol relation. Then you can just simulate data for that part. You can add further information ‚Äî including simulated characteristics that are not in the original data!‚Äì - as needed.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#summarize-the-original-data-then-re-create",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#summarize-the-original-data-then-re-create",
    "title": "Why create and use synthetic data?",
    "section": "Summarize the original data, then re-create",
    "text": "Summarize the original data, then re-create\nEven if you fully make up your synthetic data, you want it to somewhat resemble the original one. The easiest way is to just glimpse at the variables in the original data and generate new data that looks approximately similar.\nIf you want to be a bit more rigorous and get closer to the original data, you can also statistically summarize the original data, then use those summary distributions to generate new data. For instance, let‚Äôs say you find that the cholesterol variable can be well approximated with a normal distribution. If you fit the normal distribution to the data, you can get the mean and standard deviation. Then you can generate synthetic cholesterol data coming from a normal distribution with those estimated mean and standard deviation values.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#scramble-the-data",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#scramble-the-data",
    "title": "Why create and use synthetic data?",
    "section": "Scramble the data",
    "text": "Scramble the data\nIf you already have the data in a clean form that you can work with, you can use that data and scramble things to make new data. Basically you can randomly re-arrange variable values for different individuals, such that the new data does not correspond to the real data anymore, but has the overall same structure, and the same values (just re-arranged between individuals).\nFor instance you can take the age of each person, and randomly re-assign it to someone else. Note that this breaks potential patterns. For instance if in the original data, there happen to be an association between cholesterol and age, once you re-shuffled age values, this pattern will change. So the results you get from scrambled data will possibly be different, but since it has exactly the same structure and the same values as the original data, your whole analysis pipeline should work on the scrambled data. Of course, since you didn‚Äôt create the data, you don‚Äôt know the ‚Äútruth‚Äù and as such can‚Äôt assess fully if your analysis gives you the right results.\nIf you have already done your analysis and want/need scrambled data for sharing, e.g.¬†as part of a submission to a journal where you can‚Äôt submit the original data, you can try to do the reshuffling in a way that preserves patterns you identified. For instance if you saw that age was correlated with LDL, you can do your reshuffle such that age and LDL pairs stay together, while other variables (e.g., sex) get randomly reassigned. Of course you need to change at least some values for each individual, you can‚Äôt just move all variables for an individual from one row to another, it will still be the same data for that person, thus associated with potential confidentiality issues.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#do-a-mix",
    "href": "content/module-synthetic-data/synthetic-data-introduction/synthetic-data-introduction.html#do-a-mix",
    "title": "Why create and use synthetic data?",
    "section": "Do a mix",
    "text": "Do a mix\nIt is of course entirely possible to combine the two approaches above. For instance, you can start with the original data and do some scrambling if needed. Then you can replace some variables with the simulated data you generated. This allows you to test your analysis more thoroughly since now you know what you put in, so you can check if you get that out.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Introduction and Motivation"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html",
    "title": "Generating synthetic data based on existing data",
    "section": "",
    "text": "In this unit, we discuss how to generate synthetic data based on existing data.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data based on existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#ai-help",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#ai-help",
    "title": "Generating synthetic data based on existing data",
    "section": "AI help",
    "text": "AI help\nYou probably don‚Äôt want to feed your real data to the AI. So the first part of summarizing existing data has to be done in a safe/secure space. Once you have good data summaries, you can use AI to generate synthetic data and code.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data based on existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#example",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#example",
    "title": "Generating synthetic data based on existing data",
    "section": "Example",
    "text": "Example\nWe‚Äôll use the data we generated in example 1 of the previous tutorial and now assume that this is ‚Äúreal‚Äù data and that we want to generate synthetic data that‚Äôs similar to the real data.\n\nSetup\nFirst, we do the usual setup steps of package loading and other housekeeping steps.\n\n# make sure the packages are installed\n# Load required packages\nlibrary(here)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(gtsummary)\n\n\n# Set a seed for reproducibility\nset.seed(123)\n# Define the number of observations (patients) to generate\nn_patients &lt;- 100\n\n\n\nLoad and explore data\nIn a first step, we want to understand how each variable in the real data set is distributed, so we can create synthetic data that looks very similar.\nWe can use various helper functions from different packages to get good descriptive summaries of the data and variables. This code below shows two such helper functions.\nNote that we added more information to the table output. The table summary function treats the date variable and patient ID variable as numeric, so the output is somewhat nonsensical. But this is just for internal ‚Äúquick and dirty‚Äù use, so we don‚Äôt need to make things pretty. Of course you could if you needed/wanted.\n\ndat &lt;- readRDS(\"syn_dat.rds\")\nskimr::skim(dat)\n\n\nData summary\n\n\nName\ndat\n\n\nNumber of rows\n100\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nDate\n1\n\n\nnumeric\n5\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n4\n6\n0\n2\n0\n\n\nTreatmentGroup\n0\n1\n1\n7\n0\n3\n0\n\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\n\nEnrollmentDate\n0\n1\n2022-01-08\n2022-12-30\n2022-06-25\n86\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nPatientID\n0\n1\n50.50\n29.01\n1.0\n25.75\n50.50\n75.25\n100.0\n‚ñá‚ñá‚ñá‚ñá‚ñá\n\n\nAge\n0\n1\n45.90\n9.14\n21.9\n40.08\n45.60\n51.92\n66.9\n‚ñÅ‚ñÉ‚ñá‚ñÖ‚ñÇ\n\n\nBloodPressure\n0\n1\n127.96\n21.31\n91.3\n110.67\n130.85\n147.38\n159.5\n‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñá\n\n\nCholesterol\n0\n1\n174.67\n32.58\n88.9\n153.23\n171.85\n196.62\n271.0\n‚ñÅ‚ñÜ‚ñá‚ñÉ‚ñÅ\n\n\nAdverseEvent\n0\n1\n0.29\n0.46\n0.0\n0.00\n0.00\n1.00\n1.0\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÉ\n\n\n\n\ngtsummary::tbl_summary(dat, statistic = list(\n  all_continuous() ~ \"{mean}/{median}/{min}/{max}/{sd}\",\n  all_categorical() ~ \"{n} / {N} ({p}%)\"\n),)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1001\n\n\n\n\nPatientID\n51/51/1/100/29\n\n\nAge\n46/46/22/67/9\n\n\nGender\n\n\n\n\n¬†¬†¬†¬†Female\n49 / 100 (49%)\n\n\n¬†¬†¬†¬†Male\n51 / 100 (51%)\n\n\nTreatmentGroup\n\n\n\n\n¬†¬†¬†¬†A\n43 / 100 (43%)\n\n\n¬†¬†¬†¬†B\n30 / 100 (30%)\n\n\n¬†¬†¬†¬†Placebo\n27 / 100 (27%)\n\n\nEnrollmentDate\n2022-06-30/2022-06-25/2022-01-08/2022-12-30/107.06744593852\n\n\nBloodPressure\n128/131/91/160/21\n\n\nCholesterol\n175/172/89/271/33\n\n\nAdverseEvent\n29 / 100 (29%)\n\n\n\n1 Mean/Median/Min/Max/SD; n / N (%)\n\n\n\n\n\n\n\n\nWe can also look at the distribution of the different variables individually, using e.g., base R commands (or any other package of your choice).\n\n# using base R to explore variable distributions\ntable(dat$Gender)\n\n\nFemale   Male \n    49     51 \n\ntable(dat$TreatmentGroup)\n\n\n      A       B Placebo \n     43      30      27 \n\ntable(dat$AdverseEvent)\n\n\n 0  1 \n71 29 \n\nhist(dat$Age)\n\n\n\n\n\n\n\nhist(dat$BloodPressure)\n\n\n\n\n\n\n\nhist(dat$Cholesterol)\n\n\n\n\n\n\n\n\nLooks like a normal distribution for age and uniform distribution for blood pressure should work well. That‚Äôs of course not surprising since we produced the data that way in an earlier tutorial. But for real data you don‚Äôt know what process produced it, you just want to see how things are distributed so you can recreate it that way.\nFor Cholesterol, the distribution doesn‚Äôt look quite normal. That‚Äôs because in the original data generating process, we made it dependent on treatment.\nIf you want to see if there are correlations in the data that you might want to also have in your synthetic data, you can explore those with tables or plots like these.\n\n# explore some correlations between variables\ntable(dat$AdverseEvent, dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 28 19      24\n  1 15 11       3\n\nplot(dat$Age, dat$BloodPressure)\n\n\n\n\n\n\n\nggplot(dat) + geom_histogram(aes(x = Cholesterol, fill = TreatmentGroup)) \n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nAt this stage, it is up to you to decide if you want to try to include correlations between variables that might or might not exist in the real data, or if you just want to give each variable an independent distribution.\n\n\nGenerate data\nNow we‚Äôll create synthetic data that is similar to the real data. In this code example, we pull directly from the actual data stored in dat. However, you can also save that information into an intermediary object or file (e.g., save the mean and standard deviation of age) and then just use those summary statistics to generate the synthetic data. This prevents for instance issues with confidentiality if you use AI to help write the synthetic data code.\n\n# Create an empty data frame with placeholders for variables\nsyn_dat &lt;- data.frame(\n  PatientID = numeric(n_patients),\n  Age = numeric(n_patients),\n  Gender = character(n_patients),\n  TreatmentGroup = character(n_patients),\n  EnrollmentDate = lubridate::as_date(character(n_patients)),\n  BloodPressure = numeric(n_patients),\n  Cholesterol = numeric(n_patients),\n  AdverseEvent = integer(n_patients)\n)\n\n# Variable 1: Patient ID\n# can be exactly the same as the original\nsyn_dat$PatientID &lt;- 1:n_patients\n\n# Variable 2: Age (numeric variable)\n# creating normally distributed values with the mean and SD taken \n# from the real data\nsyn_dat$Age &lt;- round(rnorm(n_patients, mean = mean(dat$Age), sd = sd(dat$Age)), 1)\n\n# Variable 3: Gender (categorical variable)\n# create with probabilities based on real data distribution\nsyn_dat$Gender &lt;- sample(c(\"Male\", \"Female\"), \n                         n_patients, replace = TRUE, \n                         prob = as.numeric(table(dat$Gender)/100))\n\n# Variable 4: Treatment Group (categorical variable)\n# create with probabilities based on real data distribution\nsyn_dat$TreatmentGroup &lt;- sample(c(\"A\", \"B\", \"Placebo\"), \n                                 n_patients, \n                                 replace = TRUE,\n                                 prob = as.numeric(table(dat$TreatmentGroup)/100))\n\n# Variable 5: Date of Enrollment (date variable)\n# use same start and end dates as real data\nsyn_dat$EnrollmentDate &lt;- lubridate::as_date(sample(seq(from = min(dat$EnrollmentDate), \n                                                        to = max(dat$EnrollmentDate), \n                                                        by = \"days\"), n_patients, replace = TRUE))\n\n# Variable 6: Blood Pressure (numeric variable)\n# use uniform distribution as indicated by histogram of real data\n# use same min and max values as real data\nsyn_dat$BloodPressure &lt;- round(runif(n_patients, \n                                     min = min(dat$BloodPressure), \n                                     max = max(dat$BloodPressure)), 1)\n\n# Variable 7: Cholesterol Level (numeric variable)\n# here, we re-create it based on the overall data distribution pattern\n# since the data didn't quite look like a normal distribution, \n# here we'll just use it as its own distribution and sample right from the data\n# note that this breaks the association with treatment group \n# for real data, we wouldn't know if there is any, but if we suspect, we could\n# generate data with and without such associations and explore its impact on model performance\nsyn_dat$Cholesterol &lt;- sample(dat$Cholesterol, \n                                    size = n_patients, \n                                    replace = TRUE)\n\n\n# Variable 8: Adverse Event (binary variable, 0 = No, 1 = Yes)\n# we implement this variable by taking into account different probabilities stratified by treatment\nprobA = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,1])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,1])\nprobB = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,2])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,2]) \nprobP = as.numeric(table(dat$AdverseEvent,dat$TreatmentGroup)[,3])/sum(table(dat$AdverseEvent,dat$TreatmentGroup)[,3])\n\n# this re-creates the correlation we find between those two variables\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"A\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"A\"), replace = TRUE, prob = probA)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"B\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"B\"), replace = TRUE, prob = probB)\nsyn_dat$AdverseEvent[syn_dat$TreatmentGroup == \"Placebo\"] &lt;- sample(0:1, sum(syn_dat$TreatmentGroup == \"Placebo\"), replace = TRUE, prob = probP)\n\nYou can always make your synthetic data partially different from the real data for important quantities (e.g., your main input/exposure or outcome of interest) to explore different implications and model performance.\n\n\nCheck and save data\nQuick peek at generated data to make sure things look ok, then we can save it.\n\n# Print the first few rows of the generated data\nhead(syn_dat)\n\n  PatientID  Age Gender TreatmentGroup EnrollmentDate BloodPressure Cholesterol\n1         1 34.7   Male        Placebo     2022-05-02         152.9       218.7\n2         2 49.9   Male              B     2022-10-12         150.3       140.6\n3         3 53.2 Female        Placebo     2022-04-11         144.2       186.7\n4         4 44.4 Female              A     2022-07-28         117.0       205.4\n5         5 57.3 Female        Placebo     2022-01-23          94.2       140.9\n6         6 37.4 Female              B     2022-07-23         116.2       160.1\n  AdverseEvent\n1            0\n2            1\n3            0\n4            1\n5            0\n6            1\n\n# quick summaries\nsummary(syn_dat)\n\n   PatientID           Age           Gender          TreatmentGroup    \n Min.   :  1.00   Min.   :23.10   Length:100         Length:100        \n 1st Qu.: 25.75   1st Qu.:40.65   Class :character   Class :character  \n Median : 50.50   Median :46.20   Mode  :character   Mode  :character  \n Mean   : 50.50   Mean   :46.71                                        \n 3rd Qu.: 75.25   3rd Qu.:51.60                                        \n Max.   :100.00   Max.   :65.50                                        \n EnrollmentDate       BloodPressure    Cholesterol     AdverseEvent \n Min.   :2022-01-15   Min.   : 91.6   Min.   : 97.6   Min.   :0.00  \n 1st Qu.:2022-04-15   1st Qu.:108.9   1st Qu.:158.0   1st Qu.:0.00  \n Median :2022-07-02   Median :125.0   Median :167.5   Median :0.00  \n Mean   :2022-07-01   Mean   :125.0   Mean   :175.4   Mean   :0.31  \n 3rd Qu.:2022-09-27   3rd Qu.:142.3   3rd Qu.:197.4   3rd Qu.:1.00  \n Max.   :2022-12-26   Max.   :159.2   Max.   :248.7   Max.   :1.00  \n\ndplyr::glimpse(syn_dat)  \n\nRows: 100\nColumns: 8\n$ PatientID      &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ Age            &lt;dbl&gt; 34.7, 49.9, 53.2, 44.4, 57.3, 37.4, 49.5, 49.6, 37.8, 3‚Ä¶\n$ Gender         &lt;chr&gt; \"Male\", \"Male\", \"Female\", \"Female\", \"Female\", \"Female\",‚Ä¶\n$ TreatmentGroup &lt;chr&gt; \"Placebo\", \"B\", \"Placebo\", \"A\", \"Placebo\", \"B\", \"Placeb‚Ä¶\n$ EnrollmentDate &lt;date&gt; 2022-05-02, 2022-10-12, 2022-04-11, 2022-07-28, 2022-0‚Ä¶\n$ BloodPressure  &lt;dbl&gt; 152.9, 150.3, 144.2, 117.0, 94.2, 116.2, 110.0, 149.3, ‚Ä¶\n$ Cholesterol    &lt;dbl&gt; 218.7, 140.6, 186.7, 205.4, 140.9, 160.1, 229.0, 204.7,‚Ä¶\n$ AdverseEvent   &lt;int&gt; 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0‚Ä¶\n\n# Frequency table for adverse events stratified by treatment\ntable(syn_dat$AdverseEvent,syn_dat$TreatmentGroup)\n\n   \n     A  B Placebo\n  0 31 15      23\n  1 17 12       2\n\n# ggplot2 boxplot for cholesterol by treatment group\nggplot(syn_dat, aes(x = TreatmentGroup, y = Cholesterol)) +\n  geom_boxplot() +\n  labs(x = \"Treatment Group\", y = \"Cholesterol Level\") +\n  theme_bw()\n\n\n\n\n\n\n\n# Save the simulated data to a CSV file\nwrite.csv(syn_dat, \"syn_dat_new.csv\", row.names = FALSE)",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data based on existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#summary",
    "href": "content/module-synthetic-data/synthetic-data-new-existing-r/synthetic-data-new-existing-r.html#summary",
    "title": "Generating synthetic data based on existing data",
    "section": "Summary",
    "text": "Summary\nThe process of generating synthetic data based on existing data is fairly straightforward:\n\nLoad existing data.\nLook at distributions/frequencies of variables of interest.\nGenerate new data with variables that are distributed like original data.\n\nSometimes, you can just look at your original data and make up new data that‚Äôs roughly similar. At other times you want to be very close, in that case you need to either draw new values from distributions that are well-describe the original data, or you need to re-sample the original data (with or without replacement).\nFortunately, most of the time it‚Äôs good enough to get your data somewhat similar to the original. You would hope/expect any kind of statistical method to work well on different sets of data that are roughly similar. If that‚Äôs not the case, your method might not be very robust and might require changes.",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Creating new data based on existing data"
    ]
  },
  {
    "objectID": "content/module-synthetic-data/synthetic-data-overview.html",
    "href": "content/module-synthetic-data/synthetic-data-overview.html",
    "title": "Overview of the synthetic data content",
    "section": "",
    "text": "Overview\nThis module covers why and how to generate and use synthetic (also known as simulated/artificial/fake) data.\n\n\nLearning Objectives\n\nKnow about reasons why one might want to use simulated data.\nBe familiar with ways to generate synthetic data.\nBe aware of several R packages that can be used to generate synthetic data.\nKnow how to use synthetic data for testing.\nKnow how to use synthetic data with AI tools.\n\n\n\nContent\nThe module consists of the following units:\n\nIntroduction and Motivation\nCreating new data\nCreating new data based on existing data\nScrambling existing data\nR packages for data creation\nUse synthetic data with AI tools\nUse synthetic data to test models",
    "crumbs": [
      "Content",
      "4B. Synthetic Data",
      "Synthetic Data Overview"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html",
    "href": "content/module-workflow/assessment-complete-workflow.html",
    "title": "Assessment - Complete Workflow",
    "section": "",
    "text": "Get the quiz sheet for this module from the general Assessments page. Fill it in, then submit to the online grading system before the deadline.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#overview",
    "href": "content/module-workflow/assessment-complete-workflow.html#overview",
    "title": "Assessment - Complete Workflow",
    "section": "Overview",
    "text": "Overview\nFor this exercise, we will join the R data science community and participate in Tidy Tuesday, a weekly exercise that allows you to practice playing with data. The focus for Tidy Tuesday is on getting, cleaning (i.e., tidying) and exploring a data set, though people sometimes do statistical analyses as well. We‚Äôll do a ‚Äúfull‚Äù analysis workflow.\nThis is a solo exercise and will go into your portfolio.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#what-is-tidy-tuesday",
    "href": "content/module-workflow/assessment-complete-workflow.html#what-is-tidy-tuesday",
    "title": "Assessment - Complete Workflow",
    "section": "What is Tidy Tuesday",
    "text": "What is Tidy Tuesday\nYou briefly encountered Tidy Tuesday before in the Data Analysis Motivation unit. If you can‚Äôt remember, revisit that unit and re-read that section. Also read the information at the provided links.\nIf you want to see another Tidy Tuesday example (albeit one that doesn‚Äôt include any machine learning bits, only a very silly, tongue-in-cheek linear model fit), you can take a look at this one I did, as part of a previous version of MADA.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#your-tidy-tuesday-exercise",
    "href": "content/module-workflow/assessment-complete-workflow.html#your-tidy-tuesday-exercise",
    "title": "Assessment - Complete Workflow",
    "section": "Your Tidy Tuesday Exercise",
    "text": "Your Tidy Tuesday Exercise\nYour assignment is to participate in Tidy Tuesday by analyzing this week‚Äôs dataset. You can start as soon as the dataset is posted, which is Mondays. The datasets are released on GitHub. As I‚Äôm writing this, I have no idea what the data will be the week this exercise happens. That‚Äôs part of the fun of it üòÑ.\nHere are some more detailed instructions:\nUse your portfolio website. Make sure it‚Äôs up to date and fully synced. Create a folder called tidytuesday-exercise and inside that folder, add a file called tidytuesday-exercise.qmd file. That‚Äôs where you‚Äôll write your Tidy Tuesday analysis. As needed, add the data for this exercise into the same folder.\nGo to the TidyTuesday GitHub repository. Look for the dataset for this week, and read the instructions on how to get the data. You will also be provided with a data dictionary. If the data is available for download, place it somewhere in your portfolio repository (e.g., in a new folder called data). Remember the data limit for Git/GitHub, don‚Äôt place files that are too large into your repo!",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#what-you-need-to-include",
    "href": "content/module-workflow/assessment-complete-workflow.html#what-you-need-to-include",
    "title": "Assessment - Complete Workflow",
    "section": "What you need to include",
    "text": "What you need to include\nWrite a Quarto file (with R code as part of the Quarto file or in a separate file) that loads the data, performs any needed cleaning and wrangling, does some EDA, and fits some models. Here are the details:\n\nLoad, wrangle and explore the data. The EDA part involves making tables/figures to get an idea what your data is about. By now you know this is an iterative procedure, so it‚Äôs ok to have these parts of the process/code intertwined.\nOnce you understand the data sufficiently, formulate a question/hypothesis. This will determine your outcome of interest and, if applicable, main predictor(s) of interest. Since we don‚Äôt know the data yet, it might be that the question is a bit contrived and not actually too interesting, but I‚Äôm sure there will be more than one potentially reasonable question one can ask, no matter what the data will be. If it turns out that the data just doesn‚Äôt easily work for asking a question, create some synthetic data/variables to augment the existing data such that it will be possible to ask a question and do some machine learning modeling.\nOnce you determine the question and thus your outcome and main predictors, further pre-process and clean the data as needed. Then split into train/test. (It might be that the data is too small for this split to make sense in real life but for this exercise, we‚Äôll just do it.)\nFit at least 3 different model types to the data using the tidymodels framework we practiced. Use the CV approach for model training/fitting. Explore the quality of each model by looking at performance, residuals, uncertainty, etc. All of this should still be evaluated using the training/CV data. You can of course recycle code from previous exercises, but I also encourage you to explore further, e.g.¬†try different ML models or use different metrics. You might have to do that anyway, depending on the data and your question/outcome.\nBased on the model evaluations and your scientific question/hypothesis, decide on one model you think is overall best. Explain why. It doesn‚Äôt have to be the model with the best performance. You make the choice, just explain why you picked the one you picked.\nAs a final, somewhat honest assessment of the quality of the model you chose, evaluate it (performance, residuals, uncertainty, etc.) on the test data. This is the only time you are allowed to touch the test data, and only once.\nSummarize everything you did and found in a discussion. Make sure you discuss your findings with regard to your original question/hypothesis. What did you learn? If feasible, show a summary figure or table that illustrates your main scientific finding from this analysis.\n\n\nRemember to add ample information/documentation in the form of both code comments and explanatory text You should provide a running commentary on what you do, why you do it, and how your R code accomplishes that (so that if you stare at it in a few weeks, you can remember what in the world you did back then üòÑ).",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#finish-up",
    "href": "content/module-workflow/assessment-complete-workflow.html#finish-up",
    "title": "Assessment - Complete Workflow",
    "section": "Finish up",
    "text": "Finish up\nOnce done with your Tidy Tuesday analysis, edit your _quarto.yml file to include an entry for this exercise. Rebuild your portfolio site to make sure everything works and looks good, that all the links work, etc. Then push to GitHub by the deadline. Since this will be on your portfolio website, and I know where to find it, there is no need to post any link.\nAnd you just completed all the exercises for MADA! Now would also be a good time to check all of your portfolio page and further clean up/improve. This website is a nice showcase of what you‚Äôve done and learned in this course, and something you can show to others (future employers, etc.). So I suggest you make sure it looks reasonably good and presentable. Here are some specific steps you should check.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#connecting-website-and-file-repository",
    "href": "content/module-workflow/assessment-complete-workflow.html#connecting-website-and-file-repository",
    "title": "Assessment - Complete Workflow",
    "section": "Connecting website and file repository",
    "text": "Connecting website and file repository\nFirst, let‚Äôs make sure it‚Äôs easy for people to go from your portfolio website (the github.io location) to your file repository (the github.com location) and back. To that end, open the _quarto.yml file. At the bottom, it says URL-TO-THIS-REPOSITORY-HERE. Replace that with the URL to your GitHub repository. As an example, for the MADA course, the URL one would put in there is https://github.com/andreashandel/MADAcourse (while the website lives at https://andreashandel.github.io/MADAcourse/).\nOnce you done that, when people now are on your website, they can click on the GitHub icon in the top right corner and be taken to your file repository. Try to make sure it works by rebuilding your website and pushing to GitHub. It might take a minute or so and you might need to hit refresh before it shows up on the website.\nNow, let‚Äôs connect the two in reverse. Open Readme.md and update the text. This is what people see when they come to your github repository for your portfolio (as opposed to the website). I recommend adding a little bit of text and a link pointing users to the actual website. You can look at the Readme.md for this course as an example. Edit yours as you want.\nYou might also want to point to your website in the top right area of your repository on GitHub.com. If you go to your repository on GitHub, you should see an About section in the top right. If you click on the gear symbol, there is a field in which you can enter the URL for your website (the github.io location). It might already show as pre-populated, but you have to actually manually enter it before it will show. You can again see how that looks on the repository for this course.\nWith this, it is easy for anyone (including yourself) to quickly switch between the website and the file repository.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#adding-more-content",
    "href": "content/module-workflow/assessment-complete-workflow.html#adding-more-content",
    "title": "Assessment - Complete Workflow",
    "section": "Adding more content",
    "text": "Adding more content\nYou‚Äôve added most ‚Äúproducts‚Äù you generated as part of this class to the portfolio website. If you have created other noteworthy products, either as part of this course or outside, feel free to add them to your website. By now you should know how to do that. For instance, if you want to add your project, either now or once its done, feel free to do so. It probably doesn‚Äôt make sense to put the whole project content inside the portfolio. Instead, take the main product (e.g., the manuscript file), render it to html, and add it to the website. Or create a new project.qmd file where you briefly describe what you did and show a few highlights, and then provide a link to your main project repository for those interested in looking at the whole thing.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#cleanup-and-styling",
    "href": "content/module-workflow/assessment-complete-workflow.html#cleanup-and-styling",
    "title": "Assessment - Complete Workflow",
    "section": "Cleanup and Styling",
    "text": "Cleanup and Styling\nAt this point, it might be worth revisiting your already posted pages and making sure everything looks as nice and professional as possible.\nWhile the way we built the website has only limited ways of styling things (unless you want to start changing CSS and HTML code), you can still customize some. Feel free to play around and customize the look. You can find a good bit of information in the Quarto documentation. If you look into the repository for the MADA course, you will also see that I‚Äôm using my own CSS file (called MADAstyle.scss). SCSS/CSS lets you style websites. It‚Äôs not hard to write CSS code, but it is its own thing. I usually just search online to find what I‚Äôm looking for üòÅ. You certainly don‚Äôt have to, but if you want to further customize the look of your website, you can add your own CSS file and style it how you like.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "content/module-workflow/assessment-complete-workflow.html#more-comments",
    "href": "content/module-workflow/assessment-complete-workflow.html#more-comments",
    "title": "Assessment - Complete Workflow",
    "section": "More Comments",
    "text": "More Comments\nFuture employers really do look at portfolios like this, so being able to showcase something nice and polished is useful. It is also part of having a good online presence. I think for (future) professionals like yourself, a solid online presence is vital. I have in the past discussed this with our grad students in another class, if you want to see my thoughts on that, you can check out my presentation on building your brand ‚Äì which is of course made with R Markdown and posted to a Quarto based website üòÑ.",
    "crumbs": [
      "Assessments",
      "13. Complete Analysis Workflow"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html",
    "href": "courseinfo/assessments-overview.html",
    "title": "Course Assessments - Overview",
    "section": "",
    "text": "Each module comes with assessments. Those are things you need to do that will be graded. In general, expect that each module contains a Quiz, an Exercise, and a Discussion. Sometimes there are additional/alternative components, such as submission of project parts.\nDue dates are usually Friday 5 pm the week of the module. Some might be at other times, e.g., the beginning or middle of the week. See the schedule or the module specific assessment document. If unclear, ask.\nAll assessments are located in the Assessments section of the website. Their labeling matches that of the module, so it should be clear for each module which assessment to complete.\nThe assessments, especially the exercises, are the main way of learning the material. They are therefore meant as a help for you, giving you incentives to work through and learn the course material. It is very easy to take shortcuts (cheat) for any of the assessments. Of course, our academic honesty policy prohibits you from doing so. But more importantly, I assume you want to learn the material. And if you take shortcuts by getting answers from a classmate, you are only hurting your own learning. That said, I encourage getting help from any source you can, including your classmates. I let you decide how much helping each other is acceptable and at what point it turns into having someone else do the work for you that you should be doing yourself.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#assessments-overview",
    "href": "courseinfo/assessments-overview.html#assessments-overview",
    "title": "Course Assessments - Overview",
    "section": "",
    "text": "Each module comes with assessments. Those are things you need to do that will be graded. In general, expect that each module contains a Quiz, an Exercise, and a Discussion. Sometimes there are additional/alternative components, such as submission of project parts.\nDue dates are usually Friday 5 pm the week of the module. Some might be at other times, e.g., the beginning or middle of the week. See the schedule or the module specific assessment document. If unclear, ask.\nAll assessments are located in the Assessments section of the website. Their labeling matches that of the module, so it should be clear for each module which assessment to complete.\nThe assessments, especially the exercises, are the main way of learning the material. They are therefore meant as a help for you, giving you incentives to work through and learn the course material. It is very easy to take shortcuts (cheat) for any of the assessments. Of course, our academic honesty policy prohibits you from doing so. But more importantly, I assume you want to learn the material. And if you take shortcuts by getting answers from a classmate, you are only hurting your own learning. That said, I encourage getting help from any source you can, including your classmates. I let you decide how much helping each other is acceptable and at what point it turns into having someone else do the work for you that you should be doing yourself.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#quizzes",
    "href": "courseinfo/assessments-overview.html#quizzes",
    "title": "Course Assessments - Overview",
    "section": "Quizzes",
    "text": "Quizzes\nThe quizzes cover the basics and ensure that you have gone through some of the material. Each quiz consists of around 10-20 short questions. Not all answers to the quiz questions are directly found in the materials, sometimes you need to do a - hopefully straightforward - online search.\nQuizzes are auto-graded on submission based on a simple counting of the right answers. At the end, I‚Äôll average across all quizzes. Note that since there are &gt;10 quizzes and all quizzes together count for only a fraction of the total grade, each quiz counts for a small part of your grade. So try to get them as correct and complete as possible, but don‚Äôt stress out about missing a question here and there.\n\nThe link to the website for quiz submission is in the important-information Discord channel. Before submitting any quizzes, you need to set a password by filling the Google Form that was shared with you in the welcome email. The link to the Google Form can also be found in the important-information Discord channel. The submission system becomes available once everyone has submitted their information.\nUnless otherwise specified, you can submit each quiz once, and it needs to be submitted by the deadline (generally Fridays), otherwise the system will not take your submission.\n\n\nQuiz sheets\nFor each module, you are asked to fill out an Excel sheet with questions related to the course material. Download the quiz files from the links below, fill in the Answer column, and submit to the online quiz submission system by the deadline specified in the Schedule document.\n\nYou can expand and resize the columns/rows of the Excel file so you can make sure to see all content. Other than that, please only fill in the Answer column and don‚Äôt make other changes (e.g.¬†don‚Äôt rename or delete rows/columns). If you do make changes, submission might fail. If that happens and you get an error message during submission, simply come back here, download a fresh sheet and copy and paste your answers into it, making sure to not change any other part of the sheet.\n\nQuizzes will become available here as the course progresses. Once a quiz is available, you should be able to submit it at any time before the deadline.\n\nModule 1 ‚Äì Course and tools introduction - you have 2 attempts available for this one.\nModule 2 ‚Äì Data analysis introduction\nModule 3 ‚Äì READy Workflow\nModule 4 ‚Äì All about data\nModule 5 ‚Äì Data Processing",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#exercises",
    "href": "courseinfo/assessments-overview.html#exercises",
    "title": "Course Assessments - Overview",
    "section": "Exercises",
    "text": "Exercises\nThe exercises ‚Äì together with the project ‚Äì are the most important component of the course! The material we cover in this class can only be learned by doing it yourself. As such, the focus is really on the exercises (i.e.¬†the homework), with the readings and videos mainly meant to help you find and learn the material you need to do the exercises.\nBecause the exercises are so important, they are usually quite in-depth and also often time-consuming. Plan accordingly. For each exercise, I will provide detailed instructions that hopefully make it clear what you need to do.\nIf you get stuck during the exercises (quite possible), feel free to ask for help. Use the various course materials, the course discussion board as well as outside resources. But do try to do as much as possible yourself (of course with the help of Google).\nUnless otherwise specified, exercises are due by Friday. Quite often, some part of the exercise needs to be done before that deadline to allow for group work.\n\nExercise grading\nI grade the exercises based on 4 categories:\n\n3 points = complete or only minor omissions/mistakes.\n2 points = some noticeable bits are missing or wrong.\n1 points = some parts are present, but overall very incomplete.\n0 points = completely or almost completely missing.\n\nAt my discretion, I might consider assignments even if they are submitted after the deadline. In that case, I might take off a point for lateness.\nDon‚Äôt interpret these scores as grades, I just use them to differentiate, then I‚Äôll adjust at the end when I compute the overall project score.\nFor exercises that include multiple parts (e.g., you start something, then your colleague takes over), each part will be assigned a score.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#discussions",
    "href": "courseinfo/assessments-overview.html#discussions",
    "title": "Course Assessments - Overview",
    "section": "Discussions",
    "text": "Discussions\nWe can‚Äôt have in-class discussions in an online course, but hopefully we can still have robust discussions as part of the course. I hope that lively online discussions will allow us to create a sense of class community that usually comes more naturally in an in-class course. I want everyone to feel like they are part of a class, a community of students learning together, instead of just being on their own interacting with the course in front of a computer screen. To achieve that, having a lot of discussion activity is vital.\nYou will be asked to participate in discussions throughout the course, and discussion participation is part of the grade. The discussions are meant to connect the module content to the broader context and hopefully will spark interesting further thoughts and insights.\nUnless otherwise specified, the first posting of the discussion is due by Wednesday, the discussion replies are due by Friday. Post to the discussion channel for the applicable module (e.g.¬†module2_discussion, module3_discussion).\n\nDiscussion guidelines\nTo make sure discussions work well for everyone, here are some general guidelines.\n\nYou can and will be asked to both start new threads/posts or reply to others. If you start a new topic, make sure you are in the right channel. (If needed, you can also move posts to other channels later).\nYou are encouraged to include links to additional material that you find online, including but not limited to research papers, or sources like Wikipedia, and other useful resources. There are no rules on formatting other than that it should be readable.\nYou don‚Äôt always have to provide references/links to resources, nor do you need to be sure that ‚Äòyou got it right‚Äô. It‚Äôs ok to post speculations/ruminations/thoughts occasionally.\nMake meaningful contributions. You are welcome to react to each others‚Äô posts with üòÉ or üëç or other such emotions, but that does not replace more substantial responses. Give reasons why you think something is one way or another.\nFor the purpose of assessment, you need to participate in the discussions at the level specified in each module‚Äôs assessment document. I encourage you to engage in further discussions. Those further posts won‚Äôt be counted toward the requirement for a given module but will factor into the overall participation portion of the grade.\nIt should be obvious that any posts should be helpful and civilized. I will remove any posts that are demeaning or otherwise inappropriate. Please let me know if you see such posts.\nDiscussions will remain available for the whole course, and you can keep contributing at any time.\n\n\n\nDiscussion grading\nFor most modules, you‚Äôll have to do the following (unless specifically directed otherwise):\n\nYou need to write a post (generally a few hundred words) as instructed. Post it to the appropriate channel by Wednesday evening.\nYou need to write at least 3 replies to posts from classmates. Those can be replies to 3 different posts or multiple back-and-forth (of substance) for the same post/thread. You must post your replies by Friday evening.\nFor each module (i.e.¬†each week) you need to post a minimum of 4 posts (1 initial, 3 replies) with around 500 meaningful words total by the deadlines. I won‚Äôt count words (unless it looks really short), and probably also won‚Äôt count the number of posts, unless both seem well below the minimum.\nI will use the same 0-3 point score for each discussion as for the quizzes.\nGoing above the minimum (more posts, longer contributions, especially thoughtful, etc.) will count positively toward the participation portion of the final grade.\nFor each module, the assessment document contains specific discussion instructions. If the specific discussion instructions are different than the general ones here, you should follow the specific ones.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/assessments-overview.html#projects",
    "href": "courseinfo/assessments-overview.html#projects",
    "title": "Course Assessments - Overview",
    "section": "Projects",
    "text": "Projects\nSee the Projects section on the website for more information.",
    "crumbs": [
      "Assessments",
      "Assessments Overview"
    ]
  },
  {
    "objectID": "courseinfo/course-overview.html",
    "href": "courseinfo/course-overview.html",
    "title": "Course Information Overview",
    "section": "",
    "text": "This section contains most of the logistic information. The Syllabus page explains all the general logistic parts, the Schedule page lists what content is covered and what assessments you need to do each week, and the Communication page provides details on how we‚Äôll interact throughout this course.",
    "crumbs": [
      "General Information",
      "Course Overview"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html",
    "href": "courseinfo/course-syllabus.html",
    "title": "MADA Syllabus",
    "section": "",
    "text": "This course provides a survey of modern statistical approaches to analyze data. We will cover a variety of modern approaches for analyzing and interpreting data commonly encountered in public health, biomedical sciences, and related areas. This is an applied, hands-on class. We will use real data (bring your own data if you have it) to learn different methods of analysis. We will discuss all the steps of a data analysis, including obtaining and cleaning data, exploratory and full analysis, and presentation of results.\nWe will discuss how to formulate scientifically solid questions for a given set of data, how to decide on the right method of analysis, how to implement the analysis in R, and how to present and communicate the results. We will cover statistical topics such as regression, tree based models, cross-validation, bootstrapping, and model selection.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#overview",
    "href": "courseinfo/course-syllabus.html#overview",
    "title": "MADA Syllabus",
    "section": "",
    "text": "This course provides a survey of modern statistical approaches to analyze data. We will cover a variety of modern approaches for analyzing and interpreting data commonly encountered in public health, biomedical sciences, and related areas. This is an applied, hands-on class. We will use real data (bring your own data if you have it) to learn different methods of analysis. We will discuss all the steps of a data analysis, including obtaining and cleaning data, exploratory and full analysis, and presentation of results.\nWe will discuss how to formulate scientifically solid questions for a given set of data, how to decide on the right method of analysis, how to implement the analysis in R, and how to present and communicate the results. We will cover statistical topics such as regression, tree based models, cross-validation, bootstrapping, and model selection.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#learning-objectives",
    "href": "courseinfo/course-syllabus.html#learning-objectives",
    "title": "MADA Syllabus",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nThe main goal for this course is for you to learn the whole process of performing a data analysis project. This starts with identifying a suitable question-data pair, proceeds to getting, cleaning and exploring the data, culminates with fitting statistical models and producing materials that communicate what you did and found. A second goal is to introduce you to some modern analysis approaches that these days often go by the name of ‚ÄòMachine Learning‚Äô. Finally, a related goal is to introduce you to a set of tools that allow for a modern, reproducible workflow of your analysis.\nThe specific learning objectives that I hope you will achieve by going through this course are:\nDefine meaningful data analysis questions and assess the feasibility of answering these questions with the available data.\n\nGiven a data set, define the questions that can be answered and formulate and implement suitable analytic approaches.\nGiven a data analysis question, determine the type of data and analytic approach needed to answer it.\n\nBe able to obtain, organize and process data for detailed analysis.\n\nKnow how to obtain data from a variety of different sources.\nBe knowledgeable about data types and standards and how to process them.\nBe able to organize and process data in a reproducible, automated and documented manner.\nBe able to thoughtfully and critically assess strengths and weaknesses of specific data sets and process the data appropriately.\n\nBe knowledgeable of different data analysis methods and select the appropriate approach for a given project based on data and question.\n\nCritically compare and evaluate the strengths and weaknesses of different data analysis approaches.\nJudge the appropriateness of different approaches for specific questions and data sets and know how to apply an appropriate analytic approach.\nDesign and implement successful data analyses using state-of-the-art analysis software to translate data to information and knowledge that leads to actionable insights.\n\nEfficiently communicate results from data analyses to a variety of stakeholders.\n\nSummarize analysis results in ways that provide actionable conclusions and that are easily understandable by different audiences, such as laypersons, decision makers, and expert colleagues.\nAssess the strengths and weaknesses of different formats for representing the results of data analyses.\n\nUse modern coding and analysis tools to implement automated, reproducible analysis and project management workflows.\n\nExplain the importance of workflow, project management, and reproducibility tools, and know how to use those tools.\nBe proficient in R coding to implement and execute a complete data analysis project in a reproducible and automated manner.\nBe comfortable using R and GitHub to do data analysis in a reproducible manner.\nBe able to quickly learn how to use new software and tools, figure out how to get help when stuck, and make it work for you.\n\nDevelop skills to critically assess your own and others‚Äô analyses and conclusions.\n\nJudge the usefulness and appropriateness of data analyses described in the primary research literature.\nLearn to look over your shoulder and critically assess what you are doing, what assumptions you make by doing certain things, and if and how you can justify these.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#topics",
    "href": "courseinfo/course-syllabus.html#topics",
    "title": "MADA Syllabus",
    "section": "Topics",
    "text": "Topics\nHere is a non-exhaustive lists of topics that this course does and does not cover.\n\nWe will cover\n\nHow to set up an analysis workflow that is as reproducible and automated as possible.\nGetting, cleaning and processing messy real-world data.\nData visualization.\nModern tools for data analysis (e.g., R, Quarto, Git/GitHub, AI systems).\nThe tidyverse for data processing and tidymodels for fitting models.\nAn introduction to some Machine Learning tools and techniques.\n\n\n\nWe will not (or barely) cover\n\nAdvanced visualization techniques using interactive tools such as R/Shiny.\nDealing with ‚Äúnon-rectangular‚Äù data, such as time-series data, images, audio, complex -omics data, etc. (we won‚Äôt cover it, but you can use such data for your class project).\nStatistical tests and basic statistical modeling (linear and generalized linear models). Some familiarity with those techniques is assumed and they do show up a few times, but won‚Äôt be covered in any depth.\n\nHow to code in R. We will use R, but this course doesn‚Äôt teach R. With enough effort, a student can learn both the subject matter and pick up enough R at the same time, but this will require extra effort. In general, some basic familiarity with R or another programming language will be assumed.\nAnything in depth. This is a survey course and covers a lot of material, thus we won‚Äôt be able to go into much depth for any topic. Resources are provided to allow anyone interested to go deeper on their own.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#prerequisites",
    "href": "courseinfo/course-syllabus.html#prerequisites",
    "title": "MADA Syllabus",
    "section": "Prerequisites",
    "text": "Prerequisites\nFormal requirement for the course is BIOS 7010. Knowledge of material from BIOS 7010 is assumed. If your lack pre-requisites prevent you from enrolling in this course, please contact me to get permission to enroll.\nThis is a quantitative course. We will not discuss the mathematical details of specific data analysis approaches, however some statistical background and being comfortable with quantitative thinking are needed. Knowledge of statistics at the level of fitting linear or logistic models to data (e.g., as obtained in our BIOS 7010 and 7020 courses) is assumed. Some R coding skills (e.g., as obtained in our EPID 7500 class) are also needed.\nIf you do not have any coding or statistics knowledge, you can still take the class, but you need to be prepared to spend extra time and effort to fill any gaps. This will be especially true for the R coding part. Some of you likely have previous R experience, while others might have little to none. If you are in the little to none category, expect to spend extra time getting up to speed. I believe it‚Äôs doable and worth the effort, but you need to be prepared for it. There will be plenty of help from myself, classmates, and the internet if you end up getting stuck with some of the coding, but your effort and commitment are still required.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#should-i-take-this-course",
    "href": "courseinfo/course-syllabus.html#should-i-take-this-course",
    "title": "MADA Syllabus",
    "section": "Should I take this course?",
    "text": "Should I take this course?\nHere are a few more pointers to what to expect from the course, and comments from previous students (so you can hear it from them, not just from me).\n\nThe course is a high-level survey course. We cover a lot of material.\nA lot of the content is fairly conceptual/broad/big-picture, we don‚Äôt go much into technical details on specific topics.\nThe pace of the course is fast and covers a lot of topics.\nThere is a good bit of hands-on work/exercises that you need to do each week, which can be quite time-consuming.\nThis course comes with a good bit of team work.\nYou will learn skills that are likely useful for your research/work career.\nYou will be using R, GitHub, Quarto and other useful tools.\n\n\nTo re-emphasize: This course requires a significant time commitment, especially if you have not used R or any other programming language before. Budget your time accordingly and plan ahead! If you do, I‚Äôm fairly certain you will find it worth it. If you are not able or willing to allot the time needed to learn enough R (and GitHub) to make things work, this course might not be ideal for you.\n\n\nSome feedback from past students\n‚ÄúI have thoroughly enjoyed this class from start to finish, and I have gained a lot of knowledge, starting from GitHub to the entire data analysis workflow. However, I feel that we have covered a lot of material in just one semester.‚Äù\n‚ÄúI have learned so much from this class in what feels like such a short amount of time, and it has gotten me a lot more comfortable working with and interpreting my data and I now feel that I have a strong grasp on data analysis workflows. However, sometimes I feel like I‚Äôve learned about so many new things that I feel dumb all over again because there is so much to take in when it comes to machine learning and I want the opportunity to explore each facet in greater detail.‚Äù\n‚ÄúThe last half of the semester has seemed like a whirlwind. We have reviewed everything from modeling to training and even machine learning.‚Äù\n‚ÄúThis course has been very intense, with a very steep learning curve and high workload. I think that I will always describe this semester as my MADA semester. But I am very thankful that I took it and I have learned so much! This was an eye opener to the world of data analysis and how many components there are to it and how you should be thinking about things and structuring your work. I would recommend this course to all graduate students working with data.‚Äù\n‚ÄúThis class has challenged me more than many other classes I have taken but it has also been one of the most beneficial courses I have ever taken.‚Äù",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#course-setup",
    "href": "courseinfo/course-syllabus.html#course-setup",
    "title": "MADA Syllabus",
    "section": "Course Setup",
    "text": "Course Setup\nThis course is a fully online, asynchronous, cohort-based course. That means there are weekly deadlines, but other than the fixed deadlines, you can do the work whenever it is convenient for you.\nAll course materials are freely available online. We will make use of several freely available textbooks and other materials. All course materials are listed on the course website. We will use the R software for data analysis. We will also use a few other software tools. All are freely available.\nThis course is very hands-on. The weekly exercises (aka homework) are usually quite in-depth and also often time-consuming. Plan accordingly. For each exercise, I will provide detailed instructions that hopefully make it clear what you need to do. The materials provided on the course website are not meant to be memorized, but to be used to be able to do the hands-on activities, such as the exercises and the class project.\nFor more details on course logistics, see the other information in the General Information section of the course website.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#privacyferpa",
    "href": "courseinfo/course-syllabus.html#privacyferpa",
    "title": "MADA Syllabus",
    "section": "Privacy/FERPA",
    "text": "Privacy/FERPA\nThe course is set up such that you work ‚Äúin the open‚Äù by having a GitHub repository and accompanying website that showcases the work you are doing in this class as an online portfolio. This is a great way to showcase your work to potential employers. However, it also means that your work is publicly visible.\nAs a student, under the Family Educational Rights and Privacy Act (FERPA) you have the right to have all your student-related information kept private, including the fact that you are taking this class. Keeping everything private can be done using private GitHub repositories. But it means you won‚Äôt be able to create a public website/portfolio, and you can‚Äôt follow the general instructions in the exercises. Instead I‚Äôll need to give you special instructions. If for some reason you want to keep the fact that you are taking this class a secret, and thus do not want to use public GitHub repositories/create a publicly accessible online portfolio please let me know. If you take this course and do not inform me that you want your attendance in this course to be kept private, I consider this as you consenting with producing public materials as part of this course, and thus make your participation in this course public. If you have any concerns about this, please contact me to discuss.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#grading",
    "href": "courseinfo/course-syllabus.html#grading",
    "title": "MADA Syllabus",
    "section": "Grading",
    "text": "Grading\nThe grade will be made up as follows:\n\n20% quizzes\n30% exercises/homework\n10% participation/discussions\n40% a course long project, broken up into pieces.\n\nThe following grading scale will be used, final grades might be curved (upward, never down): A 93-100, A- 90-93, B+ 87-90, B 83-87, B- 80-83, C+ 77-80, C 73-77, C- 70-73, D 60-70, F &lt; 60\nMore detailed descriptions of the different assessments is provided on the Assessments page.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#class-attendance-make-up-policy",
    "href": "courseinfo/course-syllabus.html#class-attendance-make-up-policy",
    "title": "MADA Syllabus",
    "section": "Class Attendance, Make-up Policy",
    "text": "Class Attendance, Make-up Policy\nThis class is online. You are expected to submit all assignments by their due dates. Excused misses of due dates are only provided by prior agreement with the instructor or for special reasons (medical, etc.).",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#getting-help",
    "href": "courseinfo/course-syllabus.html#getting-help",
    "title": "MADA Syllabus",
    "section": "Getting Help",
    "text": "Getting Help\nIf you have questions about any aspect of the course, please do not hesitate to ask for help. The course materials describe in detail the ways you can ask for help.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#plagiarism-ai-and-related-topics",
    "href": "courseinfo/course-syllabus.html#plagiarism-ai-and-related-topics",
    "title": "MADA Syllabus",
    "section": "Plagiarism, AI, and related topics",
    "text": "Plagiarism, AI, and related topics\nI strongly encourage you to use AI tools as part of this course. They have become a vital part of doing data analysis and coding, and you need to learn how to use those tools properly and efficiently. That said, you should be transparent and judicious about your use of AI tools. If you use AI tools to help you, you should indicate where. I expect you might have AI running ‚Äòin the background‚Äô as you code. If that is the case, your code should have a statement that it is partially written with the help of AI. Similarly, any other work that is not your own (e.g.¬†you found someone else‚Äôs code to do a certain thing for you) should be acknowledged. You should generally provide a link or reference to the source of any code or text that is not your own. If in doubt, be explicit and detailed and state what you did and where you got help from.\nOverall, the goal of any course assignment is to help you learn. AI is a powerful tool to help you learn, but it‚Äôs also a powerful tool to avoid learning. Not doing the work, or not understanding and supervising the tools you are using (e.g.¬†AI), means you won‚Äôt learn much. So use them to enhance your learning, not to avoid it.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#university-honor-code-and-academic-honesty-policy",
    "href": "courseinfo/course-syllabus.html#university-honor-code-and-academic-honesty-policy",
    "title": "MADA Syllabus",
    "section": "University Honor Code and Academic Honesty Policy",
    "text": "University Honor Code and Academic Honesty Policy\nAll academic work must meet the standards contained in A Culture of Honesty. All students are responsible to inform themselves about those standards before performing any academic work. More detailed information about academic honesty can be found on UGA‚Äôs honesty webpage.\nDiscussions with your classmates and the instructor are encouraged. However, the final work should be your own.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#students-with-disabilities",
    "href": "courseinfo/course-syllabus.html#students-with-disabilities",
    "title": "MADA Syllabus",
    "section": "Students with Disabilities",
    "text": "Students with Disabilities\nStudents with disabilities who require reasonable accommodations in order to participate in course activities or meet course requirements should contact the instructor.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#general-disclaimers",
    "href": "courseinfo/course-syllabus.html#general-disclaimers",
    "title": "MADA Syllabus",
    "section": "General Disclaimers",
    "text": "General Disclaimers\nThis syllabus is a general plan, deviations announced to the class by the instructor may be necessary.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "courseinfo/course-syllabus.html#course-outline",
    "href": "courseinfo/course-syllabus.html#course-outline",
    "title": "MADA Syllabus",
    "section": "Course Outline",
    "text": "Course Outline\nFor an outline of the course, please see the Course Schedule document.",
    "crumbs": [
      "General Information",
      "Syllabus"
    ]
  },
  {
    "objectID": "project/project-details.html",
    "href": "project/project-details.html",
    "title": "Class Project Details",
    "section": "",
    "text": "Overview\nThis documents provides more detailed instructions and grading rubrics for each part of the project.\nNote that you should not consider the scores below as corresponding to a letter grade. Instead, I use those to differentiate between different aspects of the project. Only at the end while I make actual grade-like scores, which will likely involve curving (up, never down).\nAt my discretion, I might consider assignments even if they are submitted after the deadline. In that case, I might take off a point for lateness.\n\n\nGetting started\nAs soon as possible - definitely a few weeks before the part 1 deadline - start thinking about possible topics and data sources.\nTo practice the vital skill of working in a team, I strongly encourage you to team up with a classmate for this project. Post your topic idea or general topic interest in the project-related channel as soon as you have one and use that channel to find a partner. If for some reason you want/need to do the project on your own, that is also acceptable.\nOnce you got your team formed, follow the setup instructions on the Project Overview page.\nThe following sections describe in some more detail what the deliverables are for each part of the project.\n\n\nPart 1\nThe main objective for part 1 is to describe data and question in enough detail that I can assess if the planned project is feasible.\nUse the manuscript.qmd file. Remove or replace the template information. Add your proposed project title and your name(s).\nTo that end, provide the following pieces of information:\n\nDescribe what the data is, how it was collected, where you will get (or got) it from. How many observations do you have, what was measured? How many variables are included in the data? You will want some data that has enough detail (observations and variables) to allow you to apply some of the modeling approaches we cover in this course. Anything else important to report about the data?\nAt this stage you are not required to already have and show the data, but if you do, even better. Have a few lines of code which load the data and using some of the commands you learned about, provide summary descriptions of the data.\nExplain the question you want to answer using the data. What will be your outcome(s) of interest (if any)? What (if any) specific predictors will you focus on? What relations/patterns are you looking for in the data?\nAs much as you know, suggest how you will analyze it. At this stage in the course, we haven‚Äôt covered analysis approaches yet, so you can keep things vague and non-technical here.\nAt this stage, you can (but don‚Äôt have to yet) provide background information for the question you plan to answer. For instance you can describe why it‚Äôs an interesting question, who else has done similar analyses, how your analysis will be new/different, etc. Similar to what you read in an introduction to a research paper. For the final report, you‚Äôll need these parts. For part 1, they are not required, but you are welcome to already write down some of that.\nEventually, for your final report, what you write for this part will go into different sections of the full report. Some will go into the introduction, some in the methods section. You can already place these items there, or for now just write them as a single section.\n\nGrading for this part will follow the following rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete, contains enough information to allow assessment of feasibility of proposed project. Fully reproducible.\n3\n\n\nSomewhat insufficient\nMost components are present, but noticeable gaps exist, or existing materials can‚Äôt be fully reproduced.\n2\n\n\nNot sufficient\nSubmission is rather incomplete, has major missing parts which does not allow assessment of feasibility of proposed project\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\nDo not use any paths to load/save files that only work on your computer! If I can‚Äôt run your code and render your qmd files because of that, I‚Äôll take points off. Use the here() package mentioned many times in the course materials.\n\n\n\nPart 2\nThe main objective for part 2 is to have mostly completed the data loading/cleaning/wrangling/exploring part.\nTo that end, provide the following pieces of information:\n\nEverything from part 1. That doesn‚Äôt mean you need to keep what you wrote for part 1 frozen. Just that the description you provided there should be part of this submission. As appropriate, you can rewrite/reformat things to get it closer to a final report structure (e.g.¬†start moving some parts into a method section).\nA somewhat detailed description containing text and code showing your cleaning/wrangling/exploring steps. Place this in separate Quarto/R files inside the code folder or its sub-folders. Make sure it‚Äôs clear which files are the relevant ones, delete any non-relevant files.\nUpdate the main manuscript. Add a few of the exploratory results and any other content you think should be shown.\nRemoval or replacement of any left-over files and leftover text and code from the templates. Update all readme files, delete any files and folders that are not part of your project. Remove any comments and bits of code that are not relevant. At this stage, only information, code and files relevant to your project should be present, with appropriate documentation.\nThe main text should show plots or tables that explore the data, with a focus on the quantities of main interest (outcome, main predictor, co-variates of specific interest, etc.).\nIt is up to you how you structure things. You can use a combination of R or Quarto scripts. As long as things are well documented, reproducible and logical, the exact setup is your choice.\nEverything needs to be fully reproducible and you need to provide somewhere (e.g.¬†in the main text file or in the readme file in your repository) instructions on what one needs to do to completely reproduce everything.\nYour main article and - if applicable supplementary files - should knit into a word or pdf or html documents.\nIf you start including references, you should use a reference manager and a bibtex file from which you cite references in your manuscript. I recommend managing the bibtex file with the free Zotero reference manager, but if you have another reference manager that can handle bibtex files, you can use that too. Your bib file should be part of the project repository (for instance in the same folder as the manuscript). Feel free to pick any citation style you like (you can get CSL files from e.g.¬†Zotero‚Äôs style repository).\n\nGrading for this part will follow the following rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\n\nPart 3\nThe main objective for part 3 is to have started the analysis part of the project and continued to improve everything.\nTo that end, provide the following pieces of information:\n\nAll relevant files and documents needed to reproduce everything.\nAll non-relevant files (e.g., leftovers from the template) removed or updated.\nSome documentation (e.g.¬†a readme.md file) explaining how your project is set up and which scripts need to be run in what order to reproduce everything.\nFiles with well-documented code (either R scripts or Quarto files) that do all the previous tasks (cleaning/processing), as well as running a few analyses. You can start with simple bivariate ones, looking for patterns between your outcome(s) and individual predictors of interest. I suggest as much as possible you use the tidymodels framework. You can also try a few multivariable GLM. Results from those explorations should be saved in whatever form you consider most appropriate (figures or tables).\nA main article/manuscript file which contains the most pertinent results and findings from everything you have done so far. You can include more figures/tables here than you would in a regular manuscript. However, it should still be nicely readable and somewhat focused, so don‚Äôt produce page-long raw R output or a ton of exploratory figures or similar things. The main results of your analysis should be in this manuscript. Any further explorations and results (figures and tables) should go into a separate Quarto file that comprises the supplementary material.\n\nGrading for this part will follow this rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, minor parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\nThis part of your project will be assessed by some of your classmates. See the Project Review document for more details.\n\n\nPart 4\nThe objective is to have mostly completed implementation of analyses following the approaches covered in the course.\nTo that end, provide the following pieces of information:\n\nAll relevant files and documents needed to reproduce everything.\nDocumentation for everything you are doing. Any files/documentation not related to your project should be removed, only relevant information should be present.\nCode that performs statistical analyses of your data using the approaches we cover in class, such as: train/test split, cross-validation, trying different models, exploring model quality (performance, uncertainty, diagnostics, etc.). Depending on your data and question, not all approaches will make sense for your data. Choose the ones that make sense. E.g., if you happen to do an analysis of text or high-dimensional data, use methods/models appropriate for that data. The main point is that you should show you understand the main concepts regarding analysis and model evaluation and can apply them to your data with the tools we covered.\nUpdate your manuscript and supplementary files with the new results (figures/tables) from your analysis.\nAt this point, make sure you are also far along with your background section, including referencing (using a reference manager and bibtex, file, no manual references) and everything starts looking like a full analysis similar to what one could submit to a journal for publication.\n\nGrading for this part will follow this rubric:\n\n\n\n\n\n\n\n\nCategory\nDescription\nScore\n\n\n\n\nSufficient\nSubmission is (almost) complete\n3\n\n\nSomewhat insufficient\nSubmission is somewhat incomplete, minor parts missing or not reproducible\n2\n\n\nInsufficient\nSubmission is very incomplete, major parts missing or not reproducible\n1\n\n\nAbsent\n(Almost) everything of submission is missing\n0\n\n\n\n\n\nPart 5\nThe objective for part 5 is to have a fully completed project, ready for peer review.\nTo that end, provide the following pieces of information:\n\nEvery file needed to reproduce your complete analysis and report in your project repository (that includes files such as readme, bibtex, style files, etc).\nDetailed instructions (e.g.¬†as a readme.md file) explaining how to reproduce all your results.\nA complete, nicely readable and well formatted report, written in structure like a research paper (Abstract/Introduction/Methods/Results/Discussion/Citations).\nWell documented supplementary material\nWell-documented code as Quarto files or R scripts that explain clearly all steps in your analysis (including the wrangling/exploring parts).\nMeta-data explaining your data as needed.\nEverything very well documented and polished as much as possible.\n\nI will not grade this part but it will be assessed by your peers, as described in the Project Review document. Check there to learn more about this review and to find the template that everyone will use to assess this submission.\nA strong project would be at the level of one of the projects on the Project Examples page.\n\n\nPart 6\nThe main objective for part 6 is to have a finished project, ready for final grading.\nTo that end, provide the following pieces of information:\n\nA complete project, with as many further improvements as you want to implement, based on feedback from your classmates and any other improvements you can think of.\n\nI will grade the final project using the same criteria your classmates used for peer review. This will be graded on a 100 points scale and combined with the other project scores for a final overall project score.",
    "crumbs": [
      "Project",
      "Project Details"
    ]
  },
  {
    "objectID": "project/project-overview.html",
    "href": "project/project-overview.html",
    "title": "Class Project Overview",
    "section": "",
    "text": "Some of this will only fully make sense once we are a few weeks into the course. It‚Äôs nevertheless a good idea to read through it and get an overall idea before we have discussed all the different tools and details mentioned. Re-visit/re-read as needed.",
    "crumbs": [
      "Project",
      "Project Overview"
    ]
  },
  {
    "objectID": "project/project-overview.html#your-own-data",
    "href": "project/project-overview.html#your-own-data",
    "title": "Class Project Overview",
    "section": "Your ‚Äúown‚Äù data",
    "text": "Your ‚Äúown‚Äù data\nIf you have data that you are using for some research project(s) you are doing, you are welcome and encouraged to work on this as part of the class project. Of course, what you do for this class project needs to be new work, not a previously done and recycled analysis. Also, since the analysis needs to be fully reproducible, you need to provide the data at least within the class (no need to make it publicly available). I encourage you to use the class project toward helping you do an analysis and write a report that can help you with a project you want to publish as part of your research!",
    "crumbs": [
      "Project",
      "Project Overview"
    ]
  },
  {
    "objectID": "project/project-overview.html#publicly-available-data",
    "href": "project/project-overview.html#publicly-available-data",
    "title": "Class Project Overview",
    "section": "Publicly available data",
    "text": "Publicly available data\nYou can use any data you can get access to. Since it needs to be reproducible, you need to be able to share the data at least with me and the classmates who will review your project. If you need some ideas for data, check out the ones listed in the Data Resources page. Of course you are not limited to data listed there.\nNote that if you use a publicly available data set, it needs to be somewhat ‚Äòmessy‚Äô and not fully cleaned. Some data files on sites such as Kaggle are already too processed/cleaned to generally be suitable for the class project. If unsure, ask me.\nOften, the most interesting questions can be asked by combining data from more than one source.",
    "crumbs": [
      "Project",
      "Project Overview"
    ]
  },
  {
    "objectID": "project/templates/project-review-template.html",
    "href": "project/templates/project-review-template.html",
    "title": "Project Review Template",
    "section": "",
    "text": "Title of project:\nName of project author(s):\nName of project reviewer:"
  },
  {
    "objectID": "project/templates/project-review-template.html#background-context-and-motivation",
    "href": "project/templates/project-review-template.html#background-context-and-motivation",
    "title": "Project Review Template",
    "section": "3.1 Background, Context and Motivation",
    "text": "3.1 Background, Context and Motivation\nHow well is the context of the project described? Is a comprehensive background, including summary of previous/related work given? Is the project well placed into the context of existing work (including proper referencing of existing work). Is it clear why the project was undertaken and what new information it hopes to provide?\n\n3.1.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.1.2 Summary assessment (PICK ONE, DELETE THE OTHERS)\n\nvery poor contextualization and motivation\nsome contextualization and motivation\nstrong contextualization and motivation"
  },
  {
    "objectID": "project/templates/project-review-template.html#question-description",
    "href": "project/templates/project-review-template.html#question-description",
    "title": "Project Review Template",
    "section": "3.2 Question description",
    "text": "3.2 Question description\nHow well and clear are the question(s)/hypotheses the project aims to address described? Is it clear how the questions relate to the data?\n\n3.2.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.2.2 Summary assessment\n\nquestion/hypotheses unclear\nquestion/hypotheses somewhat explained\nquestion/hypotheses fully clear"
  },
  {
    "objectID": "project/templates/project-review-template.html#data-description",
    "href": "project/templates/project-review-template.html#data-description",
    "title": "Project Review Template",
    "section": "3.3 Data description",
    "text": "3.3 Data description\nHow well is the data overall described? Is the source provided? Is a codebook or other meta-information available that makes it clear what the data is?\n\n3.3.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.3.2 Summary assessment\n\nsource and overall structure of data poorly explained\nsource and overall structure of data somewhat explained\nsource and overall structure of data well explained"
  },
  {
    "objectID": "project/templates/project-review-template.html#data-wrangling-and-exploratory-analysis",
    "href": "project/templates/project-review-template.html#data-wrangling-and-exploratory-analysis",
    "title": "Project Review Template",
    "section": "3.4 Data wrangling and exploratory analysis",
    "text": "3.4 Data wrangling and exploratory analysis\nHow well is the data cleaned/processed and explored? Are all steps reasonable and well explained? Are alternatives discussed and considered? Are meaningful exploratory results shown (e.g.¬†in the supplementary materials)?\n\n3.4.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.4.2 Summary assessment\n\nmajor weaknesses in wrangling and exploratory component\nsome weaknesses in wrangling and exploratory component\nessentially no weaknesses in wrangling and exploratory component"
  },
  {
    "objectID": "project/templates/project-review-template.html#appropriateness-of-analysis",
    "href": "project/templates/project-review-template.html#appropriateness-of-analysis",
    "title": "Project Review Template",
    "section": "3.5 Appropriateness of Analysis",
    "text": "3.5 Appropriateness of Analysis\nWere the analysis methods appropriate for the data? Was the analysis done properly? Were different components of the analysis (e.g.¬†performance measure, variable selection, data pre-processing, model evaluation) done in the best way possible and explained well?\n\n3.5.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.5.2 Summary assessment\n\nwrong/inadequate analysis\ndefensible but not optimal analysis\nstrong and reasonable analysis"
  },
  {
    "objectID": "project/templates/project-review-template.html#presentation",
    "href": "project/templates/project-review-template.html#presentation",
    "title": "Project Review Template",
    "section": "3.6 Presentation",
    "text": "3.6 Presentation\nHow well are results presented? Are tables and figures easy to read and understand? Are the main figures/tables publication level quality?\n\n3.6.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.6.2 Summary assessment\n\nresults are poorly presented, hard to understand, poor quality\nresults are presented ok, with room for improvement\nresults are very well presented"
  },
  {
    "objectID": "project/templates/project-review-template.html#discussionconclusions",
    "href": "project/templates/project-review-template.html#discussionconclusions",
    "title": "Project Review Template",
    "section": "3.7 Discussion/Conclusions",
    "text": "3.7 Discussion/Conclusions\nAre the study findings properly discussed? Are strengths and limitations acknowledged? Are findings interpreted properly?\n\n3.7.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n3.7.2 Summary assessment\n\nmajor parts of discussion missing or wrong\nminor parts wrong, missing or unclear\nstrong, complete and clear discussion"
  },
  {
    "objectID": "project/templates/project-review-template.html#further-comments",
    "href": "project/templates/project-review-template.html#further-comments",
    "title": "Project Review Template",
    "section": "3.8 Further comments",
    "text": "3.8 Further comments\nAdd any other comments regarding the different aspects of the project here. Write anything you think can help your classmate improve their project."
  },
  {
    "objectID": "project/templates/project-review-template.html#structure",
    "href": "project/templates/project-review-template.html#structure",
    "title": "Project Review Template",
    "section": "4.1 Structure",
    "text": "4.1 Structure\nIs the project well structured? Are files in well labeled folders? Do files have reasonable names? Are all ‚Äújunk‚Äù files not needed for analysis/reproduction removed? By just looking at files and folders, can you get an idea of how things fit together?\n\n4.1.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n4.1.2 Summary assessment\n\npoor and confusing structure\nmostly clear, but some confusing parts (e.g.¬†useless files, things in the wrong folders)\nwell structured"
  },
  {
    "objectID": "project/templates/project-review-template.html#documentation",
    "href": "project/templates/project-review-template.html#documentation",
    "title": "Project Review Template",
    "section": "4.2 Documentation",
    "text": "4.2 Documentation\nHow well is the project documented? Are you able to understand each step of the whole analysis, each decision that was made, and each line of code? Is enough information provided as comments in code or as part of Rmd files?\n\n4.2.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n4.2.2 Summary assessment\n\npoorly documented\ndecently documented with some gaps\nfully and well documented"
  },
  {
    "objectID": "project/templates/project-review-template.html#reproducibility",
    "href": "project/templates/project-review-template.html#reproducibility",
    "title": "Project Review Template",
    "section": "4.3 Reproducibility",
    "text": "4.3 Reproducibility\nAre all results fully reproducible? Is documentation provided which clearly explains how to reproduce things, and does it work without the need for any manual intervention? Are you able to re-run the whole analysis without having to do manual interventions/edits?\n\n4.3.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n4.3.2 Summary assessment\n\nmajor parts not reproducible\nsmall parts not reproducible or required manual intervention\nfully reproducible without issues"
  },
  {
    "objectID": "project/templates/project-review-template.html#thoroughness",
    "href": "project/templates/project-review-template.html#thoroughness",
    "title": "Project Review Template",
    "section": "4.4 Thoroughness",
    "text": "4.4 Thoroughness\nHow thorough was the overall study? Were alternatives (e.g.¬†different ways of processing the data or different models) considered? Were alternatives discussed? Were the questions/hypotheses fully and thoroughly addressed?\n\n4.4.1 Feedback and Comments\nWRITE YOUR FEEDBACK HERE\n\n\n4.4.2 Summary assessment\n\nweak level of thoroughness\ndecent level of thoroughness\nstrong level of thorougness"
  },
  {
    "objectID": "project/templates/project-review-template.html#further-comments-1",
    "href": "project/templates/project-review-template.html#further-comments-1",
    "title": "Project Review Template",
    "section": "4.5 Further comments",
    "text": "4.5 Further comments\nAdd any other comments regarding the overall project here. Write anything you think can help your classmate improve their project."
  },
  {
    "objectID": "resources/resources-data.html",
    "href": "resources/resources-data.html",
    "title": "Data Resources",
    "section": "",
    "text": "This page lists possible resources for data that might be suitable for the course project, or also generally for research.\nThis list needs cleaning.",
    "crumbs": [
      "Resources",
      "Data Resources"
    ]
  },
  {
    "objectID": "resources/resources-data.html#infectious-disease-data",
    "href": "resources/resources-data.html#infectious-disease-data",
    "title": "Data Resources",
    "section": "Infectious disease data",
    "text": "Infectious disease data\n\nProject Tycho - infectious disease data\nhttp://www.viprbrc.orgrelated\nhttp://eupathdb.org\nClinEpiDB - a database of (a few) clinical epidemiology studies, focusing on infectious diseases.\nhttp://www.cdc.gov/flu/index.htm\nhttp://www.ncbi.nlm.nih.gov/genomes/FLU/FLU.html\nSystemsInfluenza.org\nOTIS on CDC WONDER: http://wonder.cdc.gov/tb.html",
    "crumbs": [
      "Resources",
      "Data Resources"
    ]
  },
  {
    "objectID": "resources/resources-glossary.html",
    "href": "resources/resources-glossary.html",
    "title": "Course Glossary",
    "section": "",
    "text": "This page is work in progress. Feel free to contribute via a GitHub pull request.\nThere are lots of technical terms related to data and its analysis. In this course, I try to explain them whenever they show up first. However, I figured it might also be useful to have one central place for quick reference. Terms are ordered alphabetically. This is how I define them, others might have different definitions - and so might I in the future, I try to change/update my understanding of things every so often üòÅ.\nNote that in general, there is no regulatory terminology agency, so everyone can use these words to mean whatever they want. While some are fairly well defined, other terms are often used and abused in ways outside the definitions I provide. Always look carefully at what is described to decide what exactly is going on.\n\nNot surprisingly, I‚Äôm not the 1st one with the idea of compiling a list of data science related terms and definitions. After starting this, I found another one by Frank Harrell, which you can find on his website. I‚Äôm sure I‚Äôll find and list more alternatives soon.\n\nArtificial Intelligence: The use of Deep Learning and related approaches applied to ‚Äúcomplex‚Äù problems. Historically, it was used for trying to solve problems using modeling approaches that mimic in a simplified form a brain (i.e.¬†neural net models). Currently this term has become hot and is used more widely than it should.\nBig Data: Any dataset that doesn‚Äôt easily fit into the memory of a regular computer (or cluster) and thus needs to be analyzed using special tools. Alternatively, data that is so big that doing analysis on it takes too long using standard tools (e.g.¬†R on a regular computer) and instead requires special treatment. Of course this also depends on the type of analysis, not only the type of data. As computers keep getting faster and tools more flexible and integrated, the label big data is a moving target.\nBinary Variable: A categorical variable with 2 categories, e.g., yes/no, dead/alive, diseased/healthy.\nCategorical Variable: A variable that can take on discrete values. Those categories can be ordered (at which point it is an Ordinal variable - see there) or not. Examples of non-ordered variables are hair color or ethnicity. No ordering is possible. A special and common case of categorical variables are Binary variables.\nCausal Modeling: Analysis of data with the goal to make causal inferences between variables (e.g.¬†X (partially) caused Y).\nClassification: Analysis approaches for Categorical Variables.\nContinuous Variable: A variable that can take on any numeric value. E.g. weight. Note that in practice the values are often discrete, e.g. while age is in principle a continuous variable, it is usually expressed in some units (say years) and individual values are reported in discrete units (e.g.¬†whole years). For analysis purposes, we still generally treat it as if it could have been any value.\nData Mining: Often used interchangeably with Machine Learning. It might sometimes indicate specifically a ‚Äúfishing‚Äù approach of combing through data to look for patterns, without pre-defined hypotheses to be tested. I that sense it is similar to Exploratory Data Analysis or Secondary Data Analysis, though those two are often done using smaller (Statistical) as opposed to larger (Machine Learning) models.\nDeep Learning: Generally applied to a specific class of machine learning models, namely neural nets. The ‚Äúdeep‚Äù part comes from the fact that the neural net models generally have multiple layers stacked on top of it (it has nothing to do with deep as in especially insightful).\nDependent (variable): An alternative name for Outcome.\nDescriptive Analysis: Describing and presenting data in meaningful ways using tables and figures, without trying to perform statistical modeling, i.e.¬†without looking for correlations/patterns.\nExploratory Data Analysis: Looking for patterns in data, without a hypothesis specified before data collection (or at least before looking at the data). Very useful, but any result needs to be tested on independent data.\nExposure (variable): A name for a predictor variable of particular interest. For instance, if we wanted to study if the daily duration of exercise had an impact on BMI. We would consider BMI our outcome, exercise duration our exposure (our main predictor of interest), and any other variable we record (e.g., a person‚Äôs age and gender) as other predictors (sometimes called covariates). The term exposure is common in the biomedical and related disciplines, not so much in other areas.\nFeature (variable): An alternative name for Predictor often used in the machine learning literature.\nIndependent variable: An alternative name for predictor, most often used in the statistical literature.\nInterval scale variable: A numerical (quantitative) variable for which taking differences makes sense. Unlike ratio scale variables, the zero point for interval data may be arbitrary. Addition and subtraction are meaningful for interval data, but in general, taking ratios is less meaningful. For example, temperatures in Celsius are on the interval scale. If the temperature was 70 degrees yesterday and 65 degrees today, it makes perfect sense to say ‚Äúit is 5 degrees cooler than it was yesterday.‚Äù But a day that is 70 degrees is not exactly twice as warm than a day that is 35 degrees, so division does not make sense here. There are also temperatures lower than 0 degrees, so the zero point is arbitrary.\nLabeled data: If we have data for which there is a specific outcome of interest and we know it, it is called labeled data. For instance, if we had a lot of pictures of tissue samples, and someone had gone through them and labeled them as cancerous or not cancerous, it is labeled data. Labeled data (the most common type) is usually analyzed using Supervised Learning/Analysis approaches.\nMachine Learning: An approach to data analysis that tends to use more complex models. The goal is mainly to obtain a model that is good at prediction. Understanding how different inputs lead to different outcomes is of secondary importance. Data is often abundant, so more complex models can be used. Often this term and Statistics/Statistical Learning are used interchangeably (though some people try to distinguish them, see e.g.¬†Frank Harrell‚Äôs blog post).\nMechanistic Modeling: Building and using models that explicitly incorporate mechanisms and processes of the system under study to understand how things interact and lead to specific outcomes. Many models in the hard sciences are of this type. A common way to formulate such models is with differential equations.\nNominal variable: A qualitative measurement that can take on distinct categories, but there is no natural order to these categories. For example, apples can be red, yellow, or green, but there is no inherent way to order these colors, even if the data is useful.\nObservation: An observation is a recording of the different variables for a single unit of analysis. Usually an individual, e.g., a single human or animal, but it could also be a picture or video, a county, a city, or whatever our level of observation is. For each observation, values for the different variables should be available. In R, it is most common that each observation is stored as a row in a data frame.\nOrdinal variable: A variable that can take on distinct categories which can be ordered, but the difference between levels might not allow for mathematical operations. For instance, if a question asks a person to rank their level of a pain on a scale from 1-10, a 7 is clearly higher than a 6, and a 6 higher than a 5. But it‚Äôs unclear if the difference between 5 and 6 is the same as 6 and 7.\nOutcome (variable): The variable of main interest for our analysis. This can be a single outcome (most common) or multiple. For instance, the main outcome might be if an individual survives or dies. Or it could be their BMI, or it could be if a given picture contains a cat or not. Also called response (variable) or dependent (variable).\nPredictive Modeling: Using models with the main goal of predicting future outcomes, given a collection of predictors.\nPredictor (variable): All variables that are not the outcome, which we use to see if we can predict the outcome. For instance, if we wanted to predict the price of houses, we could use the square footage of each house and the school district as the predictors.\nRatio scale variable: Ratio data are a type of quantitative data that are continuous, have consistent differences, and have a true zero. Addition/subtraction and multiplication/division are all meaningful for ratio data. For example, distance is ratio scale because the difference between two distances is meaningful, and a distance of 0 is truly the lowest possible distance value. Ratios are meaningful for distance: a distance of 4 miles is exactly twice as long as a distance of 2 miles.\nRegression: A type of supervised learning/modeling, where the outcome of interest is quantitative (or can be treated as such).\nResponse (variable): See Outcome (variable).\nSecondary Data Analysis: Analysis of a dataset that was not specifically collected for the purpose of answering the question one wants to answer. With an increasing abundance and routine collection of data, such secondary analyses are becoming very common. If a clear hypothesis is formulated before one looks at the data, one might consider any results confirmatory, otherwise it is an Exploratory Analysis. Even just looking at the data a little bit during the cleaning process should move such a secondary analysis into the exploratory category.\nStatistics: The basic/classical machinery for data analysis. Depending on the type of data, many different approaches have been developed (parametric vs.¬†non-parametric methods, longitudinal analysis, time-series analysis, and many more). Models tend to be simple and interpretable, the goal is to understand how inputs (predictors) relate to outcomes. Statistics was developed when data was sparse, computers didn‚Äôt exist, and mainly scientists interested in a deep understanding of their data used it. Because of this, statistical models tend to be simple and work well on small datasets.\nStatistical Learning: A term that seems to become more widely used in recent years. While some people distinguish this term from Statistics and consider it a sub-field (see e.g.¬†Chapter 1 of Introduction to Statistical Learning), the two terms are often used interchangeably.\nSupervised Learning/Analysis: Fitting Labeled data. The two types of supervised learning are Regression and Classification.\nUnlabeled data: If we have data for which there is either no specific outcome variable of interest or we do not know it, it is called unlabeled data. For instance, if we had a lot of pictures of tissue samples, and we knew that some showed cancerous tissue and others not, but we didn‚Äôt know which are which, it is unlabeled data. Similarly, if we had pictures of different tissue samples (or say a number of gene sequences) and all we wanted to know is if some samples are more related to each other than others, but there is no main outcome, it is considered unlabeled data. Unlabeled data is usually analyzed using unsupervised analysis approaches.\nVariable: Any quantity that we record like height, weight, income, or species type. In R, it is most common, that each variable is stored as a column in a data frame. The column name should be the name of the variable.",
    "crumbs": [
      "Resources",
      "Glossary"
    ]
  }
]